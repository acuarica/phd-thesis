\section{Performance Evaluation}\label{sec:jnif-evaluation}

We evaluated the performance of a \jnif{}-based dynamic instrumentation approach 
versus an approach using an ASM-based instrumentation server.

\subsection*{Measurement Contexts}

We ran our experiments on three different machines:
(1) A machine with two Intel Xeon E5-2620 2~GHz CPUs, 
each with 6 cores and 2 threads per core, 
and 8~GB RAM,
running Debian Linux x86 64 3.10.11-1.
(2) A Dell PowerEdge M620, 2 NUMA node with 64 GB of RAM, 
Intel Xeon E5-2680 2.7~GHz CPU
with 8 cores, 
CPU frequency scaling and Turbo Mode disabled,
running Ubuntu Linux x86 64 3.8.0-38.
For consistent memory access speed, 
we bound our program to a specific NUMA node using \code{numactl}.
(3) A MacBook Pro with an Intel Core i7 2.7~GHz CPU 
with 4 cores and 16~GB
running Mac OS X 10.8.2.

\subsection*{Benchmarks}

We used the Dacapo benchmarks, 
except for tradebeans and tradesoap,
which suffer from a well known issue.%
\footnote{\url{http://sourceforge.net/p/dacapobench/bugs/70/}}
We also include the Scala benchmarks
(except for the subset identical to Dacapo).

\subsection*{Subjects}
We compare \jnif{} to \asm{} for the purpose of performing dynamic instrumentation.
For \jnif{} we built a \jvmti{} agent that directly includes \jnif{} to instrument loaded classes.
For \asm{}, we use a \jvmti{} agent that forwards loaded classes to an instrumentation server
that uses \asm{}'s streaming \api{} (which is faster than \asm{}'s tree API).


\subsection*{Results}

Figure~\ref{fig:instr-time} shows the results of our performance evaluation
in terms of time spent instrumenting classes.
The figure shows the results from our first machine.
The other machines produced results similar to Figure~\ref{fig:instr-time}.
The figure shows box plots summarizing five measurement runs.
It shows one box for \jnif{} and two boxes for ASM.
The ``ASM Server'' box represents the time as measured on the instrumentation server.
This is equivalent to the time a static instrumentation tool would take.
It excludes the time spent in the \jvmti{} agent and the time for the IPC between the agent and the server.
The ``ASM Server on Client'' box represents the total time needed for instrumentation, 
as measured in the \jvmti{} agent,
and thus includes the IPC and \jvmti{} agent time.

Each chart in the figure consists of five groups of boxes:
``Empty'' is the time when using a \jvmti{} agent that does not process bytecodes at all.
``Identity'' is for an agent that simply decodes and encodes each class, without any instrumentation, and without recomputing stack maps.
``ComputeFrames'' also includes recomputing stack maps.
``Allocations'' represents a useful dynamic analysis that captures all allocations.
``Nop Padding'' is a different dynamic analysis that injects NOPs after each bytecode instruction. 

The figure shows that frame computation adds significant overhead,
on ASM as well as \jnif{}.
Moreover, it shows that except for dacapo-eclipse, dacapo-jython, and scala-scalatest,
\jnif{} is faster even than just the \asm{} Server time.

% \ignore{
% \begin{figure*}[htb]
% \centering
% \vspace{-1cm}
% \includegraphics[width=\textwidth]{../eval-dacapo-chart-instr}

% \vspace{-5mm}
% \includegraphics[width=\textwidth]{../eval-scala-chart-instr}

% \vspace{-5mm}
% \caption{Instrumentation time on DaCapo and Scala benchmarks}
% \label{fig:instr-time}
% \end{figure*}
% }

\begin{figure}[t]
\includegraphics[width=\textwidth]{chapters/jnif/eval-all-chart-instr}
\caption{Instrumentation time on DaCapo and Scala benchmarks}
\label{fig:instr-time}
\end{figure}

%\chart{eval-dacapo-chart-instr}{Instrumentation time on DaCapo}{fig:instr-time-dacapo}
%\chart{eval-scala-chart-instr}{Instrumentation time on Scala and JRuby}{fig:instr-time-scala}

\subsection*{Reproducibility}

To run these evaluations, a Makefile script is provided in the git repository.
These tasks take care of the compilation of the JNIF library and also all java files needed. 
The repository is self-contained, no need to download dacapo benchmarks separately.

\begin{listing}
\begin{minted}[linenos=false]{text}
> make testapp
\end{minted}
\caption{Running testapp}
\label{usage-parse2}
\end{listing}

\begin{listing}
\begin{minted}[linenos=false]{text}
> make testapp
\end{minted}
\caption{Running dacapo}
\label{usage-parse3}
\end{listing}

To run a particular dacapo benchmark with default settings

\begin{listing}
\begin{minted}[linenos=false]{text}
> make dacapo BENCH=avrora
\end{minted}
\caption{Running dacapo}
\label{usage-parse4}
\end{listing}

To run a full evaluation with all dacapo benchmarks in all configuration a task -eval- is provided. You can set how many times run each configuration with the variable times, like

\begin{listing}
\begin{minted}[linenos=false]{text}
> make eval times=5
\end{minted}
\caption{Running full eval five times}
\label{usage-parse5}
\end{listing}

Finally, there is a task to create plots for the evaluation.
This task needs R with the package ggplot2.

\begin{listing}
\begin{minted}[linenos=false]{text}
> make plots
\end{minted}
\caption{Plots}
\label{usage-parse6}
\end{listing}

%1. Stand-alone test-unit:
%In this stage we only test basic functionality of the API such as get the
%right size and the ability to parse and write without correctly without
%any modification and with them. The use of this kind of tests rely on the
%property of the API that if no modification is done to the model the 
%writer is able to recover the original bytes of the class files (except with
%the non-unique property of the stack map tables).
%
%2. Minimal native agent:
%At this stage we prepared several kind of transformations (instrumentation)
%including the identity transformation to test if model/parser/analysis/writer
%can be executed correctly inside a JVM.
%
%3. Dacapo benchmarks:
%As the final tests we run those instrumentations on the Dacapo benchmarks 
%to stress the API. Moreover we use this stage to perform our evaluation.
