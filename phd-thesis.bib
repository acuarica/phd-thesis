
@inproceedings{mastrangelo_jnif:_2014,
	address = {New York, NY, USA},
	series = {{PPPJ} '14},
	title = {{JNIF}: {Java} {Native} {Instrumentation} {Framework}},
	isbn = {978-1-4503-2926-2},
	shorttitle = {{JNIF}},
	url = {http://doi.acm.org/10.1145/2647508.2647516},
	doi = {10.1145/2647508.2647516},
	abstract = {The development of instrumentation-based dynamic analyses for Java bytecode is enabled by various bytecode rewriting frameworks. Those frameworks are all implemented in Java. This complicates their use for developing full-coverage analyses that not only observe application code, but that also observe the execution of the complete Java class library. Moreover, it makes it hard to avoid perturbation due to the Java code of the instrumentation tool interfering with the Java code of the observed program. So far, workarounds for these problems required either statically instrumenting the runtime library or running a separate JVM as an instrumentation server. This paper solves this problem. It introduces JNIF, the first complete bytecode rewriting framework implemented in native code. JNIF can be used in a JVMTI agent to create isolated, full-coverage, in-process dynamic instrumentation tools. JNIF is written in C++ and has an object-oriented design familiar to users of Java-based rewriting libraries. JNIF is able to decode, analyze, edit, and encode Java class files. This includes the generation of stack maps required by split-time verifiers of modern JVMs. Our performance evaluation shows that JNIF is often faster than the most performant competitive approach based on ASM.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Principles} and {Practices} of {Programming} on the {Java} {Platform}: {Virtual} {Machines}, {Languages}, and {Tools}},
	publisher = {ACM},
	author = {Mastrangelo, Luis and Hauswirth, Matthias},
	year = {2014},
	keywords = {bytecode instrumentation, Java, program analysis},
	pages = {194--199},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/8IXP8DSD/Mastrangelo and Hauswirth - 2014 - JNIF Java Native Instrumentation Framework.pdf:application/pdf}
}

@inproceedings{coelho_unveiling_2015,
	address = {Piscataway, NJ, USA},
	series = {{MSR} '15},
	title = {Unveiling {Exception} {Handling} {Bug} {Hazards} in {Android} {Based} on {GitHub} and {Google} {Code} {Issues}},
	isbn = {978-0-7695-5594-2},
	url = {http://dl.acm.org/citation.cfm?id=2820518.2820536},
	abstract = {This paper reports on a study mining the exception stack traces included in 159,048 issues reported on Android projects hosted in GitHub (482 projects) and Google Code (157 projects). The goal of this study is to investigate whether stack trace information can reveal bug hazards related to exception handling code that may lead to a decrease in application robustness. Overall 6,005 exception stack traces were extracted, and subjected to source code and bytecode analysis. The outcomes of this study include the identification of the following bug hazards: (i) unexpected cross-type exception wrappings (for instance, trying to handle an instance of OutOfMemoryError "hidden" in a checked exception) which can make the exception-related code more complex and negatively impact the application robustness; (ii) undocumented runtime exceptions thrown by both the Android platform and third party libraries; and (iii) undocumented checked exceptions thrown by the Android Platform. Such undocumented exceptions make it difficult, and most of the times infeasible for the client code to protect against "unforeseen" situations that may happen while calling third-party code. This study provides further insights on such bug hazards and the robustness threats they impose to Android apps as well as to other systems based on the Java exception model.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 12th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Coelho, Roberta and Almeida, Lucas and Gousios, Georgios and van Deursen, Arie},
	year = {2015},
	pages = {134--145},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TGLYE77G/Coelho et al. - 2015 - Unveiling Exception Handling Bug Hazards in Androi.pdf:application/pdf}
}

@article{knuth_empirical_1971,
	title = {An empirical study of {FORTRAN} programs},
	volume = {1},
	issn = {1097-024X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.4380010203/abstract},
	doi = {10.1002/spe.4380010203},
	abstract = {A sample of programs, written in FORTRAN by a wide variety of people for a wide variety of applications, was chosen {\textquoteleft}at random{\textquoteright} in an attempt to discover quantitatively {\textquoteleft}what programmers really do{\textquoteright}. Statistical results of this survey are presented here, together with some of their apparent implications for future work in compiler design. The principal conclusion which may be drawn is the importance of a program {\textquoteleft}profile{\textquoteright}, namely a table of frequency counts which record how often each statement is performed in a typical run; there are strong indications that profile-keeping should become a standard practice in all computer systems, for casual users as well as system programmers. This paper is the report of a three month study undertaken by the author and about a dozen students and representatives of the software industry during the summer of 1970. It is hoped that a reader who studies this report will obtain a fairly clear conception of how FORTRAN is being used, and what compilers can do about it.},
	language = {en},
	number = {2},
	urldate = {2017-11-11},
	journal = {Software: Practice and Experience},
	author = {Knuth, Donald E.},
	month = apr,
	year = {1971},
	keywords = {Compiler, Efficiency, FORTRAN, Optimization},
	pages = {105--133},
	file = {empirical-fortran.pdf:/Users/luigi/work/zotero/storage/VDMLM4DI/empirical-fortran.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/WPWCCP4B/abstract.html:text/html}
}

@article{fletcher_appropriate_1972,
	title = {On the {Appropriate} {Language} for {System} {Programming}},
	volume = {7},
	issn = {0362-1340},
	url = {http://doi.acm.org/10.1145/953360.953361},
	doi = {10.1145/953360.953361},
	number = {7},
	urldate = {2017-11-11},
	journal = {SIGPLAN Not.},
	author = {Fletcher, J. G. and Badger, C. S. and Boer, G. L. and Marshall, G. G.},
	month = jul,
	year = {1972},
	keywords = {implementation, language, system},
	pages = {28--30},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/Q34Z72E7/Fletcher et al. - 1972 - On the Appropriate Language for System Programming.pdf:application/pdf}
}

@inproceedings{kildall_unified_1973,
	address = {New York, NY, USA},
	series = {{POPL} '73},
	title = {A {Unified} {Approach} to {Global} {Program} {Optimization}},
	url = {http://doi.acm.org/10.1145/512927.512945},
	doi = {10.1145/512927.512945},
	abstract = {A technique is presented for global analysis of program structure in order to perform compile time optimization of object code generated for expressions. The global expression optimization presented includes constant propagation, common subexpression elimination, elimination of redundant register load operations, and live expression analysis. A general purpose program flow analysis algorithm is developed which depends upon the existence of an "optimizing function." The algorithm is defined formally using a directed graph model of program flow structure, and is shown to be correct. Several optimizing functions are defined which, when used in conjunction with the flow analysis algorithm, provide the various forms of code optimization. The flow analysis algorithm is sufficiently general that additional functions can easily be defined for other forms of global code optimization.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 1st {Annual} {ACM} {SIGACT}-{SIGPLAN} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Kildall, Gary A.},
	year = {1973},
	pages = {194--206},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/ELKWJNV8/Kildall - 1973 - A Unified Approach to Global Program Optimization.pdf:application/pdf}
}

@inproceedings{frailey_should_1975,
	address = {New York, NY, USA},
	series = {{ACM} '75},
	title = {Should {High} {Level} {Languages} {Be} {Used} to {Write} {Systems} {Software}?},
	url = {http://doi.acm.org/10.1145/800181.810317},
	doi = {10.1145/800181.810317},
	abstract = {Most of us write our programs in whatever language is most convenient for the problem at hand. Often this means, not so much that the language is well suited to the problem, but simply that it's the best suited of the choices available. Particularly with microprocessors and many minicomputers, we don't have a very wide choice of available software. Perhaps we have only an assembler or only a Basic interpreter. Those who have a choice, or who are responsible for developing compilers and other basic systems software, must determine how much money to spend and where to spend it, becoming embroiled in such questions as what high level languages, if any, should be used or how important it is to develop a good assembler versus a good high level language compiler for our systems work.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 1975 {Annual} {Conference}},
	publisher = {ACM},
	author = {Frailey, Dennis J.},
	year = {1975},
	pages = {205--},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/K22D4IMH/Frailey - 1975 - Should High Level Languages Be Used to Write Syste.pdf:application/pdf}
}

@inproceedings{fletcher_no!_1975,
	address = {New York, NY, USA},
	series = {{ACM} '75},
	title = {No! {High} {Level} {Languages} {Should} {Not} {Be} {Used} to {Write} {Systems} {Software}},
	url = {http://doi.acm.org/10.1145/800181.810319},
	doi = {10.1145/800181.810319},
	abstract = {The views expressed here derive from the experience of the author and his colleagues in designing and implementing the Octopus computer network at the Lawrence Livermore Laboratory. This network serves five major time-shared computers (CDC 7600's and STAR-100's), connecting them to over 800 interactive terminals, about 200 television monitor displays, printers that operate at up to 18,000 lines/minute, and more than a trillion bits of storage. The software for the network has been written entirely in assembly language (for PDP-8's, 10's, and 11's, MODCOMP II's, and TI 980's) and from scratch, basing none of it on manufacturers' or other commercial software. The same persons who create the design also do the programming and debugging. In most cases one or two persons program a computer; four persons were used on the largest system (the PDP-10's). Our experience does not accord with much of what we read in the computing literature, leading us to conclude that it is written by persons unaware the real problems of systems work. We have had little or no trouble with deadlocks, security loopholes, and other logical flaws that are belabored at length in the literature. Most of our effort has gone into devising ways for the system to survive in the presence of intermittent and random failures of hardware components and for it to maintain high data transfer rates among multiply-interconnected devices and computers of varying speeds, matters that are seldom discussed in the literature at all. It is certainly not the case that the difficulties encountered with operating systems are the same as those encountered with other large programs, such as compilers.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 1975 {Annual} {Conference}},
	publisher = {ACM},
	author = {Fletcher, John G.},
	year = {1975},
	pages = {209--211},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/WRJQZAFW/Fletcher - 1975 - No! High Level Languages Should Not Be Used to Wri.pdf:application/pdf}
}

@inproceedings{horning_yes!_1975,
	address = {New York, NY, USA},
	series = {{ACM} '75},
	title = {Yes! {High} {Level} {Languages} {Should} {Be} {Used} to {Write} {Systems} {Software}},
	url = {http://doi.acm.org/10.1145/800181.810318},
	doi = {10.1145/800181.810318},
	abstract = {It has frequently been remarked that it is easier recognize {\textquotedblleft}high level{\textquotedblright} languages than to define the concept. For the purposes of this debate, however, I think that we agree that a language is high level to the extent that it discourages (forbids) the specification of machine details (register numbers, absolute addresses, op codes, word-packing, etc.) as a routine part of program composition and low level to the extent that it encourages (requires) such specification. (Note that, by this definition, assembly languages occupy a position intermediate between machine languages and compiled languages.) Thus, I take the point at issue to be: {\textquotedblleft}To what extent is it desirable for the system programmer to specify machine details?{\textquotedblright}},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 1975 {Annual} {Conference}},
	publisher = {ACM},
	author = {Horning, James J.},
	year = {1975},
	pages = {206--208},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/AYFN2GJM/Horning - 1975 - Yes! High Level Languages Should Be Used to Write .pdf:application/pdf}
}

@article{hammond_basic_1977,
	title = {{BASIC} - an evaluation of processing methods and a study of some programs},
	volume = {7},
	issn = {1097-024X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.4380070605/abstract},
	doi = {10.1002/spe.4380070605},
	abstract = {The relative merits of compiling and interpreting BASIC are examined, and these methods are compared with the technique called throw-away compiling. The comparison reveals that a throw-away compiler has much to recommend it, and some reasons for its superior performance are explained. The BASIC programs used for the performance tests are analysed, both statically and dynamically, and certain features are picked out for comment.},
	language = {en},
	number = {6},
	urldate = {2017-11-11},
	journal = {Software: Practice and Experience},
	author = {Hammond, John},
	month = nov,
	year = {1977},
	keywords = {Compiler, Efficiency, BASIC, Interpreter, Throw-away compiler},
	pages = {697--711},
	file = {Snapshot:/Users/luigi/work/zotero/storage/MH8LMVNV/abstract.html:text/html}
}

@article{lamport_time_1978,
	title = {Time, {Clocks}, and the {Ordering} of {Events} in a {Distributed} {System}},
	volume = {21},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/359545.359563},
	doi = {10.1145/359545.359563},
	abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
	number = {7},
	urldate = {2017-11-11},
	journal = {Commun. ACM},
	author = {Lamport, Leslie},
	month = jul,
	year = {1978},
	keywords = {clock synchronization, computer networks, distributed systems, multiprocess systems},
	pages = {558--565},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NRN7SRHY/Lamport - 1978 - Time, Clocks, and the Ordering of Events in a Dist.pdf:application/pdf}
}

@article{herlihy_wait-free_1991,
	title = {Wait-free {Synchronization}},
	volume = {13},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/114005.102808},
	doi = {10.1145/114005.102808},
	abstract = {A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, {\textquotedblleft}there is no wait-free implementation of X by Y.{\textquotedblright} We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that    atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such astest\&set and fetch\&add, while more powerful than read and write, are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object.},
	number = {1},
	urldate = {2017-11-11},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Herlihy, Maurice},
	month = jan,
	year = {1991},
	keywords = {linearization, wait-free synchronization},
	pages = {124--149},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/7ZNQEGFS/Herlihy - 1991 - Wait-free Synchronization.pdf:application/pdf}
}

@article{liskov_behavioral_1994,
	title = {A {Behavioral} {Notion} of {Subtyping}},
	volume = {16},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/197320.197383},
	doi = {10.1145/197320.197383},
	abstract = {The use of hierarchy is an important component of object-oriented design. Hierarchy allows the use of type families, in which higher level supertypes capture the behavior that all of their subtypes have in common. For this methodology to be effective, it is necessary to have a clear understanding of how subtypes and supertypes are related. This paper takes the position that the relationship should ensure that any property proved about supertype objects also holds for its subtype objects. It presents two ways of defining the subtype relation, each of which meets this criterion, and each of which is easy for programmers to use. The subtype relation is based on the specifications of the sub- and supertypes; the paper presents a way of specifying types that makes it convenient to define  the subtype relation. The paper also discusses the ramifications of this notion of subtyping on the design of type families.},
	number = {6},
	urldate = {2017-11-11},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Liskov, Barbara H. and Wing, Jeannette M.},
	month = nov,
	year = {1994},
	keywords = {formal specifications, Larch, subtyping},
	pages = {1811--1841},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/CTJTTACC/Liskov and Wing - 1994 - A Behavioral Notion of Subtyping.pdf:application/pdf}
}

@article{bershad_spinextensible_1995,
	title = {{SPIN}{\textemdash}an {Extensible} {Microkernel} for {Application}-specific {Operating} {System} {Services}},
	volume = {29},
	issn = {0163-5980},
	url = {http://doi.acm.org/10.1145/202453.202472},
	doi = {10.1145/202453.202472},
	abstract = {Application domains such as multimedia, databases, and parallel computing, require operating system services with high performance and high functionality. Existing operating systems provide fixed interfaces and implementations to system services and resources. This makes them inappropriate for applications whose resource demands and usage patterns are poorly matched by the services provided. The SPIN operating system enables system services to be defined in an application-specific fashion through an extensible microkernel. It offers applications fine-grained control over a machine's logical and physical resources through run-time adaptation of the system to application requirements.},
	number = {1},
	urldate = {2017-11-11},
	journal = {SIGOPS Oper. Syst. Rev.},
	author = {Bershad, Brian N. and Chambers, Craig and Eggers, Susan and Maeda, Chris and McNamee, Dylan and Pardyak, Przemys{\textbackslash}law and Savage, Stefan and Sirer, Emin G{\"u}n},
	month = jan,
	year = {1995},
	pages = {74--77},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TVGKU4UV/Bershad et al. - 1995 - SPIN{\textemdash}an Extensible Microkernel for Application-spe.pdf:application/pdf}
}

@inproceedings{necula_proof-carrying_1997,
	address = {New York, NY, USA},
	series = {{POPL} '97},
	title = {Proof-carrying {Code}},
	isbn = {978-0-89791-853-4},
	url = {http://doi.acm.org/10.1145/263699.263712},
	doi = {10.1145/263699.263712},
	abstract = {This paper describes proof-carrying code (PCC), a mechanism by which a host system can determine with certainty that it is safe to execute a program supplied (possibly in binary form) by an untrusted source. For this to be possible, the untrusted code producer must supply with the code a safety proof that attests to the code's adherence to a previously defined safety policy. The host can then easily and quickly validate the proof without using cryptography and without consulting any external agents.In order to gain preliminary experience with PCC, we have performed several case studies. We show in this paper how proof-carrying code might be used to develop safe assembly-language extensions of ML programs. In the context of this case study, we present and prove the adequacy of concrete representations for the safety policy, the safety proofs, and the proof validation. Finally, we briefly discuss how we use proof-carrying code to develop network packet filters that are faster than similar filters developed using other techniques and are formally guaranteed to be safe with respect to a given operating system safety policy.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 24th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Necula, George C.},
	year = {1997},
	pages = {106--119},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/JFCFZE54/Necula - 1997 - Proof-carrying Code.pdf:application/pdf}
}

@article{ritchie_systems_1997,
	title = {Systems programming in {Java}},
	volume = {17},
	issn = {0272-1732},
	doi = {10.1109/40.591652},
	abstract = {The Java programming language has been widely accepted as a general purpose language for developing portable applications, toolkits, and applets. With so much activity in industry and academia in these user-level areas, is it surprising that Java is also an equally capable systems programming language? This article describes our experiences at JavaSoft with using Java as a systems-level programming language during the development of JavaOS. The author discusses the motivations for using Java and shows code examples to demonstrate various system-level primitives, including an Ethernet device driver},
	number = {3},
	journal = {IEEE Micro},
	author = {Ritchie, S.},
	month = may,
	year = {1997},
	keywords = {Java, Debugging, Ethernet networks, general purpose language, high level languages, Java programming language, JavaOS, JavaSoft, Mice, Object oriented modeling, Operating systems, operating systems (computers), portable applications, Programming profession, Protection, Protocols, Runtime, system-level primitives, systems analysis, systems programming},
	pages = {30--35},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/BZU7XIFI/591652.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/NI5IITLU/Ritchie - 1997 - Systems programming in Java.pdf:application/pdf}
}

@inproceedings{vallee-rai_soot_1999,
	address = {Mississauga, Ontario, Canada},
	series = {{CASCON} '99},
	title = {Soot - a {Java} {Bytecode} {Optimization} {Framework}},
	url = {http://dl.acm.org/citation.cfm?id=781995.782008},
	abstract = {This paper presents Soot, a framework for optimizing Java bytecode. The framework is implemented in Java and supports three intermediate representations for representing Java bytecode: Baf, a streamlined representation of bytecode which is simple to manipulate; Jimple, a typed 3-address intermediate representation suitable for optimization; and Grimp, an aggregated version of Jimple suitable for decompilation. We describe the motivation for each representation, and the salient points in translating from one representation to another.In order to demonstrate the usefulness of the framework, we have implemented intraprocedural and whole program optimizations. To show that whole program bytecode optimization can give performance improvements, we provide experimental results for 12 large benchmarks, including 8 SPECjvm98 benchmarks running on JDK 1.2 for GNU/Linuxtm. These results show up to 8\% improvement when the optimized bytecode is run using the interpreter and up to 21\% when run using the JIT compiler.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 1999 {Conference} of the {Centre} for {Advanced} {Studies} on {Collaborative} {Research}},
	publisher = {IBM Press},
	author = {Vall{\'e}e-Rai, Raja and Co, Phong and Gagnon, Etienne and Hendren, Laurie and Lam, Patrick and Sundaresan, Vijay},
	year = {1999},
	pages = {13--},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DXB3947K/Vall{\'e}e-Rai et al. - 1999 - Soot - a Java Bytecode Optimization Framework.pdf:application/pdf}
}

@inproceedings{siebert_eliminating_2000,
	address = {New York, NY, USA},
	series = {{CASES} '00},
	title = {Eliminating {External} {Fragmentation} in a {Non}-moving {Garbage} {Collector} for {Java}},
	isbn = {978-1-58113-338-7},
	url = {http://doi.acm.org/10.1145/354880.354883},
	doi = {10.1145/354880.354883},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2000 {International} {Conference} on {Compilers}, {Architecture}, and {Synthesis} for {Embedded} {Systems}},
	publisher = {ACM},
	author = {Siebert, Fridtjof},
	year = {2000},
	pages = {9--17},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/FBLQV5QS/Siebert - 2000 - Eliminating External Fragmentation in a Non-moving.pdf:application/pdf}
}

@inproceedings{lea_java_2000,
	address = {New York, NY, USA},
	series = {{JAVA} '00},
	title = {A {Java} {Fork}/{Join} {Framework}},
	isbn = {978-1-58113-288-5},
	url = {http://doi.acm.org/10.1145/337449.337465},
	doi = {10.1145/337449.337465},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {ACM} 2000 {Conference} on {Java} {Grande}},
	publisher = {ACM},
	author = {Lea, Doug},
	year = {2000},
	pages = {36--43},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GFMTENEJ/Lea - 2000 - A Java ForkJoin Framework.pdf:application/pdf}
}

@inproceedings{moonen_generating_2001,
	title = {Generating robust parsers using island grammars},
	doi = {10.1109/WCRE.2001.957806},
	abstract = {Source model extraction, the automated extraction of information from system artifacts, is a common phase in reverse engineering tools. One of the major challenges of this phase is creating extractors that can deal with irregularities in the artifacts that are typical for the reverse engineering domain (for example, syntactic errors, incomplete source code, language dialects and embedded languages). The paper proposes a solution in the form of island grammars, a special kind of grammar that combines the detailed specification possibilities of grammars with the liberal behavior of lexical approaches. We show how island grammars can be used to generate robust parsers that combine the accuracy of syntactical analysis with the speed, flexibility and tolerance usually only found in lexical analysis. We conclude with a discussion of the development of MANGROVE, a generator for source model extractors based on island grammars and describe its application to a number of case studies},
	booktitle = {Proceedings {Eighth} {Working} {Conference} on {Reverse} {Engineering}},
	author = {Moonen, L.},
	year = {2001},
	keywords = {program analysis, Application software, automated information extraction, case studies, computational linguistics, Computer languages, Data mining, detailed specification, embedded languages, fuzzy parsing, grammars, incomplete source code, island grammars, language dialects, lexical approaches, Libraries, Maintenance engineering, MANGROVE, Mars, parser generation, partial parsing, program compilers, reverse engineering, Reverse engineering, reverse engineering domain, reverse engineering tools, robust parser generation, robust parsers, Robustness, Software maintenance, source model extraction, source model extractors, syntactic errors, syntactical analysis, system artifacts, Transaction databases},
	pages = {13--22},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/B8PQQ46Q/957806.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/4H8D6X2H/Moonen - 2001 - Generating robust parsers using island grammars.pdf:application/pdf}
}

@inproceedings{leroy_java_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Java {Bytecode} {Verification}: {An} {Overview}},
	isbn = {978-3-540-42345-4 978-3-540-44585-2},
	shorttitle = {Java {Bytecode} {Verification}},
	url = {https://link.springer.com/chapter/10.1007/3-540-44585-4_26},
	doi = {10.1007/3-540-44585-4_26},
	abstract = {Bytecode verification is a crucial security component for Java applets, on the Web and on embedded devices such as smart cards. This paper describes the main bytecode verification algorithms and surveys the variety of formal methods that have been applied to bytecode verification in order to establish its correctness.},
	language = {en},
	urldate = {2017-11-11},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Leroy, Xavier},
	month = jul,
	year = {2001},
	pages = {265--285},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/NDKZZIJ2/Leroy - 2001 - Java Bytecode Verification An Overview.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/Y296DJRG/10.html:text/html}
}

@inproceedings{bruneton_asm:_2002,
	title = {{ASM}: {A} code manipulation tool to implement adaptable systems},
	shorttitle = {{ASM}},
	abstract = {ABSTRACT. ASM is a Java class manipulation tool designed to dynamically generate and manipulate Java classes, which are useful techniques to implement adaptable systems. ASM is based on a new approach, compared to equivalent existing tools, which consists in using the \&quot;visitor\&quot; design pattern without explicitly representing the visited tree with objects. This new approach gives much better performances than those of existing tools, for most of practical needs. R{\'E}SUM{\'E}. ASM est un outil de manipulation de classes Java con{\c c}u pour la g{\'e}n{\'e}ration et la manipulation dynamiques de code, qui sont des techniques tr{\`e}s utiles pour la r{\'e}alisation de syst{\`e}mes adaptables. ASM est bas{\'e} sur une approche originale, par rapport aux outils existants {\'e}quivalents, qui consiste {\`a} utiliser le patron de conception {\guillemotleft} visiteur {\guillemotright} sans repr{\'e}senter explicitement l{\textquoteright}arborescence visit{\'e}e sous forme d{\textquoteright}objets. Cette nouvelle approche permet d{\textquoteright}obtenir des performances bien sup{\'e}rieures {\`a} celles des outils existants, pour la plupart des besoins courants.},
	booktitle = {In {Adaptable} and extensible component systems},
	author = {Bruneton, Eric and Lenglet, Romain and Coupaye, Thierry},
	year = {2002},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/X2Q7NC9L/Bruneton et al. - 2002 - ASM A code manipulation tool to implement adaptab.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/KL9Z4HDH/summary.html:text/html}
}

@book{jim_cyclone:_nodate,
	title = {Cyclone: {A} safe dialect of {C}},
	shorttitle = {Cyclone},
	abstract = {Cyclone is a safe dialect of C. It has been designed from the ground up to prevent the buffer overflows, format string attacks, and memory management errors that are common in C programs, while retaining C's syntax and semantics. This paper examines safety violations enabled by C's design, and shows how Cyclone avoids them, without giving up C's hallmark control over low-level details such as data representation and memory management.},
	author = {Jim, Trevor and Morrisett, Greg and Grossman, Dan and Hicks, Michael and Cheney, James and Wang, Yanling},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/7E5RTS32/Jim et al. - Cyclone A safe dialect of C.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/BIBZRDU6/summary.html:text/html}
}

@article{barnett_verification_2003,
	title = {Verification of {Object}-{Oriented} {Programs} {With} {Invariants}},
	volume = {3, No. 6},
	url = {https://www.microsoft.com/en-us/research/publication/verification-of-object-oriented-programs-with-invariants-2/},
	abstract = {An object invariant defines what it means for an object{\textquoteright}s data to be in a consistent state. Object invariants are central to the design and correctness of object-oriented programs. This paper defines a programming methodology for using object invariants. The methodology, which enriches a program{\textquoteright}s state space to express when each object invariant holds, deals {\textellipsis}},
	urldate = {2017-11-11},
	journal = {Journal of Object Technology, Special issue: ECOOP 2003 workshop on FTfJP},
	author = {Barnett, Mike and DeLine, Rob and Fahndrich, Manuel and Leino, Rustan and Schulte, Wolfram},
	month = jul,
	year = {2003},
	file = {article2.pdf:/Users/luigi/work/zotero/storage/7Z5SLUBY/article2.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/YLL6KZ28/verification-of-object-oriented-programs-with-invariants-2.html:text/html}
}

@inproceedings{nystrom_polyglot:_2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Polyglot: {An} {Extensible} {Compiler} {Framework} for {Java}},
	isbn = {978-3-540-00904-7 978-3-540-36579-2},
	shorttitle = {Polyglot},
	url = {https://link.springer.com/chapter/10.1007/3-540-36579-6_11},
	doi = {10.1007/3-540-36579-6_11},
	abstract = {Polyglot is an extensible compiler framework that supports the easy creation of compilers for languages similar to Java, while avoiding code duplication. The Polyglot framework is useful for domain-specific languages, exploration of language design, and for simplified versions of Java for pedagogical use. We have used Polyglot to implement several major and minor modifications to Java; the cost of implementing language extensions scales well with the degree to which the language differs from Java. This paper focuses on the design choices in Polyglot that are important for making the framework usable and highly extensible. Polyglot source code is available.},
	language = {en},
	urldate = {2017-11-11},
	booktitle = {Compiler {Construction}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Nystrom, Nathaniel and Clarkson, Michael R. and Myers, Andrew C.},
	month = apr,
	year = {2003},
	pages = {138--152},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/7R2QK48C/Nystrom et al. - 2003 - Polyglot An Extensible Compiler Framework for Jav.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/SAJRSBBV/10.html:text/html}
}

@inproceedings{chen_heap_2003,
	address = {New York, NY, USA},
	series = {{OOPSLA} '03},
	title = {Heap {Compression} for {Memory}-constrained {Java} {Environments}},
	isbn = {978-1-58113-712-5},
	url = {http://doi.acm.org/10.1145/949305.949330},
	doi = {10.1145/949305.949330},
	abstract = {Java is becoming the main software platform for consumer and embedded devices such as mobile phones, PDAs, TV set-top boxes, and in-vehicle systems. Since many of these systems are memory constrained, it is extremely important to keep the memory footprint of Java applications under control.The goal of this work is to enable the execution of Java applications using a smaller heap footprint than that possible using current embedded JVMs. We propose a set of memory management strategies to reduce heap footprint of embedded Java applications that execute under severe memory constraints. Our first contribution is a new garbage collector, referred to as the Mark-Compact-Compress (MCC) collector, that allows an application to run with a heap smaller than its footprint. An important characteristic of this collector is that it compresses objects when heap compaction is not sufficient for creating space for the current allocation request. In addition to employing compression, we also consider a heap management strategy and associated garbage collector, called MCL (Mark-Compact-Lazy Allocate), based on lazy allocation of object portions. This new collector operates like the conventional Mark-Compact (MC) collector, but takes advantage of the observation that many Java applications create large objects, of which only a small portion is actually used. In addition, we also combine MCC and MCL, and present MCCL (Mark-Compact-Compress-Lazy Al-locate), which outperforms both MCC and MCL.We have implemented these collectors using KVM, and performed extensive experiments using a set of ten embedded Java applications. We have found our new garbage collection strategies to be useful in two main aspects. First, they reduce the minimum heap size necessary to execute an application without out-of-memory exception. Second, our strategies reduce the heap occupancy. That is, at a given time, they reduce the heap memory requirement of the application being executed. We have also conducted experiments with a more aggressive object compression strategy and discussed its main advantages.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 18th {Annual} {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programing}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Chen, G. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M. J. and Mathiske, B. and Wolczko, M.},
	year = {2003},
	keywords = {garbage collection, heap, Java virtual machine, memory compression},
	pages = {282--301},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/WHRRN5EQ/Chen et al. - 2003 - Heap Compression for Memory-constrained Java Envir.pdf:application/pdf}
}

@inproceedings{bacon_real-time_2003,
	address = {New York, NY, USA},
	series = {{POPL} '03},
	title = {A {Real}-time {Garbage} {Collector} with {Low} {Overhead} and {Consistent} {Utilization}},
	isbn = {978-1-58113-628-9},
	url = {http://doi.acm.org/10.1145/604131.604155},
	doi = {10.1145/604131.604155},
	abstract = {Now that the use of garbage collection in languages like Java is becoming widely accepted due to the safety and software engineering benefits it provides, there is significant interest in applying garbage collection to hard real-time systems. Past approaches have generally suffered from one of two major flaws: either they were not provably real-time, or they imposed large space overheads to meet the real-time bounds. We present a mostly non-moving, dynamically defragmenting collector that overcomes both of these limitations: by avoiding copying in most cases, space requirements are kept low; and by fully incrementalizing the collector we are able to meet real-time bounds. We implemented our algorithm in the Jikes RVM and show that at real-time resolution we are able to obtain mutator utilization rates of 45\% with only 1.6--2.5 times the actual space required by the application, a factor of 4 improvement in utilization over the best previously published results. Defragmentation causes no more than 4\% of the traced data to be copied.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 30th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Bacon, David F. and Cheng, Perry and Rajan, V. T.},
	year = {2003},
	keywords = {defragmentation, read barrier, real-time scheduling, utilization},
	pages = {285--298},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/I37SF3CL/Bacon et al. - 2003 - A Real-time Garbage Collector with Low Overhead an.pdf:application/pdf}
}

@article{leroy_java_2003,
	title = {Java {Bytecode} {Verification}: {Algorithms} and {Formalizations}},
	volume = {30},
	issn = {0168-7433, 1573-0670},
	shorttitle = {Java {Bytecode} {Verification}},
	url = {https://link.springer.com/article/10.1023/A:1025055424017},
	doi = {10.1023/A:1025055424017},
	abstract = {Bytecode verification is a crucial security component for Java applets, on the Web and on embedded devices such as smart cards. This paper reviews the various bytecode verification algorithms that have been proposed, recasts them in a common framework of dataflow analysis, and surveys the use of proof assistants to specify bytecode verification and prove its correctness.},
	language = {en},
	number = {3-4},
	urldate = {2017-11-11},
	journal = {Journal of Automated Reasoning},
	author = {Leroy, Xavier},
	month = may,
	year = {2003},
	pages = {235--269},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/BDMRQBEU/Leroy - 2003 - Java Bytecode Verification Algorithms and Formali.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/AVBMVBXU/10.html:text/html}
}

@article{hunt_singularity_2004,
	title = {Singularity {Design} {Motivation}},
	url = {https://www.microsoft.com/en-us/research/publication/singularity-design-motivation/},
	abstract = {Singularity is a cross-discipline research project in Microsoft Research building a managed code operating system. This technical report describes the motivation and priorities for Singularity. Other technical reports describe the abstractions and implementations of Singularity features.},
	urldate = {2017-11-11},
	journal = {Microsoft Research},
	author = {Hunt, Galen and Larus, Jim},
	month = nov,
	year = {2004},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/V4FM6DPL/Hunt and Larus - 2004 - Singularity Design Motivation.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/9CHFMXKS/singularity-design-motivation.html:text/html}
}

@inproceedings{barnett_spec_2004,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Spec}\# {Programming} {System}: {An} {Overview}},
	isbn = {978-3-540-24287-1 978-3-540-30569-9},
	shorttitle = {The {Spec}\# {Programming} {System}},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-30569-9_3},
	doi = {10.1007/978-3-540-30569-9_3},
	abstract = {The Spec\# programming system is a new attempt at a more cost effective way to develop and maintain high-quality software. This paper describes the goals and architecture of the Spec\# programming system, consisting of the object-oriented Spec\# programming language, the Spec\# compiler, and the Boogie static program verifier. The language includes constructs for writing specifications that capture programmer intentions about how methods and data are to be used, the compiler emits run-time checks to enforce these specifications, and the verifier can check the consistency between a program and its specifications.},
	language = {en},
	urldate = {2017-11-11},
	booktitle = {Construction and {Analysis} of {Safe}, {Secure}, and {Interoperable} {Smart} {Devices}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Barnett, Mike and Leino, K. Rustan M. and Schulte, Wolfram},
	month = mar,
	year = {2004},
	pages = {49--69},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/5FZ9KSD7/Barnett et al. - 2004 - The Spec# Programming System An Overview.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/CB4CFN9F/10.html:text/html}
}

@inproceedings{lisitsa_towards_2005,
	title = {Towards verification via supercompilation},
	volume = {2},
	doi = {10.1109/COMPSAC.2005.159},
	abstract = {Supercompilation, or supervised compilation is a technique for program specialization, optimization and, more generally, program transformation. We present an idea to use supercompilation for verification of parameterized programs and protocols, present a case study and report on our initial experiments.},
	booktitle = {29th {Annual} {International} {Computer} {Software} and {Applications} {Conference} ({COMPSAC}'05)},
	author = {Lisitsa, A. and Nemytykh, A.},
	month = jul,
	year = {2005},
	keywords = {Protocols, Computer languages, program compilers, Computer applications, Computer science, Concrete, formal verification, History, Power system modeling, program optimization, program specialization, program transformation, Resource description framework, Software testing, supercompilation, System testing},
	pages = {9--10 Vol. 1},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/Z4WLQ6QE/1508067.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/XPALV5Q5/Lisitsa and Nemytykh - 2005 - Towards verification via supercompilation.pdf:application/pdf}
}

@inproceedings{hallgren_principled_2005,
	address = {New York, NY, USA},
	series = {{ICFP} '05},
	title = {A {Principled} {Approach} to {Operating} {System} {Construction} in {Haskell}},
	isbn = {978-1-59593-064-4},
	url = {http://doi.acm.org/10.1145/1086365.1086380},
	doi = {10.1145/1086365.1086380},
	abstract = {We describe a monadic interface to low-level hardware features that is a suitable basis for building operating systems in Haskell. The interface includes primitives for controlling memory management hardware, user-mode process execution, and low-level device I/O. The interface enforces memory safety in nearly all circumstances. Its behavior is specified in part by formal assertions written in a programming logic called P-Logic. The interface has been implemented on bare IA32 hardware using the Glasgow Haskell Compiler (GHC) runtime system. We show how a variety of simple O/S kernels can be constructed on top of the interface, including a simple separation kernel and a demonstration system in which the kernel, window system, and all device drivers are written in Haskell.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {Tenth} {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Hallgren, Thomas and Jones, Mark P. and Leslie, Rebekah and Tolmach, Andrew},
	year = {2005},
	keywords = {hardware interface, Haskell, monads, operating systems, programming logic, verification},
	pages = {116--128},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NHIE7IWP/Hallgren et al. - 2005 - A Principled Approach to Operating System Construc.pdf:application/pdf}
}

@article{alpern_jikes_2005,
	title = {The {Jikes} {Research} {Virtual} {Machine} project: {Building} an open-source research community},
	volume = {44},
	issn = {0018-8670},
	shorttitle = {The {Jikes} {Research} {Virtual} {Machine} project},
	doi = {10.1147/sj.442.0399},
	abstract = {This paper describes the evolution of the Jikes{\texttrademark} Research Virtual Machine project from an IBM internal research project, called Jalape{\~n}o, into an open-source project. After summarizing the original goals of the project, we discuss the motivation for releasing it as an open-source project and the activities performed to ensure the success of the project. Throughout, we highlight the unique challenges of developing and maintaining an open-source project designed specifically to support a research community.},
	number = {2},
	journal = {IBM Systems Journal},
	author = {Alpern, B. and Augart, S. and Blackburn, S. M. and Butrico, M. and Cocchi, A. and Cheng, P. and Dolby, J. and Fink, S. and Grove, D. and Hind, M. and McKinley, K. S. and Mergen, M. and Moss, J. E. B. and Ngo, T. and Sarkar, V. and Trapp, M.},
	year = {2005},
	pages = {399--417},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/QPE9ZXNA/5386722.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/EYDPZ64K/Alpern et al. - 2005 - The Jikes Research Virtual Machine project Buildi.pdf:application/pdf}
}

@article{hunt_overview_2005,
	title = {An {Overview} of the {Singularity} {Project}},
	url = {https://www.microsoft.com/en-us/research/publication/an-overview-of-the-singularity-project/},
	abstract = {Singularity is a research project in Microsoft Research that started with the question: what would a software platform look like if it was designed from scratch with the primary goal of dependability? Singularity is working to answer this question by building on advances in programming languages and tools to develop a new system architecture and {\textellipsis}},
	urldate = {2017-11-11},
	journal = {Microsoft Research},
	author = {Hunt, Galen and Abadi, Martin and Barham, Paul and Fahndrich, Manuel and Hawblitzel, Chris and Hodson, Orion and Levi, Steven and Wobber, Ted and Zill, Brian and Larus, Jim},
	month = oct,
	year = {2005},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/964VBIY4/Hunt et al. - 2005 - An Overview of the Singularity Project.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/HKNJXFFK/an-overview-of-the-singularity-project.html:text/html}
}

@inproceedings{hindman_atomicity_2006,
	address = {New York, NY, USA},
	series = {{MSPC} '06},
	title = {Atomicity via {Source}-to-source {Translation}},
	isbn = {978-1-59593-578-6},
	url = {http://doi.acm.org/10.1145/1178597.1178611},
	doi = {10.1145/1178597.1178611},
	abstract = {We present an implementation and evaluation of atomicity (also known as software transactions) for a dialect of Java. Our implementation is fundamentally different from prior work in three respects: (1) It is entirely a source-to-source translation, producing Java source code that can be compiled by any Java compiler and run on any Java Virtual Machine. (2) It can enforce "strong" atomicity without assuming special hardware or a uniprocessor. (3) The implementation uses locks rather than optimistic concurrency, but it cannot deadlock and requires inter-thread communication only when there is data contention.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2006 {Workshop} on {Memory} {System} {Performance} and {Correctness}},
	publisher = {ACM},
	author = {Hindman, Benjamin and Grossman, Dan},
	year = {2006},
	keywords = {Java, atomicity, concurrent programming, transactional memory},
	pages = {82--91},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/CD9LTTAT/Hindman and Grossman - 2006 - Atomicity via Source-to-source Translation.pdf:application/pdf;ACM Full Text PDF:/Users/luigi/work/zotero/storage/JDFYI5HZ/Hindman and Grossman - 2006 - Atomicity via Source-to-source Translation.pdf:application/pdf}
}

@inproceedings{yamauchi_writing_2006,
	address = {New York, NY, USA},
	series = {{PLOS} '06},
	title = {Writing {Solaris} {Device} {Drivers} in {Java}},
	isbn = {978-1-59593-577-9},
	url = {http://doi.acm.org/10.1145/1215995.1215998},
	doi = {10.1145/1215995.1215998},
	abstract = {Operating system kernels have been traditionally written in C for flexibility and efficiency. They, however, often suffer from bugs and security vulnerabilities because C is inherently error-prone and unsafe. While there have been attempts to experimentally construct a complete operating system in a type safe language such as Java for higher safety and reliability, such type safe operating systems are not mainstream as yet. In this paper, we present an experimental implementation of the Java Virtual Machine that runs inside the kernel of the Solaris operating system. Our approach is to extend the existing operating system, rather than creating a new operating system from scratch, in order to reap the benefits of a type safe language in the kernel without expensive development and transition cost for a new operating system architecture. We implemented our system by porting an existing small, portable JVM, Squawk, into the Solaris kernel. Our first application of this system is to allow device drivers to be written in Java. A simple device driver was ported from C to Java. Characteristics of the Java device driver and our device driver interface are described.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 3rd {Workshop} on {Programming} {Languages} and {Operating} {Systems}: {Linguistic} {Support} for {Modern} {Operating} {Systems}},
	publisher = {ACM},
	author = {Yamauchi, Hiroshi and Wolczko, Mario},
	year = {2006},
	keywords = {operating systems, device drivers, type-safe languages},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/LPT7G8NB/Yamauchi and Wolczko - 2006 - Writing Solaris Device Drivers in Java.pdf:application/pdf;ACM Full Text PDF:/Users/luigi/work/zotero/storage/DSMYR96U/Yamauchi and Wolczko - 2006 - Writing Solaris Device Drivers in Java.pdf:application/pdf}
}

@article{cachopo_versioned_2006,
	series = {Special issue on synchronization and concurrency in object-oriented languages},
	title = {Versioned boxes as the basis for memory transactions},
	volume = {63},
	issn = {0167-6423},
	url = {http://www.sciencedirect.com/science/article/pii/S0167642306001171},
	doi = {10.1016/j.scico.2006.05.009},
	abstract = {In this paper, we propose the use of Versioned Boxes, which keep a history of values, as the basis for language-level memory transactions. Unlike previous work on software transactional memory, in our proposal read-only transactions never conflict with any other concurrent transaction. This may improve significantly the concurrency on applications which have longer transactions and a high read/write ratio. Furthermore, we discuss how we can reduce transaction conflicts by delaying computations and re-executing only parts of a transaction in case of a conflict. We propose two language-level abstractions to support these strategies: the per-transaction boxes and the restartable transactions. Finally, we lay out the basis for a more generic model, which better supports fine-grained restartable transactions. The goal of this new model is to generalize the previous two abstractions to reduce conflicts.},
	number = {2},
	urldate = {2017-11-11},
	journal = {Science of Computer Programming},
	author = {Cachopo, Jo{\~a}o and Rito-Silva, Ant{\'o}nio},
	month = dec,
	year = {2006},
	keywords = {Conflict reduction, Multi-version concurrency control, Software transactional memory, Transactions},
	pages = {172--185},
	file = {ScienceDirect Full Text PDF:/Users/luigi/work/zotero/storage/77L9QTGT/Cachopo and Rito-Silva - 2006 - Versioned boxes as the basis for memory transactio.pdf:application/pdf;ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/HLH3QHIB/S0167642306001171.html:text/html}
}

@article{fahndrich_language_2006,
	title = {Language {Support} for {Fast} and {Reliable} {Message}-based {Communication} in {Singularity} {OS}},
	url = {https://www.microsoft.com/en-us/research/publication/language-support-for-fast-and-reliable-message-based-communication-in-singularity-os/},
	abstract = {Message-based communication offers the potential benefits of providing stronger specification and cleaner separation between components. Compared with shared-memory interactions, message passing has the potential disadvantages of more expensive data exchange (no direct sharing) and more complicated programming. In this paper we report on the language, verification, and run-time system features that make messages practical as {\textellipsis}},
	urldate = {2017-11-11},
	journal = {Microsoft Research},
	author = {Fahndrich, Manuel and Aiken, Mark and Hawblitzel, Chris and Hodson, Orion and Hunt, Galen and Larus, Jim and Levi, Steven},
	month = apr,
	year = {2006},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/UKFJ96R8/Fahndrich et al. - 2006 - Language Support for Fast and Reliable Message-bas.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/B3NEXJB9/language-support-for-fast-and-reliable-message-based-communication-in-singularity-os.html:text/html}
}

@article{collberg_empirical_2007,
	title = {An empirical study of {Java} bytecode programs},
	volume = {37},
	issn = {1097-024X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.776/abstract},
	doi = {10.1002/spe.776},
	abstract = {We present a study of the static structure of real Java bytecode programs. A total of 1132 Java jar-files were collected from the Internet and analyzed. In addition to simple counts (number of methods per class, number of bytecode instructions per method, etc.), structural metrics such as the complexity of control-flow and inheritance graphs were computed. We believe this study will be valuable in the design of future programming languages and virtual machine instruction sets, as well as in the efficient implementation of compilers and other language processors. Copyright {\textcopyright} 2006 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2017-11-11},
	journal = {Software: Practice and Experience},
	author = {Collberg, Christian and Myles, Ginger and Stepp, Michael},
	month = may,
	year = {2007},
	keywords = {Java, bytecode, measure, software complexity metrics},
	pages = {581--641},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/P2N3ATDN/Collberg et al. - 2007 - An empirical study of Java bytecode programs.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/S9IRIET3/abstract.html:text/html}
}

@inproceedings{gay_safe_2007,
	address = {New York, NY, USA},
	series = {{ISMM} '07},
	title = {Safe {Manual} {Memory} {Management}},
	isbn = {978-1-59593-893-0},
	url = {http://doi.acm.org/10.1145/1296907.1296911},
	doi = {10.1145/1296907.1296911},
	abstract = {We present HeapSafe, a tool that uses reference counting to dynamically verify the soundness of manual memory management of C programs. HeapSafe relies on asimple extension to the usual malloc/free memory management API: delayed free scopes during which otherwise dangling references can exist. Porting programs for use with HeapSafe typically requires little effort (on average 0.6\% oflines change), adds an average 11\% time overhead (84\% in the worst case), and increases space usage by an average of 13\%. These results are based on portingover half a million lines of C code, including perl where we found sixpreviously unknown bugs.Many existing C programs continue to use unchecked manual memorymanagement. One reason is that programmers fear that moving to garbage collection is too big a risk. We believe that HeapSafe is a practical way toprovide safe memory management for such programs. Since HeapSafe checks existing memory management rather than changing it, programmers need not worrythat HeapSafe will introduce new bugs; and, since HeapSafe does not managememory itself, programmers can choose to deploy their programs without HeapSafe if performance is critical (a simple header file allows HeapSafe programs to compile and run with a regular C compiler). In contrast, we foundthat garbage collection, although faster, had much higher space overhead, and occasionally caused a space-usage explosion that made the program unusable.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 6th {International} {Symposium} on {Memory} {Management}},
	publisher = {ACM},
	author = {Gay, David and Ennals, Rob and Brewer, Eric},
	year = {2007},
	keywords = {C, memory management, reference counting, safety},
	pages = {2--14},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HZI238P2/Gay et al. - 2007 - Safe Manual Memory Management.pdf:application/pdf}
}

@inproceedings{kinneer_sofya:_2007,
	title = {Sofya: {Supporting} {Rapid} {Development} of {Dynamic} {Program} {Analyses} for {Java}},
	shorttitle = {Sofya},
	doi = {10.1109/ICSECOMPANION.2007.68},
	abstract = {Dynamic analysis is an increasingly important means of supporting software validation and maintenance. To date, developers of dynamic analyses have used low-level instrumentation and debug interfaces to realize their analyses. Many dynamic analyses, however, share multiple common high-level requirements, e.g., capture of program data state as well as events, and efficient and accurate event capture in the presence of threading. We present SOFYA - an infra-structure designed to provide high-level, efficient, concurrency-aw are support for building analyses that reason about rich observations of program data and events. It provides a layered, modular architecture, which has been successfully used to rapidly develop and evaluate a variety of demanding dynamic program analyses. In this paper, we describe the SOFYA framework, the challenges it addresses, and survey several such analyses.},
	booktitle = {29th {International} {Conference} on {Software} {Engineering} - {Companion}, 2007. {ICSE} 2007 {Companion}},
	author = {Kinneer, A. and Dwyer, M. B. and Rothermel, G.},
	month = may,
	year = {2007},
	keywords = {Java, Software maintenance, Computer science, Buildings, Concurrent computing, debug interfaces, dynamic program analyses, Instruments, modular architecture, Monitoring, Payloads, Performance analysis, program data observations, program diagnostics, program verification, software maintenance, software validation, SOFYA framework, Yarn},
	pages = {51--52},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/ISXRMTMN/4222676.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/V6JL39T3/Kinneer et al. - 2007 - Sofya Supporting Rapid Development of Dynamic Pro.pdf:application/pdf}
}

@book{kuleshov_using_2007,
	title = {Using the {ASM} framework to implement common {Java} bytecode transformation patterns},
	author = {Kuleshov, Eugene},
	year = {2007},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/8ECNLPNX/Kuleshov - 2007 - Using the ASM framework to implement common Java b.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/AS2T2AXP/summary.html:text/html}
}

@inproceedings{mitchell_supercompiler_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Supercompiler} for {Core} {Haskell}},
	isbn = {978-3-540-85372-5 978-3-540-85373-2},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-85373-2_9},
	doi = {10.1007/978-3-540-85373-2_9},
	abstract = {Haskell is a functional language, with features such as higher order functions and lazy evaluation, which allow succinct programs. These high-level features present many challenges for optimising compilers. We report practical experiments using novel variants of supercompilation, with special attention to let bindings and the generalisation technique.},
	language = {en},
	urldate = {2017-11-11},
	booktitle = {Implementation and {Application} of {Functional} {Languages}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Mitchell, Neil and Runciman, Colin},
	month = sep,
	year = {2007},
	pages = {147--164},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/3T9SUZ5H/Mitchell and Runciman - 2007 - A Supercompiler for Core Haskell.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/JYRJPCMS/10.html:text/html}
}

@inproceedings{binder_reengineering_2007,
	title = {Reengineering {Standard} {Java} {Runtime} {Systems} through {Dynamic} {Bytecode} {Instrumentation}},
	doi = {10.1109/SCAM.2007.20},
	abstract = {Java bytecode instrumentation is a widely used technique, especially for profiling purposes. In order to ensure the instrumentation of all classes in the system, including dynamically generated or downloaded code, instrumentation has to be performed at runtime. The standard JDK offers some mechanisms for dynamic instrumentation, which however either require the use of native code or impose severe restrictions on the instrumentation of certain core classes of the JDK. These limitations prevent several instrumentation techniques that are important for efficient, calling context-sensitive profiling. In this paper we present a generic bytecode instrumentation framework that goes beyond these restrictions and enables the customized, dynamic instrumentation of all classes in pure Java. Our framework addresses important issues, such as bootstrapping an instrumented JDK, as well as avoiding measurement perturbations due to dynamic instrumentation or execution of instrumentation code. We validated and evaluated our framework using an instrumentation for exact profiling which generates complete calling context trees of various platform-independent dynamic metrics.},
	booktitle = {Seventh {IEEE} {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM} 2007)},
	author = {Binder, W. and Hulaas, J. and Moret, P.},
	month = sep,
	year = {2007},
	keywords = {Java, Runtime, Libraries, Instruments, Performance analysis, aspect-oriented programming, bootstrapping, Code standards, Communication standards, computer bootstrapping, context-sensitive profiling, dynamic bytecode instrumentation, dynamic metrics, Dynamic programming, Informatics, Java dynamic bytecode instrumentation, JVM, Manipulator dynamics, profiling, program transformations, software libraries, standard Java runtime system reengineering, systems re-engineering, virtual machines},
	pages = {91--100},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/XBF7PL7V/4362901.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/LQNG6DB6/Binder et al. - 2007 - Reengineering Standard Java Runtime Systems throug.pdf:application/pdf}
}

@article{erren_ten_2007,
	title = {Ten {Simple} {Rules} for {Doing} {Your} {Best} {Research}, {According} to {Hamming}},
	volume = {3},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0030213},
	doi = {10.1371/journal.pcbi.0030213},
	number = {10},
	urldate = {2017-11-11},
	journal = {PLOS Computational Biology},
	author = {Erren, Thomas C. and Cullen, Paul and Erren, Michael and Bourne, Philip E.},
	month = oct,
	year = {2007},
	keywords = {Computer and information sciences, Creativity, Distillation, Fathers, Information theory, Physicists, Scientists, Telecommunications},
	pages = {e213},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/RFK49KQ7/Erren et al. - 2007 - Ten Simple Rules for Doing Your Best Research, Acc.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/UPHAN9SH/article.html:text/html}
}

@article{lhotak_evaluating_2008,
	title = {Evaluating the {Benefits} of {Context}-sensitive {Points}-to {Analysis} {Using} a {BDD}-based {Implementation}},
	volume = {18},
	issn = {1049-331X},
	url = {http://doi.acm.org/10.1145/1391984.1391987},
	doi = {10.1145/1391984.1391987},
	abstract = {We present Paddle, a framework of BDD-based context-sensitive points-to and call graph analyses for Java, as well as client analyses that use their results. Paddle supports several variations of context-sensitive analyses, including call site strings and object sensitivity, and context-sensitively specializes both pointer variables and the heap abstraction. We empirically evaluate the precision of these context-sensitive analyses on significant Java programs. We find that that object-sensitive analyses are more precise than comparable variations of the other approaches, and that specializing the heap abstraction improves precision more than extending the length of context strings.},
	number = {1},
	urldate = {2017-11-11},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Lhot{\'a}k, Ond{\v r}ej and Hendren, Laurie},
	month = oct,
	year = {2008},
	keywords = {Java, binary decision diagrams, call graph construction, cast safety analysis, context sensitivity, Interprocedural program analysis, points-to analysis},
	pages = {3:1--3:53},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/96WDZGXD/Lhot{\'a}k and Hendren - 2008 - Evaluating the Benefits of Context-sensitive Point.pdf:application/pdf}
}

@inproceedings{holkner_evaluating_2009,
	address = {Darlinghurst, Australia, Australia},
	series = {{ACSC} '09},
	title = {Evaluating the {Dynamic} {Behaviour} of {Python} {Applications}},
	isbn = {978-1-920682-72-9},
	url = {http://dl.acm.org/citation.cfm?id=1862659.1862665},
	abstract = {The Python programming language is typical among dynamic languages in that programs written in it are not susceptible to static analysis. This makes efficient static program compilation difficult, as well as limiting the amount of early error detection that can be performed. Prior research in this area tends to make assumptions about the nature of programs written in Python, restricting the expressiveness of the language. One may question why programmers are drawn to these languages at all, if only to use them in a static-friendly style. In this paper we present our results after measuring the dynamic behaviour of 24 production-stage open source Python programs. The programs tested included arcade games, GUI applications and non-interactive batch programs. We found that while most dynamic activity occurs during program startup, dynamic activity after startup cannot be discounted entirely.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {Thirty}-{Second} {Australasian} {Conference} on {Computer} {Science} - {Volume} 91},
	publisher = {Australian Computer Society, Inc.},
	author = {Holkner, Alex and Harland, James},
	year = {2009},
	keywords = {dynamic languages, Python and compilers},
	pages = {19--28},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/EGCHB39A/Holkner and Harland - 2009 - Evaluating the Dynamic Behaviour of Python Applica.pdf:application/pdf}
}

@inproceedings{frampton_demystifying_2009,
	address = {New York, NY, USA},
	series = {{VEE} '09},
	title = {Demystifying {Magic}: {High}-level {Low}-level {Programming}},
	isbn = {978-1-60558-375-4},
	shorttitle = {Demystifying {Magic}},
	url = {http://doi.acm.org/10.1145/1508293.1508305},
	doi = {10.1145/1508293.1508305},
	abstract = {The power of high-level languages lies in their abstraction over hardware and software complexity, leading to greater security, better reliability, and lower development costs. However, opaque abstractions are often show-stoppers for systems programmers, forcing them to either break the abstraction, or more often, simply give up and use a different language. This paper addresses the challenge of opening up a high-level language to allow practical low-level programming without forsaking integrity or performance. The contribution of this paper is three-fold: 1) we draw together common threads in a diverse literature, 2) we identify a framework for extending high-level languages for low-level programming, and 3) we show the power of this approach through concrete case studies. Our framework leverages just three core ideas: extending semantics via intrinsic methods, extending types via unboxing and architectural-width primitives, and controlling semantics via scoped semantic regimes. We develop these ideas through the context of a rich literature and substantial practical experience. We show that they provide the power necessary to implement substantial artifacts such as a high-performance virtual machine, while preserving the software engineering benefits of the host language. The time has come for high-level low-level programming to be taken more seriously: 1) more projects now use high-level languages for systems programming, 2) increasing architectural heterogeneity and parallelism heighten the need for abstraction, and 3) a new generation of high-level languages are under development and ripe to be influenced.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2009 {ACM} {SIGPLAN}/{SIGOPS} {International} {Conference} on {Virtual} {Execution} {Environments}},
	publisher = {ACM},
	author = {Frampton, Daniel and Blackburn, Stephen M. and Cheng, Perry and Garner, Robin J. and Grove, David and Moss, J. Eliot B. and Salishev, Sergey I.},
	year = {2009},
	keywords = {systems programming, debugging, intrinsics, jikes rvm, magic, mmtk, virtualization, vmmagic},
	pages = {81--90},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HFNEJJ25/Frampton et al. - 2009 - Demystifying Magic High-level Low-level Programmi.pdf:application/pdf}
}

@inproceedings{che_rodinia:_2009,
	title = {Rodinia: {A} benchmark suite for heterogeneous computing},
	shorttitle = {Rodinia},
	doi = {10.1109/IISWC.2009.5306797},
	abstract = {This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley's dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout.},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Che, S. and Boyer, M. and Meng, J. and Tarjan, D. and Sheaffer, J. W. and Lee, S. H. and Skadron, K.},
	month = oct,
	year = {2009},
	keywords = {Application software, Yarn, Benchmark testing, Berkeleys dwarf taxonomy, Central Processing Unit, Computer architecture, data layout, Energy consumption, heterogeneous computing, Kernel, memory-bandwidth limitation, Microprocessors, multicore CPU platform, multicore GPU platform, Multicore processing, parallel communication pattern, Parallel processing, parallel program, parallel programming, power consumption, Rodinia-a benchmark suite, synchronization technique},
	pages = {44--54},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/G6CVUCGG/5306797.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/5VHPSFJX/Che et al. - 2009 - Rodinia A benchmark suite for heterogeneous compu.pdf:application/pdf}
}

@inproceedings{qi_masked_2009,
	address = {New York, NY, USA},
	series = {{POPL} '09},
	title = {Masked {Types} for {Sound} {Object} {Initialization}},
	isbn = {978-1-60558-379-2},
	url = {http://doi.acm.org/10.1145/1480881.1480890},
	doi = {10.1145/1480881.1480890},
	abstract = {This paper presents a type-based solution to the long-standing problem of object initialization. Constructors, the conventional mechanism for object initialization, have semantics that are surprising to programmers and that lead to bugs. They also contribute to the problem of null-pointer exceptions, which make software less reliable. Masked types are a new type-state mechanism that explicitly tracks the initialization state of objects and prevents reading from uninitialized fields. In the resulting language, constructors are ordinary methods that operate on uninitialized objects, and no special default value (null) is needed in the language. Initialization of cyclic data structures is achieved with the use of conditionally masked types. Masked types are modular and compatible with data abstraction. The type system is presented in a simplified object calculus and is proved to soundly prevent reading from uninitialized fields. Masked types have been implemented as an extension to Java, in which compilation simply erases extra type information. Experience using the extended language suggests that masked types work well on real code.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 36th {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Qi, Xin and Myers, Andrew C.},
	year = {2009},
	keywords = {conditional masks, cyclic data structures, data abstraction, invariants, null pointer exceptions},
	pages = {53--65},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GDDXXPWT/Qi and Myers - 2009 - Masked Types for Sound Object Initialization.pdf:application/pdf}
}

@inproceedings{bloom_thorn:_2009,
	address = {New York, NY, USA},
	series = {{OOPSLA} '09},
	title = {Thorn: {Robust}, {Concurrent}, {Extensible} {Scripting} on the {JVM}},
	isbn = {978-1-60558-766-0},
	shorttitle = {Thorn},
	url = {http://doi.acm.org/10.1145/1640089.1640098},
	doi = {10.1145/1640089.1640098},
	abstract = {Scripting languages enjoy great popularity due to their support for rapid and exploratory development. They typically have lightweight syntax, weak data privacy, dynamic typing, powerful aggregate data types, and allow execution of the completed parts of incomplete programs. The price of these features comes later in the software life cycle. Scripts are hard to evolve and compose, and often slow. An additional weakness of most scripting languages is lack of support for concurrency - though concurrency is required for scalability and interacting with remote services. This paper reports on the design and implementation of Thorn, a novel programming language targeting the JVM. Our principal contributions are a careful selection of features that support the evolution of scripts into industrial grade programs - e.g., an expressive module system, an optional type annotation facility for declarations, and support for concurrency based on message passing between lightweight, isolated processes. On the implementation side, Thorn has been designed to accommodate the evolution of the language itself through a compiler plugin mechanism and target the Java virtual machine.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 24th {ACM} {SIGPLAN} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {Bloom, Bard and Field, John and Nystrom, Nathaniel and {\"O}stlund, Johan and Richards, Gregor and Strni{\v s}a, Rok and Vitek, Jan and Wrigstad, Tobias},
	year = {2009},
	keywords = {actors, pattern matching, scripting},
	pages = {117--136},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/G36L8MEC/Bloom et al. - 2009 - Thorn Robust, Concurrent, Extensible Scripting on.pdf:application/pdf}
}

@article{phadtare_scientific_2009,
	title = {Scientific writing: a randomized controlled trial comparing standard and on-line instruction},
	volume = {9},
	issn = {1472-6920},
	shorttitle = {Scientific writing},
	url = {https://doi.org/10.1186/1472-6920-9-27},
	doi = {10.1186/1472-6920-9-27},
	abstract = {Writing plays a central role in the communication of scientific ideas and is therefore a key aspect in researcher education, ultimately determining the success and long-term sustainability of their careers. Despite the growing popularity of e-learning, we are not aware of any existing study comparing on-line vs. traditional classroom-based methods for teaching scientific writing.},
	urldate = {2017-11-11},
	journal = {BMC Medical Education},
	author = {Phadtare, Amruta and Bahmani, Anu and Shah, Anand and Pietrobon, Ricardo},
	month = may,
	year = {2009},
	pages = {27},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/QXBXNY93/Phadtare et al. - 2009 - Scientific writing a randomized controlled trial .pdf:application/pdf}
}

@inproceedings{auerbach_lime:_2010,
	address = {New York, NY, USA},
	series = {{OOPSLA} '10},
	title = {Lime: {A} {Java}-compatible and {Synthesizable} {Language} for {Heterogeneous} {Architectures}},
	isbn = {978-1-4503-0203-6},
	shorttitle = {Lime},
	url = {http://doi.acm.org/10.1145/1869459.1869469},
	doi = {10.1145/1869459.1869469},
	abstract = {The halt in clock frequency scaling has forced architects and language designers to look elsewhere for continued improvements in performance. We believe that extracting maximum performance will require compilation to highly heterogeneous architectures that include reconfigurable hardware. We present a new language, Lime, which is designed to be executable across a broad range of architectures, from FPGAs to conventional CPUs. We present the language as a whole, focusing on its novel features for limiting side-effects and integration of the streaming paradigm into an object- oriented language. We conclude with some initial results demonstrating applications running either on a CPU or co- executing on a CPU and an FPGA.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {Auerbach, Joshua and Bacon, David F. and Cheng, Perry and Rabbah, Rodric},
	year = {2010},
	keywords = {fpga, functional programming, high level synthesis, object oriented, reconfigurable architecture, streaming, value type},
	pages = {89--108},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/QNDPVMS6/Auerbach et al. - 2010 - Lime A Java-compatible and Synthesizable Language.pdf:application/pdf}
}

@inproceedings{posnett_thex:_2010,
	title = {{THEX}: {Mining} metapatterns from java},
	shorttitle = {{THEX}},
	doi = {10.1109/MSR.2010.5463349},
	abstract = {Design patterns are codified solutions to common object-oriented design (OOD) problems in software development. One of the proclaimed benefits of the use of design patterns is that they decouple functionality and enable different parts of a system to change frequently without undue disruption throughout the system. These OOD patterns have received a wealth of attention in the research community since their introduction; however, identifying them in source code is a difficult problem. In contrast, metapatterns have similar effects on software design by enabling portions of the system to be extended or modified easily, but are purely structural in nature, and thus easier to detect. Our long-term goal is to evaluate the effects of different OOD patterns on coordination in software teams as well as outcomes such as developer productivity and software quality. we present THEX, a metapattern detector that scales to large codebases and works on any Java bytecode. We evaluate THEX by examining its performance on codebases with known design patterns (and therefore metapatterns) and find that it performs quite well, with recall of over 90\%.},
	booktitle = {2010 7th {IEEE} {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR} 2010)},
	author = {Posnett, D. and Bird, C. and Devanbu, P.},
	month = may,
	year = {2010},
	keywords = {Java, Object oriented modeling, Computer science, Birds, data mining, design patterns, Detectors, developer productivity, Java bytecode, metapattern detector, metapattern mining, object-oriented design, object-oriented programming, Productivity, Programming, software design, Software design, software development, software development management, Software performance, software quality, Software quality, software team coordination, source code, team working, THEX},
	pages = {122--125},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/NK9GS3HR/5463349.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/F4PA2HXC/Posnett et al. - 2010 - THEX Mining metapatterns from java.pdf:application/pdf}
}

@inproceedings{mitchell_rethinking_2010,
	address = {New York, NY, USA},
	series = {{ICFP} '10},
	title = {Rethinking {Supercompilation}},
	isbn = {978-1-60558-794-3},
	url = {http://doi.acm.org/10.1145/1863543.1863588},
	doi = {10.1145/1863543.1863588},
	abstract = {Supercompilation is a program optimisation technique that is particularly effective at eliminating unnecessary overheads. We have designed a new supercompiler, making many novel choices, including different termination criteria and handling of let bindings. The result is a supercompiler that focuses on simplicity, compiles programs quickly and optimises programs well. We have benchmarked our supercompiler, with some programs running more than twice as fast than when compiled with GHC.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 15th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Mitchell, Neil},
	year = {2010},
	keywords = {supercompilation, haskell, optimisation},
	pages = {309--320},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/4Z6QI4LW/Mitchell - 2010 - Rethinking Supercompilation.pdf:application/pdf}
}

@inproceedings{bolingbroke_supercompilation_2010,
	address = {New York, NY, USA},
	series = {Haskell '10},
	title = {Supercompilation by {Evaluation}},
	isbn = {978-1-4503-0252-4},
	url = {http://doi.acm.org/10.1145/1863523.1863540},
	doi = {10.1145/1863523.1863540},
	abstract = {This paper shows how call-by-need supercompilation can be recast to be based explicitly on an evaluator, contrasting with standard presentations which are specified as algorithms that mix evaluation rules with reductions that are unique to supercompilation. Building on standard operational-semantics technology for call-by-need languages, we show how to extend the supercompilation algorithm to deal with recursive let expressions.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {Third} {ACM} {Haskell} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Bolingbroke, Maximilian and Peyton Jones, Simon},
	year = {2010},
	keywords = {supercompilation, haskell, optimisation, deforestation, specialisation},
	pages = {135--146},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/P88II97P/Bolingbroke and Peyton Jones - 2010 - Supercompilation by Evaluation.pdf:application/pdf}
}

@inproceedings{grechanik_empirical_2010,
	address = {New York, NY, USA},
	series = {{ESEM} '10},
	title = {An {Empirical} {Investigation} into a {Large}-scale {Java} {Open} {Source} {Code} {Repository}},
	isbn = {978-1-4503-0039-1},
	url = {http://doi.acm.org/10.1145/1852786.1852801},
	doi = {10.1145/1852786.1852801},
	abstract = {Getting insight into different aspects of source code artifacts is increasingly important -- yet there is little empirical research using large bodies of source code, and subsequently there are not much statistically significant evidence of common patterns and facts of how programmers write source code. We pose 32 research questions, explain rationale behind them, and obtain facts from 2,080 randomly chosen Java applications from Sourceforge. Among these facts we find that most methods have one or zero arguments or they do not return any values, few methods are overridden, most inheritance hierarchies have the depth of one, close to 50\% of classes are not explicitly inherited from any classes, and the number of methods is strongly correlated with the number of fields in a class.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2010 {ACM}-{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {ACM},
	author = {Grechanik, Mark and McMillan, Collin and DeFerrari, Luca and Comi, Marco and Crespi, Stefano and Poshyvanyk, Denys and Fu, Chen and Xie, Qing and Ghezzi, Carlo},
	year = {2010},
	keywords = {empirical study, large-scale software, mining software repositories, open source, patterns, practice, software repository},
	pages = {11:1--11:10},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NV6YI8WT/Grechanik et al. - 2010 - An Empirical Investigation into a Large-scale Java.pdf:application/pdf}
}

@inproceedings{flanagan_fasttrack:_2009,
	address = {New York, NY, USA},
	series = {{PLDI} '09},
	title = {{FastTrack}: {Efficient} and {Precise} {Dynamic} {Race} {Detection}},
	isbn = {978-1-60558-392-1},
	shorttitle = {{FastTrack}},
	url = {http://doi.acm.org/10.1145/1542476.1542490},
	doi = {10.1145/1542476.1542490},
	abstract = {{\textbackslash}begin\{abstract\} Multithreaded programs are notoriously prone to race conditions. Prior work on dynamic race detectors includes fast but imprecise race detectors that report false alarms, as well as slow but precise race detectors that never report false alarms. The latter typically use expensive vector clock operations that require time linear in the number of program threads. This paper exploits the insight that the full generality of vector clocks is unnecessary in most cases. That is, we can replace heavyweight vector clocks with an adaptive lightweight representation that, for almost all operations of the target program, requires only constant space and supports constant-time operations. This representation change significantly improves time and space performance, with no loss in precision. Experimental results on Java benchmarks including the Eclipse development environment show that our FastTrack race detector is an order of magnitude faster than a traditional vector-clock race detector, and roughly twice as fast as the high-performance DJIT+ algorithm. FastTrack is even comparable in speed to Eraser on our Java benchmarks, while never reporting false alarms.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 30th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Flanagan, Cormac and Freund, Stephen N.},
	year = {2009},
	keywords = {concurrency, dynamic analysis, race conditions},
	pages = {121--133},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/XN94VAUL/Flanagan and Freund - 2009 - FastTrack Efficient and Precise Dynamic Race Dete.pdf:application/pdf}
}

@inproceedings{richards_analysis_2010,
	address = {New York, NY, USA},
	series = {{PLDI} '10},
	title = {An {Analysis} of the {Dynamic} {Behavior} of {JavaScript} {Programs}},
	isbn = {978-1-4503-0019-3},
	url = {http://doi.acm.org/10.1145/1806596.1806598},
	doi = {10.1145/1806596.1806598},
	abstract = {The JavaScript programming language is widely used for web programming and, increasingly, for general purpose computing. As such, improving the correctness, security and performance of JavaScript applications has been the driving force for research in type systems, static analysis and compiler techniques for this language. Many of these techniques aim to reign in some of the most dynamic features of the language, yet little seems to be known about how programmers actually utilize the language or these features. In this paper we perform an empirical study of the dynamic behavior of a corpus of widely-used JavaScript programs, and analyze how and why the dynamic features are used. We report on the degree of dynamism that is exhibited by these JavaScript programs and compare that with assumptions commonly made in the literature and accepted industry benchmark suites.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 31st {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Richards, Gregor and Lebresne, Sylvain and Burg, Brian and Vitek, Jan},
	year = {2010},
	keywords = {program analysis, dynamic metrics, dynamic behavior, execution tracing, javascript},
	pages = {1--12},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/PT4IYUIL/Richards et al. - 2010 - An Analysis of the Dynamic Behavior of JavaScript .pdf:application/pdf}
}

@article{korland_noninvasive_2010,
	title = {Noninvasive concurrency with {Java} {STM}},
	abstract = {In this paper we present a complete Java STM framework, called Deuce, intended as a platform for developing scalable concurrent applications and as a research tool for designing new STM algorithms. It was not clear if one could build an ecient Java STM without compiler support. Deuce provides several benets over existing Java STM frameworks: it avoids any changes or additions to the JVM, it does not require language extensions or intrusive APIs, and it does not impose any memory footprint or GC overhead. To support legacy libraries, Deuce dynamically instruments classes at load time and uses an original eld-based locking strategy to improve concur-rency. Deuce also provides a simple internal API allowing dierent STMs algorithms to be plugged in. We show empirical results that highlight the scalability of our framework running benchmarks with hundreds of concur-rent threads. This paper shows for the rst time that one can actually design a Java STM with reasonable performance without compiler support.},
	author = {Korland, Guy and Shavit, Nir and Felber, Pascal},
	month = jan,
	year = {2010},
	file = {e023f5543ad1b1128f84e463cc00d3e7775a.pdf:/Users/luigi/work/zotero/storage/E7ND7BU7/e023f5543ad1b1128f84e463cc00d3e7775a.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/F8S48DQY/267398972_Noninvasive_concurrency_with_Java_STM.html:text/html}
}

@inproceedings{flanagan_roadrunner_2010,
	address = {New York, NY, USA},
	series = {{PASTE} '10},
	title = {The {RoadRunner} {Dynamic} {Analysis} {Framework} for {Concurrent} {Programs}},
	isbn = {978-1-4503-0082-7},
	url = {http://doi.acm.org/10.1145/1806672.1806674},
	doi = {10.1145/1806672.1806674},
	abstract = {RoadRunner is a dynamic analysis framework designed to facilitate rapid prototyping and experimentation with dynamic analyses for concurrent Java programs. It provides a clean API for communicating an event stream to back-end analyses, where each event describes some operation of interest performed by the target program, such as accessing memory, synchronizing on a lock, forking a new thread, and so on. This API enables the developer to focus on the essential algorithmic issues of the dynamic analysis, rather than on orthogonal infrastructure complexities. Each back-end analysis tool is expressed as a filter over the event stream, allowing easy composition of analyses into tool chains. This tool-chain architecture permits complex analyses to be described and implemented as a sequence of more simple, modular steps, and it facilitates experimentation with different tool compositions. Moreover, the ability to insert various monitoring tools into the tool chain facilitates debugging and performance tuning. Despite RoadRunner's flexibility, careful implementation and optimization choices enable RoadRunner-based analyses to offer comparable performance to traditional, monolithic analysis prototypes, while being up to an order of magnitude smaller in code size. We have used RoadRunner to develop several dozen tools and have successfully applied them to programs as large as the Eclipse programming environment.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 9th {ACM} {SIGPLAN}-{SIGSOFT} {Workshop} on {Program} {Analysis} for {Software} {Tools} and {Engineering}},
	publisher = {ACM},
	author = {Flanagan, Cormac and Freund, Stephen N.},
	year = {2010},
	keywords = {concurrency, dynamic analysis},
	pages = {1--8},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/62KZXRZW/Flanagan and Freund - 2010 - The RoadRunner Dynamic Analysis Framework for Conc.pdf:application/pdf}
}

@article{stone_opencl:_2010,
	title = {{OpenCL}: {A} {Parallel} {Programming} {Standard} for {Heterogeneous} {Computing} {Systems}},
	volume = {12},
	issn = {1521-9615},
	shorttitle = {{OpenCL}},
	doi = {10.1109/MCSE.2010.69},
	abstract = {The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.},
	number = {3},
	journal = {Computing in Science Engineering},
	author = {Stone, J. E. and Gohara, D. and Shi, G.},
	month = may,
	year = {2010},
	keywords = {Runtime, Concurrent computing, Computer architecture, Kernel, Microprocessors, parallel programming, API, application program interfaces, computational devices, computer graphic equipment, Computer interfaces, coprocessors, GPU, Hardware, heterogeneous computing systems, High performance computing, multicore CPU, OpenCL standard, Parallel programming, parallel programming standard, program execution, Software standards},
	pages = {66--73},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/8M6598NZ/5457293.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/PNLEY8EW/Stone et al. - 2010 - OpenCL A Parallel Programming Standard for Hetero.pdf:application/pdf}
}

@inproceedings{tempero_qualitas_2010,
	title = {The {Qualitas} {Corpus}: {A} {Curated} {Collection} of {Java} {Code} for {Empirical} {Studies}},
	shorttitle = {The {Qualitas} {Corpus}},
	doi = {10.1109/APSEC.2010.46},
	abstract = {In order to increase our ability to use measurement to support software development practise we need to do more analysis of code. However, empirical studies of code are expensive and their results are difficult to compare. We describe the Qualitas Corpus, a large curated collection of open source Java systems. The corpus reduces the cost of performing large empirical studies of code and supports comparison of measurements of the same artifacts. We discuss its design, organisation, and issues associated with its development.},
	booktitle = {2010 {Asia} {Pacific} {Software} {Engineering} {Conference}},
	author = {Tempero, E. and Anslow, C. and Dietrich, J. and Han, T. and Li, J. and Lumpe, M. and Melton, H. and Noble, J.},
	month = nov,
	year = {2010},
	keywords = {Java, Libraries, Benchmark testing, software development, codes, curated code corpus, curated collection, Empirical studies, experimental infrastructure, Java code, open source Java systems, Pragmatics, Qualitas Corpus, Software, software engineering, Software engineering},
	pages = {336--345},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/2T9DTH3U/5693210.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/Z3DLRYSK/Tempero et al. - 2010 - The Qualitas Corpus A Curated Collection of Java .pdf:application/pdf}
}

@inproceedings{counsell_is_2010,
	address = {New York, NY, USA},
	series = {{WETSoM} '10},
	title = {Is a {Strategy} for {Code} {Smell} {Assessment} {Long} {Overdue}?},
	isbn = {978-1-60558-976-3},
	url = {http://doi.acm.org/10.1145/1809223.1809228},
	doi = {10.1145/1809223.1809228},
	abstract = {Code smells reflect code decay and, as such, developers should seek to eradicate such smells through application of 'deodorant' in the form of one or more refactorings. However, a dearth of studies exploring code smells either theoretically or empirically suggests that there are reasons why smell eradication is neither being applied in anger, nor the subject of significant research. In this paper, we present three studies as supporting evidence for this claim. The first is an analysis of a set of five, open-source Java systems, the second an empirical study of a sub-system of a proprietary, C\# web-based application and the third, a theoretical enumeration of smell-related refactorings. Key findings of the study were first, that developers seemed to avoid eradicating superficially 'simple' smells in favor of more 'obscure' ones; second, a wide range of conflicts and anomalies soon emerged when trying to identify smelly code. Finally, perceived effort to eradicate a smell may be a key factor. The study highlights the need for a clearer research strategy on the issue of code smells and all aspects of their identification and measurement.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2010 {ICSE} {Workshop} on {Emerging} {Trends} in {Software} {Metrics}},
	publisher = {ACM},
	author = {Counsell, S. and Hierons, R. M. and Hamza, H. and Black, S. and Durrand, M.},
	year = {2010},
	keywords = {Java, C\#, code smells, empirical studies, refactoring},
	pages = {32--38},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/M2SJU7SP/Counsell et al. - 2010 - Is a Strategy for Code Smell Assessment Long Overd.pdf:application/pdf}
}

@inproceedings{jonsson_taming_2011,
	address = {New York, NY, USA},
	series = {{PEPM} '11},
	title = {Taming {Code} {Explosion} in {Supercompilation}},
	isbn = {978-1-4503-0485-6},
	url = {http://doi.acm.org/10.1145/1929501.1929507},
	doi = {10.1145/1929501.1929507},
	abstract = {Supercompilation algorithms can perform great optimizations but sometimes suffer from the problem of code explosion. This results in huge binaries which might hurt the performance on a modern processor. We present a supercompilation algorithm that is fast enough to speculatively supercompile expressions and discard the result if it turned out bad. This allows us to supercompile large parts of the imaginary and spectral parts of nofib in a matter of seconds while keeping the binary size increase below 5\%.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {Workshop} on {Partial} {Evaluation} and {Program} {Manipulation}},
	publisher = {ACM},
	author = {Jonsson, Peter A. and Nordlander, Johan},
	year = {2011},
	keywords = {supercompilation, haskell, deforestation},
	pages = {33--42},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TFWSKD9T/Jonsson and Nordlander - 2011 - Taming Code Explosion in Supercompilation.pdf:application/pdf}
}

@inproceedings{winther_guarded_2011,
	address = {New York, NY, USA},
	series = {{FTfJP} '11},
	title = {Guarded {Type} {Promotion}: {Eliminating} {Redundant} {Casts} in {Java}},
	isbn = {978-1-4503-0893-9},
	shorttitle = {Guarded {Type} {Promotion}},
	url = {http://doi.acm.org/10.1145/2076674.2076680},
	doi = {10.1145/2076674.2076680},
	abstract = {In Java, explicit casts are ubiquitous since they bridge the gap between compile-time and runtime type safety. Since casts potentially throw a ClassCastException, many programmers use a defensive programming style of guarded casts. In this programming style casts are protected by a preceding conditional using the instanceof operator and thus the cast type is redundantly mentioned twice. We propose a new typing rule for Java called Guarded Type Promotion aimed at eliminating the need for the explicit casts when guarded. This new typing rule is backward compatible and has been fully implemented in a Java 6 compiler. Through our extensive testing of real-life code we show that guarded casts account for approximately one fourth of all casts and that Guarded Type Promotion can eliminate the need for 95 percent of these guarded casts.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 13th {Workshop} on {Formal} {Techniques} for {Java}-{Like} {Programs}},
	publisher = {ACM},
	author = {Winther, Johnni},
	year = {2011},
	keywords = {Java, type cast, type checking},
	pages = {6:1--6:8},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/2LVFNFIF/Winther - 2011 - Guarded Type Promotion Eliminating Redundant Cast.pdf:application/pdf}
}

@inproceedings{moret_polymorphic_2011,
	address = {New York, NY, USA},
	series = {{AOSD} '11},
	title = {Polymorphic {Bytecode} {Instrumentation}},
	isbn = {978-1-4503-0605-8},
	url = {http://doi.acm.org/10.1145/1960275.1960292},
	doi = {10.1145/1960275.1960292},
	abstract = {Bytecode instrumentation is a widely used technique to implement aspect weaving and dynamic analyses in virtual machines such as the Java Virtual Machine. Aspect weavers and other instrumentations are usually developed independently and combining them often requires significant engineering effort, if at all possible. In this paper we introduce polymorphic bytecode instrumentation (PBI), a simple but effective technique that allows dynamic dispatch amongst several, possibly independent instrumentations. PBI enables complete bytecode coverage, that is, any method with a bytecode representation can be instrumented. We illustrate further benefits of PBI with three case studies. First, we provide an implementation of execution levels for AspectJ, which avoid infinite regression and unwanted interference between aspects. Second, we present a framework for adaptive dynamic analysis, where the analysis to be performed can be changed at runtime by the user. Third, we describe how PBI can be used to support a form of dynamic mixin layers. We provide thorough performance evaluations with dynamic analysis aspects applied to standard benchmarks. We show that PBI-based execution levels are much faster than control flow pointcuts to avoid interference between aspects, and that their efficient integration in a practical aspect language is possible. We also demonstrate that PBI enables adaptive dynamic analysis tools that are more reactive to user inputs than existing tools that rely on dynamic aspect-oriented programming with runtime weaving.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {Tenth} {International} {Conference} on {Aspect}-oriented {Software} {Development}},
	publisher = {ACM},
	author = {Moret, Philippe and Binder, Walter and Tanter, {\'E}ric},
	year = {2011},
	keywords = {bytecode instrumentation, aspect-oriented programming, dynamic program analysis, java virtual machine, mixin layers, modularity constructs},
	pages = {129--140},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/AYI4M7ZN/Moret et al. - 2011 - Polymorphic Bytecode Instrumentation.pdf:application/pdf}
}

@inproceedings{gligoric_codese:_2011,
	address = {New York, NY, USA},
	series = {{ISSTA} '11},
	title = {{CoDeSe}: {Fast} {Deserialization} via {Code} {Generation}},
	isbn = {978-1-4503-0562-4},
	shorttitle = {{CoDeSe}},
	url = {http://doi.acm.org/10.1145/2001420.2001456},
	doi = {10.1145/2001420.2001456},
	abstract = {Many tools for automated testing, model checking, and debugging store and restore program states multiple times. Storing/restoring a program state is commonly done with serialization/deserialization. Traditionally, the format for stored states is based on data: serialization generates the data that encodes the state, and deserialization interprets this data to restore the state. We propose a new approach, called CoDeSe, where the format for stored states is based on code: serialization generates code whose execution restores the state, and deserialization simply executes the code. We implemented CoDeSe in Java and performed a number of experiments on deserialization of states. CoDeSe provides on average more than 6X speedup over the highly optimized deserialization from the standard Java library. Our new format also allows simple parallel deserialization that can provide additional speedup on top of the sequential CoDeSe but only for larger states.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2011 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Gligoric, Milos and Marinov, Darko and Kamin, Sam},
	year = {2011},
	keywords = {code generation, deserialization},
	pages = {298--308},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TZAETHML/Gligoric et al. - 2011 - CoDeSe Fast Deserialization via Code Generation.pdf:application/pdf}
}

@inproceedings{parnin_java_2011,
	address = {New York, NY, USA},
	series = {{MSR} '11},
	title = {Java {Generics} {Adoption}: {How} {New} {Features} {Are} {Introduced}, {Championed}, or {Ignored}},
	isbn = {978-1-4503-0574-7},
	shorttitle = {Java {Generics} {Adoption}},
	url = {http://doi.acm.org/10.1145/1985441.1985446},
	doi = {10.1145/1985441.1985446},
	abstract = {Support for generic programming was added to the Java language in 2004, representing perhaps the most significant change to one of the most widely used programming languages today. Researchers and language designers anticipated this addition would relieve many long-standing problems plaguing developers, but surprisingly, no one has yet measured whether generics actually provide such relief. In this paper, we report on the first empirical investigation into how Java generics have been integrated into open source software by automatically mining the history of 20 popular open source Java programs, traversing more than 500 million lines of code in the process. We evaluate five hypotheses, each based on assertions made by prior researchers, about how Java developers use generics. For example, our results suggest that generics do not significantly reduce the number of type casts and that generics are usually adopted by a single champion in a project, rather than all committers.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 8th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Parnin, Chris and Bird, Christian and Murphy-Hill, Emerson},
	year = {2011},
	keywords = {generics, java, languages, post-mortem analysis},
	pages = {3--12},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/IPUEDD6G/Parnin et al. - 2011 - Java Generics Adoption How New Features Are Intro.pdf:application/pdf}
}

@inproceedings{richards_eval_2011,
	address = {Berlin, Heidelberg},
	series = {{ECOOP}'11},
	title = {The {Eval} {That} {Men} {Do}: {A} {Large}-scale {Study} of the {Use} of {Eval} in {Javascript} {Applications}},
	isbn = {978-3-642-22654-0},
	shorttitle = {The {Eval} {That} {Men} {Do}},
	url = {http://dl.acm.org/citation.cfm?id=2032497.2032503},
	abstract = {Transforming text into executable code with a function such as Java-Script's eval endows programmers with the ability to extend applications, at any time, and in almost any way they choose. But, this expressive power comes at a price: reasoning about the dynamic behavior of programs that use this feature becomes challenging. Any ahead-of-time analysis, to remain sound, is forced to make pessimistic assumptions about the impact of dynamically created code. This pessimism affects the optimizations that can be applied to programs and significantly limits the kinds of errors that can be caught statically and the security guarantees that can be enforced. A better understanding of how eval is used could lead to increased performance and security. This paper presents a large-scale study of the use of eval in JavaScript-based web applications. We have recorded the behavior of 337 MB of strings given as arguments to 550,358 calls to the eval function exercised in over 10,000 web sites. We provide statistics on the nature and content of strings used in eval expressions, as well as their provenance and data obtained by observing their dynamic behavior.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 25th {European} {Conference} on {Object}-oriented {Programming}},
	publisher = {Springer-Verlag},
	author = {Richards, Gregor and Hammer, Christian and Burg, Brian and Vitek, Jan},
	year = {2011},
	pages = {52--78},
	file = {10.1007%2F978-3-642-22655-7_4.pdf:/Users/luigi/work/zotero/storage/YVZCQDBG/10.1007%2F978-3-642-22655-7_4.pdf:application/pdf}
}

@inproceedings{bacchelli_extracting_2011,
	title = {Extracting structured data from natural language documents with island parsing},
	doi = {10.1109/ASE.2011.6100103},
	abstract = {The design and evolution of a software system leave traces in various kinds of artifacts. In software, produced by humans for humans, many artifacts are written in natural language by people involved in the project. Such entities contain structured information which constitute a valuable source of knowledge for analyzing and comprehending a system's design and evolution. However, the ambiguous and informal nature of narrative is a serious challenge in gathering such information, which is scattered throughout natural language text. We present an approach-based on island parsing-to recognize and enable the parsing of structured information that occur in natural language artifacts. We evaluate our approach by applying it to mailing lists pertaining to three software systems. We show that this approach allows us to extract structured data from emails with high precision and recall.},
	booktitle = {2011 26th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE} 2011)},
	author = {Bacchelli, A. and Cleve, A. and Lanza, M. and Mocci, A.},
	month = nov,
	year = {2011},
	keywords = {Java, Data mining, grammars, History, data mining, Electronic mail, Grammar, island parsing, natural language document, natural language processing, Natural languages, Production, structured information},
	pages = {476--479},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/5BKB8IXM/6100103.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/UZCFXMZC/Bacchelli et al. - 2011 - Extracting structured data from natural language d.pdf:application/pdf}
}

@inproceedings{stuchlik_static_2011,
	address = {New York, NY, USA},
	series = {{DLS} '11},
	title = {Static vs. {Dynamic} {Type} {Systems}: {An} {Empirical} {Study} {About} the {Relationship} {Between} {Type} {Casts} and {Development} {Time}},
	isbn = {978-1-4503-0939-4},
	shorttitle = {Static vs. {Dynamic} {Type} {Systems}},
	url = {http://doi.acm.org/10.1145/2047849.2047861},
	doi = {10.1145/2047849.2047861},
	abstract = {Static type systems are essential in computer science. However, there is hardly any knowledge about the impact of type systems on the resulting piece of software. While there are authors that state that static types increase the development speed, other authors argue the other way around. A previous experiment suggests that there are multiple factors that play a role for a comparison of statically and dynamically typed language. As a follow-up, this paper presents an empirical study with 21 subjects that compares programming tasks performed in Java and Groovy - programming tasks where the number of expected type casts vary in the statically typed language. The result of the study is, that the dynamically typed group solved the complete programming tasks significantly faster for most tasks - but that for larger tasks with a higher number of type casts no significant difference could be found.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 7th {Symposium} on {Dynamic} {Languages}},
	publisher = {ACM},
	author = {Stuchlik, Andreas and Hanenberg, Stefan},
	year = {2011},
	keywords = {software engineering, empirical research, programming language research, type systems},
	pages = {97--106},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/65LVDU64/Stuchlik and Hanenberg - 2011 - Static vs. Dynamic Type Systems An Empirical Stud.pdf:application/pdf}
}

@inproceedings{zheng_turbo_2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Turbo {DiSL}: {Partial} {Evaluation} for {High}-{Level} {Bytecode} {Instrumentation}},
	isbn = {978-3-642-30560-3 978-3-642-30561-0},
	shorttitle = {Turbo {DiSL}},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-30561-0_24},
	doi = {10.1007/978-3-642-30561-0_24},
	abstract = {Bytecode instrumentation is a key technique for the implementation of dynamic program analysis tools such as profilers and debuggers. Traditionally, bytecode instrumentation has been supported by low-level bytecode engineering libraries that are difficult to use. Recently, the domain-specific aspect language DiSL has been proposed to provide high-level abstractions for the rapid development of efficient bytecode instrumentations. While DiSL supports user-defined expressions that are evaluated at weave-time, the DiSL programming model requires these expressions to be implemented in separate classes, thus increasing code size and impairing code readability and maintenance. In addition, the DiSL weaver may produce a significant amount of dead code, which may impair some optimizations performed by the runtime. In this paper we introduce Turbo, a novel partial evaluator for DiSL, which processes the generated instrumentation code, performs constant propagation, conditional reduction, and pattern-based code simplification, and executes pure methods at weave-time. With Turbo, it is often unnecessary to wrap expressions for evaluation at weave-time in separate classes, thus simplifying the programming model. We present Turbo{\textquoteright}s partial evaluation algorithm and illustrate its benefits with several case studies. We evaluate the impact of Turbo on weave-time performance and on runtime performance of the instrumented application.},
	language = {en},
	urldate = {2017-11-11},
	booktitle = {Objects, {Models}, {Components}, {Patterns}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Zheng, Yudi and Ansaloni, Danilo and Marek, Lukas and Sewe, Andreas and Binder, Walter and Villaz{\'o}n, Alex and Tuma, Petr and Qi, Zhengwei and Mezini, Mira},
	month = may,
	year = {2012},
	pages = {353--368},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/AIXMU4U5/Zheng et al. - 2012 - Turbo DiSL Partial Evaluation for High-Level Byte.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/LWD86U3M/978-3-642-30561-0_24.html:text/html}
}

@inproceedings{jensen_remedying_2012,
	address = {New York, NY, USA},
	series = {{ISSTA} 2012},
	title = {Remedying the {Eval} {That} {Men} {Do}},
	isbn = {978-1-4503-1454-1},
	url = {http://doi.acm.org/10.1145/2338965.2336758},
	doi = {10.1145/2338965.2336758},
	abstract = {A range of static analysis tools and techniques have been developed in recent years with the aim of helping JavaScript web application programmers produce code that is more robust, safe, and efficient. However, as shown in a previous large-scale study, many web applications use the JavaScript eval function to dynamically construct code from text strings in ways that obstruct existing static analyses. As a consequence, the analyses either fail to reason about the web applications or produce unsound or useless results.   We present an approach to soundly and automatically transform many common uses of eval into other language constructs to enable sound static analysis of web applications. By eliminating calls to eval, we expand the applicability of static analysis for JavaScript web applications in general.   The transformation we propose works by incorporating a refactoring technique into a dataflow analyzer. We report on our experimental results with a small collection of programming patterns extracted from popular web sites. Although there are inevitably cases where the transformation must give up, our technique succeeds in eliminating many nontrivial occurrences of eval.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2012 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Jensen, Simon Holm and Jonsson, Peter A. and M{\o}ller, Anders},
	year = {2012},
	pages = {34--44},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/H9PRP9GC/Jensen et al. - 2012 - Remedying the Eval That Men Do.pdf:application/pdf}
}

@inproceedings{mayer_empirical_2012,
	address = {New York, NY, USA},
	series = {{OOPSLA} '12},
	title = {An {Empirical} {Study} of the {Influence} of {Static} {Type} {Systems} on the {Usability} of {Undocumented} {Software}},
	isbn = {978-1-4503-1561-6},
	url = {http://doi.acm.org/10.1145/2384616.2384666},
	doi = {10.1145/2384616.2384666},
	abstract = {Abstract Although the study of static and dynamic type systems plays a major role in research, relatively little is known about the impact of type systems on software development. Perhaps one of the more common arguments for static type systems in languages such as Java or C++ is that they require developers to annotate their code with type names, which is thus claimed to improve the documentation of software. In contrast, one common argument against static type systems is that they decrease flexibility, which may make them harder to use. While these arguments are found in the literature, rigorous empirical evidence is lacking. We report on a controlled experiment where 27 subjects performed programming tasks on an undocumented API with a static type system (requiring type annotations) as well as a dynamic type system (which does not). Our results show that for some tasks, programmers had faster completion times using a static type system, while for others, the opposite held. We conduct an exploratory study to try and theorize why.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {Mayer, Clemens and Hanenberg, Stefan and Robbes, Romain and Tanter, {\'E}ric and Stefik, Andreas},
	year = {2012},
	keywords = {empirical research, type systems, programming languages},
	pages = {683--702},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/SPSTV566/Mayer et al. - 2012 - An Empirical Study of the Influence of Static Type.pdf:application/pdf}
}

@inproceedings{dyer_declarative_2013,
	address = {New York, NY, USA},
	series = {{GPCE} '13},
	title = {Declarative {Visitors} to {Ease} {Fine}-grained {Source} {Code} {Mining} with {Full} {History} on {Billions} of {AST} {Nodes}},
	isbn = {978-1-4503-2373-4},
	url = {http://doi.acm.org/10.1145/2517208.2517226},
	doi = {10.1145/2517208.2517226},
	abstract = {Software repositories contain a vast wealth of information about software development. Mining these repositories has proven useful for detecting patterns in software development, testing hypotheses for new software engineering approaches, etc. Specifically, mining source code has yielded significant insights into software development artifacts and processes. Unfortunately, mining source code at a large-scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse-grained, or sacrifice studying the history of the code due to both human and computational scalability issues. In this paper we address the substantial challenges of mining source code: a) at a very large scale; b) at a fine-grained level of detail; and c) with full history information. To address these challenges, we present domain-specific language features for source code mining. Our language features are inspired by object-oriented visitors and provide a default depth-first traversal strategy along with two expressions for defining custom traversals. We provide an implementation of these features in the Boa infrastructure for software repository mining and describe a code generation strategy into Java code. To show the usability of our domain-specific language features, we reproduced over 40 source code mining tasks from two large-scale previous studies in just 2 person-weeks. The resulting code for these tasks show between 2.0x--4.8x reduction in code size. Finally we perform a small controlled experiment to gain insights into how easily mining tasks written using our language features can be understood, with no prior training. We show a substantial number of tasks (77\%) were understood by study participants, in about 3 minutes per task.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Generative} {Programming}: {Concepts} \& {Experiences}},
	publisher = {ACM},
	author = {Dyer, Robert and Rajan, Hridesh and Nguyen, Tien N.},
	year = {2013},
	keywords = {boa, source code mining, visitor pattern},
	pages = {23--32},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/2SY4FFZ5/Dyer et al. - 2013 - Declarative Visitors to Ease Fine-grained Source C.pdf:application/pdf}
}

@inproceedings{gousios_ghtorent_2013,
	address = {Piscataway, NJ, USA},
	series = {{MSR} '13},
	title = {The {GHTorent} {Dataset} and {Tool} {Suite}},
	isbn = {978-1-4673-2936-1},
	url = {http://dl.acm.org/citation.cfm?id=2487085.2487132},
	abstract = {During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 10th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Gousios, Georgios},
	year = {2013},
	pages = {233--236},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/H84M3ICU/Gousios - 2013 - The GHTorent Dataset and Tool Suite.pdf:application/pdf}
}

@inproceedings{mohamedin_bytestm:_2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ByteSTM}: {Virtual} {Machine}-{Level} {Java} {Software} {Transactional} {Memory}},
	isbn = {978-3-642-38492-9 978-3-642-38493-6},
	shorttitle = {{ByteSTM}},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-38493-6_12},
	doi = {10.1007/978-3-642-38493-6_12},
	abstract = {We present ByteSTM, a virtual machine-level Java STM implementation that is built by extending the Jikes RVM. We modify Jikes RVM{\textquoteright}s optimizing compiler to transparently support implicit transactions. Being implemented at the VM-level, it accesses memory directly, avoids Java garbage collection overhead by manually managing memory for transactional metadata, and provides pluggable support for implementing different STM algorithms to the VM. Our experimental studies reveal throughput improvement over other non-VM STMs by 6{\textendash}70\% on micro-benchmarks and by 7{\textendash}60\% on macro-benchmarks.},
	language = {en},
	urldate = {2017-11-11},
	booktitle = {Coordination {Models} and {Languages}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Mohamedin, Mohamed and Ravindran, Binoy and Palmieri, Roberto},
	month = jun,
	year = {2013},
	pages = {166--180},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/BP7RVPQA/Mohamedin et al. - 2013 - ByteSTM Virtual Machine-Level Java Software Trans.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/VRHBRWK9/10.html:text/html}
}

@inproceedings{raemaekers_maven_2013,
	title = {The {Maven} repository dataset of metrics, changes, and dependencies},
	doi = {10.1109/MSR.2013.6624031},
	abstract = {We present the Maven Dependency Dataset (MDD), containing metrics, changes and dependencies of 148,253 jar files. Metrics and changes have been calculated at the level of individual methods, classes and packages of multiple library versions. A complete call graph is also presented which includes call, inheritance, containment and historical relationships between all units of the entire repository. In this paper, we describe our dataset and the methodology used to obtain it. We present different conceptual views of MDD and we also describe limitations and data quality issues that researchers using this data should be aware of.},
	booktitle = {2013 10th {Working} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Raemaekers, S. and Deursen, A. van and Visser, J.},
	month = may,
	year = {2013},
	keywords = {Java, Data mining, Libraries, software libraries, data mining, Software, complete call graph, data quality issues, Dataset, Indexes, jar file changes, jar file dependencies, jar file metrics, library version packages, Maven repository, Maven repository dataset, MDD, Measurement, software metrics, software packages, Supercomputers},
	pages = {221--224},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/G84GZZZ7/6624031.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/6IQXLTHP/Raemaekers et al. - 2013 - The Maven repository dataset of metrics, changes, .pdf:application/pdf}
}

@inproceedings{marek_disl:_2012,
	address = {New York, NY, USA},
	series = {{DSAL} '12},
	title = {{DiSL}: {An} {Extensible} {Language} for {Efficient} and {Comprehensive} {Dynamic} {Program} {Analysis}},
	isbn = {978-1-4503-1128-1},
	shorttitle = {{DiSL}},
	url = {http://doi.acm.org/10.1145/2162037.2162046},
	doi = {10.1145/2162037.2162046},
	abstract = {Dynamic program analysis tools support numerous software engineering tasks, including profiling, debugging, and reverse engineering. Prevailing techniques for building dynamic analysis tools are based on low-level abstractions that make tool development tedious, error-prone, and expensive. To simplify the development of dynamic analysis tools, some researchers promoted the use of aspect-oriented programming (AOP). However, as mainstream AOP languages have not been designed to meet the requirements of dynamic analysis, the success of using AOP in this context remains limited. For example, in AspectJ, join points that are important for dynamic program analysis (e.g., the execution of bytecodes or basic blocks of code) are missing, access to reflective dynamic join{\textasciitilde}point information is expensive, data passing between woven advice in local variables is not supported, and the mixing of low-level bytecode instrumentation and high-level AOP code is not foreseen. In this talk, we present DiSL [1], a new domain-specific aspect language for bytecode instrumentation. DiSL uses Java annotation syntax such that standard Java compilers can be used for compiling DiSL code. The language features an open join point model, novel constructs inspired by weave-time evaluation of conditional join{\textasciitilde}points and by staged execution, and access to custom static and dynamic context information. Moreover, the DiSL weaver guarantees complete bytecode coverage. We have implemented several dynamic analysis tools in DiSL, including profilers for the inter- and intra-procedural control flow, debuggers, dynamic metrics collectors integrated in the Eclipse IDE to augment the static source views with dynamic information, and tools for workload characterization. These tools are concise and perform equally well as implementations using low-level techniques. DiSL has also been conceived as an intermediate language for future domain-specific analysis languages, as well as for AOP languages.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {Seventh} {Workshop} on {Domain}-{Specific} {Aspect} {Languages}},
	publisher = {ACM},
	author = {Marek, Luk{\'a}{\v s} and Zheng, Yudi and Ansaloni, Danilo and Binder, Walter and Qi, Zhengwei and Tuma, Petr},
	year = {2012},
	keywords = {bytecode instrumentation, aspect-oriented programming, JVM, dynamic program analysis},
	pages = {27--28},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/SNUW3RAU/Marek et al. - 2012 - DiSL An Extensible Language for Efficient and Com.pdf:application/pdf}
}

@article{pukall_javadaptor-flexible_2013,
	title = {{JavAdaptor}-{Flexible} {Runtime} {Updates} of {Java} {Applications}},
	volume = {43},
	issn = {0038-0644},
	url = {http://dx.doi.org/10.1002/spe.2107},
	doi = {10.1002/spe.2107},
	abstract = {Software is changed frequently during its life cycle. New requirements come, and bugs must be fixed. To update an application, it usually must be stopped, patched, and restarted. This causes time periods of unavailability, which is always a problem for highly available applications. Even for the development of complex applications, restarts to test new program parts can be time consuming and annoying. Thus, we aim at dynamic software updates to update programs at runtime. There is a large body of research on dynamic software updates, but so far, existing approaches have shortcomings either in terms of flexibility or performance. In addition, some of them depend on specific runtime environments and dictate the program's architecture. We present JavAdaptor, the first runtime update approach based on Java that a offers flexible dynamic software updates, b is platform independent, c introduces only minimal performance overhead, and d does not dictate the program architecture. JavAdaptor combines schema changing class replacements by class renaming and caller updates with Java HotSwap using containers and proxies. It runs on top of all major standard Java virtual machines. We evaluate our approach's applicability and performance in non-trivial case studies and compare it with existing dynamic software update approaches. Copyright {\textcopyright} 2012 John Wiley \& Sons, Ltd.},
	number = {2},
	urldate = {2017-11-11},
	journal = {Softw. Pract. Exper.},
	author = {Pukall, Mario and K{\"a}stner, Christian and Cazzola, Walter and G{\"o}tz, Sebastian and Grebhahn, Alexander and Schr{\"o}ter, Reimar and Saake, Gunter},
	month = feb,
	year = {2013},
	keywords = {dynamic software updates, program evolution, state migration: tool support},
	pages = {153--185},
	file = {Pukall_et_al-2013-Software__Practice_and_Experience.pdf:/Users/luigi/work/zotero/storage/73H4BFZ6/Pukall_et_al-2013-Software__Practice_and_Experience.pdf:application/pdf}
}

@article{callau_how_2013,
	title = {How (and why) developers use the dynamic features of programming languages: the case of smalltalk},
	volume = {18},
	issn = {1382-3256, 1573-7616},
	shorttitle = {How (and why) developers use the dynamic features of programming languages},
	url = {https://link.springer.com/article/10.1007/s10664-012-9203-2},
	doi = {10.1007/s10664-012-9203-2},
	abstract = {The dynamic and reflective features of programming languages are powerful constructs that programmers often mention as extremely useful. However, the ability to modify a program at runtime can be both a boon{\textemdash}in terms of flexibility{\textemdash}, and a curse{\textemdash}in terms of tool support. For instance, usage of these features hampers the design of type systems, the accuracy of static analysis techniques, or the introduction of optimizations by compilers. In this paper, we perform an empirical study of a large Smalltalk codebase{\textemdash}often regarded as the poster-child in terms of availability of these features{\textemdash}, in order to assess how much these features are actually used in practice, whether some are used more than others, and in which kinds of projects. In addition, we performed a qualitative analysis of a representative sample of usages of dynamic features in order to uncover (1) the principal reasons that drive people to use dynamic features, and (2) whether and how these dynamic feature usages can be removed or converted to safer usages. These results are useful to make informed decisions about which features to consider when designing language extensions or tool support.},
	language = {en},
	number = {6},
	urldate = {2017-11-11},
	journal = {Empirical Software Engineering},
	author = {Calla{\'u}, Oscar and Robbes, Romain and Tanter, {\'E}ric and R{\"o}thlisberger, David},
	month = dec,
	year = {2013},
	pages = {1156--1194},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/ZZ2WU8Y3/Calla{\'u} et al. - 2013 - How (and why) developers use the dynamic features .pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/HPIRJ2GK/10.html:text/html}
}

@inproceedings{dyer_boa:_2013,
	title = {Boa: {A} language and infrastructure for analyzing ultra-large-scale software repositories},
	shorttitle = {Boa},
	doi = {10.1109/ICSE.2013.6606588},
	abstract = {In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers.},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Dyer, R. and Nguyen, H. A. and Rajan, H. and Nguyen, T. N.},
	month = may,
	year = {2013},
	keywords = {Java, Protocols, Runtime, Data mining, Libraries, Software, software packages, Alexandria new library, Boa, Boa infrastructure, domain specific language, ease of use, GitHub, Google code, Internet, lower barrier to entry, mining, mining software repository, MSR related hypotheses, repository, reproducible, scalable, software, software centric world, SourceForge, systematic extraction, ultra-large-scale software repositories analysis},
	pages = {422--431},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/DASN4FQY/6606588.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/27Z4QYMP/Dyer et al. - 2013 - Boa A language and infrastructure for analyzing u.pdf:application/pdf}
}

@article{parnin_adoption_2013,
	title = {Adoption and use of {Java} generics},
	volume = {18},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/article/10.1007/s10664-012-9236-6},
	doi = {10.1007/s10664-012-9236-6},
	abstract = {Support for generic programming was added to the Java language in 2004, representing perhaps the most significant change to one of the most widely used programming languages today. Researchers and language designers anticipated this addition would relieve many long-standing problems plaguing developers, but surprisingly, no one has yet measured how generics have been adopted and used in practice. In this paper, we report on the first empirical investigation into how Java generics have been integrated into open source software by automatically mining the history of 40 popular open source Java programs, traversing more than 650 million lines of code in the process. We evaluate five hypotheses and research questions about how Java developers use generics. For example, our results suggest that generics sometimes reduce the number of type casts and that generics are usually adopted by a single champion in a project, rather than all committers. We also offer insights into why some features may be adopted sooner and others features may be held back.},
	language = {en},
	number = {6},
	urldate = {2017-11-11},
	journal = {Empirical Software Engineering},
	author = {Parnin, Chris and Bird, Christian and Murphy-Hill, Emerson},
	month = dec,
	year = {2013},
	pages = {1047--1089},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/3QIDK2PH/Parnin et al. - 2013 - Adoption and use of Java generics.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/J7SWD45J/10.html:text/html}
}

@article{hall_kardashian_2014,
	title = {The {Kardashian} index: a measure of discrepant social media profile for scientists},
	volume = {15},
	issn = {1474-760X},
	shorttitle = {The {Kardashian} index},
	url = {https://doi.org/10.1186/s13059-014-0424-0},
	doi = {10.1186/s13059-014-0424-0},
	abstract = {In the era of social media there are now many different ways that a scientist can build their public profile; the publication of high-quality scientific papers being just one. While social media is a valuable tool for outreach and the sharing of ideas, there is a danger that this form of communication is gaining too high a value and that we are losing sight of key metrics of scientific value, such as citation indices. To help quantify this, I propose the {\textquoteleft}Kardashian Index{\textquoteright}, a measure of discrepancy between a scientist{\textquoteright}s social media profile and publication record based on the direct comparison of numbers of citations and Twitter followers.},
	urldate = {2017-11-11},
	journal = {Genome Biology},
	author = {Hall, Neil},
	month = jul,
	year = {2014},
	pages = {424},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/SWBETZRL/Hall - 2014 - The Kardashian index a measure of discrepant soci.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/S8BEALCR/s13059-014-0424-0.html:text/html}
}

@inproceedings{stefik_what_2014,
	address = {New York, NY, USA},
	series = {{ICPC} 2014},
	title = {What is the {Foundation} of {Evidence} of {Human} {Factors} {Decisions} in {Language} {Design}? {An} {Empirical} {Study} on {Programming} {Language} {Workshops}},
	isbn = {978-1-4503-2879-1},
	shorttitle = {What is the {Foundation} of {Evidence} of {Human} {Factors} {Decisions} in {Language} {Design}?},
	url = {http://doi.acm.org/10.1145/2597008.2597154},
	doi = {10.1145/2597008.2597154},
	abstract = {In recent years, the programming language design community has engaged in rigorous debate on the role of empirical evidence in the design of general purpose programming languages. Some scholars contend that the language community has failed to embrace a form of evidence that is non-controversial in other disciplines (e.g., medicine, biology, psychology, sociology, physics, chemistry), while others argue that a science of language design is unrealistic. While the discussion will likely persist for some time, we begin here a systematic evaluation of the use of empirical evidence with human users, documenting, paper-by-paper, the evidence provided for human factors decisions, beginning with 359 papers from the workshops PPIG, Plateau, and ESP. This preliminary work provides the following contributions: an analysis of the 1) overall quantity and quality of empirical evidence used in the workshops, and of the 2) overall significant challenges to reliably coding academic papers. We hope that, once complete, this long-term research project will serve as a practical catalog designers can use when evaluating the impact of a language feature on human users.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 22Nd {International} {Conference} on {Program} {Comprehension}},
	publisher = {ACM},
	author = {Stefik, Andreas and Hanenberg, Stefan and McKenney, Mark and Andrews, Anneliese and Yellanki, Srinivas Kalyan and Siebert, Susanna},
	year = {2014},
	keywords = {Empirical Evidence, Meta-analysis, The Programming Language Wars},
	pages = {223--231},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/4KUAXMTV/Stefik et al. - 2014 - What is the Foundation of Evidence of Human Factor.pdf:application/pdf}
}

@inproceedings{marek_shadowvm:_2013,
	address = {New York, NY, USA},
	series = {{GPCE} '13},
	title = {{ShadowVM}: {Robust} and {Comprehensive} {Dynamic} {Program} {Analysis} for the {Java} {Platform}},
	isbn = {978-1-4503-2373-4},
	shorttitle = {{ShadowVM}},
	url = {http://doi.acm.org/10.1145/2517208.2517219},
	doi = {10.1145/2517208.2517219},
	abstract = {Dynamic analysis tools are often implemented using instrumentation, particularly on managed runtimes including the Java Virtual Machine (JVM). Performing instrumentation robustly is especially complex on such runtimes: existing frameworks offer limited coverage and poor isolation, while previous work has shown that apparently innocuous instrumentation can cause deadlocks or crashes in the observed application. This paper describes ShadowVM, a system for instrumentation-based dynamic analyses on the JVM which combines a number of techniques to greatly improve both isolation and coverage. These centre on the offload of analysis to a separate process; we believe our design is the first system to enable genuinely full bytecode coverage on the JVM. We describe a working implementation, and use a case study to demonstrate its improved coverage and to evaluate its runtime overhead.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Generative} {Programming}: {Concepts} \& {Experiences}},
	publisher = {ACM},
	author = {Marek, Luk{\'a}{\v s} and Kell, Stephen and Zheng, Yudi and Bulej, Lubom{\'i}r and Binder, Walter and T{\r u}ma, Petr and Ansaloni, Danilo and Sarimbekov, Aibek and Sewe, Andreas},
	year = {2013},
	keywords = {dynamic analysis, instrumentation, jvm},
	pages = {105--114},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/CWFCG7TE/Marek et al. - 2013 - ShadowVM Robust and Comprehensive Dynamic Program.pdf:application/pdf}
}

@inproceedings{petersen_empirical_2014,
	address = {New York, NY, USA},
	series = {{ICPC} 2014},
	title = {An {Empirical} {Comparison} of {Static} and {Dynamic} {Type} {Systems} on {API} {Usage} in the {Presence} of an {IDE}: {Java} vs. {Groovy} with {Eclipse}},
	isbn = {978-1-4503-2879-1},
	shorttitle = {An {Empirical} {Comparison} of {Static} and {Dynamic} {Type} {Systems} on {API} {Usage} in the {Presence} of an {IDE}},
	url = {http://doi.acm.org/10.1145/2597008.2597152},
	doi = {10.1145/2597008.2597152},
	abstract = {Several studies have concluded that static type systems offer an advantage over dynamic type systems for programming tasks involving the discovery of a new API. However, these studies did not take into account modern IDE features; the advanced navigation and code completion techniques available in modern IDEs could drastically alter their conclusions. This study describes an experiment that compares the usage of an unknown API using Java and Groovy using the IDE Eclipse. It turns out that the previous finding that static type systems improve the usability of an unknown API still holds, even in the presence of a modern IDE.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 22Nd {International} {Conference} on {Program} {Comprehension}},
	publisher = {ACM},
	author = {Petersen, Pujan and Hanenberg, Stefan and Robbes, Romain},
	year = {2014},
	keywords = {empirical research, type systems, programming languages},
	pages = {212--222},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/E2NHYDB3/Petersen et al. - 2014 - An Empirical Comparison of Static and Dynamic Type.pdf:application/pdf}
}

@inproceedings{dyer_mining_2014,
	address = {New York, NY, USA},
	series = {{ICSE} 2014},
	title = {Mining {Billions} of {AST} {Nodes} to {Study} {Actual} and {Potential} {Usage} of {Java} {Language} {Features}},
	isbn = {978-1-4503-2756-5},
	url = {http://doi.acm.org/10.1145/2568225.2568295},
	doi = {10.1145/2568225.2568295},
	abstract = {Programming languages evolve over time, adding additional language features to simplify common tasks and make the language easier to use. For example, the Java Language Specification has four editions and is currently drafting a fifth. While the addition of language features is driven by an assumed need by the community (often with direct requests for such features), there is little empirical evidence demonstrating how these new features are adopted by developers once released. In this paper, we analyze over 31k open-source Java projects representing over 9 million Java files, which when parsed contain over 18 billion AST nodes. We analyze this corpus to find uses of new Java language features over time. Our study gives interesting insights, such as: there are millions of places features could potentially be used but weren't; developers convert existing code to use new features; and we found thousands of instances of potential resource handling bugs.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Dyer, Robert and Rajan, Hridesh and Nguyen, Hoan Anh and Nguyen, Tien N.},
	year = {2014},
	keywords = {Java, empirical study, language feature use, software mining},
	pages = {779--790},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/BTC3S8VE/Dyer et al. - 2014 - Mining Billions of AST Nodes to Study Actual and P.pdf:application/pdf}
}

@inproceedings{gorla_checking_2014,
	address = {New York, NY, USA},
	series = {{ICSE} 2014},
	title = {Checking {App} {Behavior} {Against} {App} {Descriptions}},
	isbn = {978-1-4503-2756-5},
	url = {http://doi.acm.org/10.1145/2568225.2568276},
	doi = {10.1145/2568225.2568276},
	abstract = {How do we know a program does what it claims to do? After clustering Android apps by their description topics, we identify outliers in each cluster with respect to their API usage. A "weather" app that sends messages thus becomes an anomaly; likewise, a "messaging" app would typically not be expected to access the current location. Applied on a set of 22,500+ Android applications, our CHABADA prototype identified several anomalies; additionally, it flagged 56\% of novel malware as such, without requiring any known malware patterns.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Gorla, Alessandra and Tavecchia, Ilaria and Gross, Florian and Zeller, Andreas},
	year = {2014},
	keywords = {Android, clustering, description analysis, malware detection},
	pages = {1025--1035},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/N63DCL4D/Gorla et al. - 2014 - Checking App Behavior Against App Descriptions.pdf:application/pdf}
}

@inproceedings{gousios_lean_2014,
	address = {New York, NY, USA},
	series = {{MSR} 2014},
	title = {Lean {GHTorrent}: {GitHub} {Data} on {Demand}},
	isbn = {978-1-4503-2863-0},
	shorttitle = {Lean {GHTorrent}},
	url = {http://doi.acm.org/10.1145/2597073.2597126},
	doi = {10.1145/2597073.2597126},
	abstract = {In recent years, GitHub has become the largest code host in the world, with more than 5M developers collaborating across 10M repositories. Numerous popular open source projects (such as Ruby on Rails, Homebrew, Bootstrap, Django or jQuery) have chosen GitHub as their host and have migrated their code base to it. GitHub offers a tremendous research potential. For instance, it is a flagship for current open source development, a place for developers to showcase their expertise to peers or potential recruiters, and the platform where social coding features or pull requests emerged. However, GitHub data is, to date, largely underexplored. To facilitate studies of GitHub, we have created GHTorrent, a scalable, queriable, offline mirror of the data offered through the GitHub REST API. In this paper we present a novel feature of GHTorrent designed to offer customisable data dumps on demand. The new GHTorrent data-on-demand service offers users the possibility to request via a web form up-to-date GHTorrent data dumps for any collection of GitHub repositories. We hope that by offering customisable GHTorrent data dumps we will not only lower the "barrier for entry" even further for researchers interested in mining GitHub data (thus encourage researchers to intensify their mining efforts), but also enhance the replicability of GitHub studies (since a snapshot of the data on which the results were obtained can now easily accompany each study).},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 11th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Gousios, Georgios and Vasilescu, Bogdan and Serebrenik, Alexander and Zaidman, Andy},
	year = {2014},
	keywords = {GitHub, data on demand, dataset},
	pages = {384--387},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HP67IK6T/Gousios et al. - 2014 - Lean GHTorrent GitHub Data on Demand.pdf:application/pdf}
}

@inproceedings{ponzanelli_stormed:_2015,
	title = {{StORMeD}: {Stack} {Overflow} {Ready} {Made} {Data}},
	shorttitle = {{StORMeD}},
	doi = {10.1109/MSR.2015.67},
	abstract = {Stack Overflow is the de facto Question and Answer (Q\&A) website for developers, and it has been used in many approaches by software engineering researchers to mine useful data. However, the contents of a Stack Overflow discussion are inherently heterogeneous, mixing natural language, source code, stack traces and configuration files in XML or JSON format. We constructed a full island grammar capable of modeling the set of 700,000 Stack Overflow discussions talking about Java, building a heterogeneous abstract syntax tree (H-AST) of each post (question, answer or comment) in a discussion. The resulting dataset models every Stack Overflow discussion, providing a full H-AST for each type of structured fragment (i.e., JSON, XML, Java, Stack traces), and complementing this information with a set of basic meta-information like term frequency to enable natural language analyses. Our dataset allows the end-user to perform combined analyses of the Stack Overflow by visiting the H-AST of a discussion.},
	booktitle = {2015 {IEEE}/{ACM} 12th {Working} {Conference} on {Mining} {Software} {Repositories}},
	author = {Ponzanelli, L. and Mocci, A. and Lanza, M.},
	month = may,
	year = {2015},
	keywords = {Java, Data mining, source code, Software, software engineering, Grammar, island parsing, Natural languages, configuration files, Data models, h-ast, heterogeneous abstract syntax tree, JSON format, natural language, question and answer Website, question answering (information retrieval), software engineering researchers, stack overflow ready made data, stack traces, StORMeD, term frequency, unstructured data, Web sites, XML},
	pages = {474--477},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/ZAFQEIGF/7180121.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/GSDBBEKF/Ponzanelli et al. - 2015 - StORMeD Stack Overflow Ready Made Data.pdf:application/pdf}
}

@inproceedings{sawant_dataset_2015,
	title = {A {Dataset} for {API} {Usage}},
	doi = {10.1109/MSR.2015.75},
	abstract = {An Application Programming Interface (API) provides a specific set of functionalities to a developer. The main aim of an API is to encourage the reuse of already existing functionality. There has been some work done into API popularity trends, API evolution and API usage. For all the aforementioned research avenues there has been a need to mine the usage of an API in order to perform any kind of analysis. Each one of the approaches that has been employed in the past involved a certain degree of inaccuracy as there was no type check that takes place. We introduce an approach that takes type information into account while mining API method invocations and annotation usages. This approach accurately makes a connection between a method invocation and the class of the API to which the method belongs to. We try collecting as many usages of an API as possible, this is achieved by targeting projects hosted on GitHub. Additionally, we look at the history of every project to collect the usage of an API from earliest version onwards. By making such a large and rich dataset public, we hope to stimulate some more research in the field of APIs with the aid of accurate API usage samples.},
	booktitle = {2015 {IEEE}/{ACM} 12th {Working} {Conference} on {Mining} {Software} {Repositories}},
	author = {Sawant, A. A. and Bacchelli, A.},
	month = may,
	year = {2015},
	keywords = {Java, Data mining, Libraries, History, application program interfaces, Software, GitHub, dataset, API evolution, API method annotation usage mining, API method invocationusage mining, API popularity trends, API usage, application programming interface, Databases, functionality reuse, Market research, public dataset, software reusability},
	pages = {506--509},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/VSFRDBNM/7180129.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/8FXAQWVX/Sawant and Bacchelli - 2015 - A Dataset for API Usage.pdf:application/pdf}
}

@article{livshits_defense_2015,
	title = {In {Defense} of {Soundiness}: {A} {Manifesto}},
	volume = {58},
	issn = {0001-0782},
	shorttitle = {In {Defense} of {Soundiness}},
	url = {http://doi.acm.org/10.1145/2644805},
	doi = {10.1145/2644805},
	abstract = {Soundy is the new sound.},
	number = {2},
	urldate = {2017-11-11},
	journal = {Commun. ACM},
	author = {Livshits, Benjamin and Sridharan, Manu and Smaragdakis, Yannis and Lhot{\'a}k, Ond{\v r}ej and Amaral, J. Nelson and Chang, Bor-Yuh Evan and Guyer, Samuel Z. and Khedker, Uday P. and M{\o}ller, Anders and Vardoulakis, Dimitrios},
	month = jan,
	year = {2015},
	pages = {44--46},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/K2DFZ9IK/Livshits et al. - 2015 - In Defense of Soundiness A Manifesto.pdf:application/pdf}
}

@inproceedings{stefanescu_semantics-based_2016,
	address = {New York, NY, USA},
	series = {{OOPSLA} 2016},
	title = {Semantics-based {Program} {Verifiers} for {All} {Languages}},
	isbn = {978-1-4503-4444-9},
	url = {http://doi.acm.org/10.1145/2983990.2984027},
	doi = {10.1145/2983990.2984027},
	abstract = {We present a language-independent verification framework that can be instantiated with an operational semantics to automatically generate a program verifier. The framework treats both the operational semantics and the program correctness specifications as reachability rules between matching logic patterns, and uses the sound and relatively complete reachability logic proof system to prove the specifications using the semantics. We instantiate the framework with the semantics of one academic language, KernelC, as well as with three recent semantics of real-world languages, C, Java, and JavaScript, developed independently of our verification infrastructure. We evaluate our approach empirically and show that the generated program verifiers can check automatically the full functional correctness of challenging heap-manipulating programs implementing operations on list and tree data structures, like AVL trees. This is the first approach that can turn the operational semantics of real-world languages into correct-by-construction automatic verifiers.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} {International} {Conference} on {Object}-{Oriented} {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Stef{\u a}nescu, Andrei and Park, Daejun and Yuwen, Shijiao and Li, Yilong and Ro{\c s}u, Grigore},
	year = {2016},
	keywords = {K framework, matching logic, reachability logic},
	pages = {74--91},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/EHPB7J75/Stef{\u a}nescu et al. - 2016 - Semantics-based Program Verifiers for All Language.pdf:application/pdf}
}

@inproceedings{nakshatri_analysis_2016,
	address = {New York, NY, USA},
	series = {{MSR} '16},
	title = {Analysis of {Exception} {Handling} {Patterns} in {Java} {Projects}: {An} {Empirical} {Study}},
	isbn = {978-1-4503-4186-8},
	shorttitle = {Analysis of {Exception} {Handling} {Patterns} in {Java} {Projects}},
	url = {http://doi.acm.org/10.1145/2901739.2903499},
	doi = {10.1145/2901739.2903499},
	abstract = {Exception handling is a powerful tool provided by many programming languages to help developers deal with unforeseen conditions. Java is one of the few programming languages to enforce an additional compilation check on certain subclasses of the Exception class through checked exceptions. As part of this study, empirical data was extracted from software projects developed in Java. The intent is to explore how developers respond to checked exceptions and identify common patterns used by them to deal with exceptions, checked or otherwise. Bloch's book - "Effective Java" [1] was used as reference for best practices in exception handling - these recommendations were compared against results from the empirical data. Results of this study indicate that most programmers ignore checked exceptions and leave them unnoticed. Additionally, it is observed that classes higher in the exception class hierarchy are more frequently used as compared to specific exception subclasses.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Nakshatri, Suman and Hegde, Maithri and Thandra, Sahithi},
	year = {2016},
	keywords = {Boa, best practices, Github, Java exception handling},
	pages = {500--503},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/5IMZFABG/Nakshatri et al. - 2016 - Analysis of Exception Handling Patterns in Java Pr.pdf:application/pdf}
}

@inproceedings{asaduzzaman_how_2016,
	address = {New York, NY, USA},
	series = {{MSR} '16},
	title = {How {Developers} {Use} {Exception} {Handling} in {Java}?},
	isbn = {978-1-4503-4186-8},
	url = {http://doi.acm.org/10.1145/2901739.2903500},
	doi = {10.1145/2901739.2903500},
	abstract = {Exception handling is a technique that addresses exceptional conditions in applications, allowing the normal flow of execution to continue in the event of an exception and/or to report on such events. Although exception handling techniques, features and bad coding practices have been discussed both in developer communities and in the literature, there is a marked lack of empirical evidence on how developers use exception handling in practice. In this paper we use the Boa language and infrastructure to analyze 274k open source Java projects in GitHub to discover how developers use exception handling. We not only consider various exception handling features but also explore bad coding practices and their relation to the experience of developers. Our results provide some interesting insights. For example, we found that bad exception handling coding practices are common in open source Java projects and regardless of experience all developers use bad exception handling coding practices.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Asaduzzaman, Muhammad and Ahasanuzzaman, Muhammad and Roy, Chanchal K. and Schneider, Kevin A.},
	year = {2016},
	keywords = {Java, source code mining, exception, language feature},
	pages = {516--519},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/JD8WHSL5/Asaduzzaman et al. - 2016 - How Developers Use Exception Handling in Java.pdf:application/pdf}
}

@inproceedings{kery_examining_2016,
	address = {New York, NY, USA},
	series = {{MSR} '16},
	title = {Examining {Programmer} {Practices} for {Locally} {Handling} {Exceptions}},
	isbn = {978-1-4503-4186-8},
	url = {http://doi.acm.org/10.1145/2901739.2903497},
	doi = {10.1145/2901739.2903497},
	abstract = {Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed. A major complaint is that programmers often write minimal and low quality handlers. We used the Boa tool to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions. We found that programmers handle exceptions locally in catch blocks much of the time, rather than propagating by throwing an Exception. Programmers make heavy use of actions like Log, Print, Return, or Throw in catch blocks, and also frequently copy code between handlers. We found bad practices like empty catch blocks or catching Exception are indeed widespread. We discuss evidence that programmers may misjudge risk when catching Exception, and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception. Some of these issues might be addressed by future tools which autocomplete more complete handlers.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Kery, Mary Beth and Le Goues, Claire and Myers, Brad A.},
	year = {2016},
	keywords = {Boa, GitHub, error handlers, Java exceptions},
	pages = {484--487},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/2H8HKBJF/Kery et al. - 2016 - Examining Programmer Practices for Locally Handlin.pdf:application/pdf}
}

@inproceedings{sena_understanding_2016,
	address = {New York, NY, USA},
	series = {{MSR} '16},
	title = {Understanding the {Exception} {Handling} {Strategies} of {Java} {Libraries}: {An} {Empirical} {Study}},
	isbn = {978-1-4503-4186-8},
	shorttitle = {Understanding the {Exception} {Handling} {Strategies} of {Java} {Libraries}},
	url = {http://doi.acm.org/10.1145/2901739.2901757},
	doi = {10.1145/2901739.2901757},
	abstract = {This paper presents an empirical study whose goal was to investigate the exception handling strategies adopted by Java libraries and their potential impact on the client applications. In this study, exception flow analysis was used in combination with manual inspections in order: (i) to characterize the exception handling strategies of existing Java libraries from the perspective of their users; and (ii) to identify exception handling anti-patterns. We extended an existing static analysis tool to reason about exception flows and handler actions of 656 Java libraries selected from 145 categories in the Maven Central Repository. The study findings suggest a current trend of a high number of undocumented API runtime exceptions (i.e., @throws in Javadoc) and Unintended Handler problem. Moreover, we could also identify a considerable number of occurrences of exception handling anti-patterns (e.g. Catch and Ignore). Finally, we have also analyzed 647 bug issues of the 7 most popular libraries and identified that 20.71\% of the reports are defects related to the problems of the exception strategies and anti-patterns identified in our study. The results of this study point to the need of tools to better understand and document the exception handling behavior of libraries.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Sena, Dem{\'o}stenes and Coelho, Roberta and Kulesza, Uir{\'a} and Bonif{\'a}cio, Rodrigo},
	year = {2016},
	keywords = {software libraries, empirical study, exception flows analysis, exception handling, exception handling anti-patterns, static analysis tool},
	pages = {212--222},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HSRBLDIX/Sena et al. - 2016 - Understanding the Exception Handling Strategies of.pdf:application/pdf}
}

@article{lopes_dejavu:_2017,
	title = {D{{\'e}J{\`a}Vu}: {A} {Map} of {Code} {Duplicates} on {GitHub}},
	volume = {1},
	issn = {2475-1421},
	shorttitle = {D{{\'e}J{\`a}Vu}},
	url = {http://doi.acm.org/10.1145/3133908},
	doi = {10.1145/3133908},
	abstract = {Previous studies have shown that there is a non-trivial amount of duplication in source code. This paper analyzes a corpus of 4.5 million non-fork projects hosted on GitHub representing over 428 million files written in Java, C++, Python, and JavaScript. We found that this corpus has a mere 85 million unique files. In other words, 70\% of the code on GitHub consists of clones of previously created files. There is considerable variation between language ecosystems. JavaScript has the highest rate of file duplication, only 6\% of the files are distinct. Java, on the other hand, has the least duplication, 60\% of files are distinct. Lastly, a project-level analysis shows that between 9\% and 31\% of the projects contain at least 80\% of files that can be found elsewhere. These rates of duplication have implications for systems built on open source software as well as for researchers interested in analyzing large code bases. As a concrete artifact of this study, we have created D{\'e}j{\`a}Vu, a publicly available map of code duplicates in GitHub repositories.},
	number = {OOPSLA},
	urldate = {2017-11-11},
	journal = {Proc. ACM Program. Lang.},
	author = {Lopes, Cristina V. and Maj, Petr and Martins, Pedro and Saini, Vaibhav and Yang, Di and Zitny, Jakub and Sajnani, Hitesh and Vitek, Jan},
	month = oct,
	year = {2017},
	keywords = {Clone Detection, Source Code Analysis},
	pages = {84:1--84:28},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/P84LN2ED/Lopes et al. - 2017 - D{\'e}J{\`a}Vu A Map of Code Duplicates on GitHub.pdf:application/pdf}
}

@inproceedings{harlin_impact_2017,
	title = {Impact of {Using} a {Static}-{Type} {System} in {Computer} {Programming}},
	doi = {10.1109/HASE.2017.17},
	abstract = {Static-type systems are a major topic in programming language research and the software industry because they should reduce the development time and increase the code quality. Additionally, they are predicted to decrease the number of defects in a code due to early error detection. However, only a few empirical experiments exist on the potential benefits of static-type systems in programming activities. This paper describes an experiment that tests whether static-type systems help developers create solutions for certain programming tasks. The results indicate that although the existence of a static-type system has no positive impact when subjects code a program from scratch, it does allow more errors in program debugging to be fixed.},
	booktitle = {2017 {IEEE} 18th {International} {Symposium} on {High} {Assurance} {Systems} {Engineering} ({HASE})},
	author = {Harlin, I. R. and Washizaki, H. and Fukazawa, Y.},
	month = jan,
	year = {2017},
	keywords = {Debugging, Computer languages, Programming, empirical study, Software, Encryption, Measurement uncertainty, program debugging, programming language, static-type systems},
	pages = {116--119},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/M5GFH7LE/7911881.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/2VQI6AB3/Harlin et al. - 2017 - Impact of Using a Static-Type System in Computer P.pdf:application/pdf}
}

@inproceedings{dilorenzo_incremental_2016,
	address = {New York, NY, USA},
	series = {{OOPSLA} 2016},
	title = {Incremental {Forest}: {A} {DSL} for {Efficiently} {Managing} {Filestores}},
	isbn = {978-1-4503-4444-9},
	shorttitle = {Incremental {Forest}},
	url = {http://doi.acm.org/10.1145/2983990.2984034},
	doi = {10.1145/2983990.2984034},
	abstract = {File systems are often used to store persistent application data, but manipulating file systems using standard APIs can be difficult for programmers. Forest is a domain-specific language that bridges the gap between the on-disk and in-memory representations of file system data. Given a high-level specification of the structure, contents, and properties of a collection of directories, files, and symbolic links, the Forest compiler generates tools for loading, storing, and validating that data. Unfortunately, the initial implementation of Forest offered few mechanisms for controlling cost{\^a}??e.g., the run-time system could load gigabytes of data, even if only a few bytes were needed. This paper introduces Incremental Forest (iForest), an extension to Forest with an explicit delay construct that programmers can use to precisely control costs. We describe the design of iForest using a series of running examples, present a formal semantics in a core calculus, and define a simple cost model that accurately characterizes the resources needed to use a given specification. We propose skins, which allow programmers to modify the delay structure of a specification in a compositional way, and develop a static type system for ensuring compatibility between specifications and skins. We prove the soundness and completeness of the type system and a variety of algebraic properties of skins. We describe an OCaml implementation and evaluate its performance on applications developed in collaboration with watershed hydrologists.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} {International} {Conference} on {Object}-{Oriented} {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {DiLorenzo, Jonathan and Zhang, Richard and Menzies, Erin and Fisher, Kathleen and Foster, Nate},
	year = {2016},
	keywords = {ad hoc data, Data description languages, domain-specific languages, file systems, filestores, laziness},
	pages = {252--271},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/U2TGPEQE/DiLorenzo et al. - 2016 - Incremental Forest A DSL for Efficiently Managing.pdf:application/pdf}
}

@article{arnold_survey_2005,
	title = {A {Survey} of {Adaptive} {Optimization} in {Virtual} {Machines}},
	volume = {93},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2004.840305},
	abstract = {Virtual machines face significant performance challenges beyond those confronted by traditional static optimizers. First, portable program representations and dynamic language features, such as dynamic class loading, force the deferral of most optimizations until runtime, inducing runtime optimization overhead. Second, modular program representations preclude many forms of whole-program interprocedural optimization. Third, virtual machines incur additional costs for runtime services such as security guarantees and automatic memory management. To address these challenges, vendors have invested considerable resources into adaptive optimization systems in production virtual machines. Today, mainstream virtual machine implementations include substantial infrastructure for online monitoring and profiling, runtime compilation, and feedback-directed optimization. As a result, adaptive optimization has begun to mature as a widespread production-level technology. This paper surveys the evolution and current state of adaptive optimization technology in virtual machines.},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Arnold, M. and Fink, S. J. and Grove, D. and Hind, M. and Sweeney, P. F.},
	month = feb,
	year = {2005},
	keywords = {Runtime, virtual machines, optimisation, Adaptive optimization, adaptive optimization systems, Adaptive systems, automatic memory management, Condition monitoring, Costs, dynamic optimization, feedback directed optimization, feedback-directed optimization (FDO), Memory management, modular program representations, online monitoring, online profiling, optimising compilers, Optimized production technology, production level technology, Production systems, runtime compilation, Security, software performance evaluation, static optimizers, Virtual machine monitors, Virtual machining},
	pages = {449--466},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/K9ZEXRL4/1386662.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/DNPCDW47/Arnold et al. - 2005 - A Survey of Adaptive Optimization in Virtual Machi.pdf:application/pdf}
}

@inproceedings{ancona_semantic_2016,
	address = {New York, NY, USA},
	series = {{OOPSLA} 2016},
	title = {Semantic {Subtyping} for {Imperative} {Object}-oriented {Languages}},
	isbn = {978-1-4503-4444-9},
	url = {http://doi.acm.org/10.1145/2983990.2983992},
	doi = {10.1145/2983990.2983992},
	abstract = {Semantic subtyping is an approach for defining sound and complete procedures to decide subtyping for expressive types, including union and intersection types; although it has been exploited especially in functional languages for XML based programming, recently it has been partially investigated in the context of object-oriented languages, and a sound and complete subtyping algorithm has been proposed for record types, but restricted to immutable fields, with union and recursive types interpreted coinductively to support cyclic objects. In this work we address the problem of studying semantic subtyping for imperative object-oriented languages, where fields can be mutable; in particular, we add read/write field annotations to record types, and, besides union, we consider intersection types as well, while maintaining coinductive interpretation of recursive types. In this way, we get a richer notion of type with a flexible subtyping relation, able to express a variety of type invariants useful for enforcing static guarantees for mutable objects. The addition of these features radically changes the defi- nition of subtyping, and, hence, the corresponding decision procedure, and surprisingly invalidates some subtyping laws that hold in the functional setting. We propose an intuitive model where mutable record val- ues contain type information to specify the values that can be correctly stored in fields. Such a model, and the correspond- ing subtyping rules, require particular care to avoid circularity between coinductive judgments and their negations which, by duality, have to be interpreted inductively. A sound and complete subtyping algorithm is provided, together with a prototype implementation.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} {International} {Conference} on {Object}-{Oriented} {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Ancona, Davide and Corradi, Andrea},
	year = {2016},
	keywords = {Read/Write Field Annotations, Semantic Subtyp- ing, Structural Types for Objects},
	pages = {568--587},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/SS9HNA3A/Ancona and Corradi - 2016 - Semantic Subtyping for Imperative Object-oriented .pdf:application/pdf}
}

@inproceedings{landman_challenges_2017,
	title = {Challenges for {Static} {Analysis} of {Java} {Reflection} - {Literature} {Review} and {Empirical} {Study}},
	doi = {10.1109/ICSE.2017.53},
	abstract = {The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78\% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Landman, D. and Serebrenik, A. and Vinju, J. J.},
	month = may,
	year = {2017},
	keywords = {Java, computational linguistics, program diagnostics, source code, application program interfaces, Software, Grammar, static analysis tool, abstract syntax trees, Bibliographies, code idioms, collected descriptive statistics, collections semantics, dynamic proxies, Empirical Study, Java projects, Java Reflection API, Java systems, literature review, nonexceptional exceptions, programmatic filtering meta objects, public domain software, real-world Java code analysis, Reflection, reflection code analysis, reflective Java code, Semantics, software behavior, software tools, source code (software), Static Analysis, Systematic Literature Review, Systematics, Tools, trees (mathematics)},
	pages = {507--518},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/XSVPXWLC/7985689.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/64TAFNJG/Landman et al. - 2017 - Challenges for Static Analysis of Java Reflection .pdf:application/pdf}
}

@article{ray_large-scale_2017,
	title = {A {Large}-scale {Study} of {Programming} {Languages} and {Code} {Quality} in {GitHub}},
	volume = {60},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/3126905},
	doi = {10.1145/3126905},
	abstract = {What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (728 projects, 63 million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static versus dynamic typing and allowing versus disallowing type confusion on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that disallowing type confusion is modestly better than allowing it, and among functional languages, static typing is also somewhat better than dynamic typing. We also find that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, for example, the preference of certain personality types for functional, static languages that disallow type confusion.},
	number = {10},
	urldate = {2017-11-11},
	journal = {Commun. ACM},
	author = {Ray, Baishakhi and Posnett, Daryl and Devanbu, Premkumar and Filkov, Vladimir},
	month = sep,
	year = {2017},
	pages = {91--100},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/WME4ZMHT/Ray et al. - 2017 - A Large-scale Study of Programming Languages and C.pdf:application/pdf}
}

@inproceedings{nystrom_scala_2017,
	address = {New York, NY, USA},
	series = {{SCALA} 2017},
	title = {A {Scala} {Framework} for {Supercompilation}},
	isbn = {978-1-4503-5529-2},
	url = {http://doi.acm.org/10.1145/3136000.3136011},
	doi = {10.1145/3136000.3136011},
	abstract = {Supercompilation is a program transformation technique that attempts to evaluate programs as much as possible at compile time. Supercompilation has been used for theorem proving, function inversion, and most notably optimization, especially of functional programs. However, the technique has numerous practical problems that prevent it from being applied in mainstream compilers. In this paper, we describe a framework that can be used for experimenting with supercompilation techniques. Our framework allows supercompilers to be constructed directly from an interpreter. The user specifies the interpreter using rewrite rules and the framework handles termination checking, generalization, and residualization. We demonstrate the approach by implementing a supercompiler for JavaScript.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {International} {Symposium} on {Scala}},
	publisher = {ACM},
	author = {Nystrom, Nathaniel},
	year = {2017},
	keywords = {supercompilation, abstract machines, language frameworks, partial evaluation},
	pages = {18--28},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/J4X53MU6/Nystrom - 2017 - A Scala Framework for Supercompilation.pdf:application/pdf}
}

@article{wu_how_2017,
	title = {How {Type} {Errors} {Were} {Fixed} and {What} {Students} {Did}?},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133929},
	doi = {10.1145/3133929},
	abstract = {Providing better supports for debugging type errors has been an active research area in the last three decades. Numerous approaches from different perspectives have been developed. Most approaches work well under certain conditions only, for example, when type errors are caused by single leaves and when type annotations are correct. However, the research community is still unaware of which conditions hold in practice and what the real debugging situations look like. We address this problem with a study of 3 program data sets, which were written in different years, using different compilers, and were of diverse sizes. They include more than 55,000 programs, among which more than 2,700 are ill typed. We investigated all the ill-typed programs, and our results indicate that current error debugging support is far from sufficient in practice since only about 35\% of all type errors were caused by single leaves. In addition, type annotations cannot always be trusted in error debuggers since about 30\% of the time type errors were caused by wrong type annotations. Our study also provides many insights about the debugging behaviors of students in functional programming, which could be exploited for developing more effective error debuggers.},
	number = {OOPSLA},
	urldate = {2017-11-11},
	journal = {Proc. ACM Program. Lang.},
	author = {Wu, Baijun and Chen, Sheng},
	month = oct,
	year = {2017},
	keywords = {empirical study, type inference, Type-error debugging},
	pages = {105:1--105:27},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/RDJHMS2I/Wu and Chen - 2017 - How Type Errors Were Fixed and What Students Did.pdf:application/pdf}
}

@article{dietrich_contracts_2017,
	title = {Contracts in the {Wild}: {A} {Study} of {Java} {Programs} ({Artifact})},
	volume = {3},
	issn = {2509-8195},
	shorttitle = {Contracts in the {Wild}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2017/7287},
	doi = {10.4230/DARTS.3.2.6},
	number = {1},
	urldate = {2017-11-11},
	journal = {Dagstuhl Artifacts Series},
	author = {Dietrich, Jens and Pearce, David J. and Jezek, Kamil and Brada, Premek},
	year = {2017},
	keywords = {verification, java, assertions, design-by-contract, input validation, postconditions, preconditions, runtime checking},
	pages = {6:1--6:4},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/GAC558CJ/Dietrich et al. - 2017 - Contracts in the Wild A Study of Java Programs (A.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/R668MBWW/7287.html:text/html}
}

@inproceedings{brandauer_spencer:_2017,
	title = {Spencer: {Interactive} {Heap} {Analysis} for the {Masses}},
	shorttitle = {Spencer},
	doi = {10.1109/MSR.2017.35},
	abstract = {Programming language-design and run-time-implementation require detailed knowledge about the programs that users want to implement. Acquiring this knowledge is hard, and there is little tool support to effectively estimate whether a proposed tradeoff actually makes sense in the context of real world applications. Ideally, knowledge about behaviour of "typical" programs is 1) easily obtainable, 2) easily reproducible, and 3) easily sharable. We present Spencer, an open source web service and APIframework for dynamic analysis of a continuously growing set of traces of standard program corpora. Users do not obtain traces on their own, but can instead send queries to the web service that will be executed on a set of program traces. Queries are built in terms of a set of query combinators that present a high level interface for working with trace data. Since the framework is high level, and there is a hosted collection of recorded traces, queries are easy to implement. Since the data sets are shared by the research community, results are reproducible. Since the actual queries run on one (or many) servers that provide analysis as a service, obtaining results is possible on commodity hardware. Data in Spencer is meant to be obtained once, and analysed often, making the overhead of data collection mostly irrelevant. This allows Spencer to collect more data than traditional tracing tools can afford within their performance budget. Results in Spencer are cached, making complicated analyses that build on cached primitive queries speedy.},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Brandauer, S. and Wrigstad, T.},
	month = may,
	year = {2017},
	keywords = {Optimization, Computer languages, Performance analysis, program diagnostics, dynamic analysis, public domain software, Tools, APIframework, data analysis, Data visualization, heap analysis, interactive heap analysis, masses, open source Web service, program knowledge, program traces, programming language-design, query combinators, query processing, Resource management, run-time-implementation, Spencer, tracing, Web services},
	pages = {113--123},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/FZDGWGCB/7962361.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/9MELSS2W/Brandauer and Wrigstad - 2017 - Spencer Interactive Heap Analysis for the Masses.pdf:application/pdf}
}

@inproceedings{costa_empirical_2017,
	address = {New York, NY, USA},
	series = {{ICPE} '17},
	title = {Empirical {Study} of {Usage} and {Performance} of {Java} {Collections}},
	isbn = {978-1-4503-4404-3},
	url = {http://doi.acm.org/10.1145/3030207.3030221},
	doi = {10.1145/3030207.3030221},
	abstract = {Collection data structures have a major impact on the performance of applications, especially in languages such as Java, C\#, or C++. This requires a developer to select an appropriate collection from a large set of possibilities, including different abstractions (e.g. list, map, set, queue), and multiple implementations. In Java, the default implementation of collections is provided by the standard Java Collection Framework (JCF). However, there exist a large variety of less known third-party collection libraries which can provide substantial performance benefits with minimal code changes. In this paper, we first study the popularity and usage patterns of collection implementations by mining a code corpus comprised of 10,986 Java projects. We use the results to evaluate and compare the performance of the six most popular alternative collection libraries in a large variety of scenarios. We found that for almost every scenario and JCF collection type there is an alternative implementation that greatly decreases memory consumption while offering comparable or even better execution time. Memory savings range from 60\% to 88\% thanks to reduced overhead and some operations execute 1.5x to 50x faster. We present our results as a comprehensive guideline to help developers in identifying the scenarios in which an alternative implementation can provide a substantial performance improvement. Finally, we discuss how some coding patterns result in substantial performance differences of collections.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 8th {ACM}/{SPEC} on {International} {Conference} on {Performance} {Engineering}},
	publisher = {ACM},
	author = {Costa, Diego and Andrzejak, Artur and Seboek, Janos and Lo, David},
	year = {2017},
	keywords = {empirical study, java, collections, execution time, memory, performance},
	pages = {389--400},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/SIKHR6LM/Costa et al. - 2017 - Empirical Study of Usage and Performance of Java C.pdf:application/pdf}
}

@inproceedings{dietrich_contracts_2017-1,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {Contracts in the {Wild}: {A} {Study} of {Java} {Programs}},
	volume = {74},
	isbn = {978-3-95977-035-4},
	shorttitle = {Contracts in the {Wild}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2017/7259},
	doi = {10.4230/LIPIcs.ECOOP.2017.9},
	urldate = {2017-11-11},
	booktitle = {31st {European} {Conference} on {Object}-{Oriented} {Programming} ({ECOOP} 2017)},
	publisher = {Schloss Dagstuhl{\textendash}Leibniz-Zentrum fuer Informatik},
	author = {Dietrich, Jens and Pearce, David J. and Jezek, Kamil and Brada, Premek},
	editor = {M{\"u}ller, Peter},
	year = {2017},
	keywords = {verification, java, assertions, design-by-contract, input validation, postconditions, preconditions, runtime checking},
	pages = {9:1--9:29},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/MA2KGGVP/Dietrich et al. - 2017 - Contracts in the Wild A Study of Java Programs.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/Y2BVGBRZ/7259.html:text/html}
}

@inproceedings{tiwari_candoia:_2017,
	title = {Candoia: {A} {Platform} for {Building} and {Sharing} {Mining} {Software} {Repositories} {Tools} as {Apps}},
	shorttitle = {Candoia},
	doi = {10.1109/MSR.2017.56},
	abstract = {We propose Candoia, a novel platform and ecosystemfor building and sharing Mining Software Repositories(MSR) tools. Using Candoia, MSR tools are built as apps, and Candoia ecosystem, acting as an appstore, allows effective sharing. Candoia platform provides, data extraction tools for curating custom datasets for user projects, and data abstractions for enabling uniform access to MSR artifacts from disparate sources, which makes apps portable and adoptable across diverse software project settings of MSR researchers and practitioners. The structured design of a Candoia app and the languages selected for building various components of a Candoia app promotes easy customization. To evaluate Candoia we have built over two dozen MSR apps for analyzing bugs, software evolution, project management aspects, and source code and programming practices showing the applicability of the platform for buildinga variety of MSR apps. For testing portability of apps acrossdiverse project settings, we tested the apps using ten popularproject repositories, such as Apache Tomcat, JUnit, Node.js, etc, and found that apps required no changes to be portable. We performed a user study to test customizability and we found that five of eight Candoia users found it very easy to customize an existing app. Candoia is available for download.},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Tiwari, N. M. and Upadhyaya, G. and Nguyen, H. A. and Rajan, H.},
	month = may,
	year = {2017},
	keywords = {Java, Data mining, Buildings, source code, Software, program debugging, software tools, Tools, app customizability, apps sharing, appstore, Candioa ecosystem, Candoia, Candoia app, Candoia exchange, Computer bugs, data abstractions, data extraction tools, Ecosystems, mining software repositories tools, MSR, MSR apps, MSR data, MSR tools, programming practices, project management, software bugs, software evolution, software project settings},
	pages = {53--63},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/WY37L35S/7962355.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/A2BL7GWK/Tiwari et al. - 2017 - Candoia A Platform for Building and Sharing Minin.pdf:application/pdf}
}

@article{wu_learning_2017,
	title = {Learning {User} {Friendly} {Type}-error {Messages}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133930},
	doi = {10.1145/3133930},
	abstract = {Type inference is convenient by allowing programmers to elide type annotations, but this comes at the cost of often generating very confusing and opaque type error messages that are of little help to fix type errors. Though there have been many successful attempts at making type error messages better in the past thirty years, many classes of errors are still difficult to fix. In particular, current approaches still generate imprecise and uninformative error messages for type errors arising from errors in grouping constructs like parentheses and brackets. Worse, a recent study shows that these errors usually take more than 10 steps to fix and occur quite frequently (around 45\% to 60\% of all type errors) in programs written by students learning functional programming. We call this class of errors, nonstructural errors. We solve this problem by developing Learnskell, a type error debugger that uses machine learning to help diagnose and deliver high quality error messages, for programs that contain nonstructural errors. While previous approaches usually report type errors on typing constraints or on the type level, Learnskell generates suggestions on the expression level. We have performed an evaluation on more than 1,500 type errors, and the result shows that Learnskell is quite precise. It can correctly capture 86\% of all nonstructural errors and locate the error cause with a precision of 63\%/87\% with the first 1/3 messages, respectively. This is several times more than the precision of state-of-the-art compilers and debuggers. We have also studied the performance of Learnskell and found out that it scales to large programs.},
	number = {OOPSLA},
	urldate = {2017-11-11},
	journal = {Proc. ACM Program. Lang.},
	author = {Wu, Baijun and Campora III, John Peter and Chen, Sheng},
	month = oct,
	year = {2017},
	keywords = {concrete messages, machine learning, structure- changing errors, Type error debugging},
	pages = {106:1--106:29},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/8IEJVW23/Wu et al. - 2017 - Learning User Friendly Type-error Messages.pdf:application/pdf}
}

@inproceedings{avgustinov_tracking_2015,
	title = {Tracking {Static} {Analysis} {Violations} over {Time} to {Capture} {Developer} {Characteristics}},
	volume = {1},
	doi = {10.1109/ICSE.2015.62},
	abstract = {Many interesting questions about the software quality of a code base can only be answered adequately if fine-grained information about the evolution of quality metrics over time and the contributions of individual developers is known. We present an approach for tracking static analysis violations (which are often indicative of defects) over the revision history of a program, and for precisely attributing the introduction and elimination of these violations to individual developers. As one application, we demonstrate how this information can be used to compute {\textquotedblleft}fingerprints{\textquotedblright} of developers that reflect which kinds of violations they tend to introduce or to fix. We have performed an experimental study on several large open-source projects written in different languages, providing evidence that these fingerprints are well-defined and capture characteristic information about the coding habits of individual developers.},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	author = {Avgustinov, P. and Baars, A. I. and Henriksen, A. S. and Lavender, G. and Menzel, G. and Moor, O. d and Sch{\"a}fer, M. and Tibble, J.},
	month = may,
	year = {2015},
	keywords = {Java, Libraries, History, program diagnostics, software quality, Software quality, public domain software, coding habits, fine-grained information, Open source software, open-source projects, Position measurement, program revision history, quality metrics, static analysis violations tracking},
	pages = {437--447},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/38QVG2IW/7194595.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/7WFT9HDY/Avgustinov et al. - 2015 - Tracking Static Analysis Violations over Time to C.pdf:application/pdf}
}

@article{mazinanian_understanding_2017,
	title = {Understanding the {Use} of {Lambda} {Expressions} in {Java}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133909},
	doi = {10.1145/3133909},
	abstract = {Java 8 retrofitted lambda expressions, a core feature of functional programming, into a mainstream object-oriented language with an imperative paradigm. However, we do not know how Java developers have adapted to the functional style of thinking, and more importantly, what are the reasons motivating Java developers to adopt functional programming. Without such knowledge, researchers miss opportunities to improve the state of the art, tool builders use unrealistic assumptions, language designers fail to improve upon their designs, and developers are unable to explore efficient and effective use of lambdas.   We present the first large-scale, quantitative and qualitative empirical study to shed light on how imperative programmers use lambda expressions as a gateway into functional thinking. Particularly, we statically scrutinize the source code of 241 open-source projects with 19,770 contributors, to study the characteristics of 100,540 lambda expressions. Moreover, we investigate the historical trends and adoption rates of lambdas in the studied projects. To get a complementary perspective, we seek the underlying reasons on why developers introduce lambda expressions, by surveying 97 developers who are introducing lambdas in their projects, using the firehouse interview method.   Among others, our findings revealed an increasing trend in the adoption of lambdas in Java: in 2016, the ratio of lambdas introduced per added line of code increased by 54\% compared to 2015. Lambdas were used for various reasons, including but not limited to (i) making existing code more succinct and readable, (ii) avoiding code duplication, and (iii) simulating lazy evaluation of functions. Interestingly, we found out that developers are using Java's built-in functional interfaces inefficiently, i.e., they prefer to use general functional interfaces over the specialized ones, overlooking the performance overheads that might be imposed. Furthermore, developers are not adopting techniques from functional programming, e.g., currying. Finally, we present the implications of our findings for researchers, tool builders, language designers, and developers.},
	number = {OOPSLA},
	urldate = {2017-11-11},
	journal = {Proc. ACM Program. Lang.},
	author = {Mazinanian, Davood and Ketkar, Ameya and Tsantalis, Nikolaos and Dig, Danny},
	month = oct,
	year = {2017},
	keywords = {Empirical Studies, Functional Programming, Java 8, Lambda Expressions, Multi-paradigm Programming, The Firehouse Interview Method},
	pages = {85:1--85:31},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/Q8CU569X/Mazinanian et al. - 2017 - Understanding the Use of Lambda Expressions in Jav.pdf:application/pdf}
}

@inproceedings{mastrangelo_use_2015,
	address = {New York, NY, USA},
	series = {{OOPSLA} 2015},
	title = {Use at {Your} {Own} {Risk}: {The} {Java} {Unsafe} {API} in the {Wild}},
	isbn = {978-1-4503-3689-5},
	shorttitle = {Use at {Your} {Own} {Risk}},
	url = {http://doi.acm.org/10.1145/2814270.2814313},
	doi = {10.1145/2814270.2814313},
	abstract = {Java is a safe language. Its runtime environment provides strong safety guarantees that any Java application can rely on. Or so we think. We show that the runtime actually does not provide these guarantees---for a large fraction of today's Java code. Unbeknownst to many application developers, the Java runtime includes a "backdoor" that allows expert library and framework developers to circumvent Java's safety guarantees. This backdoor is there by design, and is well known to experts, as it enables them to write high-performance "systems-level" code in Java. For much the same reasons that safe languages are preferred over unsafe languages, these powerful---but unsafe---capabilities in Java should be restricted. They should be made safe by changing the language, the runtime system, or the libraries. At the very least, their use should be restricted. This paper is a step in that direction. We analyzed 74 GB of compiled Java code, spread over 86,479 Java archives, to determine how Java's unsafe capabilities are used in real-world libraries and applications. We found that 25\% of Java bytecode archives depend on unsafe third-party Java code, and thus Java's safety guarantees cannot be trusted. We identify 14 different usage patterns of Java's unsafe capabilities, and we provide supporting evidence for why real-world code needs these capabilities. Our long-term goal is to provide a foundation for the design of new language features to regain safety in Java.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2015 {ACM} {SIGPLAN} {International} {Conference} on {Object}-{Oriented} {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Mastrangelo, Luis and Ponzanelli, Luca and Mocci, Andrea and Lanza, Michele and Hauswirth, Matthias and Nystrom, Nathaniel},
	year = {2015},
	keywords = {Java, patterns, mining, Maven Central, Stack Overflow, unsafe},
	pages = {695--710},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/SKY5CYFY/Mastrangelo et al. - 2015 - Use at Your Own Risk The Java Unsafe API in the W.pdf:application/pdf}
}

@article{shen_empirical_1990,
	title = {An empirical study of {Fortran} programs for parallelizing compilers},
	volume = {1},
	issn = {1045-9219},
	doi = {10.1109/71.80162},
	abstract = {Some results are reported from an empirical study of program characteristics, that are important in parallelizing compiler writers, especially in the area of data dependence analysis and program transformations. The state of the art in data dependence analysis and some parallel execution techniques are examined. The major findings are included. Many subscripts contain symbolic terms with unknown values. A few methods of determining their values at compile time are evaluated. Array references with coupled subscripts appear quite frequently; these subscripts must be handled simultaneously in a dependence test, rather than being handled separately as in current test algorithms. Nonzero coefficients of loop indexes in most subscripts are found to be simple: they are either 1 or -1. This allows an exact real-valued test to be as accurate as an exact integer-valued test for one-dimensional or two-dimensional arrays. Dependencies with uncertain distance are found to be rather common, and one of the main reasons is the frequent appearance of symbolic terms with unknown values},
	number = {3},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Shen, Z. and Li, Z. and Yew, P. C.},
	month = jul,
	year = {1990},
	keywords = {FORTRAN, program compilers, program transformations, array references, Councils, Data analysis, data dependence analysis, Fortran programs, Helium, integer-valued test, NASA, parallelizing compilers, program characteristics, Program processors, Scheduling, Space technology, Statistics, Testing, US Department of Energy},
	pages = {356--364},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/EAI25Q9Z/80162.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/EFFFV5FN/Shen et al. - 1990 - An empirical study of Fortran programs for paralle.pdf:application/pdf}
}

@article{chevance_static_1978,
	title = {Static {Profile} and {Dynamic} {Behavior} of {COBOL} {Programs}},
	volume = {13},
	issn = {0362-1340},
	url = {http://doi.acm.org/10.1145/953411.953414},
	doi = {10.1145/953411.953414},
	abstract = {A measurement system for gathering a static profile and dynamic characteristics of COBOL programs at the source language level is described. Static and dynamic results are presented and discussed.},
	number = {4},
	urldate = {2017-11-11},
	journal = {SIGPLAN Not.},
	author = {Chevance, R. J. and Heidet, T.},
	month = apr,
	year = {1978},
	pages = {44--57},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/YZUTQ85D/Chevance and Heidet - 1978 - Static Profile and Dynamic Behavior of COBOL Progr.pdf:application/pdf}
}

@article{salvadori_static_1975,
	title = {Static {Profile} of {COBOL} {Programs}},
	volume = {10},
	issn = {0362-1340},
	url = {http://doi.acm.org/10.1145/956028.956031},
	doi = {10.1145/956028.956031},
	number = {8},
	urldate = {2017-11-11},
	journal = {SIGPLAN Not.},
	author = {Salvadori, A and Gordon, J. and Capstick, C.},
	month = aug,
	year = {1975},
	pages = {20--33},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/QLJAPQNC/Salvadori et al. - 1975 - Static Profile of COBOL Programs.pdf:application/pdf}
}

@inproceedings{saal_properties_1975,
	address = {New York, NY, USA},
	series = {{APL} '75},
	title = {Some {Properties} of {APL} {Programs}},
	url = {http://doi.acm.org/10.1145/800117.803819},
	doi = {10.1145/800117.803819},
	abstract = {Some results of a study of the static usage of features of the APL language is presented. We compare several characterizations of APL programs with previously measured FORTRAN data, and discuss the significant differences observed. The verity of popular rumors and intuitions about APL programs is also examined. APL users appear to take advantage of the unique matrix features inherent in APL, but in general, use extremely heavily only a small fraction of the available features. The distribution of use agrees well with the so-called {\textquotedblleft}80-20 rule{\textquotedblright}.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of {Seventh} {International} {Conference} on {APL}},
	publisher = {ACM},
	author = {Saal, Harry J. and Weiss, Zvi},
	year = {1975},
	pages = {292--297},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/L2NLKES5/Saal and Weiss - 1975 - Some Properties of APL Programs.pdf:application/pdf}
}

@article{cook_contextual_1982,
	title = {A contextual analysis of {Pascal} programs},
	volume = {12},
	issn = {1097-024X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.4380120209/abstract},
	doi = {10.1002/spe.4380120209},
	abstract = {More than 120,000 lines of Pascal programs, written by graduate students and faculty members, have been statically analysed to provide a better understanding of how the language is {\textquoteleft}really{\textquoteright} used. The analysis was done within twelve distinct contexts to discover differences in usage patterns among the various contexts. For example, it was found that 47 per cent of the operands in arguments lists were constants. The results are displayed as tables of frequency counts which show how often each construct is used within a context. Also, we have compared our findings to the results from studies of other languages, such as FORTRAN, SAL and XPL.},
	language = {en},
	number = {2},
	urldate = {2017-11-11},
	journal = {Software: Practice and Experience},
	author = {Cook, Robert P. and Lee, Insup},
	month = feb,
	year = {1982},
	keywords = {Contextual analysis, Pascal, Static analysis},
	pages = {195--203},
	file = {Snapshot:/Users/luigi/work/zotero/storage/T2FVEF3Y/abstract.html:text/html}
}

@article{saal_empirical_1977,
	title = {An empirical study of {APL} programs},
	volume = {2},
	issn = {0096-0551},
	url = {http://www.sciencedirect.com/science/article/pii/0096055177900078},
	doi = {10.1016/0096-0551(77)90007-8},
	abstract = {The statistical results of a study of the static usage of features of the APL language is presented. The distributions of the appearance of the APL primitive functions and the functions derived from the APL operators are presented. APL users appear to use extremely heavily only a small fraction of the available features. The paper compares several characterizations of APL programs with previously measured FORTRAN data, and discusses the significant differences observed.},
	number = {3},
	urldate = {2017-11-11},
	journal = {Computer Languages},
	author = {Saal, Harry J. and Weiss, Zvi},
	month = jan,
	year = {1977},
	keywords = {80-20 rule, APL, a programming language, Programming style, Static measurements, Usage statistics, Very high level languages},
	pages = {47--59},
	file = {ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/EFI525BJ/0096055177900078.html:text/html}
}

@article{bohm_automatic_1985,
	series = {Third {Conference} on {Foundations} of {Software} {Technology} and {Theoretical} {Computer} {Science}},
	title = {Automatic synthesis of typed $\Lambda$-programs on term algebras},
	volume = {39},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/0304397585901355},
	doi = {10.1016/0304-3975(85)90135-5},
	abstract = {The notion of iteratively defined functions from and to heterogeneous term algebras is introduced as the solution of a finite set of equations of a special shape. Such a notion has remarkable consequences: (1) Choosing the second-order typed lamdda-calculus ($\Lambda$ for short) as a programming language enables one to represent algebra elements and iterative functions by automatic uniform synthesis paradigms, using neither conditional nor recursive constructs. (2) A completeness theorem for $\Lambda$-terms with type of degree at most two and a companion corollary for $\Lambda$-programs have been proved. (3) A new congruence relation for the last-mentioned $\Lambda$-terms which is stronger than $\Lambda$-convertibility is introduced and proved to have the meaning of a $\Lambda$-program equivalence. Moreover, an extension of the paradigms to the synthesis of functions of higher complexity is considered and exemplified. All the concepts are explained and motivated by examples over integers, list- and tree-structures.},
	number = {Supplement C},
	urldate = {2017-11-11},
	journal = {Theoretical Computer Science},
	author = {B{\"o}hm, Corrado and Berarducci, Alessandro},
	month = jan,
	year = {1985},
	pages = {135--154},
	file = {ScienceDirect Full Text PDF:/Users/luigi/work/zotero/storage/HKM388G4/B{\"o}hm and Berarducci - 1985 - Automatic synthesis of typed $\Lambda$-programs on term al.pdf:application/pdf;ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/5DEDNES5/0304397585901355.html:text/html}
}

@article{cook_empirical_1989,
	title = {An empirical analysis of the {Lilith} instruction set},
	volume = {38},
	issn = {0018-9340},
	doi = {10.1109/12.8740},
	abstract = {A static analysis of the instructions used to implement all of the system software on the Lilith computer is described. The results are compared to those of a similar analysis performed on the Mesa instruction set architecture. The data provide a good illustration of how code generation strategies and language usage can affect opcode statistics, even for machines with similar architectures},
	number = {1},
	journal = {IEEE Transactions on Computers},
	author = {Cook, R. P.},
	month = jan,
	year = {1989},
	keywords = {Operating systems, Performance analysis, Computer architecture, Hardware, Statistics, code generation strategies, Computer aided instruction, High level languages, instruction sets, language usage, Lilith instruction set, Lilith software environment, Mesa instruction set architecture, Modula, opcode statistics, Packaging machines, Software packages, static analysis, system software, System software, systems software},
	pages = {156--158},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/SFFJIB9S/8740.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/6R6EB6W8/Cook - 1989 - An empirical analysis of the Lilith instruction se.pdf:application/pdf}
}

@inproceedings{odonoghue_bigram_2002,
	address = {Maynooth, County Kildare, Ireland, Ireland},
	series = {{PPPJ} '02/{IRE} '02},
	title = {Bigram {Analysis} of {Java} {Bytecode} {Sequences}},
	isbn = {978-0-901519-87-0},
	url = {http://dl.acm.org/citation.cfm?id=638476.638513},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the {Inaugural} {Conference} on the {Principles} and {Practice} of {Programming}, 2002 and {Proceedings} of the {Second} {Workshop} on {Intermediate} {Representation} {Engineering} for {Virtual} {Machines}, 2002},
	publisher = {National University of Ireland},
	author = {O'Donoghue, Diarmuid and Leddy, Aine and Power, James and Waldron, John},
	year = {2002},
	pages = {187--192},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/MNCE7CVB/O'Donoghue et al. - 2002 - Bigram Analysis of Java Bytecode Sequences.pdf:application/pdf}
}

@article{kaijanaho_evidence-based_2015,
	title = {Evidence-based programming language design : a philosophical and methodological exploration},
	issn = {1456-5390},
	shorttitle = {Evidence-based programming language design},
	url = {https://jyx.jyu.fi/dspace/handle/123456789/47698},
	abstract = {Background: Programming language design is not usually informed by empirical 
studies. In other fields similar problems have inspired an evidence-based paradigm 
of practice. Such a paradigm is practically inevitable in language design, as well. 
Aims: The content of evidence-based programming design (EB-PLD) is explored, 
as is the concept of evidence in general. Additionally, the extent of evidence 
potentially useful for EB-PLD is mapped, and the appropriateness of Cohen{\textquoteright}s 
kappa for evaluating coder agreement in a secondary study is evaluated. Method: 
Philosophical analysis and explication are used to clarify the unclear. A systematic mapping study was conducted to map out the existing body of evidence. 
Results: Evidence is a report of observations that affects the strength of an argument. There is some but not much evidence. EB-PLD is a five-step process for 
resolving uncertainty about design problems. Cohen{\textquoteright}s kappa is inappropriate for 
coder agreement evaluation in systematic secondary studies. Conclusions: Coder 
agreement evaluation should use Scott{\textquoteright}s pi, Fleiss{\textquoteright} kappa, or Krippendorff{\textquoteright}s alpha. EB-PLD is worthy of further research, although its usefulness was out of 
scope here.},
	language = {eng},
	urldate = {2017-11-11},
	journal = {Jyv{\"a}skyl{\"a} studies in computing 222.},
	author = {Kaijanaho, Antti-Juhani},
	year = {2015},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/767TLGZR/Kaijanaho - 2015 - Evidence-based programming language design  a phi.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/Q6QQ6TJ2/47698.html:text/html}
}

@inproceedings{li_accessing_2016,
	title = {Accessing {Inaccessible} {Android} {APIs}: {An} {Empirical} {Study}},
	shorttitle = {Accessing {Inaccessible} {Android} {APIs}},
	doi = {10.1109/ICSME.2016.35},
	abstract = {As Android becomes a de-facto choice of development platform for mobile apps, developers extensively leverage its accompanying Software Development Kit to quickly build their apps. This SDK comes with a set of APIs which developers may find limited in comparison to what system apps can do or what framework developers are preparing to harness capabilities of new generation devices. Thus, developers may attempt to explore in advance the normally "inaccessible" APIs for building unique API-based functionality in their app. The Android programming model is unique in its kind. Inaccessible APIs, which however are used by developers, constitute yet another specificity of Android development, and is worth investigating to understand what they are, how they evolve over time, and who uses them. To that end, in this work, we empirically investigate 17 important releases of the Android framework source code base, and we find that inaccessible APIs are commonly implemented in the Android framework, which are further neither forward nor backward compatible. Moreover, a small set of inaccessible APIs can eventually become publicly accessible, while most of them are removed during the evolution, resulting in risks for such apps that have leveraged inaccessible APIs. Finally, we show that inaccessible APIs are indeed accessed by third-party apps, and the official Google Play store has tolerated the proliferation of apps leveraging inaccessible API methods.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Li, L. and Bissyand{\'e}, T. F. and Traon, Y. L. and Klein, J.},
	month = oct,
	year = {2016},
	keywords = {Runtime, Libraries, application program interfaces, Software, software engineering, source code (software), Ecosystems, unsafe, Android (operating system), Android programming model, Androids, API-based functionality, application program interface, authorisation, Google, Humanoid robots, inaccessible Android API, mobile app risk, mobile computing, risk management, SDK, smart phones, software development kit, source code base},
	pages = {411--422},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/4C4Q9XKX/7816486.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/QFQSUX6L/Li et al. - 2016 - Accessing Inaccessible Android APIs An Empirical .pdf:application/pdf}
}

@inproceedings{hora_when_2016,
	address = {New York, NY, USA},
	series = {{FSE} 2016},
	title = {When {Should} {Internal} {Interfaces} {Be} {Promoted} to {Public}?},
	isbn = {978-1-4503-4218-6},
	url = {http://doi.acm.org/10.1145/2950290.2950306},
	doi = {10.1145/2950290.2950306},
	abstract = {Commonly, software systems have public (and stable) interfaces, and internal (and possibly unstable) interfaces. Despite being discouraged, client developers often use internal interfaces, which may cause their systems to fail when they evolve. To overcome this problem, API producers may promote internal interfaces to public. In practice, however, API producers have no assistance to identify public interface candidates. In this paper, we study the transition from internal to public interfaces. We aim to help API producers to deliver a better product and API clients to benefit sooner from public interfaces. Our empirical investigation on five widely adopted Java systems present the following observations. First, we identified 195 promotions from 2,722 internal interfaces. Second, we found that promoted internal interfaces have more clients. Third, we predicted internal interface promotion with precision between 50\%-80\%, recall 26\%-82\%, and AUC 74\%-85\%. Finally, by applying our predictor on the last version of the analyzed systems, we automatically detected 382 public interface candidates.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Hora, Andr{\'e} and Valente, Marco Tulio and Robbes, Romain and Anquetil, Nicolas},
	year = {2016},
	keywords = {unsafe, API Usage, Internal Interface Analysis, Software Evolution},
	pages = {278--289},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/5N3TT5YR/Hora et al. - 2016 - When Should Internal Interfaces Be Promoted to Pub.pdf:application/pdf}
}

@inproceedings{saied_cooperative_2016,
	title = {A cooperative approach for combining client-based and library-based {API} usage pattern mining},
	doi = {10.1109/ICPC.2016.7503717},
	abstract = {Software developers need to cope with the complexity of Application Programming Interfaces (APIs) of external libraries or frameworks. Typical APIs provide thousands of methods to their client programs, and these methods are not used independently of each other. Much existing work has provided different techniques to mine API usage patterns based on client programs in order to help developers understanding and using existing libraries. Other techniques propose to overcome the strong constraint of clients' dependency and infer API usage patterns only using the library source code. In this paper, we propose a cooperative usage pattern mining technique (COUPminer) that combines client-based and library-based usage pattern mining. We evaluated our technique through four APIs and the obtained results show that the cooperative approach allows taking advantage at the same time from the precision of client-based technique and from the generalizability of library-based techniques.},
	booktitle = {2016 {IEEE} 24th {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Saied, M. A. and Sahraoui, H.},
	month = may,
	year = {2016},
	keywords = {Java, Libraries, software libraries, data mining, application program interfaces, Semantics, source code (software), unsafe, API Usage, API Documentation, application programming interfaces, client programs, client-based API usage pattern mining, client-based technique, Context, cooperative usage pattern mining technique, COUPminer, Digital signatures, Documentation, library source code, library-based API usage pattern mining, library-based techniques, Software Clustering, software developers, Usage Pattern},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/N2Y6FBVG/7503717.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/JDFHH96C/Saied and Sahraoui - 2016 - A cooperative approach for combining client-based .pdf:application/pdf}
}

@inproceedings{bruno_ng2c:_2017,
	address = {New York, NY, USA},
	series = {{ISMM} 2017},
	title = {{NG}2C: {Pretenuring} {Garbage} {Collection} with {Dynamic} {Generations} for {HotSpot} {Big} {Data} {Applications}},
	isbn = {978-1-4503-5044-0},
	shorttitle = {{NG}2C},
	url = {http://doi.acm.org/10.1145/3092255.3092272},
	doi = {10.1145/3092255.3092272},
	abstract = {Big Data applications suffer from unpredictable and unacceptably high pause times due to Garbage Collection (GC). This is the case in latency-sensitive applications such as on-line credit-card fraud detection, graph-based computing for analysis on social networks, etc. Such pauses compromise latency requirements of the whole application stack and result from applications' aggressive buffering/caching of data, exposing an ill-suited GC design, which assumes that most objects will die young and does not consider that applications hold large amounts of middle-lived data in memory.   To avoid such pauses, we propose NG2C, a new GC algorithm that combines pretenuring with user-defined dynamic generations. By being able to allocate objects into different generations, NG2C is able to group objects with similar lifetime profiles in the same generation. By allocating objects with similar lifetime profiles close to each other, i.e. in the same generation, we avoid object promotion (copying between generations) and heap fragmentation (which leads to heap compactions) both responsible for most of the duration of HotSpot GC pause times.   NG2C is implemented for the OpenJDK 8 HotSpot Java Virtual Machine, as an extension of the Garbage First GC. We evaluate NG2C using Cassandra, Lucene, and GraphChi with three different GCs: Garbage First (G1), Concurrent Mark Sweep (CMS), and NG2C. Results show that NG2C decreases the worst observable GC pause time by up to 94.8\% for Cassandra, 85.0\% for Lucene and 96.45\% for GraphChi, when compared to current collectors (G1 and CMS). In addition, NG2c has no negative impact on application throughput or memory usage.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2017 {ACM} {SIGPLAN} {International} {Symposium} on {Memory} {Management}},
	publisher = {ACM},
	author = {Bruno, Rodrigo and Oliveira, Lu{\'i}s Picciochi and Ferreira, Paulo},
	year = {2017},
	keywords = {unsafe, Big Data, Garbage Collection, Latency},
	pages = {2--13},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/PWDU74BA/Bruno et al. - 2017 - NG2C Pretenuring Garbage Collection with Dynamic .pdf:application/pdf}
}

@inproceedings{holzinger_-depth_2016,
	address = {New York, NY, USA},
	series = {{CCS} '16},
	title = {An {In}-{Depth} {Study} of {More} {Than} {Ten} {Years} of {Java} {Exploitation}},
	isbn = {978-1-4503-4139-4},
	url = {http://doi.acm.org/10.1145/2976749.2978361},
	doi = {10.1145/2976749.2978361},
	abstract = {When created, the Java platform was among the first runtimes designed with security in mind. Yet, numerous Java versions were shown to contain far-reaching vulnerabilities, permitting denial-of-service attacks or even worse allowing intruders to bypass the runtime's sandbox mechanisms, opening the host system up to many kinds of further attacks. This paper presents a systematic in-depth study of 87 publicly available Java exploits found in the wild. By collecting, minimizing and categorizing those exploits, we identify their commonalities and root causes, with the goal of determining the weak spots in the Java security architecture and possible countermeasures. Our findings reveal that the exploits heavily rely on a set of nine weaknesses, including unauthorized use of restricted classes and confused deputies in combination with caller-sensitive methods. We further show that all attack vectors implemented by the exploits belong to one of three categories: single-step attacks, restricted-class attacks, and information hiding attacks. The analysis allows us to propose ideas for improving the security architecture to spawn further research in this area.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Holzinger, Philipp and Triller, Stefan and Bartel, Alexandre and Bodden, Eric},
	year = {2016},
	keywords = {unsafe, access control, exploits, java security, security analysis},
	pages = {779--790},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/VDLLXUBH/Holzinger et al. - 2016 - An In-Depth Study of More Than Ten Years of Java E.pdf:application/pdf}
}

@inproceedings{reboucas_empirical_2016,
	title = {An {Empirical} {Study} on the {Usage} of the {Swift} {Programming} {Language}},
	volume = {1},
	doi = {10.1109/SANER.2016.66},
	abstract = {Recently, Apple released Swift, a modern programming language built to be the successor of Objective-C. In less than a year and a half after its first release, Swift became one of the most popular programming languages in the world, considering different popularity measures. A significant part of this success is due to Apple's strict control over its ecosystem, and the clear message that it will replace Objective-C in a near future. According to Apple, "Swift is a powerful and intuitive programming language[...]. Writing Swift code is interactive and fun, the syntax is concise yet expressive." However, little is known about how Swift developers perceive these benefits. In this paper, we conducted two studies aimed at uncovering the questions and strains that arise from this early adoption. First, we perform a thorough analysis on 59,156 questions asked about Swift on StackOverflow. Second, we interviewed 12 Swift developers to cross-validate the initial results. Our study reveals that developers do seem to find the language easy to understand and adopt, although 17.5\% of the questions are about basic elements of the language. Still, there are many questions about problems in the toolset (compiler, Xcode, libraries). Some of our interviewees reinforced these problems.},
	booktitle = {2016 {IEEE} 23rd {International} {Conference} on {Software} {Analysis}, {Evolution}, and {Reengineering} ({SANER})},
	author = {Rebou{\c c}as, M. and Pinto, G. and Ebert, F. and Torres, W. and Serebrenik, A. and Castor, F.},
	month = mar,
	year = {2016},
	keywords = {computational linguistics, Computer languages, Libraries, Programming, Software, programming languages, unsafe, Testing, compiler, Interviews, libraries, StackOverflow, Standards, Swift developers, Swift programming languages, Xcode},
	pages = {634--638},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/JY8EQ5RC/7476687.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/8WTKH9YQ/Rebou{\c c}as et al. - 2016 - An Empirical Study on the Usage of the Swift Progr.pdf:application/pdf}
}

@inproceedings{zhang_accepting_2016,
	address = {New York, NY, USA},
	series = {{PLDI} '16},
	title = {Accepting {Blame} for {Safe} {Tunneled} {Exceptions}},
	isbn = {978-1-4503-4261-2},
	url = {http://doi.acm.org/10.1145/2908080.2908086},
	doi = {10.1145/2908080.2908086},
	abstract = {Unhandled exceptions crash programs, so a compile-time check that exceptions are handled should in principle make software more reliable. But designers of some recent languages have argued that the benefits of statically checked exceptions are not worth the costs. We introduce a new statically checked exception mechanism that addresses the problems with existing checked-exception mechanisms. In particular, it interacts well with higher-order functions and other design patterns. The key insight is that whether an exception should be treated as a "checked" exception is not a property of its type but rather of the context in which the exception propagates. Statically checked exceptions can "tunnel" through code that is oblivious to their presence, but the type system nevertheless checks that these exceptions are handled. Further, exceptions can be tunneled without being accidentally caught, by expanding the space of exception identifiers to identify the exception-handling context. The resulting mechanism is expressive and syntactically light, and can be implemented efficiently. We demonstrate the expressiveness of the mechanism using significant codebases and evaluate its performance. We have implemented this new exception mechanism as part of the new Genus programming language, but the mechanism could equally well be applied to other programming languages.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 37th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Zhang, Yizhou and Salvaneschi, Guido and Beightol, Quinn and Liskov, Barbara and Myers, Andrew C.},
	year = {2016},
	keywords = {exception handling, unsafe, Exception tunneling, Genus},
	pages = {281--295},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/JQTY67XV/Zhang et al. - 2016 - Accepting Blame for Safe Tunneled Exceptions.pdf:application/pdf}
}

@inproceedings{jiang_unsupervised_2017,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '17},
	title = {An {Unsupervised} {Approach} for {Discovering} {Relevant} {Tutorial} {Fragments} for {APIs}},
	isbn = {978-1-5386-3868-2},
	url = {https://doi.org/10.1109/ICSE.2017.12},
	doi = {10.1109/ICSE.2017.12},
	abstract = {Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely {\textless}u{\textgreater}F{\textless}/u{\textgreater}ragment {\textless}u{\textgreater}R{\textless}/u{\textgreater}ecommender for {\textless}u{\textgreater}A{\textless}/u{\textgreater}PIs with {\textless}u{\textgreater}P{\textless}/u{\textgreater}ageRank and {\textless}u{\textgreater}T{\textless}/u{\textgreater}opic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-the-art approach by 8.77\% and 12.32\% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.},
	urldate = {2017-11-11},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Jiang, He and Zhang, Jingxuan and Ren, Zhilei and Zhang, Tao},
	year = {2017},
	keywords = {application programming interface, unsafe, pagerank algorithm, topic model, unsupervised approaches},
	pages = {38--48},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DK8GYWNG/Jiang et al. - 2017 - An Unsupervised Approach for Discovering Relevant .pdf:application/pdf}
}

@article{staicu_understanding_2017,
	title = {Understanding and {Automatically} {Preventing} {Injection} {Attacks} on {Node}.js},
	url = {https://www.microsoft.com/en-us/research/publication/understanding-automatically-preventing-injection-attacks-node-js/},
	abstract = {The NODE.JS ecosystem has lead to the creation of many modern applications, such as server-side web applications and desktop applications. Unlike client-side JavaScript code, NODE.JS applications can interact freely with the operating system without the benefits of a security sandbox. The complex interplay between NODE.JS modules leads to subtle injection vulnerabilities being introduced across module {\textellipsis}},
	urldate = {2017-11-11},
	journal = {Microsoft Research},
	author = {Staicu, Cristian-Alexandru and Pradel, Michael and Livshits, Ben},
	month = jan,
	year = {2017},
	keywords = {unsafe},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/GLMVIEMT/Staicu et al. - 2017 - Understanding and Automatically Preventing Injecti.pdf:application/pdf;Full Text PDF:/Users/luigi/work/zotero/storage/FKSXN8IE/Staicu et al. - 2017 - Understanding and Automatically Preventing Injecti.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/JFUQJN4L/understanding-automatically-preventing-injection-attacks-node-js.html:text/html;Snapshot:/Users/luigi/work/zotero/storage/2MG3B8CL/understanding-automatically-preventing-injection-attacks-node-js.html:text/html}
}

@article{diaconescu_logical_2002,
	series = {Rewriting {Logic} and its {Applications}},
	title = {Logical foundations of {CafeOBJ}},
	volume = {285},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397501003619},
	doi = {10.1016/S0304-3975(01)00361-9},
	abstract = {This paper surveys the logical and mathematical foundations of CafeOBJ, which is a successor of the famous algebraic specification language OBJ but adds to it several new primitive paradigms such as behavioural concurrent specification and rewriting logic. We first give a concise overview of CafeOBJ. Then we focus on the actual logical foundations of the language at two different levels: basic specification and structured specification, including also the definition of the CafeOBJ institution. We survey some novel or more classical theoretical concepts supporting the logical foundations of CafeOBJ, pointing out the main results but without giving proofs and without discussing all mathematical details. Novel theoretical concepts include the coherent hidden algebra formalism and its combination with rewriting logic, and Grothendieck (or fibred) institutions. However, for proofs and for some of the mathematical details not discussed here we give pointers to relevant publications. The logical foundations of CafeOBJ are structured by the concept of institution. Moreover, the design of CafeOBJ emerged from its logical foundations, and institution concepts played a crucial r{\^o}le in structuring the language design.},
	number = {2},
	urldate = {2017-11-12},
	journal = {Theoretical Computer Science},
	author = {Diaconescu, R{\u a}zvan and Futatsugi, Kokichi},
	month = aug,
	year = {2002},
	keywords = {Algebraic specification, Behavioural specification, Institutions},
	pages = {289--318},
	file = {ScienceDirect Full Text PDF:/Users/luigi/work/zotero/storage/57XXM964/Diaconescu and Futatsugi - 2002 - Logical foundations of CafeOBJ.pdf:application/pdf;ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/DZSU2EG9/S0304397501003619.html:text/html}
}

@article{fruhwirth_theory_1998,
	title = {Theory and practice of constraint handling rules},
	volume = {37},
	issn = {0743-1066},
	url = {http://www.sciencedirect.com/science/article/pii/S0743106698100055},
	doi = {10.1016/S0743-1066(98)10005-5},
	abstract = {Constraint Handling Rules (CHR) are our proposal to allow more flexibility and application-oriented customization of constraint systems. CHR are a declarative language extension especially designed for writing user-defined constraints. CHR are essentially a committed-choice language consisting of multi-headed guarded rules that rewrite constraints into simpler ones until they are solved. In this broad survey we aim at covering all aspects of CHR as they currently present themselves. Going from theory to practice, we will define syntax and semantics for CHR, introduce an important decidable property, confluence, of CHR programs and define a tight integration of CHR with constraint logic programming languages. This survey then describes implementations of the language before we review several constraint solvers {\textendash} both traditional and nonstandard ones {\textendash} written in the CHR language. Finally we introduce two innovative applications that benefited from using CHR.},
	number = {1},
	urldate = {2017-11-12},
	journal = {The Journal of Logic Programming},
	author = {Fr{\"u}hwirth, Thom},
	month = oct,
	year = {1998},
	pages = {95--138},
	file = {ScienceDirect Full Text PDF:/Users/luigi/work/zotero/storage/UCH4KIFM/Fr{\"u}hwirth - 1998 - Theory and practice of constraint handling rules.pdf:application/pdf;ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/U72YJZ6F/S0743106698100055.html:text/html}
}

@book{gluck_roadmap_1996,
	title = {A {Roadmap} to {Metacomputation} by {Supercompilation}},
	abstract = {This paper gives a gentle introduction to Turchin's supercompilation and its applications in metacomputation with an emphasis on recent developments. First, a complete supercompiler, including positive driving and generalization, is defined for a functional language and illustrated with examples. Then a taxonomy of related transformers is given and compared to the supercompiler. Finally, we put supercompilation into the larger perspective of metacomputation and consider three metacomputation tasks: specialization, composition, and inversion.},
	author = {Gl{\"u}ck, Robert and S{\o}rensen, Morten Heine},
	year = {1996},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/G53PFBYS/Gl{\"u}ck and S{\o}rensen - 1996 - A Roadmap to Metacomputation by Supercompilation.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/EPQP6QQF/summary.html:text/html}
}

@inproceedings{coutts_stream_2007,
	address = {New York, NY, USA},
	series = {{ICFP} '07},
	title = {Stream {Fusion}: {From} {Lists} to {Streams} to {Nothing} at {All}},
	isbn = {978-1-59593-815-2},
	shorttitle = {Stream {Fusion}},
	url = {http://doi.acm.org/10.1145/1291151.1291199},
	doi = {10.1145/1291151.1291199},
	abstract = {This paper presents an automatic deforestation system, stream fusion, based on equational transformations, that fuses a wider range of functions than existing short-cut fusion systems. In particular, stream fusion is able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing feature of the framework is its simplicity: by transforming list functions to expose their structure, intermediate values are eliminated by general purpose compiler optimisations. We have reimplemented the Haskell standard List library on top of our framework, providing stream fusion for Haskell lists. By allowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion in typical Haskell programs. We present benchmarks documenting time and space improvements.},
	urldate = {2017-11-12},
	booktitle = {Proceedings of the 12th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Coutts, Duncan and Leshchinskiy, Roman and Stewart, Don},
	year = {2007},
	keywords = {program transformation, functional programming, deforestation, program fusion, program optimisation},
	pages = {315--326},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NRDHG6ZW/Coutts et al. - 2007 - Stream Fusion From Lists to Streams to Nothing at.pdf:application/pdf}
}

@inproceedings{altenkirch_why_2005,
	title = {Why {Dependent} {Types} {Matter}},
	abstract = {We exhibit the rationale behind the design of Epigram, a dependently typed programming language and interactive program development system, using refinements of a well known program{\textemdash}merge sort{\textemdash}as a running example. We discuss its relationship with other proposals to introduce aspects of dependent types into functional programming languages and sketch some topics for further work in this area. 1.},
	booktitle = {In preparation, http://www.e-pig.org/downloads/ydtm.pdf},
	author = {Altenkirch, Thorsten and Mcbride, Conor and Mckinna, James},
	year = {2005},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/F7Z66P8B/Altenkirch et al. - 2005 - Why dependent types matter.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/Y4KDBFCK/summary.html:text/html}
}

@inproceedings{yang_survey_2006,
	address = {New York, NY, USA},
	series = {{AST} '06},
	title = {A {Survey} of {Coverage} {Based} {Testing} {Tools}},
	isbn = {978-1-59593-408-6},
	url = {http://doi.acm.org/10.1145/1138929.1138949},
	doi = {10.1145/1138929.1138949},
	abstract = {Test coverage is sometimes used as a way to measure how thoroughly software is tested. Coverage is used by software developers and sometimes by vendors to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools focusing on, but not restricted to coverage measurement. We also survey additional features, including program prioritization for testing, assistance in debugging, automatic generation of test cases, and customization of test reports. Such features make tools more useful and practical, especially for large-scale, real-life commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage1 (a tool suite that includes code coverage testing, debugging, performance profiling, and reporting). Our study shows that each tool has its unique features tailored to its application domains. Therefore this study can be used to pick the right coverage testing tools depending on various requirements.},
	urldate = {2017-11-12},
	booktitle = {Proceedings of the 2006 {International} {Workshop} on {Automation} of {Software} {Test}},
	publisher = {ACM},
	author = {Yang, Qian and Li, J. Jenny and Weiss, David},
	year = {2006},
	keywords = {automate test case generation, code coverage, coverage-based testing tool, dominator analysis, eXVantage, prioritization},
	pages = {99--103},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NQXDWMNJ/Yang et al. - 2006 - A Survey of Coverage Based Testing Tools.pdf:application/pdf}
}

@inproceedings{madsen_string_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {String {Analysis} for {Dynamic} {Field} {Access}},
	isbn = {978-3-642-54806-2 978-3-642-54807-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-54807-9_12},
	doi = {10.1007/978-3-642-54807-9_12},
	abstract = {In JavaScript, and scripting languages in general, dynamic field access is a commonly used feature. Unfortunately, current static analysis tools either completely ignore dynamic field access or use overly conservative approximations that lead to poor precision and scalability.We present new string domains to reason about dynamic field access in a static analysis tool. A key feature of the domains is that the equal, concatenate and join operations take ?O{\textbackslash}mathcal\{O\}(1) time.Experimental evaluation on four common JavaScript libraries, including jQuery and Prototype, shows that traditional string domains are insufficient. For instance, the commonly used constant string domain can only ensure that at most 21\% dynamic field accesses are without false positives. In contrast, our string domain ?H{\textbackslash}mathcal\{H\} ensures no false positives for up to 90\% of all dynamic field accesses.We demonstrate that a dataflow analysis equipped with the ?H{\textbackslash}mathcal\{H\} domain gains significant precision resulting in an analysis speedup of more than 1.5x for 7 out of 10 benchmark programs.},
	language = {en},
	urldate = {2017-11-12},
	booktitle = {Compiler {Construction}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Madsen, Magnus and Andreasen, Esben},
	month = apr,
	year = {2014},
	pages = {197--217},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/I28CW7EC/Madsen and Andreasen - 2014 - String Analysis for Dynamic Field Access.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/LT2L58YF/978-3-642-54807-9_12.html:text/html}
}

@inproceedings{blackburn_dacapo_2006,
	address = {New York, NY, USA},
	series = {{OOPSLA} '06},
	title = {The {DaCapo} {Benchmarks}: {Java} {Benchmarking} {Development} and {Analysis}},
	isbn = {978-1-59593-348-5},
	shorttitle = {The {DaCapo} {Benchmarks}},
	url = {http://doi.acm.org/10.1145/1167473.1167488},
	doi = {10.1145/1167473.1167488},
	abstract = {Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.},
	urldate = {2017-11-12},
	booktitle = {Proceedings of the 21st {Annual} {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programming} {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Blackburn, Stephen M. and Garner, Robin and Hoffmann, Chris and Khang, Asjad M. and McKinley, Kathryn S. and Bentzur, Rotem and Diwan, Amer and Feinberg, Daniel and Frampton, Daniel and Guyer, Samuel Z. and Hirzel, Martin and Hosking, Antony and Jump, Maria and Lee, Han and Moss, J. Eliot B. and Phansalkar, Aashish and Stefanovi{\'c}, Darko and VanDrunen, Thomas and von Dincklage, Daniel and Wiedermann, Ben},
	year = {2006},
	keywords = {Java, benchmark, DaCapo, methodology, SPEC},
	pages = {169--190},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GCUIDM8X/Blackburn et al. - 2006 - The DaCapo Benchmarks Java Benchmarking Developme.pdf:application/pdf}
}

@article{brady_hints_1977,
	title = {Hints on proofs by recursion induction},
	volume = {20},
	issn = {0010-4620},
	url = {https://academic.oup.com/comjnl/article/20/4/353/393896},
	doi = {10.1093/comjnl/20.4.353},
	abstract = {In 1963 John McCarthy proposed a formalism based on conditional expression and recursion for use in the emergent theory of computation. Included in his proposals was a proof technique, known as recursive induction, which could be used to establish the equivalence of recursively defined functions. This paper shows that the discovery of an equations to serve in a proof by recursive induction does not have to rely on luck or inspiration, but can be developed rationally hand in hand with the development of the proof.},
	number = {4},
	urldate = {2017-11-12},
	journal = {The Computer Journal},
	author = {Brady, J. M.},
	month = jan,
	year = {1977},
	pages = {353--355},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/CZ7CGRRC/Brady - 1977 - Hints on proofs by recursion induction.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/7KNADDWH/Hints-on-proofs-by-recursion-induction.html:text/html}
}

@inproceedings{jhala_structural_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Structural {Invariants}},
	isbn = {978-3-540-37756-6 978-3-540-37758-0},
	url = {https://link.springer.com/chapter/10.1007/11823230_6},
	doi = {10.1007/11823230_6},
	abstract = {We present structural invariants (SI), a new technique for incrementally overapproximating the verification condition of a program in static single assignment form by making a linear pass over the dominator tree of the program. The 1-level SI at a program location is the conjunction of all dominating program statements viewed as constraints. For any k, we define a k-level SI by recursively strengthening the dominating join points of the 1-level SI with the (k {\textendash} 1)-level SI of the predecessors of the join point, thereby providing a tunable selector to add path-sensitivity incrementally. By ignoring program paths, the size of the SI and correspondingly the time to discharge the validity query remains small, allowing the technique to scale to large programs. We show experimentally that even with k <=2, for a set of open-source programs totaling 570K lines and properties for which specialized analyses have been previously devised, our method provides an automatic and scalable algorithm with a low false positive rate.},
	language = {en},
	urldate = {2017-11-12},
	booktitle = {Static {Analysis}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Jhala, Ranjit and Majumdar, Rupak and Xu, Ru-Gang},
	month = aug,
	year = {2006},
	pages = {71--87},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/HZ9JIP9B/Jhala et al. - 2006 - Structural Invariants.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/IQKXKB4E/11823230_6.html:text/html}
}

@article{lamport_state_1978,
	title = {State the {Problem} {Before} {Describing} the {Solution}},
	volume = {3},
	issn = {0163-5948},
	url = {http://doi.acm.org/10.1145/1010734.1010737},
	doi = {10.1145/1010734.1010737},
	number = {1},
	urldate = {2017-11-12},
	journal = {SIGSOFT Softw. Eng. Notes},
	author = {Lamport, Leslie},
	month = jan,
	year = {1978},
	pages = {26--26},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/5D2JKFWN/Lamport - 1978 - State the Problem Before Describing the Solution.pdf:application/pdf}
}

@inproceedings{weiser_program_1981,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '81},
	title = {Program {Slicing}},
	isbn = {978-0-89791-146-7},
	url = {http://dl.acm.org/citation.cfm?id=800078.802557},
	abstract = {Program slicing is a method used by experienced computer programmers for abstracting from programs. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a {\textquotedblleft}slice{\textquotedblright}, is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior. Finding a slice is in general unsolvable. A dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. Experimental evidence is presented that these slices are used by programmers during debugging. Experience with two automatic slicing tools is summarized. New measures of program complexity are suggested based on the organization of a program's slices.},
	urldate = {2017-11-12},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Weiser, Mark},
	year = {1981},
	keywords = {Debugging, Data flow analysis, Human factors, Program maintenance, Program metrics, Software tools},
	pages = {439--449},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/YY44SUIU/Weiser - 1981 - Program Slicing.pdf:application/pdf}
}

@article{tip_survey_1995,
	title = {A {Survey} of {Program} {Slicing} {Techniques}},
	volume = {3},
	abstract = {A program slice consists of the parts of a program that (potentially) affect the  values computed at some point of interest, referred to as a slicing criterion. The task  of computing program slices is called program slicing. The original definition of a  program slice was presented by Weiser in 1979. Since then, various slightly different  notions of program slices have been proposed, as well as a number of methods to  compute them. An important distinction is that between a static and a dynamic slice.  The former notion is computed without making assumptions regarding a program's  input, whereas the latter relies on some specific test case.  Procedures, arbitrary control flow, composite datatypes and pointers, and interprocess  communication each require a specific solution. We classify static and dynamic  slicing methods for each of these features, and compare their accuracy and efficiency.  Moreover, the possibilities for combining solutions for different features are investigated....},
	journal = {Journal of Programming Languages},
	author = {Tip, F.},
	year = {1995},
	pages = {121--189},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/D9S2386I/Tip - 1995 - A Survey of Program Slicing Techniques.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/E8HSKQXM/summary.html:text/html}
}

@inproceedings{hu_dynamic_2008,
	title = {Dynamic {Analysis} and {Design} {Pattern} {Detection} in {Java} {Programs}.},
	abstract = {Identifying design patterns within an existing software system can support understandability and reuse of the system's core functionality. In this context, incorporating behavioral features into the design pattern recovery would enhance the scalability of the process. The main advantage of the new approach in this paper over the existing approaches is incorporating dynamic analysis and feature localization in source code. This allows us to perform a goal-driven design pattern detection and focus ourselves on patterns that implement specific software functionality, as opposed to conducting a general pattern detection which is susceptible to high complexity problem. Using a new pattern description language and a matching process we identify the instances of these patterns within the obtained classes and interactions. We use a two-phase matching process: i) an approximate matching of class attributes generates a list of candidate patterns; and ii) a structural matching of classes identifies exact matched patterns. One target application domain can be software product line which emphasizes on reusing core software artifacts to construct a reference architecture for several similar products. Finally, we present the result of a case study.},
	booktitle = {20th {International} {Conference} on {Software} {Engineering} and {Knowledge} {Engineering}, {SEKE} 2008},
	author = {Hu, Lei and Sartipi, Kamran},
	month = jan,
	year = {2008},
	pages = {842--846},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/J7BRZ7SC/Hu and Sartipi - Dynamic Analysis and Design Pattern Detection in J.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/3UVHHRUH/221391328_Dynamic_Analysis_and_Design_Pattern_Detection_in_Java_Programs.html:text/html}
}

@inproceedings{arcelli_design_2008,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Design {Pattern} {Detection} in {Java} {Systems}: {A} {Dynamic} {Analysis} {Based} {Approach}},
	isbn = {978-3-642-14818-7 978-3-642-14819-4},
	shorttitle = {Design {Pattern} {Detection} in {Java} {Systems}},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-14819-4_12},
	doi = {10.1007/978-3-642-14819-4_12},
	abstract = {In the context of reverse engineering, the recognition of design patterns provides additional information related to the rationale behind the design. This paper presents our approach to the recognition of the behavioral design patterns based on dynamic analysis of Java software systems. The idea behind our solution is to identify a set of rules capturing information necessary to identify a design pattern instance. Rules are characterized by weights indicating their importance in the detection of a specific design pattern. The core behavior of each design pattern may be described through a subset of these rules forming a macrorule. Macrorules define the main traits of a pattern. JADEPT (JAva DEsign Pattern deTector) is our software for design pattern identification based on this idea. It captures static and dynamic aspects through a dynamic analysis of the software by exploiting the JPDA (Java Platform Debugger Architecture). The extracted information is stored in a database. Queries to the database implement the rules defined to recognize the design patterns. The tool has been validated with positive results on different implementations of design patterns and on systems such as JADEPT itself.},
	language = {en},
	urldate = {2017-11-12},
	booktitle = {Evaluation of {Novel} {Approaches} to {Software} {Engineering}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Arcelli, Francesca and Perin, Fabrizio and Raibulet, Claudia and Ravani, Stefano},
	month = may,
	year = {2008},
	pages = {163--179},
	file = {Snapshot:/Users/luigi/work/zotero/storage/KS9GYNA3/978-3-642-14819-4_12.html:text/html}
}

@inproceedings{jang_empirical_2010,
	address = {New York, NY, USA},
	series = {{CCS} '10},
	title = {An {Empirical} {Study} of {Privacy}-violating {Information} {Flows} in {JavaScript} {Web} {Applications}},
	isbn = {978-1-4503-0245-6},
	url = {http://doi.acm.org/10.1145/1866307.1866339},
	doi = {10.1145/1866307.1866339},
	abstract = {The dynamic nature of JavaScript web applications has given rise to the possibility of privacy violating information flows. We present an empirical study of the prevalence of such flows on a large number of popular websites. We have (1) designed an expressive, fine-grained information flow policy language that allows us to specify and detect different kinds of privacy-violating flows in JavaScript code,(2) implemented a new rewriting-based JavaScript information flow engine within the Chrome browser, and (3) used the enhanced browser to conduct a large-scale empirical study over the Alexa global top 50,000 websites of four privacy-violating flows: cookie stealing, location hijacking, history sniffing, and behavior tracking. Our survey shows that several popular sites, including Alexa global top-100 sites, use privacy-violating flows to exfiltrate information about users' browsing behavior. Our findings show that steps must be taken to mitigate the privacy threat from covert flows in browsers.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 17th {ACM} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Jang, Dongseok and Jhala, Ranjit and Lerner, Sorin and Shacham, Hovav},
	year = {2010},
	keywords = {dynamic analysis, JavaScript, history sniffing, information flow, privacy, rewriting, web application, web security},
	pages = {270--283},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/G2QBZDRK/Jang et al. - 2010 - An Empirical Study of Privacy-violating Informatio.pdf:application/pdf}
}

@inproceedings{latoza_developers_2010,
	address = {New York, NY, USA},
	series = {{ICSE} '10},
	title = {Developers {Ask} {Reachability} {Questions}},
	isbn = {978-1-60558-719-6},
	url = {http://doi.acm.org/10.1145/1806799.1806829},
	doi = {10.1145/1806799.1806829},
	abstract = {A reachability question is a search across feasible paths through a program for target statements matching search criteria. In three separate studies, we found that reachability questions are common and often time consuming to answer. In the first study, we observed 13 developers in the lab and found that half of the bugs developers inserted were associated with reachability questions. In the second study, 460 professional software developers reported asking questions that may be answered using reachability questions more than 9 times a day, and 82\% rated one or more as at least somewhat hard to answer. In the third study, we observed 17 developers in the field and found that 9 of the 10 longest activities were associated with reachability questions. These findings suggest that answering reachability questions is an important source of difficulty understanding large, complex codebases.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 32Nd {ACM}/{IEEE} {International} {Conference} on {Software} {Engineering} - {Volume} 1},
	publisher = {ACM},
	author = {LaToza, Thomas D. and Myers, Brad A.},
	year = {2010},
	keywords = {software maintenance, empirical study, code navigation, developer questions, program comprehension},
	pages = {185--194},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/N3RPZDCU/LaToza and Myers - 2010 - Developers Ask Reachability Questions.pdf:application/pdf}
}

@inproceedings{dietrich_broken_2014,
	title = {Broken promises: {An} empirical study into evolution problems in {Java} programs caused by library upgrades},
	shorttitle = {Broken promises},
	doi = {10.1109/CSMR-WCRE.2014.6747226},
	abstract = {It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs. We have studied the extent of the problem on the qualitas corpus, a data set consisting of Java open-source programs widely used in empirical studies. In this paper, we describe the study and report its key findings. We found that the above mentioned issues do occur in practice, albeit not on a wide scale.},
	booktitle = {2014 {Software} {Evolution} {Week} - {IEEE} {Conference} on {Software} {Maintenance}, {Reengineering}, and {Reverse} {Engineering} ({CSMR}-{WCRE})},
	author = {Dietrich, J. and Jezek, K. and Brada, P.},
	month = feb,
	year = {2014},
	keywords = {Java, Runtime, Libraries, program compilers, virtual machines, object-oriented programming, application program interface, compiler, API changes, Contracts, Couplings, Educational institutions, Java open-source programs, Joining processes, library upgrades, library versions, partially upgrading systems, qualitas corpus, reuse approach, system runtime failures, virtual machine},
	pages = {64--73},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/5CQSS7AJ/6747226.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/YFBBZSD3/Dietrich et al. - 2014 - Broken promises An empirical study into evolution.pdf:application/pdf}
}

@inproceedings{sorensen_algorithm_1995,
	title = {An {Algorithm} of {Generalization} in {Positive} {Supercompilation}},
	abstract = {This paper presents a termination technique for positive supercompilation, based on notions from term algebra. The technique is not particularily biased towards positive supercompilation, but also works for deforestation and partial evaluation. It appears to be well suited for partial deduction too. The technique guarantees termination, yet it is not overly conservative. Our technique can be viewed as an instance of Martens ' and Gallagher's recent framework for global termination of partial deduction, but it is more general in some important respects, e.g. it uses well-quasi orderings rather than well-founded orderings. Its merits are illustrated on several examples.},
	booktitle = {Proceedings of {ILPS}'95, the {International} {Logic} {Programming} {Symposium}},
	publisher = {MIT Press},
	author = {S{\o}rensen, Morten H. and Gl{\"u}ck, Robert},
	year = {1995},
	pages = {465--479},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/FZRVPQDD/S{\o}rensen and Gl{\"u}ck - 1995 - An Algorithm of Generalization in Positive Superco.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/AB294Z57/summary.html:text/html}
}

@book{sorensen_turchins_1996,
	title = {Turchin's {Supercompiler} {Revisited} - {An} operational theory of positive information propagation},
	abstract = {Turchin`s supercompiler is a program transformer that includes both partial evaluation and deforestation. Although known in the West since 1979, the essence of its techniques, its more precise relations to other transformers, and the properties of the programs that it produces are only now becoming apparent in the Western functional programming community. This thesis gives a new formulation of the supercompiler in familiar terms; we study the essence of it, how it achieves its effects, and its relations to related transformers; and we develop results dealing with the problems of preserving semantics, assessing the efficiency of transformed programs, and ensuring termination.},
	author = {S{\o}rensen, Morten Heine},
	year = {1996},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/KMWFFDQC/S{\o}rensen - 1996 - Turchin's Supercompiler Revisited - An operational.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/Q79BHCQY/summary.html:text/html}
}

@inproceedings{saraswat_concurrent_1990,
	address = {New York, NY, USA},
	series = {{POPL} '90},
	title = {Concurrent {Constraint} {Programming}},
	isbn = {978-0-89791-343-0},
	url = {http://doi.acm.org/10.1145/96709.96733},
	doi = {10.1145/96709.96733},
	abstract = {This paper presents a new and very rich class of (concurrent) programming languages, based on the notion of computing with partial information, and the concomitant notions of consistency and entailment.1 In this framework, computation emerges from the interaction of concurrently executing agents that communicate by placing, checking and instantiating constraints on shared variables. Such a view of computation is interesting in the context of programming languages because of the ability to represent and manipulate partial information about the domain of discourse, in the context of concurrency because of the use of constraints for communication and control, and in the context of AI because of the availability of simple yet powerful mechanisms for controlling inference, and the promise that very rich representational/programming languages, sharing the same set of abstract properties, may be possible.
To reflect this view of computation, [Sar89] develops the cc family of languages. We present here one member of the family, cc({\textdownarrow}, {\textrightarrow}) (pronounced {\textquotedblleft}cc with Ask and Choose{\textquotedblright}) which provides the basic operations of blocking Ask and atomic Tell and an algebra of behaviors closed under prefixing, indeterministic choice, interleaving, and hiding, and provides a mutual recursion operator. cc({\textdownarrow}, {\textrightarrow}) is (intentionally!) very similar to Milner's CCS, but for the radically different underlying concept of communication, which, in fact, provides a general{\textemdash}and elegant{\textemdash}alternative approach to {\textquotedblleft}value-passing{\textquotedblright} in CCS. At the same time it adequately captures the framework of committed choice concurrent logic programming languages. We present the rules of behavior for cc agents, motivate a notion of {\textquotedblleft}visible action{\textquotedblright} for them, and develop the notion of c-trees and reactive congruence analogous to Milner's synchronization trees and observational congruence. We also present an equational characterization of reactive congruence for Finitary cc({\textdownarrow}, {\textrightarrow}).},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Saraswat, Vijay A. and Rinard, Martin},
	year = {1990},
	pages = {232--245},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/QZKCNCZQ/Saraswat and Rinard - 1990 - Concurrent Constraint Programming.pdf:application/pdf}
}

@book{sorensen_lectures_1998,
	title = {Lectures on the {Curry}-{Howard} {Isomorphism}},
	abstract = {The Curry-Howard isomorphism states an amazing correspondence between systems of formal logic as encountered in proof theory and computational calculi as found in type theory. For instance, minimal propositional logic corresponds to simply typed-calculus, first-order logic corresponds to dependent types, second-order logic corresponds to polymorphic types, etc. The isomorphism has many aspects, even at the syntactic level: formulas correspond to types, proofs correspond to terms, provability corresponds to inhabitation, proof normalization corresponds to term reduction, etc. But there is much more to the isomorphism than this. For instance, it is an old idea---due to Brouwer, Kolmogorov, and Heyting, and later formalized by Kleene's realizability interpretation---that a constructive proof of an implication is a procedure that transforms proofs of the antecedent into proofs of the succedent; the Curry-Howard isomorphism gives syntactic representations of such procedures. These notes give an introduction to parts of proof theory and related aspects of type theory relevant for the Curry-Howard isomorphism.},
	author = {S{\o}rensen, Morten Heine B. and Urzyczyn, Pawel},
	year = {1998},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/TQY43C4X/S{\o}rensen and Urzyczyn - 1998 - Lectures on the Curry-Howard Isomorphism.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/S3L5JDT3/summary.html:text/html}
}

@inproceedings{sorensen_towards_1994,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards unifying partial evaluation, deforestation, supercompilation, and {GPC}},
	isbn = {978-3-540-57880-2 978-3-540-48376-2},
	url = {https://link.springer.com/chapter/10.1007/3-540-57880-3_32},
	doi = {10.1007/3-540-57880-3_32},
	abstract = {We study four transformation methodologies which are automatic instances of Burstall and Darlington's fold/unfold framework: partial evaluation, deforestation, supercompilation, and generalized partial computation (GPC). One can classify these and other fold/unfold based transformers by how much information they maintain during transformation.We introduce the positive supercompiler, a version of deforestation including more information propagation, to study such a classification in detail. Via the study of positive supercompilation we are able to show that partial evaluation and deforestation have simple information propagation, positive supercompilation has more information propagation, and supercompilation and GPC have even more information propagation. The amount of information propagation is significant: positive supercompilation, GPC, and supercompilation can specialize a general pattern matcher to a fixed pattern so as to obtain efficient output similar to that of the Knuth-Morris-Pratt algorithm. In the case of partial evaluation and deforestation, the general matcher must be rewritten to achieve this.},
	language = {en},
	urldate = {2017-11-13},
	booktitle = {Programming {Languages} and {Systems} {\textemdash} {ESOP} '94},
	publisher = {Springer, Berlin, Heidelberg},
	author = {S{\o}rensen, Morten Heine and Gl{\"u}ck, Robert and Jones, Neil D.},
	month = apr,
	year = {1994},
	pages = {485--500},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/7PPPEJ67/S{\o}rensen et al. - 1994 - Towards unifying partial evaluation, deforestation.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/2T5DUQUW/3-540-57880-3_32.html:text/html}
}

@inproceedings{horridge_igniting_2007,
	title = {Igniting the {OWL} 1.1 {Touch} {Paper}: {The} {OWL} {API}},
	shorttitle = {Igniting the {OWL} 1.1 {Touch} {Paper}},
	abstract = {Abstract. This paper describes the design and implementation of an OWL 1.1 API, herein referred to as the OWL API. The API is designed to facilitate the manipulation of OWL 1.1 ontologies at a high level of abstraction for use by editors, reasoners and other tools. The API is based on the OWL 1.1 specification and influenced by the experience of designing and using the WonderWeb API and OWL-based applications. An overview of the basis for the design of the API is discussed along with major API functionality. The API is available from Source Forge:},
	booktitle = {In {Proc}. {OWL}-{ED} 2007, volume 258 of {CEUR}},
	author = {Horridge, Matthew and Bechhofer, Sean},
	year = {2007},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/7DAAFDMR/Horridge and Bechhofer - 2007 - Igniting the OWL 1.1 Touch Paper The OWL API.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/6NLQHGZK/summary.html:text/html}
}

@inproceedings{mitchell_run-time_2004,
	address = {Las Vegas, Nevada, USA},
	title = {Run-{Time} {Cohesion} {Metrics}: {An} {Empirical} {Investigation}},
	shorttitle = {Run-{Time} {Cohesion} {Metrics}},
	url = {http://eprints.maynoothuniversity.ie/6436/},
	abstract = {Cohesion is one of the fundamental measures of the
{\textquoteright}goodness{\textquoteright} of a software design. The most accepted and
widely studied object-oriented cohesion metric is Chidamber
and Kemerer{\textquoteright}s Lack of Cohesion in Methods measure.
However due to the nature of object-oriented programs,
static design metrics fail to quantify all the underlying
dimensions of cohesion, as program behaviour is a
function of it operational environment as well as the complexity
of the source code. For these reasons two run-time
object-oriented cohesion metrics are described in this paper,
and applied to Java programs from the SPECjvm98
benchmark suite. A statistical analysis is conducted to assess
the fundamental properties of the measures and investigate
whether they are redundant with respect to the static
cohesion metric. Results to date indicate that run-time cohesion
metrics can provide an interesting and informative
qualitative analysis of a program and complement existing
static cohesion metrics.},
	language = {en},
	urldate = {2017-11-13},
	author = {Mitchell, Aine and Power, James F.},
	year = {2004},
	pages = {532--537},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/RNTGZH5E/Mitchell and Power - 2004 - Run-Time Cohesion Metrics An Empirical Investigat.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/D7IQXXRG/6436.html:text/html}
}

@inproceedings{mcbride_epigram:_2004,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Epigram: {Practical} {Programming} with {Dependent} {Types}},
	isbn = {978-3-540-28540-3 978-3-540-31872-9},
	shorttitle = {Epigram},
	url = {https://link.springer.com/chapter/10.1007/11546382_3},
	doi = {10.1007/11546382_3},
	abstract = {Find the type error in the following Haskell expression:if null xs then tail xs else xsYou can{\textquoteright}t, of course: this program is obviously nonsense unless you{\textquoteright}re a typechecker. The trouble is that only certain computations make sense if the null xs test is True, whilst others make sense if it is False. However, as far as the type system is concerned, the type of the then branch is the type of the else branch is the type of the entire conditional. Statically, the test is irrelevant. Which is odd, because if the test really were irrelevant, we wouldn{\textquoteright}t do it. Of course, tail [] doesn{\textquoteright}t go wrong{\textemdash}well-typed programs don{\textquoteright}t go wrong{\textemdash}so we{\textquoteright}d better pick a different word for the way they do go.Abstraction and application, tupling and projection: these provide the {\textquoteleft}software engineering{\textquoteright} superstructure for programs, and our familiar type systems ensure that these operations are used compatibly. However, sooner or later, most programs inspect data and make a choice{\textemdash}at that point our familiar type systems fall silent. They simply can{\textquoteright}t talk about specific data. All this time, we thought our programming was strongly typed, when it was just our software engineering. In order to do better, we need a static language capable of expressing the significance of particular values in legitimizing some computations rather than others. We should not give up on programming.},
	language = {en},
	urldate = {2017-11-13},
	booktitle = {Advanced {Functional} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {McBride, Conor},
	month = aug,
	year = {2004},
	pages = {130--170},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/KYZWA445/McBride - 2004 - Epigram Practical Programming with Dependent Type.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/IC4XWP98/11546382_3.html:text/html}
}

@book{skalski_syntax-extending_2005,
	title = {Syntax-extending and type-reflecting macros in an object-oriented language},
	author = {Skalski, Kamil},
	year = {2005},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/PRU7VGXM/Skalski - 2005 - Syntax-extending and type-reflecting macros in an .pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/FVXIVRPW/summary.html:text/html}
}

@inproceedings{gluck_occams_1993,
	title = {Occam{\textquoteright}s {Razor} in {Metacomputation}: the {Notion} of a {Perfect} {Process} {Tree}},
	shorttitle = {Occam{\textquoteright}s {Razor} in {Metacomputation}},
	abstract = {Abstract. We introduce the notion of a perfect process tree as a model for the full propagation of information in metacomputation. Starting with constant propagation we construct step-by-step the driving mechanism used in super-compila tion which ensures the perfect propagation of information. The concept of a simple supercompiler based on perfect driving coupled with a simple folding strategy is explained. As an example we demonstrate that specializing a naive pattern matcher with respect to a fixed pattern obtains the efficiency of a matcher generated by the Knuth, Morris \& Pratt algorithm. 1},
	booktitle = {In {Proc}. of the 3rd {Int}{\textquoteright}l {Workshop} on {Static} {Analysis} ({WSA}{\textquoteright}93). {Springer} {LNCS} 724},
	author = {Gl{\"u}ck, Robert and Klimov, Andrei V.},
	year = {1993},
	pages = {112--123},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/VVUHB3HW/Gl{\"u}ck and Klimov - 1993 - Occam{\textquoteright}s Razor in Metacomputation the Notion of a .pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/CR5GYJJW/summary.html:text/html}
}

@inproceedings{leuschel_power_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Power} of {Homeomorphic} {Embedding} for {Online} {Termination}},
	isbn = {978-3-540-65014-0 978-3-540-49727-1},
	url = {https://link.springer.com/chapter/10.1007/3-540-49727-7_14},
	doi = {10.1007/3-540-49727-7_14},
	abstract = {Recently well-quasi orders in general, and homeomorphic embedding in particular, have gained popularity to ensure the termination of program analysis, specialisation and transformation techniques. In this paper we investigate and clarify for the first time, both intuitively and formally, the advantages of such an approach over one using well-founded orders. Notably we show that the homeomorphic embedding relation is strictly more powerful than a large class of involved well-founded approaches.},
	language = {en},
	urldate = {2017-11-13},
	booktitle = {Static {Analysis}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Leuschel, Michael},
	month = sep,
	year = {1998},
	pages = {230--245},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/QX76IBKW/Leuschel - 1998 - On the Power of Homeomorphic Embedding for Online .pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/GTE8ZCXQ/3-540-49727-7_14.html:text/html}
}

@article{sasirekha_program_2011,
	title = {Program slicing techniques and its applications},
	url = {http://arxiv.org/abs/1108.1352},
	abstract = {Program understanding is an important aspect in Software Maintenance and Reengineering. Understanding the program is related to execution behaviour and relationship of variable involved in the program. The task of finding all statements in a program that directly or indirectly influence the value for an occurrence of a variable gives the set of statements that can affect the value of a variable at some point in a program is called a program slice. Program slicing is a technique for extracting parts of computer programs by tracing the programs' control and data flow related to some data item. This technique is applicable in various areas such as debugging, program comprehension and understanding, program integration, cohesion measurement, re-engineering, maintenance, testing where it is useful to be able to focus on relevant parts of large programs. This paper focuses on the various slicing techniques (not limited to) like static slicing, quasi static slicing, dynamic slicing and conditional slicing. This paper also includes various methods in performing the slicing like forward slicing, backward slicing, syntactic slicing and semantic slicing. The slicing of a program is carried out using Java which is a object oriented programming language.},
	urldate = {2017-11-13},
	journal = {arXiv:1108.1352 [cs]},
	author = {Sasirekha, N. and Robert, A. Edwin and Hemalatha, Dr M.},
	month = aug,
	year = {2011},
	note = {arXiv: 1108.1352},
	keywords = {Computer Science - Software Engineering},
	file = {arXiv\:1108.1352 PDF:/Users/luigi/work/zotero/storage/KVUSJNFH/Sasirekha et al. - 2011 - Program slicing techniques and its applications.pdf:application/pdf;arXiv.org Snapshot:/Users/luigi/work/zotero/storage/78ETG8PT/1108.html:text/html}
}

@article{galeotti_inferring_2015,
	title = {Inferring {Loop} {Invariants} by {Mutation}, {Dynamic} {Analysis}, and {Static} {Checking}},
	volume = {41},
	issn = {0098-5589},
	doi = {10.1109/TSE.2015.2431688},
	abstract = {Verifiers that can prove programs correct against their full functional specification require, for programs with loops, additional annotations in the form of loop invariants-properties that hold for every iteration of a loop. We show that significant loop invariant candidates can be generated by systematically mutating postconditions; then, dynamic checking (based on automatically generated tests) weeds out invalid candidates, and static checking selects provably valid ones. We present a framework that automatically applies these techniques to support a program prover, paving the way for fully automatic verification without manually written loop invariants: Applied to 28 methods (including 39 different loops) from various java.util classes (occasionally modified to avoid using Java features not fully supported by the static checker), our DYNAMATE prototype automatically discharged 97 percent of all proof obligations, resulting in automatic complete correctness proofs of 25 out of the 28 methods-outperforming several state-of-the-art tools for fully automatic verification.},
	number = {10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Galeotti, J. P. and Furia, C. A. and May, E. and Fraser, G. and Zeller, A.},
	month = oct,
	year = {2015},
	keywords = {Java, Instruments, program verification, Detectors, dynamic analysis, Arrays, automatic complete correctness proofs, automatic verification, DYNAMATE prototype, formal specification, functional properties, functional specification, Generators, Heuristic algorithms, inference, Java.util classes, loop invariant inference, Loop invariants, mutation, program control structures, program prover, program testing, Prototypes, static checking, system monitoring, test automatic generation},
	pages = {1019--1037},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/ZRUIZB4J/7105412.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/NSK7PMIL/Galeotti et al. - 2015 - Inferring Loop Invariants by Mutation, Dynamic Ana.pdf:application/pdf}
}

@article{mccarthy_recursive_1960,
	title = {Recursive {Functions} of {Symbolic} {Expressions} and {Their} {Computation} by {Machine}, {Part} {I}},
	volume = {3},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/367177.367199},
	doi = {10.1145/367177.367199},
	number = {4},
	urldate = {2017-11-13},
	journal = {Commun. ACM},
	author = {McCarthy, John},
	month = apr,
	year = {1960},
	pages = {184--195},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/CN8RPXLH/McCarthy - 1960 - Recursive Functions of Symbolic Expressions and Th.pdf:application/pdf}
}

@article{conway_design_1963,
	title = {Design of a {Separable} {Transition}-diagram {Compiler}},
	volume = {6},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/366663.366704},
	doi = {10.1145/366663.366704},
	number = {7},
	urldate = {2017-11-13},
	journal = {Commun. ACM},
	author = {Conway, Melvin E.},
	month = jul,
	year = {1963},
	pages = {396--408},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/3J46KTPE/Conway - 1963 - Design of a Separable Transition-diagram Compiler.pdf:application/pdf}
}

@article{landin_next_1966,
	title = {The {Next} 700 {Programming} {Languages}},
	volume = {9},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/365230.365257},
	doi = {10.1145/365230.365257},
	abstract = {A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework. This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships. Within this framework the design of a specific language splits into two independent parts. One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, list of them, functional relations among them) that can be referred to in the language.
The system is biased towards {\textquotedblleft}expressions{\textquotedblright} rather than {\textquotedblleft}statements.{\textquotedblright} It includes a nonprocedural (purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand.},
	number = {3},
	urldate = {2017-11-13},
	journal = {Commun. ACM},
	author = {Landin, P. J.},
	month = mar,
	year = {1966},
	pages = {157--166},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/RF4R2M5C/Landin - 1966 - The Next 700 Programming Languages.pdf:application/pdf}
}

@article{backus_can_1978,
	title = {Can {Programming} {Be} {Liberated} from the {Von} {Neumann} {Style}?: {A} {Functional} {Style} and {Its} {Algebra} of {Programs}},
	volume = {21},
	issn = {0001-0782},
	shorttitle = {Can {Programming} {Be} {Liberated} from the {Von} {Neumann} {Style}?},
	url = {http://doi.acm.org/10.1145/359576.359579},
	doi = {10.1145/359576.359579},
	abstract = {Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor{\textemdash}the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.
An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.
Associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose {\textquotedblleft}unknowns{\textquotedblright} are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.
 A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states{\textemdash}only one state transition occurs per major computation.},
	number = {8},
	urldate = {2017-11-13},
	journal = {Commun. ACM},
	author = {Backus, John},
	month = aug,
	year = {1978},
	keywords = {program transformation, functional programming, programming languages, algebra of programs, applicative computing systems, applicative state transition systems, combining forms, functional forms, metacomposition, models of computing systems, program correctness, program termination, von Neumann computers, von Neumann languages},
	pages = {613--641},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/LVBPQNCD/Backus - 1978 - Can Programming Be Liberated from the Von Neumann .pdf:application/pdf}
}

@article{mcgill_variations_1978,
	title = {Variations of {Box} {Plots}},
	volume = {32},
	issn = {0003-1305},
	url = {http://www.jstor.org/stable/2683468},
	doi = {10.2307/2683468},
	abstract = {Box plots display batches of data. Five values from a set of data are conventionally used; the extremes, the upper and lower hinges (quartiles), and the median. Such plots are becoming a widely used tool in exploratory data analysis and in preparing visual summaries for statisticians and nonstatisticians alike. Three variants of the basic display, devised by the authors, are described. The first visually incorporates a measure of group size; the second incorporates an indication of rough significance of differences between medians; the third combines the features of the first two. These techniques are displayed by examples.},
	number = {1},
	urldate = {2017-11-13},
	journal = {The American Statistician},
	author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
	year = {1978},
	pages = {12--16},
	file = {JSTOR Full Text PDF:/Users/luigi/work/zotero/storage/WXMWTEK2/McGill et al. - 1978 - Variations of Box Plots.pdf:application/pdf}
}

@article{milner_theory_1978,
	title = {A theory of type polymorphism in programming},
	volume = {17},
	abstract = {The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple pro-gramming language, and a compile time type-checking algorithm w which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot {\textquotedblleft}go wrong {\textquotedblright} and a Syntactic Soundness Theorem states that if fl accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on w is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system, 1.},
	journal = {Journal of Computer and System Sciences},
	author = {Milner, Robin},
	year = {1978},
	pages = {348--375},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/NPTHLXFI/Milner - 1978 - A theory of type polymorphism in programming.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/5ZMW9IJ4/summary.html:text/html}
}

@article{bentley_programming_1986,
	title = {Programming {Pearls}: {Little} {Languages}},
	volume = {29},
	issn = {0001-0782},
	shorttitle = {Programming {Pearls}},
	url = {http://doi.acm.org/10.1145/6424.315691},
	doi = {10.1145/6424.315691},
	number = {8},
	urldate = {2017-11-13},
	journal = {Commun. ACM},
	author = {Bentley, Jon},
	month = aug,
	year = {1986},
	pages = {711--721},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/T6W5UH3X/Bentley - 1986 - Programming Pearls Little Languages.pdf:application/pdf}
}

@inproceedings{wadler_linear_1990,
	title = {Linear {Types} {Can} {Change} the {World}!},
	abstract = {The linear logic of J.-Y. Girard suggests a new type system for functional  languages, one which supports operations that "change the world". Values belonging  to a linear type must be used exactly once: like the world, they cannot be  duplicated or destroyed. Such values require no reference counting or garbage collection, and safely admit destructive array update. Linear types extend Schmidt's  notion of single threading; provide an alternative to Hudak and Bloss' update  analysis; and offer a practical complement to Lafont and Holmstr{\"o}m's elegant linear languages.},
	booktitle = {Programming {Concepts} and {Methods}},
	publisher = {North},
	author = {Wadler, Philip},
	year = {1990},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/S2RWRZGF/Wadler - 1990 - Linear Types Can Change the World!.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/9CJGCCYS/summary.html:text/html}
}

@inproceedings{ungar_self:_1987,
	address = {New York, NY, USA},
	series = {{OOPSLA} '87},
	title = {Self: {The} {Power} of {Simplicity}},
	isbn = {978-0-89791-247-1},
	shorttitle = {Self},
	url = {http://doi.acm.org/10.1145/38765.38828},
	doi = {10.1145/38765.38828},
	abstract = {Self is a new object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because Self does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. Self's simplicity and expressiveness offer new insights into object-oriented computation.},
	urldate = {2017-11-13},
	booktitle = {Conference {Proceedings} on {Object}-oriented {Programming} {Systems}, {Languages} and {Applications}},
	publisher = {ACM},
	author = {Ungar, David and Smith, Randall B.},
	year = {1987},
	pages = {227--242},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TIJCVKBQ/Ungar and Smith - 1987 - Self The Power of Simplicity.pdf:application/pdf}
}

@inproceedings{liang_dynamic_1998,
	address = {New York, NY, USA},
	series = {{OOPSLA} '98},
	title = {Dynamic {Class} {Loading} in the {Java} {Virtual} {Machine}},
	isbn = {978-1-58113-005-8},
	url = {http://doi.acm.org/10.1145/286936.286945},
	doi = {10.1145/286936.286945},
	abstract = {Class loaders are a powerful mechanism for dynamically loading software components on the Java platform. They are unusual in supporting all of the following features: laziness, type-safe linkage, user-defined extensibility, and multiple communicating namespaces.We present the notion of class loaders and demonstrate some of their interesting uses. In addition, we discuss how to maintain type safety in the presence of user-defined dynamic class loading.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 13th {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Liang, Sheng and Bracha, Gilad},
	year = {1998},
	pages = {36--44},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/3BSCK3VV/Liang and Bracha - 1998 - Dynamic Class Loading in the Java Virtual Machine.pdf:application/pdf}
}

@inproceedings{foster_theory_1999,
	address = {New York, NY, USA},
	series = {{PLDI} '99},
	title = {A {Theory} of {Type} {Qualifiers}},
	isbn = {978-1-58113-094-2},
	url = {http://doi.acm.org/10.1145/301618.301665},
	doi = {10.1145/301618.301665},
	abstract = {We describe a framework for adding type qualifiers to a language. Type qualifiers encode a simple but highly useful form of subtyping. Our framework extends standard type rules to model the flow of qualifiers through a program, where each qualifier or set of qualifiers comes with additional rules that capture its semantics. Our framework allows types to be polymorphic in the type qualifiers. We present a const-inference system for C as an example application of the framework. We show that for a set of real C programs, many more consts can be used than are actually present in the original code.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 1999 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Foster, Jeffrey S. and F{\"a}hndrich, Manuel and Aiken, Alexander},
	year = {1999},
	pages = {192--203},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DJE4GFLJ/Foster et al. - 1999 - A Theory of Type Qualifiers.pdf:application/pdf}
}

@article{alpern_jalapeno_2000,
	title = {The {Jalape{\~n}o} virtual machine},
	volume = {39},
	issn = {0018-8670},
	doi = {10.1147/sj.391.0211},
	abstract = {Jalape{\~n}o is a virtual machine for Java{\texttrademark} servers written in the Java language. To be able to address the requirements of servers (performance and scalability in particular), Jalape{\~n}o was designed {\textquotedblleft}from scratch{\textquotedblleft} to be as self-sufficient as possible. Jalape{\~n}o's unique object model and memory layout allows a hardware null-pointer check as well as fast access to array elements, fields, and methods. Run-time services conventionally provided in native code are implemented primarily in Java. Java threads are multiplexed by virtual processors (implemented as operating system threads). A family of concurrent object allocators and parallel type-accurate garbage collectors is supported. Jalape{\~n}o's interoperable compilers enable quasi-preemptive thread switching and precise location of object references. Jalape{\~n}o's dynamic optimizing compiler is designed to obtain high quality code for methods that are observed to be frequently executed or computationally intensive.},
	number = {1},
	journal = {IBM Systems Journal},
	author = {Alpern, B. and Attanasio, C. R. and Barton, J. J. and Burke, M. G. and Cheng, P. and Choi, J. D. and Cocchi, A. and Fink, S. J. and Grove, D. and Hind, M. and Hummel, S. F. and Lieber, D. and Litvinov, V. and Mergen, M. F. and Ngo, T. and Russell, J. R. and Sarkar, V. and Serrano, M. J. and Shepherd, J. C. and Smith, S. E. and Sreedhar, V. C. and Srinivasan, H. and Whaley, J.},
	year = {2000},
	pages = {211--238},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/R3HBL8CY/5387060.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/LSVPYBSW/Alpern et al. - 2000 - The Jalape #x00F1\;o virtual machine.pdf:application/pdf}
}

@inproceedings{flatt_submodules_2013,
	address = {New York, NY, USA},
	series = {{GPCE} '13},
	title = {Submodules in {Racket}: {You} {Want} {It} when, {Again}?},
	isbn = {978-1-4503-2373-4},
	shorttitle = {Submodules in {Racket}},
	url = {http://doi.acm.org/10.1145/2517208.2517211},
	doi = {10.1145/2517208.2517211},
	abstract = {In an extensible programming language, programmers write code that must run at different times - in particular, at compile time versus run time. The module system of the Racket programming language enables a programmer to reason about programs in the face of such extensibility, because the distinction between run-time and compile-time phases is built into the language model. Submodules extend Racket's module system to make the phase-separation facet of the language extensible. That is, submodules give programmers the capability to define new phases, such as "test time" or "documentation time," with the same reasoning and code-management benefits as the built-in distinction between run time and compile time.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Generative} {Programming}: {Concepts} \& {Experiences}},
	publisher = {ACM},
	author = {Flatt, Matthew},
	year = {2013},
	keywords = {language tower, macros, modules},
	pages = {13--22},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/9ZIC9MCT/Flatt - 2013 - Submodules in Racket You Want It when, Again.pdf:application/pdf}
}

@inproceedings{dice_transactional_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Transactional {Locking} {II}},
	isbn = {978-3-540-44624-8 978-3-540-44627-9},
	url = {https://link.springer.com/chapter/10.1007/11864219_14},
	doi = {10.1007/11864219_14},
	abstract = {The transactional memory programming paradigm is gaining momentum as the approach of choice for replacing locks in concurrent programming. This paper introduces the transactional locking II (TL2) algorithm, a software transactional memory (STM) algorithm based on a combination of commit-time locking and a novel global version-clock based validation technique. TL2 improves on state-of-the-art STMs in the following ways: (1) unlike all other STMs it fits seamlessly with any system{\textquoteright}s memory life-cycle, including those using malloc/free (2) unlike all other lock-based STMs it efficiently avoids periods of unsafe execution, that is, using its novel version-clock validation, user code is guaranteed to operate only on consistent memory states, and (3) in a sequence of high performance benchmarks, while providing these new properties, it delivered overall performance comparable to (and in many cases better than) that of all former STM algorithms, both lock-based and non-blocking. Perhaps more importantly, on various benchmarks, TL2 delivers performance that is competitive with the best hand-crafted fine-grained concurrent structures. Specifically, it is ten-fold faster than a single lock. We believe these characteristics make TL2 a viable candidate for deployment of transactional memory today, long before hardware transactional support is available.},
	language = {en},
	urldate = {2017-11-13},
	booktitle = {Distributed {Computing}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Dice, Dave and Shalev, Ori and Shavit, Nir},
	month = sep,
	year = {2006},
	pages = {194--208},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/WJCICUI2/Dice et al. - 2006 - Transactional Locking II.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/7L6T2IF8/11864219_14.html:text/html}
}

@inproceedings{fluet_linear_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Linear {Regions} {Are} {All} {You} {Need}},
	isbn = {978-3-540-33095-0 978-3-540-33096-7},
	url = {https://link.springer.com/chapter/10.1007/11693024_2},
	doi = {10.1007/11693024_2},
	abstract = {The type-and-effects system of the Tofte-Talpin region calculus makes it possible to safely reclaim objects without a garbage collector. However, it requires that regions have last-in-first-out (LIFO) lifetimes following the block structure of the language. We introduce $\lambda$rgnUL, a core calculus that is powerful enough to encode Tofte-Talpin-like languages, and that eliminates the LIFO restriction. The target language has an extremely simple, substructural type system. To prove the power of the language, we sketch how Tofte-Talpin-style regions, as well as the first-class dynamic regions and unique pointers of the Cyclone programming language can be encoded in $\lambda$rgnUL.},
	language = {en},
	urldate = {2017-11-13},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Fluet, Matthew and Morrisett, Greg and Ahmed, Amal},
	month = mar,
	year = {2006},
	pages = {7--21},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/TSRQCCB8/Fluet et al. - 2006 - Linear Regions Are All You Need.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/DTF65I98/11693024_2.html:text/html}
}

@inproceedings{govindaraju_memory_2006,
	address = {New York, NY, USA},
	series = {{SC} '06},
	title = {A {Memory} {Model} for {Scientific} {Algorithms} on {Graphics} {Processors}},
	isbn = {978-0-7695-2700-0},
	url = {http://doi.acm.org/10.1145/1188455.1188549},
	doi = {10.1145/1188455.1188549},
	abstract = {We present a memory model to analyze and improve the performance of scientific algorithms on graphics processing units (GPUs). Our memory model is based on texturing hardware, which uses a 2D block-based array representation to perform the underlying computations. We incorporate many characteristics of GPU architectures including smaller cache sizes, 2D block representations, and use the 3C's model to analyze the cache misses. Moreover. we present techniques to improve the performance of nested loops on GPUs. In order to demonstrate the effectiveness of our model, we highlight its performance on three memory-intensive scientific applications - sorting, fast Fourier transform and dense matrix-multiplication. In practice, our cache-efficient algorithms for these applications are able to achieve memory throughput of 30-50 GB/s on a NVIDIA 7900 GTX GPU. We also compare our results with prior GPU-based and CPU-based implementations on high-end processors. In practice, we are able to achieve 2-5 x performance improvement.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 2006 {ACM}/{IEEE} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Govindaraju, Naga K. and Larsen, Scott and Gray, Jim and Manocha, Dinesh},
	year = {2006},
	keywords = {graphics processors, memory model, scientific algorithms},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/FE48SUFG/Govindaraju et al. - 2006 - A Memory Model for Scientific Algorithms on Graphi.pdf:application/pdf}
}

@article{grossman_quantified_2006,
	title = {Quantified {Types} in an {Imperative} {Language}},
	volume = {28},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/1133651.1133653},
	doi = {10.1145/1133651.1133653},
	abstract = {We describe universal types, existential types, and type constructors in Cyclone, a strongly typed C-like language. We show how the language naturally supports first-class polymorphism and polymorphic recursion while requiring an acceptable amount of explicit type information. More importantly, we consider the soundness of type variables in the presence of C-style mutation and the address-of operator. For polymorphic references, we describe a solution more natural for the C level than the ML-style {\textquotedblleft}value restriction.{\textquotedblright} For existential types, we discover and subsequently avoid a subtle unsoundness issue resulting from the address-of operator. We develop a formal abstract machine and type-safety proof that capture the essence of type variables at the C level.},
	number = {3},
	urldate = {2017-11-13},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Grossman, Dan},
	month = may,
	year = {2006},
	keywords = {Cyclone, existential types, polymorphism, type variables},
	pages = {429--475},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/JA5CG7SD/Grossman - 2006 - Quantified Types in an Imperative Language.pdf:application/pdf}
}

@inproceedings{scherer_scalable_2006,
	address = {New York, NY, USA},
	series = {{PPoPP} '06},
	title = {Scalable {Synchronous} {Queues}},
	isbn = {978-1-59593-189-4},
	url = {http://doi.acm.org/10.1145/1122971.1122994},
	doi = {10.1145/1122971.1122994},
	abstract = {We present two new nonblocking and contention-free implementations of synchronous queues ,concurrent transfer channels in which producers wait for consumers just as consumers wait for producers. Our implementations extend our previous work in dual queues and dual stacks to effect very high-performance handoff. We present performance results on 16-processor SPARC and 4-processor Opteron machines. We compare our algorithms to commonly used alternatives from the literature and from the Java SE 5.0 class java. util. concurrent. SynchronousQueue both directly in synthetic microbenchmarks and indirectly as the core of Java's Thread-PoolExecutor mechanism (which in turn is the core of many Java server programs).Our new algorithms consistently outperform the Java SE 5.0 SynchronousQueue by factors of three in unfair mode and 14 in fair mode; this translates to factors of two and ten for the ThreadPoolExecutor. Our synchronous queues have been adopted for inclusion in Java 6.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the {Eleventh} {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {ACM},
	author = {Scherer, III, William N. and Lea, Doug and Scott, Michael L.},
	year = {2006},
	keywords = {contention freedom, dual data structures, dual queue, dual stack, lock freedom, nonblocking synchronization, synchronous queue},
	pages = {147--156},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/E84DB5D6/Scherer et al. - 2006 - Scalable Synchronous Queues.pdf:application/pdf}
}

@inproceedings{maebe_javana:_2006,
	address = {New York, NY, USA},
	series = {{OOPSLA} '06},
	title = {Javana: {A} {System} for {Building} {Customized} {Java} {Program} {Analysis} {Tools}},
	isbn = {978-1-59593-348-5},
	shorttitle = {Javana},
	url = {http://doi.acm.org/10.1145/1167473.1167487},
	doi = {10.1145/1167473.1167487},
	abstract = {Understanding the behavior of applications running on high-level language virtual machines, as is the case in Java, is non-trivial because of the tight entanglement at the lowest execution level between the application and the virtual machine. This paper proposes Javana, a system for building Java program analysis tools. Javana provides an easy-to-use instrumentation infrastructure that allows for building customized profiling tools very quickly.Javana runs a dynamic binary instrumentation tool underneath the virtual machine. The virtual machine communicates with the instrumentation layer through an event handling mechanism for building a vertical map that links low-level native instruction pointers and memory addresses to high-level language concepts such as objects, methods, threads, lines of code, etc. The dynamic binary instrumentation tool then intercepts all memory accesses and instructions executed and provides the Javana end user with high-level language information for all memory accesses and natively executed instructions.We demonstrate the power of Javana through a number of applications: memory address tracing, vertical cache simulation and object lifetime computation. For each of these applications, the instrumentation specification requires only a small number of lines of code. Developing similarly powerful profiling tools within a virtual machine (as done in current practice) is both time-consuming and error-prone; in addition, the accuracy of the obtained profiling results might be questionable as we show in this paper.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 21st {Annual} {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programming} {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Maebe, Jonas and Buytaert, Dries and Eeckhout, Lieven and De Bosschere, Koen},
	year = {2006},
	keywords = {Java, aspect-oriented instrumentation, customized program analysis tool},
	pages = {153--168},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/WY9HPGG8/Maebe et al. - 2006 - Javana A System for Building Customized Java Prog.pdf:application/pdf}
}

@article{xi_dependent_2007,
	title = {Dependent {ML} {An} approach to practical programming with dependent types},
	volume = {17},
	issn = {1469-7653, 0956-7968},
	url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/dependent-ml-an-approach-to-practical-programming-with-dependent-types/4A1FC643ACD49EF31DAF5EB955D2CCC7#},
	doi = {10.1017/S0956796806006216},
	abstract = {AbstractWe present an approach to enriching the type system of ML with a restricted form of dependent types, where type index terms are required to be drawn from a given type index language  that is completely separate from run-time programs, leading to the DML() language schema. This enrichment allows for specification and inference of significantly more precise type information, facilitating program error detection and compiler optimization. The primary contribution of the paper lies in our language design, which can effectively support the use of dependent types in practical programming. In particular, this design makes it both natural and straightforward to accommodate dependent types in the presence of effects such as references and exceptions.},
	number = {2},
	urldate = {2017-11-13},
	journal = {Journal of Functional Programming},
	author = {Xi, Hongwei},
	month = mar,
	year = {2007},
	pages = {215--286},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/FMM8TH8F/Xi - 2007 - Dependent ML An approach to practical programming .pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/3XTMU5CH/4A1FC643ACD49EF31DAF5EB955D2CCC7.html:text/html}
}

@inproceedings{beyer_relational_2006,
	address = {New York, NY, USA},
	series = {{ICSE} '06},
	title = {Relational {Programming} with {CrocoPat}},
	isbn = {978-1-59593-375-1},
	url = {http://doi.acm.org/10.1145/1134285.1134420},
	doi = {10.1145/1134285.1134420},
	abstract = {Many structural analyses of software systems are naturally formalized as relational queries, for example, the detection of design patterns, patterns of problematic design, code clones, dead code, and differences between the as-built and the as-designed architecture. This paper describes CrocoPat, an application-independent tool for relational programming. Through its efficiency and its expressive language, CrocoPat enables practically important analyses of real-world software systems that are not possible with other graph analysis tools, in particular analyses that involve transitive closures and the detection of patterns in graphs. The language is easy to use, because it is based on the well-known first-order predicate logic. The tool is easy to integrate into other software systems, because it is a small command-line tool that uses a simple text format for input and output of relations.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Beyer, Dirk},
	year = {2006},
	keywords = {pattern matching, BDD, graph models, predicate logic, relational algebra, software analysis, transitive closure},
	pages = {807--810},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/7D7EDKKI/Beyer - 2006 - Relational Programming with CrocoPat.pdf:application/pdf}
}

@inproceedings{albert_heap_2007,
	address = {New York, NY, USA},
	series = {{ISMM} '07},
	title = {Heap {Space} {Analysis} for {Java} {Bytecode}},
	isbn = {978-1-59593-893-0},
	url = {http://doi.acm.org/10.1145/1296907.1296922},
	doi = {10.1145/1296907.1296922},
	abstract = {This article presents a heap space analysis for (sequential) Java bytecode. The analysis generates heap space cost relations which define at compile-time the heap consumption of a program as a function of its data size. These relations can be used to obtain upper bounds on the heap space located during the execution of the different methods. In addition, we describe how to refine the cost relations, by relying on escape analysis, in order to take into account the heap space that can be safely deallocated by the garbage collector upon exit from a corresponding method. These refined cost relations are then used to infer upper bounds on the active heap space upon methods return. Example applications for the analysis consider inference of constant heap usage and heap usage proportional to the data size (including polynomial and exponential heap consumption). Our prototype implementation is reported and demonstrated by means of a series of examples which illustrate how the analysis naturally encompasses standard data-structures like lists, trees and arrays with several dimensions written in object-oriented programming style.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 6th {International} {Symposium} on {Memory} {Management}},
	publisher = {ACM},
	author = {Albert, Elvira and Genaim, Samir and Gomez-Zamalloa, Miguel},
	year = {2007},
	keywords = {Java bytecode, heap consumption, heap space analysis, low-level languages},
	pages = {105--116},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/MZG6ZL4H/Albert et al. - 2007 - Heap Space Analysis for Java Bytecode.pdf:application/pdf}
}

@inproceedings{binder_advanced_2007,
	address = {New York, NY, USA},
	series = {{PPPJ} '07},
	title = {Advanced {Java} {Bytecode} {Instrumentation}},
	isbn = {978-1-59593-672-1},
	url = {http://doi.acm.org/10.1145/1294325.1294344},
	doi = {10.1145/1294325.1294344},
	abstract = {Bytecode instrumentation is a valuable technique for transparently enhancing virtual execution environments for purposes such as monitoring or profiling. Current approaches to bytecode instrumentation either exclude some methods from instrumentation, severely restrict the ways certain methods may be instrumented, or require the use of native code. In this paper we compare different approaches to bytecode instrumentation in Java and come up with a novel instrumentation framework that goes beyond the aforementioned limitations. We evaluate our approach with an instrumentation for profiling which generates calling context trees of various platform-independent dynamic metrics.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 5th {International} {Symposium} on {Principles} and {Practice} of {Programming} in {Java}},
	publisher = {ACM},
	author = {Binder, Walter and Hulaas, Jarle and Moret, Philippe},
	year = {2007},
	keywords = {Java, dynamic bytecode instrumentation, dynamic metrics, JVM, profiling, program transformations},
	pages = {135--144},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/7FGNB9FA/Binder et al. - 2007 - Advanced Java Bytecode Instrumentation.pdf:application/pdf}
}

@book{blackburn_moxie_2008,
	title = {The {Moxie} {JVM} {Experience}},
	abstract = {By January 1998, only two years after the launch of the first Java virtual machine, almost all JVMs in use today had been architected. In the nine years since, technology has advanced enormously, with respect to the underlying hardware, language implementation, and in the application domain. Although JVM technology has moved forward in leaps and bounds, basic design decisions made in the 90{\textquoteright}s has anchored JVM implementation. The Moxie project set out to explore the question: {\textquoteleft}How would we design a JVM from scratch knowing what we know today?{\textquoteright} Amid the mass of design questions we faced, the tension between performance and flexibility was pervasive, persistent and problematic. In this experience paper we describe the Moxie project and its lessons, a process which began with consulting experts from industry and academia, and ended with a fully working prototype.},
	author = {Blackburn, Stephen M. and Salishev, Sergey I. and Danilov, Mikhail and Mokhovikov, Oleg A. and Nashatyrev, Anton A. and Novodvorsky, Peter A. and Bogdanov, Vadim I. and Li, Xiao Feng and Ushakov, Dennis},
	year = {2008},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/ALURTFGR/Blackburn et al. - The Moxie JVM Experience.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/AT4YQ53N/summary.html:text/html}
}

@inproceedings{mitchell_causes_2007,
	address = {New York, NY, USA},
	series = {{OOPSLA} '07},
	title = {The {Causes} of {Bloat}, the {Limits} of {Health}},
	isbn = {978-1-59593-786-5},
	url = {http://doi.acm.org/10.1145/1297027.1297046},
	doi = {10.1145/1297027.1297046},
	abstract = {Applications often have large runtime memory requirements. In some cases, large memory footprint helps accomplish an important functional, performance, or engineering requirement. A large cache,for example, may ameliorate a pernicious performance problem. In general, however, finding a good balance between memory consumption and other requirements is quite challenging. To do so, the development team must distinguish effective from excessive use of memory. We introduce health signatures to enable these distinctions. Using data from dozens of applications and benchmarks, we show that they provide concise and application-neutral summaries of footprint. We show how to use them to form value judgments about whether a design or implementation choice is good or bad. We show how being independent ofany application eases comparison across disparate implementations. We demonstrate the asymptotic nature of memory health: certain designsare limited in the health they can achieve, no matter how much the data size scales up. Finally, we show how to use health signatures to automatically generate formulas that predict this asymptotic behavior, and show how they enable powerful limit studies on memory health.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 22Nd {Annual} {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programming} {Systems} and {Applications}},
	publisher = {ACM},
	author = {Mitchell, Nick and Sevitsky, Gary},
	year = {2007},
	keywords = {bloat, limit studies, memory footprint, metrics},
	pages = {245--260},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/ZK76FBXJ/Mitchell and Sevitsky - 2007 - The Causes of Bloat, the Limits of Health.pdf:application/pdf}
}

@article{musuvathi_chess:_2007,
	title = {{CHESS}: {A} systematic testing tool for concurrent software},
	shorttitle = {{CHESS}},
	url = {https://www.microsoft.com/en-us/research/publication/chess-a-systematic-testing-tool-for-concurrent-software/},
	abstract = {Concurrency is used pervasively in the development of large systems programs. However, concurrent programming is difficult because of the possibility of unexpected interference among concurrently executing tasks. Such interference often results in {\textquotedblleft}Heisenbugs{\textquotedblright} that appear rarely and are extremely difficult to reproduce and debug. Stress testing, in which the system is run under heavy load {\textellipsis}},
	urldate = {2017-11-13},
	journal = {Microsoft Research},
	author = {Musuvathi, Madan and Qadeer, Shaz and Ball, Tom},
	month = nov,
	year = {2007},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/I8QIJRAL/Musuvathi et al. - 2007 - CHESS A systematic testing tool for concurrent so.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/K8SXA6EB/chess-a-systematic-testing-tool-for-concurrent-software.html:text/html}
}

@inproceedings{nethercote_valgrind:_2007,
	address = {New York, NY, USA},
	series = {{PLDI} '07},
	title = {Valgrind: {A} {Framework} for {Heavyweight} {Dynamic} {Binary} {Instrumentation}},
	isbn = {978-1-59593-633-2},
	shorttitle = {Valgrind},
	url = {http://doi.acm.org/10.1145/1250734.1250746},
	doi = {10.1145/1250734.1250746},
	abstract = {Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited. In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values-a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 28th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Nethercote, Nicholas and Seward, Julian},
	year = {2007},
	keywords = {dynamic binary analysis, dynamic binary instrumentation, Memcheck, shadow values, Valgrind},
	pages = {89--100},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/5QUIUZLW/Nethercote and Seward - 2007 - Valgrind A Framework for Heavyweight Dynamic Bina.pdf:application/pdf}
}

@inproceedings{phansalkar_analysis_2007,
	address = {New York, NY, USA},
	series = {{ISCA} '07},
	title = {Analysis of {Redundancy} and {Application} {Balance} in the {SPEC} {CPU}2006 {Benchmark} {Suite}},
	isbn = {978-1-59593-706-3},
	url = {http://doi.acm.org/10.1145/1250662.1250713},
	doi = {10.1145/1250662.1250713},
	abstract = {The recently released SPEC CPU2006 benchmark suite is expected to be used by computer designers and computer architecture researchers for pre-silicon early design analysis. Partial use of benchmark suites by researchers, due to simulation time constraints, compiler difficulties, or library or system call issues is likely to happen; but a random subset can lead to misleading results. This paper analyzes the SPEC CPU2006 benchmarks using performance counter based experimentation from several state of the art systems, and uses statistical techniques such as principal component analysis and clustering to draw inferences on the similarity of the benchmarks and the redundancy in the suite and arrive at meaningful subsets. The SPEC CPU2006 benchmark suite contains several programs from areas such as artificial intelligence and includes none from the electronic design automation (EDA) application area. Hence there is a concern on the application balance in the suite. An analysis from the perspective of fundamental program characteristics shows that the included programs offer characteristics broader than the EDA programs' space. A subset of 6 integer programs and 8 floating point programs can yield most of the information from the entire suite.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 34th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Phansalkar, Aashish and Joshi, Ajay and John, Lizy K.},
	year = {2007},
	keywords = {clustering, benchmark, SPEC, microprocessor performance counters},
	pages = {412--423},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/YLG5BRB8/Phansalkar et al. - 2007 - Analysis of Redundancy and Application Balance in .pdf:application/pdf}
}

@inproceedings{xi_dependently_2001,
	address = {New York, NY, USA},
	series = {{ICFP} '01},
	title = {A {Dependently} {Typed} {Assembly} {Language}},
	isbn = {978-1-58113-415-5},
	url = {http://doi.acm.org/10.1145/507635.507657},
	doi = {10.1145/507635.507657},
	abstract = {We present a dependently typed assembly language (DTAL) in which the type system supports the use of a restricted form of dependent types, reaping some benefits of dependent types at the assembly level. DTAL improves upon TAL , enabling certain important compiler optimizations such as run-time array bound check elimination and tag check elimination. Also, DTAL formally addresses the issue of representing sum types at assembly level, making it suitable for handling not only datatypes in ML but also dependent datatypes in Dependent ML (DML).},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the {Sixth} {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Xi, Hongwei and Harper, Robert},
	year = {2001},
	pages = {169--180},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DTSRVI37/Xi and Harper - 2001 - A Dependently Typed Assembly Language.pdf:application/pdf}
}

@inproceedings{jones_flexible_1982,
	address = {New York, NY, USA},
	series = {{POPL} '82},
	title = {A {Flexible} {Approach} to {Interprocedural} {Data} {Flow} {Analysis} and {Programs} with {Recursive} {Data} {Structures}},
	isbn = {978-0-89791-065-1},
	url = {http://doi.acm.org/10.1145/582153.582161},
	doi = {10.1145/582153.582161},
	abstract = {A new approach to data flow analysis of procedural programs and programs with recursive data structures is described. The method depends on simulation of the interpreter for the subject programming language using a retrieval function to approximate a program's data structures.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 9th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Jones, Neil D. and Muchnick, Steven S.},
	year = {1982},
	pages = {66--74},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/9LIQQXI8/Jones and Muchnick - 1982 - A Flexible Approach to Interprocedural Data Flow A.pdf:application/pdf}
}

@book{martin-lof_intuitionistic_1984,
	title = {Intuitionistic type theory},
	language = {en},
	publisher = {Bibliopolis},
	author = {Martin-L{\"o}f, Per},
	year = {1984},
	note = {Google-Books-ID: \_D0ZAQAAIAAJ},
	keywords = {Intuitionistic mathematics, Logic, Symbolic and mathematical, Mathematics / Logic, Proof theory, Set theory},
	file = {71779e2cd1423f6e4fd3f93f2ec668462f1f.pdf:/Users/luigi/work/zotero/storage/YJC8MIL8/71779e2cd1423f6e4fd3f93f2ec668462f1f.pdf:application/pdf}
}

@article{ferrante_program_1987,
	title = {The {Program} {Dependence} {Graph} and {Its} {Use} in {Optimization}},
	volume = {9},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/24039.24041},
	doi = {10.1145/24039.24041},
	abstract = {In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
	number = {3},
	urldate = {2017-11-13},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Ferrante, Jeanne and Ottenstein, Karl J. and Warren, Joe D.},
	month = jul,
	year = {1987},
	pages = {319--349},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/KKWRVLHT/Ferrante et al. - 1987 - The Program Dependence Graph and Its Use in Optimi.pdf:application/pdf}
}

@inproceedings{janzen_navigating_2003,
	address = {New York, NY, USA},
	series = {{AOSD} '03},
	title = {Navigating and {Querying} {Code} {Without} {Getting} {Lost}},
	isbn = {978-1-58113-660-9},
	url = {http://doi.acm.org/10.1145/643603.643622},
	doi = {10.1145/643603.643622},
	abstract = {A development task related to a crosscutting concern is challenging because a developer can easily get lost when exploring scattered elements of code and the complex tangle of relationships between them. In this paper we present a source browsing tool that improves the developer's ability to work with crosscutting concerns by providing better support for exploring code. Our tool helps the developer to remain oriented while exploring and navigating across a code base. The cognitive burden placed on a developer is reduced by avoiding disorienting view switches and by providing an explicit representation of the exploration process in terms of exploration paths. While our tool is generally useful, good navigation support is particularly important when exploring crosscutting concerns.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the 2Nd {International} {Conference} on {Aspect}-oriented {Software} {Development}},
	publisher = {ACM},
	author = {Janzen, Doug and De Volder, Kris},
	year = {2003},
	keywords = {proposal},
	pages = {178--187},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/QH8Y37R8/Janzen and De Volder - 2003 - Navigating and Querying Code Without Getting Lost.pdf:application/pdf}
}

@inproceedings{kerr_characterization_2009,
	title = {A characterization and analysis of {PTX} kernels},
	doi = {10.1109/IISWC.2009.5306801},
	abstract = {General purpose application development for GPUs (GPGPU) has recently gained momentum as a cost-effective approach for accelerating data- and compute-intensive applications. It has been driven by the introduction of C-based programming environments such as NVIDIA's CUDA, OpenCL, and Intel's Ct. While significant effort has been focused on developing and evaluating applications and software tools, comparatively little has been devoted to the analysis and characterization of applications to assist future work in compiler optimizations, application re-structuring, and micro-architecture design. This paper proposes a set of metrics for GPU workloads and uses these metrics to analyze the behavior of GPU programs. We report on an analysis of over 50 kernels and applications including the full NVIDIA CUDA SDK and UIUC's Parboil Benchmark Suite covering control flow, data flow, parallelism, and memory behavior. The analysis was performed using a full function emulator we developed that implements the NVIDIA virtual machine referred to as PTX (parallel thread execution architecture) - a machine model and low level virtual ISA that is representative of ISAs for data parallel execution. The emulator can execute compiled kernels from the CUDA compiler, currently supports the full PTX 1.4 specification, and has been validated against the full CUDA SDK. The results quantify the importance of optimizations such as those for branch reconvergence, the prevalance of sharing between threads, and highlights opportunities for additional parallelism.},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Kerr, A. and Diamos, G. and Yalamanchili, S.},
	month = oct,
	year = {2009},
	keywords = {Application software, Computer applications, Yarn, virtual machines, Kernel, Parallel processing, GPU, optimising compilers, Program processors, Software tools, virtual machine, proposal, Acceleration, application re-structuring, branch reconvergence, C language, C-based programming environment, compiler optimization, compute-intensive application, control flow, cost-effective approach, data flow, data flow analysis, data parallel execution, data-intensive application, full function emulator, general purpose application development, Instruction sets, low level virtual ISA, machine model, memory behavior, microarchitecture design, multi-threading, NVIDIA CUDA SDK, OpenCL, operating system kernels, parallel thread execution architecture, parallelism, program behavior analysis, programming environments, Programming environments, PTX kernel, software tool, UIUC Parboil Benchmark Suite},
	pages = {3--12},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/TLR2QB5N/5306801.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/LYFT8KBG/Kerr et al. - 2009 - A characterization and analysis of PTX kernels.pdf:application/pdf}
}

@inproceedings{mileva_mining_2009,
	address = {New York, NY, USA},
	series = {{IWPSE}-{Evol} '09},
	title = {Mining {Trends} of {Library} {Usage}},
	isbn = {978-1-60558-678-6},
	url = {http://doi.acm.org/10.1145/1595808.1595821},
	doi = {10.1145/1595808.1595821},
	abstract = {A library is available in multiple versions. Which one should I use? Has it been widely adopted already? Was it a good decision to switch to the newest version? We have mined hundreds of open-source projects for their library dependencies, and determined global trends in library usage. This wisdom of the crowds can be helpful for developers when deciding when to use which version of a library - by helping them avoid pitfalls experienced by other developers, and by showing important emerging trends in library usage.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the {Joint} {International} and {Annual} {ERCIM} {Workshops} on {Principles} of {Software} {Evolution} ({IWPSE}) and {Software} {Evolution} ({Evol}) {Workshops}},
	publisher = {ACM},
	author = {Mileva, Yana Momchilova and Dallmeier, Valentin and Burger, Martin and Zeller, Andreas},
	year = {2009},
	keywords = {global usage trends, library versions usage, mining software archives},
	pages = {57--62},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/Z3C7M8PS/Mileva et al. - 2009 - Mining Trends of Library Usage.pdf:application/pdf}
}

@inproceedings{nightingale_helios:_2009,
	address = {New York, NY, USA},
	series = {{SOSP} '09},
	title = {Helios: {Heterogeneous} {Multiprocessing} with {Satellite} {Kernels}},
	isbn = {978-1-60558-752-3},
	shorttitle = {Helios},
	url = {http://doi.acm.org/10.1145/1629575.1629597},
	doi = {10.1145/1629575.1629597},
	abstract = {Helios is an operating system designed to simplify the task of writing, deploying, and tuning applications for heterogeneous platforms. Helios introduces satellite kernels, which export a single, uniform set of OS abstractions across CPUs of disparate architectures and performance characteristics. Access to I/O services such as file systems are made transparent via remote message passing, which extends a standard microkernel message-passing abstraction to a satellite kernel infrastructure. Helios retargets applications to available ISAs by compiling from an intermediate language. To simplify deploying and tuning application performance, Helios exposes an affinity metric to developers. Affinity provides a hint to the operating system about whether a process would benefit from executing on the same platform as a service it depends upon. We developed satellite kernels for an XScale programmable I/O card and for cache-coherent NUMA architectures. We offloaded several applications and operating system components, often by changing only a single line of metadata. We show up to a 28\% performance improvement by offloading tasks to the XScale I/O card. On a mail-server benchmark, we show a 39\% improvement in performance by automatically splitting the application among multiple NUMA domains.},
	urldate = {2017-11-13},
	booktitle = {Proceedings of the {ACM} {SIGOPS} 22Nd {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {ACM},
	author = {Nightingale, Edmund B. and Hodson, Orion and McIlroy, Ross and Hawblitzel, Chris and Hunt, Galen},
	year = {2009},
	keywords = {operating systems, heterogeneous computing},
	pages = {221--234},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/7XJ4J8ZZ/Nightingale et al. - 2009 - Helios Heterogeneous Multiprocessing with Satelli.pdf:application/pdf}
}

@inproceedings{amin_java_2016,
	address = {New York, NY, USA},
	series = {{OOPSLA} 2016},
	title = {Java and {Scala}'s {Type} {Systems} {Are} {Unsound}: {The} {Existential} {Crisis} of {Null} {Pointers}},
	isbn = {978-1-4503-4444-9},
	shorttitle = {Java and {Scala}'s {Type} {Systems} {Are} {Unsound}},
	url = {http://doi.acm.org/10.1145/2983990.2984004},
	doi = {10.1145/2983990.2984004},
	abstract = {We present short programs that demonstrate the unsoundness of Java and Scala's current type systems. In particular, these programs provide parametrically polymorphic functions that can turn any type into any type without (down)casting. Fortunately, parametric polymorphism was not integrated into the Java Virtual Machine (JVM), so these examples do not demonstrate any unsoundness of the JVM. Nonetheless, we discuss broader implications of these findings on the field of programming languages.},
	urldate = {2017-11-15},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} {International} {Conference} on {Object}-{Oriented} {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Amin, Nada and Tate, Ross},
	year = {2016},
	keywords = {Java, Existential, Null, Scala, Unsoundness},
	pages = {838--848},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TQT6KYYP/Amin and Tate - 2016 - Java and Scala's Type Systems Are Unsound The Exi.pdf:application/pdf}
}

@inproceedings{lavazza_empirical_2016,
	address = {New York, NY, USA},
	series = {{SAC} '16},
	title = {An {Empirical} {Study} on the {Effect} of {Programming} {Languages} on {Productivity}},
	isbn = {978-1-4503-3739-7},
	url = {http://doi.acm.org/10.1145/2851613.2851780},
	doi = {10.1145/2851613.2851780},
	abstract = {Background -- Software development productivity is of great practical interest and has been widely investigated in the past. However, due to the rapid evolution of software development techniques and methods and the constant improvement in the use of existing ones, continuously updated evidence on productivity is constantly needed. Objectives -- The research documented in this paper has the main goal to identify how different programming languages may affect productivity. Method -- We analysed the ISBSG dataset, probably the largest public repository of data on software projects, with focus on the primary programming language used to develop each software project. We followed a rigorous statistical analysis approach. Moreover, we compared our analysis with the productivity data provided by Capers Jones in 1996 and 2013 and with an investigation on open-source software by Delorey et al. Results -- The implementation programming language of software projects seems to affect productivity. The comparison between the productivity level of each of the analysed programming languages shows important differences with the results by Capers Jones and Delorey et al. Conclusions --This paper provides some more evidence about how each programming language has its own productivity level and highlights some interesting divergences with the results reported by Capers Jones and Delorey et al.},
	urldate = {2017-11-15},
	booktitle = {Proceedings of the 31st {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Lavazza, Luigi and Morasca, Sandro and Tosi, Davide},
	year = {2016},
	keywords = {empirical study, programming languages, software productivity},
	pages = {1434--1439},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/A7APUC2P/Lavazza et al. - 2016 - An Empirical Study on the Effect of Programming La.pdf:application/pdf}
}

@inproceedings{hanenberg_why_2014,
	address = {New York, NY, USA},
	series = {{DLS} '14},
	title = {Why {Do} {We} {Know} {So} {Little} {About} {Programming} {Languages}, and {What} {Would} {Have} {Happened} if {We} {Had} {Known} {More}?},
	isbn = {978-1-4503-3211-8},
	url = {http://doi.acm.org/10.1145/2661088.2661102},
	doi = {10.1145/2661088.2661102},
	abstract = {Programming language research in the last decades was mainly driven by mathematical methods (such as formal semantics, correctness proofs, type soundness proofs, etc.) or run-time arguments based on benchmark tests. This happened despite the frequent discussion over programming language usability. We have now been through decade after decade of one language after another domainating the field, forcing companies to switch languages and migrate libraries. Now that Javascript seems to be the next language to dominate, people start to ask old questions anew. The first goal of this talk is to discuss why the application of empirical methods is (still) relatively rare in PL research, and to discuss what could be done in empirical methods to make them a substantial part of PL research. The second goal is to speculate about the possible effects that concrete empirical knowledge could have had on the programming language community. For example, what would have happened to programming languages if current knowledge would have been available 30 years ago? What if knowledge about programming languages from the year 2050 would be available today?},
	urldate = {2017-11-15},
	booktitle = {Proceedings of the 10th {ACM} {Symposium} on {Dynamic} {Languages}},
	publisher = {ACM},
	author = {Hanenberg, Stefan},
	year = {2014},
	keywords = {empirical studies, programming languages, controlled experiments},
	pages = {1--1},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/U3IAZ9NG/Hanenberg - 2014 - Why Do We Know So Little About Programming Languag.pdf:application/pdf}
}

@inproceedings{hanenberg_experiment_2010,
	address = {New York, NY, USA},
	series = {{OOPSLA} '10},
	title = {An {Experiment} {About} {Static} and {Dynamic} {Type} {Systems}: {Doubts} {About} the {Positive} {Impact} of {Static} {Type} {Systems} on {Development} {Time}},
	isbn = {978-1-4503-0203-6},
	shorttitle = {An {Experiment} {About} {Static} and {Dynamic} {Type} {Systems}},
	url = {http://doi.acm.org/10.1145/1869459.1869462},
	doi = {10.1145/1869459.1869462},
	abstract = {Although static type systems are an essential part in teach-ing and research in software engineering and computer science, there is hardly any knowledge about what the impact of static type systems on the development time or the resulting quality for a piece of software is. On the one hand there are authors that state that static type systems decrease an application's complexity and hence its development time (which means that the quality must be improved since developers have more time left in their projects). On the other hand there are authors that argue that static type systems increase development time (and hence decrease the code quality) since they restrict developers to express themselves in a desired way. This paper presents an empirical study with 49 subjects that studies the impact of a static type system for the development of a parser over 27 hours working time. In the experiments the existence of the static type system has neither a positive nor a negative impact on an application's development time (under the conditions of the experiment).},
	urldate = {2017-11-15},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {Hanenberg, Stefan},
	year = {2010},
	keywords = {empirical study, type systems, programming languages, dynamically typed languages},
	pages = {22--35},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/5X3ESV2L/Hanenberg - 2010 - An Experiment About Static and Dynamic Type System.pdf:application/pdf}
}

@inproceedings{latoza_answering_2008,
	address = {New York, NY, USA},
	series = {{OOPSLA} {Companion} '08},
	title = {Answering {Control} {Flow} {Questions} {About} {Code}},
	isbn = {978-1-60558-220-7},
	url = {http://doi.acm.org/10.1145/1449814.1449909},
	doi = {10.1145/1449814.1449909},
	abstract = {Empirical observations of developers editing code revealed that difficulties following control flow relationships led to poor changes, wasted time, and bugs. I am designing a static analysis to compute interprocedural path-sensitive control flow to help developers more quickly and accurately visually answer these common questions about code.},
	urldate = {2017-11-15},
	booktitle = {Companion to the 23rd {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {LaToza, Thomas D.},
	year = {2008},
	keywords = {empirical study, program comprehension, dataflow analysis},
	pages = {921--922},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/3HFACSWQ/LaToza - 2008 - Answering Control Flow Questions About Code.pdf:application/pdf}
}

@inproceedings{voinea_benefits_2016,
	address = {New York, NY, USA},
	series = {{PLATEAU} 2016},
	title = {Benefits of {Session} {Types} for {Software} {Development}},
	isbn = {978-1-4503-4638-2},
	url = {http://doi.acm.org/10.1145/3001878.3001883},
	doi = {10.1145/3001878.3001883},
	abstract = {Session types are a formalism used to specify and check the correctness of communication based systems. Within their scope, they can guarantee the absence of communication errors such as deadlock, sending an unexpected message or failing to handle an incoming message. Introduced over two decades ago, they have developed into a significant theme in programming languages. In this paper we examine the beliefs that drive research into this area and make it popular. We look at the claims and motivation behind session types throughout the literature. We identify the hypotheses upon which session types have been designed and implemented, and attempt to clarify and formulate them in a more suitable manner for testing.},
	urldate = {2017-11-15},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Evaluation} and {Usability} of {Programming} {Languages} and {Tools}},
	publisher = {ACM},
	author = {Voinea, A. Laura and Gay, Simon J.},
	year = {2016},
	keywords = {empirical studies, hypotheses, session types},
	pages = {26--29},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/H5AYDP4G/Voinea and Gay - 2016 - Benefits of Session Types for Software Development.pdf:application/pdf}
}

@inproceedings{salvaneschi_empirical_2014,
	address = {New York, NY, USA},
	series = {{FSE} 2014},
	title = {An {Empirical} {Study} on {Program} {Comprehension} with {Reactive} {Programming}},
	isbn = {978-1-4503-3056-5},
	url = {http://doi.acm.org/10.1145/2635868.2635895},
	doi = {10.1145/2635868.2635895},
	abstract = {Starting from the first investigations with strictly functional languages, reactive programming has been proposed as THE programming paradigm for reactive applications. The advantages of designs based on this style over designs based on the Observer design pattern have been studied for a long time. Over the years, researchers have enriched reactive languages with more powerful abstractions, embedded these abstractions into mainstream languages {\textendash} including object-oriented languages {\textendash} and applied reactive programming to several domains, like GUIs, animations, Web applications, robotics, and sensor networks. However, an important assumption behind this line of research {\textendash} that, beside other advantages, reactive programming makes a wide class of otherwise cumbersome applications more comprehensible {\textendash} has never been evaluated. In this paper, we present the design and the results of the first empirical study that evaluates the effect of reactive programming on comprehensibility compared to the traditional object-oriented style with the Observer design pattern. Results confirm the conjecture that comprehensibility is enhanced by reactive programming. In the experiment, the reactive programming group significantly outperforms the other group.},
	urldate = {2017-11-15},
	booktitle = {Proceedings of the 22Nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Salvaneschi, Guido and Amann, Sven and Proksch, Sebastian and Mezini, Mira},
	year = {2014},
	keywords = {Empirical Study, Controlled Experiment, Reactive Programming},
	pages = {564--575},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TWQ3GCBY/Salvaneschi et al. - 2014 - An Empirical Study on Program Comprehension with R.pdf:application/pdf}
}

@article{binkley_empirical_2007,
	title = {Empirical {Study} of {Optimization} {Techniques} for {Massive} {Slicing}},
	volume = {30},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/1290520.1290523},
	doi = {10.1145/1290520.1290523},
	abstract = {This article presents results from a study of techniques that improve the performance of graph-based interprocedural slicing of the System Dependence Graph (SDG). This is useful in {\textquotedblleft}massive slicing{\textquotedblright} where slices are required for many or all of the possible set of slicing criteria. Several different techniques are considered, including forming strongly connected components, topological sorting, and removing transitive edges. Data collected from a test bed of just over 1,000,000 lines of code are presented. This data illustrates the impact on computation time of the techniques. Together, the best combination produces a 71\% reduction in run-time (and a 64\% reduction in memory usage). The complete set of techniques also illustrates the point at which faster computation is not viable due to prohibitive preprocessing costs.},
	number = {1},
	urldate = {2017-11-15},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Binkley, David and Harman, Mark and Krinke, Jens},
	month = nov,
	year = {2007},
	keywords = {empirical study, internal representation, performance enhancement, Slicing},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/Y4TSYWTK/Binkley et al. - 2007 - Empirical Study of Optimization Techniques for Mas.pdf:application/pdf}
}

@inproceedings{gude_javascript:_2012,
	address = {New York, NY, USA},
	series = {{SPLASH} '12},
	title = {{JavaScript}: {The} {Used} {Parts}},
	isbn = {978-1-4503-1563-0},
	shorttitle = {{JavaScript}},
	url = {http://doi.acm.org/10.1145/2384716.2384762},
	doi = {10.1145/2384716.2384762},
	abstract = {We describe an empirical study to understand how JavaScript language features are used by the programmers. Our test corpus is larger than any previous work (more than 1 million scripts) and it attempts to target JS usage from various points of view. We describe the usage results of JS language features.},
	urldate = {2017-11-15},
	booktitle = {Proceedings of the 3rd {Annual} {Conference} on {Systems}, {Programming}, and {Applications}: {Software} for {Humanity}},
	publisher = {ACM},
	author = {Gude, Sharath Chowdary},
	year = {2012},
	keywords = {empirical study, javascript, variable scope},
	pages = {109--110},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/FQRPWRZF/Gude - 2012 - JavaScript The Used Parts.pdf:application/pdf}
}

@article{chen_empirical_2005,
	title = {An empirical study of programming language trends},
	volume = {22},
	issn = {0740-7459},
	doi = {10.1109/MS.2005.55},
	abstract = {Predicting software engineering trends is a strategically important asset for both developers and managers, but it's also difficult, due to the wide range of factors involved and the complexity of their interactions. This paper reveals some interesting trends and a method for studying other important software engineering trends. This article trades breadth for depth by focusing on a small, compact set of trends involving 17 high-level programming languages. We quantified many of their relevant factors, and then collected data on their evolution over 10 years. By applying statistical methods to this data, we aim to gain insight into what does and does not make a language successful.},
	number = {3},
	journal = {IEEE Software},
	author = {Chen, Yaofei and Dios, R. and Mili, A. and Wu, Lan and Wang, Kefei},
	month = may,
	year = {2005},
	keywords = {Java, high level languages, Programming profession, Computer languages, History, software engineering, Software engineering, programming languages, empirical software engineering, Engineering management, ISO standards, programming language trends, Software development management, software engineering trends, statistical analysis, statistical modeling},
	pages = {72--79},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/F7S9V39U/1438333.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/5CXE2YRH/Chen et al. - 2005 - An empirical study of programming language trends.pdf:application/pdf}
}

@inproceedings{nanz_design_2011,
	title = {Design of an {Empirical} {Study} for {Comparing} the {Usability} of {Concurrent} {Programming} {Languages}},
	doi = {10.1109/ESEM.2011.41},
	abstract = {The recent turn towards multicore processing architectures has made concurrency an important part of mainstream software development. As a result, an increasing number of developers have to learn to write concurrent programs, a task that is known to be hard even for the expert. Language designers are therefore working on languages that promise to make concurrent programming "easier". However, the claim that a new language is more usable than another cannot be supported by purely theoretical considerations, but calls for empirical studies. In this paper, we present the design of a study to compare concurrent programming languages with respect to comprehending and debugging existing programs and writing correct new programs. A critical challenge for such a study is avoiding the bias that might be introduced during the training phase and when interpreting participants' solutions. We address these issues by the use of self-study material and an evaluation scheme that exposes any subjective decisions of the corrector, or eliminates them altogether. We apply our design to a comparison of two object-oriented languages for concurrency, multithreaded Java and SCOOP (Simple Concurrent Object-Oriented Programming), in an academic setting. We obtain results in favor of SCOOP even though the study participants had previous training in writing multithreaded Java programs.},
	booktitle = {2011 {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	author = {Nanz, S. and Torshizi, F. and Pedroni, M. and Meyer, B.},
	month = sep,
	year = {2011},
	keywords = {Java, Concurrent computing, Programming, empirical study, concurrency, programming languages, Instruction sets, multi-threading, concurrent programming languages, language designers, mainstream software development, Materials, multicore processing architectures, multiprocessing systems, multithreaded Java, object oriented languages, object-oriented languages, simple concurrent object oriented programming, Synchronization, Training, usability},
	pages = {325--334},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/NJKH2G53/6092581.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/UECRDBYQ/Nanz et al. - 2011 - Design of an Empirical Study for Comparing the Usa.pdf:application/pdf}
}

@inproceedings{klint_rascal:_2009,
	title = {{RASCAL}: {A} {Domain} {Specific} {Language} for {Source} {Code} {Analysis} and {Manipulation}},
	shorttitle = {{RASCAL}},
	doi = {10.1109/SCAM.2009.28},
	abstract = {Many automated software engineering tools require tight integration of techniques for source code analysis and manipulation. State-of-the-art tools exist for both, but the domains have remained notoriously separate because different computational paradigms fit each domain best. This impedance mismatch hampers the development of new solutions because the desired functionality and scalability can only be achieved by repeated and ad hoc integration of different techniques. RASCAL is a domain-specific language that takes away most of this boilerplate by integrating source code analysis and manipulation at the conceptual, syntactic, semantic and technical level. We give an overview of the language and assess its merits by implementing a complex refactoring.},
	booktitle = {2009 {Ninth} {IEEE} {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation}},
	author = {Klint, P. and Storm, T. v d and Vinju, J.},
	month = sep,
	year = {2009},
	keywords = {Java, Libraries, program diagnostics, software maintenance, Informatics, Software engineering, domain specific language, object-oriented languages, ad hoc integration, automated software engineering tool, complex software refactoring, conceptual-syntactic-semantic-technical level, Domain specific languages, Impedance, impedance mismatch, Logic programming, meta-programming, Pattern matching, RASCAL, Scalability, source code analysis, source code manipulation, Storms, transformation},
	pages = {168--177},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/X8X2RWMI/5279910.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/KM9NZDJ2/Klint et al. - 2009 - RASCAL A Domain Specific Language for Source Code.pdf:application/pdf}
}

@inproceedings{filho_exceptions_2006,
	address = {New York, NY, USA},
	series = {{SIGSOFT} '06/{FSE}-14},
	title = {Exceptions and {Aspects}: {The} {Devil} is in the {Details}},
	isbn = {978-1-59593-468-0},
	shorttitle = {Exceptions and {Aspects}},
	url = {http://doi.acm.org/10.1145/1181775.1181794},
	doi = {10.1145/1181775.1181794},
	abstract = {It is usually assumed that the implementation of exception handling can be better modularized by the use of aspect-oriented programming (AOP). However, the trade-offs involved in using AOP with this goal are not well-understood. This paper presents an in-depth study of the adequacy of the AspectJ language for modularizing exception handling code. The study consisted in refactoring existing applications so that the code responsible for implementing heterogeneous error handling strategies was moved to separate aspects. We have performed quantitative assessments of four systems - three object-oriented and one aspect-oriented - based on four quality attributes, namely separation of concerns, coupling, cohesion, and conciseness. Our investigation also included a multi-perspective analysis of the refactored systems, including (i) the reusability of the aspectized error handling code, (ii) the beneficial and harmful aspectization scenarios, and (iii) the scalability of AOP to aspectize exception handling in the presence of other crosscutting concerns.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 14th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Filho, Fernando Castor and Cacho, Nelio and Figueiredo, Eduardo and Maranh{\~a}o, Raquel and Garcia, Alessandro and Rubira, Cec{\'i}lia Mary F.},
	year = {2006},
	keywords = {empirical studies, exception handling, aspectJ},
	pages = {152--162},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DTCA8M3X/Filho et al. - 2006 - Exceptions and Aspects The Devil is in the Detail.pdf:application/pdf}
}

@inproceedings{osman_exception_2017,
	address = {Piscataway, NJ, USA},
	series = {{MSR} '17},
	title = {Exception {Evolution} in {Long}-lived {Java} {Systems}},
	isbn = {978-1-5386-1544-7},
	url = {https://doi.org/10.1109/MSR.2017.21},
	doi = {10.1109/MSR.2017.21},
	abstract = {Exception handling allows developers to deal with abnormal situations that disrupt the execution flow of a program. There are mainly three types of exceptions: standard exceptions provided by the programming language itself, custom exceptions defined by the project developers, and third-party exceptions defined in external libraries. We conjecture that there are multiple factors that affect the use of these exception types. We perform an empirical study on long-lived Java projects to investigate these factors. In particular, we analyze how developers rely on the different types of exceptions in throw statements and exception handlers. We confirm that the domain, the type, and the development phase of a project affect the exception handling patterns. We observe that applications have significantly more error handling code than libraries and they increasingly rely on custom exceptions. Also, projects that belong to different domains have different preferences of exception types. For instance, content management systems rely more on custom exceptions than standard exceptions whereas the opposite is true in parsing frameworks.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {Osman, Haidar and Chi{\c s}, Andrei and Corrodi, Claudio and Ghafari, Mohammad and Nierstrasz, Oscar},
	year = {2017},
	keywords = {empirical study, exception handling, software evolution},
	pages = {302--311},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/LJPNZGWT/Osman et al. - 2017 - Exception Evolution in Long-lived Java Systems.pdf:application/pdf}
}

@inproceedings{dulaigh_measurement_2012,
	address = {Piscataway, NJ, USA},
	series = {{WEH} '12},
	title = {Measurement of {Exception}-handling {Code}: {An} {Exploratory} {Study}},
	isbn = {978-1-4673-1766-5},
	shorttitle = {Measurement of {Exception}-handling {Code}},
	url = {http://dl.acm.org/citation.cfm?id=2666990.2667003},
	abstract = {This paper presents some preliminary results from an empirical study of 12 Java applications from the Qualitas corpus. We measure the quantity and distribution of exception-handling constructs, and study their change as the systems evolve through several versions.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 5th {International} {Workshop} on {Exception} {Handling}},
	publisher = {IEEE Press},
	author = {D{\'u}laigh, Keith {\'O} and Power, James F. and Clarke, Peter J.},
	year = {2012},
	keywords = {software metrics, empirical software engineering, exception handling code},
	pages = {55--61},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/UIAWLLGE/D{\'u}laigh et al. - 2012 - Measurement of Exception-handling Code An Explora.pdf:application/pdf}
}

@inproceedings{cacho_trading_2014,
	address = {New York, NY, USA},
	series = {{ICSE} 2014},
	title = {Trading {Robustness} for {Maintainability}: {An} {Empirical} {Study} of {Evolving} {C}\# {Programs}},
	isbn = {978-1-4503-2756-5},
	shorttitle = {Trading {Robustness} for {Maintainability}},
	url = {http://doi.acm.org/10.1145/2568225.2568308},
	doi = {10.1145/2568225.2568308},
	abstract = {Mainstream programming languages provide built-in exception handling mechanisms to support robust and maintainable implementation of exception handling in software systems. Most of these modern languages, such as C\#, Ruby, Python and many others, are often claimed to have more appropriated exception handling mechanisms. They reduce programming constraints on exception handling to favor agile changes in the source code. These languages provide what we call maintenance-driven exception handling mechanisms. It is expected that the adoption of these mechanisms improve software maintainability without hindering software robustness. However, there is still little empirical knowledge about the impact that adopting these mechanisms have on software robustness. This paper addressed this gap by conducting an empirical study aimed at understanding the relationship between changes in C\# programs and their robustness. In particular, we evaluated how changes in the normal and exceptional code were related to exception handling faults. We applied a change impact analysis and a control flow analysis in 119 versions of 16 C\# programs. The results showed that: (i) most of the problems hindering software robustness in those programs are caused by changes in the normal code, (ii) many potential faults were introduced even when improving exception handling in C\# code, and (iii) faults are often facilitated by the maintenance-driven flexibility of the exception handling mechanism. Moreover, we present a series of change scenarios that decrease the program robustness.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Cacho, N{\'e}lio and C{\'e}sar, Thiago and Filipe, Thomas and Soares, Eliezio and Cassio, Arthur and Souza, Rafael and Garcia, Israel and Barbosa, Eiji Adachi and Garcia, Alessandro},
	year = {2014},
	keywords = {Exception handling, maintainability, robustness},
	pages = {584--595},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/VKBIQB4P/Cacho et al. - 2014 - Trading Robustness for Maintainability An Empiric.pdf:application/pdf}
}

@inproceedings{sena_integrated_2016,
	address = {New York, NY, USA},
	series = {{SAC} '16},
	title = {Integrated {Analysis} of {Exception} {Flows} and {Handler} {Actions} in {Java} {Libraries}: {An} {Empirical} {Study}},
	isbn = {978-1-4503-3739-7},
	shorttitle = {Integrated {Analysis} of {Exception} {Flows} and {Handler} {Actions} in {Java} {Libraries}},
	url = {http://doi.acm.org/10.1145/2851613.2851793},
	doi = {10.1145/2851613.2851793},
	abstract = {This paper presents an empirical study of exception handling strategies in Java libraries. The study conducts an integrated analysis of exception flows and handler actions from Java libraries with the aim to understand the impact of adopted exception handling strategies from the perspective of libraries' users. We extended an existing static analysis tool to identify exception flows in software libraries and collected data from the eight most downloaded Java libraries in Maven repository. After that, manual analysis was performed to categorize the tailored handler actions for each exception handler. Our results show that a high number of uncaught runtime exceptions and subsumption handlers were applied in some libraries. We also investigated the community anti-patterns implemented by handler actions in exception flows. Our results reveal the need to have automated support to allow the integrated analysis of exception flows and their handler actions explicitly.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 31st {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Sena, Dem{\'o}stenes and Coelho, Roberta and Kulesza, Uir{\'a}},
	year = {2016},
	keywords = {software libraries, empirical study, exception handling, static analysis},
	pages = {1520--1526},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/2MULLE48/Sena et al. - 2016 - Integrated Analysis of Exception Flows and Handler.pdf:application/pdf}
}

@inproceedings{shah_why_2008,
	address = {New York, NY, USA},
	series = {{WEH} '08},
	title = {Why {Do} {Developers} {Neglect} {Exception} {Handling}?},
	isbn = {978-1-60558-229-0},
	url = {http://doi.acm.org/10.1145/1454268.1454277},
	doi = {10.1145/1454268.1454277},
	abstract = {In this paper, we explore the problems associated with exception handling from a new dimension: the human. We designed a study that evaluates (1) different perspectives of software developers to understand how they perceive exception handling and what methods they adopt to deal with exception handling constructs, and (2) the usefulness of a visualization tool that we developed in previous work for exception handling. We describe the design of our study, present details about the study's participants, describe the interviews we conducted with the participants, present the results of the study, and discuss what we learned from the study. Based on our analysis, we suggest several future directions, including the proposal of a new role for the software-development process---exception engineer, who works closely with software developers throughout all phases of the software-development life cycle and who concentrates on the integration of exception handling into the core functionality of programs.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Exception} {Handling}},
	publisher = {ACM},
	author = {Shah, Hina and G{\"o}rg, Carsten and Harrold, Mary Jean},
	year = {2008},
	keywords = {exception handling, human aspects, interactive visualization, user studies},
	pages = {62--68},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/RDGG6SK9/Shah et al. - 2008 - Why Do Developers Neglect Exception Handling.pdf:application/pdf}
}

@inproceedings{marinescu_are_2011,
	address = {New York, NY, USA},
	series = {{IWPSE}-{EVOL} '11},
	title = {Are the {Classes} {That} {Use} {Exceptions} {Defect} {Prone}?},
	isbn = {978-1-4503-0848-9},
	url = {http://doi.acm.org/10.1145/2024445.2024456},
	doi = {10.1145/2024445.2024456},
	abstract = {Exception handling is a mechanism that highlights exceptional functionality of software systems. Currently many empirical studies point out that sometimes developers neglect exceptional functionality, minimizing its importance. In this paper we investigate if the design entities (classes) that use exceptions are more defect prone than the other classes. The results, based on analyzing three releases of Eclipse, show that indeed the classes that use exceptions are more defect prone than the other classes. Based on our results, developers are advertised to pay more attention to the way they handle exceptions.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 12th {International} {Workshop} on {Principles} of {Software} {Evolution} and the 7th {Annual} {ERCIM} {Workshop} on {Software} {Evolution}},
	publisher = {ACM},
	author = {Marinescu, Cristina},
	year = {2011},
	keywords = {defects, exceptions},
	pages = {56--60},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/JLSVXYVD/Marinescu - 2011 - Are the Classes That Use Exceptions Defect Prone.pdf:application/pdf}
}

@inproceedings{marinescu_detecting_2010,
	address = {New York, NY, USA},
	series = {{ESEM} '10},
	title = {Detecting {Missing} {Thrown} {Exceptions} in {Enterprise} {Systems}: {An} {Empirical} {Study}},
	isbn = {978-1-4503-0039-1},
	shorttitle = {Detecting {Missing} {Thrown} {Exceptions} in {Enterprise} {Systems}},
	url = {http://doi.acm.org/10.1145/1852786.1852861},
	doi = {10.1145/1852786.1852861},
	abstract = {Commonly enterprise systems are implemented using the object-oriented and relational paradigms, among which the communication is performed using various library methods for manipulating the persistent data. Most of the times the involved library methods throw different exceptions. An improper handling mechanism for these exceptions in the source code may bring different problems at runtime and hamper its maintenance. In this work we introduce an approach that automatically detects the methods from the source code which reveal an improper mechanism for handling exceptions involving database operations. The detected methods should be refactored in order to increase the reliability of the application, as well as its maintenance.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 2010 {ACM}-{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {ACM},
	author = {Marinescu, Cristina},
	year = {2010},
	keywords = {static analysis, software quality assessment},
	pages = {59:1--59:1},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/2M4KESMC/Marinescu - 2010 - Detecting Missing Thrown Exceptions in Enterprise .pdf:application/pdf}
}

@inproceedings{oliveira_exploratory_2016,
	address = {New York, NY, USA},
	series = {{SBES} '16},
	title = {An {Exploratory} {Study} of {Exception} {Handling} {Behavior} in {Evolving} {Android} and {Java} {Applications}},
	isbn = {978-1-4503-4201-8},
	url = {http://doi.acm.org/10.1145/2973839.2973843},
	doi = {10.1145/2973839.2973843},
	abstract = {Previous work has shown that robustness-related issues like functional errors and app crashes rank among the most common causes for complaints about mobile phone apps. Since most Android applications are written in Java, exception handling is the main mechanism they employ to report and handle errors, similarly to standard Java applications. Thus, the proper use of this mechanism is closely linked to app robustness. Nonetheless, to the best of our knowledge, no previous study analyzed the relationship between source code changes and uncaught exceptions, a common cause of bugs in Android apps, nor whether exception handling code in these apps evolves in the same way as in standard Java applications. This paper presents an empirical study aimed at understanding the relationship between changes in Android programs and their robustness and comparing the evolution of the exception handling code in Android and standard Java applications.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 30th {Brazilian} {Symposium} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Oliveira, Juliana and Cacho, Nelio and Borges, Deise and Silva, Thaisa and Castor, Fernando},
	year = {2016},
	keywords = {Java, Robustness, Android, Exception Handling},
	pages = {23--32},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/WB7EUK9A/Oliveira et al. - 2016 - An Exploratory Study of Exception Handling Behavio.pdf:application/pdf}
}

@inproceedings{sena_investigation_2015,
	address = {New York, NY, USA},
	series = {{SAC} '15},
	title = {An {Investigation} on the {Evolutionary} {Nature} of {Exception} {Handling} {Violations} in {Software} {Product} {Lines}},
	isbn = {978-1-4503-3196-8},
	url = {http://doi.acm.org/10.1145/2695664.2695933},
	doi = {10.1145/2695664.2695933},
	abstract = {The Exception Handling (EH) is a widely used mechanism for building robust systems and it is embedded in most of the mainstream programming languages. In the context of Software Product Lines (SPL), we can find exception handling code associated to common and variable features of an SPL. However, studies have shown that the exception handling code can also become a source of bugs which may affect the system the other way around, making it even less robust. This paper describes an empirical study whose goal was to investigate the evolutionary nature of exception handling violations. Can the exception handling behavior be preserved along SPL evolution scenarios? In order to carry out this study we extended a tool, called PLEA, which statically discovers the exception handling flows on SPLs (those implemented in Java and using annotation techniques), and it was executed over five to seven versions of two distinct SPLs. Our goal was to identify how the different kinds of evolution scenarios affected the exception handling policy initially defined. The results showed that the evolution of the SPLs exception handling code did not occur in a planned way in most of the scenarios. Consequently, it led to violations on the exception handling behavior. This paper points out the need of a set of EH behavior-preserving evolution scenarios for the SPLs exception handling code.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 30th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Sena, Dem{\'o}stenes and Coelho, Roberta and Kulesza, Uir{\'a}},
	year = {2015},
	keywords = {exception handling, software evolution, software product line},
	pages = {1616--1623},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/WGBUG7GD/Sena et al. - 2015 - An Investigation on the Evolutionary Nature of Exc.pdf:application/pdf}
}

@inproceedings{avgustinov_ql:_2016,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {{QL}: {Object}-oriented {Queries} on {Relational} {Data}},
	volume = {56},
	isbn = {978-3-95977-014-9},
	shorttitle = {{QL}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2016/6096},
	doi = {10.4230/LIPIcs.ECOOP.2016.2},
	urldate = {2017-11-16},
	booktitle = {30th {European} {Conference} on {Object}-{Oriented} {Programming} ({ECOOP} 2016)},
	publisher = {Schloss Dagstuhl{\textendash}Leibniz-Zentrum fuer Informatik},
	author = {Avgustinov, Pavel and Moor, Oege de and Jones, Michael Peyton and Sch{\"a}fer, Max},
	editor = {Krishnamurthi, Shriram and Lerner, Benjamin S.},
	year = {2016},
	keywords = {Datalog, Object orientation, prescriptive typing, query languages},
	pages = {2:1--2:25},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/X9UUZBNI/Avgustinov et al. - 2016 - QL Object-oriented Queries on Relational Data.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/SYHU6EMX/6096.html:text/html}
}

@techreport{antonioli_analysis_1998,
	title = {Analysis of the {Java} {Class} {File} {Format}},
	abstract = {The wide acceptance of Java as a network programing language has made the Java class file to one of the most popular, portable intermediate program representations. Such a representation must be as small as possible and still ideally support interpretation and code generation. This report presents the result of an analysis that examined a set of 4016 different class files for size and bytecode usage.},
	institution = {University of Zurich},
	author = {Antonioli, Denis N. and Pilz, Markus},
	year = {1998},
	file = {ifi-98.04.pdf:/Users/luigi/work/zotero/storage/D7ZGERIW/ifi-98.04.pdf:application/pdf}
}

@inproceedings{uesbeck_empirical_2016,
	address = {New York, NY, USA},
	series = {{ICSE} '16},
	title = {An {Empirical} {Study} on the {Impact} of {C}++ {Lambdas} and {Programmer} {Experience}},
	isbn = {978-1-4503-3900-1},
	url = {http://doi.acm.org/10.1145/2884781.2884849},
	doi = {10.1145/2884781.2884849},
	abstract = {Lambdas have seen increasing use in mainstream programming languages, notably in Java 8 and C++ 11. While the technical aspects of lambdas are known, we conducted the first randomized controlled trial on the human factors impact of C++ 11 lambdas compared to iterators. Because there has been recent debate on having students or professionals in experiments, we recruited undergraduates across the academic pipeline and professional programmers to evaluate these findings in a broader context. Results afford some doubt that lambdas benefit developers and show evidence that students are negatively impacted in regard to how quickly they can write correct programs to a test specification and whether they can complete a task. Analysis from log data shows that participants spent more time with compiler errors, and have more errors, when using lambdas as compared to iterators, suggesting difficulty with the syntax chosen for C++. Finally, experienced users were more likely to complete tasks, with or without lambdas, and could do so more quickly, with experience as a factor explaining 45.7\% of the variance in our sample in regard to completion time.},
	urldate = {2017-11-16},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Uesbeck, Phillip Merlin and Stefik, Andreas and Hanenberg, Stefan and Pedersen, Jan and Daleiden, Patrick},
	year = {2016},
	keywords = {C++11, human factors, lambda expressions},
	pages = {760--771},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/3WC9NQA8/Uesbeck et al. - 2016 - An Empirical Study on the Impact of C++ Lambdas an.pdf:application/pdf}
}

@inproceedings{hanenberg_faith_2010,
	address = {New York, NY, USA},
	series = {{OOPSLA} '10},
	title = {Faith, {Hope}, and {Love}: {An} {Essay} on {Software} {Science}'s {Neglect} of {Human} {Factors}},
	isbn = {978-1-4503-0203-6},
	shorttitle = {Faith, {Hope}, and {Love}},
	url = {http://doi.acm.org/10.1145/1869459.1869536},
	doi = {10.1145/1869459.1869536},
	abstract = {Research in the area of programming languages has different facets -- from formal reasoning about new programming language constructs (such as type soundness proofs for new type systems) over inventions of new abstractions, up to performance measurements of virtual machines. A closer look into the underlying research methods reveals a distressing characteristic of programming language research: developers, which are the main audience for new language constructs, are hardly considered in the research process. As a consequence, it is simply not possible to state whether a new construct that requires some kind of interaction with the developer has any positive impact on the construction of software. This paper argues for appropriate research methods in programming language research that rely on studies of developers -- and argues that the introduction of corresponding empirical methods not only requires a new understanding of research but also a different view on how to teach software science to students.},
	urldate = {2017-11-17},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {Hanenberg, Stefan},
	year = {2010},
	keywords = {software engineering, empirical research, programming language research, research methods},
	pages = {933--946},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/YQGMK2YV/Hanenberg - 2010 - Faith, Hope, and Love An Essay on Software Scienc.pdf:application/pdf}
}

@inproceedings{feigenspan_measuring_2012,
	title = {Measuring programming experience},
	doi = {10.1109/ICPC.2012.6240511},
	abstract = {Programming experience is an important confounding parameter in controlled experiments regarding program comprehension. In literature, ways to measure or control programming experience vary. Often, researchers neglect it or do not specify how they controlled it. We set out to find a well-defined understanding of programming experience and a way to measure it. From published comprehension experiments, we extracted questions that assess programming experience. In a controlled experiment, we compare the answers of 128 students to these questions with their performance in solving program-comprehension tasks. We found that self estimation seems to be a reliable way to measure programming experience. Furthermore, we applied exploratory factor analysis to extract a model of programming experience. With our analysis, we initiate a path toward measuring programming experience with a valid and reliable tool, so that we can control its influence on program comprehension.},
	booktitle = {2012 20th {IEEE} {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Feigenspan, J. and K{\"a}stner, C. and Liebig, J. and Apel, S. and Hanenberg, S.},
	month = jun,
	year = {2012},
	keywords = {Programming profession, reverse engineering, software engineering, Software engineering, Educational institutions, statistical analysis, Estimation, exploratory factor analysis, program-comprehension tasks, programming experience measurement, self estimation, Time factors},
	pages = {73--82},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/WHM8S8NF/6240511.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/KK94SATY/Feigenspan et al. - 2012 - Measuring programming experience.pdf:application/pdf}
}

@inproceedings{kleinschmager_static_2012,
	title = {Do static type systems improve the maintainability of software systems? {An} empirical study},
	shorttitle = {Do static type systems improve the maintainability of software systems?},
	doi = {10.1109/ICPC.2012.6240483},
	abstract = {Static type systems play an essential role in contemporary programming languages. Despite their importance, whether static type systems influence human software development capabilities remains an open question. One frequently mentioned argument for static type systems is that they improve the maintainability of software systems - an often used claim for which there is no empirical evidence. This paper describes an experiment which tests whether static type systems improve the maintainability of software systems. The results show rigorous empirical evidence that static type are indeed beneficial to these activities, except for fixing semantic errors.},
	booktitle = {2012 20th {IEEE} {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Kleinschmager, S. and Robbes, R. and Stefik, A. and Hanenberg, S. and Tanter, E.},
	month = jun,
	year = {2012},
	keywords = {Java, software maintenance, Programming, Software, programming languages, Semantics, Educational institutions, contemporary programming languages, software system maintainability, static type systems, Time measurement},
	pages = {153--162},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/NZEHSRGN/6240483.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/B5NN5NQP/Kleinschmager et al. - 2012 - Do static type systems improve the maintainability.pdf:application/pdf}
}

@inproceedings{fisher_next_2006,
	address = {New York, NY, USA},
	series = {{POPL} '06},
	title = {The {Next} 700 {Data} {Description} {Languages}},
	isbn = {978-1-59593-027-9},
	url = {http://doi.acm.org/10.1145/1111037.1111039},
	doi = {10.1145/1111037.1111039},
	abstract = {In the spirit of Landin, we present a calculus of dependent types to serve as the semantic foundation for a family of languages called data description languages. Such languages, which include pads, datascript, and packettypes, are designed to facilitate programming with ad hoc data, ie, data not in well-behaved relational or xml formats. In the calculus, each type describes the physical layout and semantic properties of a data source. In the semantics, we interpret types simultaneously as the in-memory representation of the data described and as parsers for the data source. The parsing functions are robust, automatically detecting and recording errors in the data stream without halting parsing. We show the parsers are type-correct, returning data whose type matches the simple-type interpretation of the specification. We also prove the parsers are "error-correct," accurately reporting the number of physical and semantic errors that occur in the returned data. We use the calculus to describe the features of various data description languages, and we discuss how we have used the calculus to improve PADS.},
	urldate = {2017-11-18},
	booktitle = {Conference {Record} of the 33rd {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Fisher, Kathleen and Mandelbaum, Yitzhak and Walker, David},
	year = {2006},
	keywords = {domain-specific languages, data description language, dependent types},
	pages = {2--15},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/3PQ96CVI/Fisher et al. - 2006 - The Next 700 Data Description Languages.pdf:application/pdf}
}

@inproceedings{hills_empirical_2013,
	address = {New York, NY, USA},
	series = {{ISSTA} 2013},
	title = {An {Empirical} {Study} of {PHP} {Feature} {Usage}: {A} {Static} {Analysis} {Perspective}},
	isbn = {978-1-4503-2159-4},
	shorttitle = {An {Empirical} {Study} of {PHP} {Feature} {Usage}},
	url = {http://doi.acm.org/10.1145/2483760.2483786},
	doi = {10.1145/2483760.2483786},
	abstract = {PHP is one of the most popular languages for server-side application development. The language is highly dynamic, providing programmers with a large amount of flexibility. However, these dynamic features also have a cost, making it difficult to apply traditional static analysis techniques used in standard code analysis and transformation tools. As part of our work on creating analysis tools for PHP, we have conducted a study over a significant corpus of open-source PHP systems, looking at the sizes of actual PHP programs, which features of PHP are actually used, how often dynamic features appear, and how distributed these features are across the files that make up a PHP website. We have also looked at whether uses of these dynamic features are truly dynamic or are, in some cases, statically understandable, allowing us to identify specific patterns of use which can then be taken into account to build more precise tools. We believe this work will be of interest to creators of analysis tools for PHP, and that the methodology we present can be leveraged for other dynamic languages with similar features.},
	urldate = {2017-11-19},
	booktitle = {Proceedings of the 2013 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Hills, Mark and Klint, Paul and Vinju, Jurgen},
	year = {2013},
	keywords = {Static analysis, Dynamic language features, PHP, Static metrics, Static program behavior},
	pages = {325--335},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/LD48V54M/Hills et al. - 2013 - An Empirical Study of PHP Feature Usage A Static .pdf:application/pdf}
}

@inproceedings{dahse_experience_2015,
	address = {New York, NY, USA},
	series = {{ISSTA} 2015},
	title = {Experience {Report}: {An} {Empirical} {Study} of {PHP} {Security} {Mechanism} {Usage}},
	isbn = {978-1-4503-3620-8},
	shorttitle = {Experience {Report}},
	url = {http://doi.acm.org/10.1145/2771783.2771787},
	doi = {10.1145/2771783.2771787},
	abstract = {The World Wide Web mainly consists of web applications written in weakly typed scripting languages, with PHP being the most popular language in practice. Empirical evidence based on the analysis of vulnerabilities suggests that security is often added as an ad-hoc solution, rather than planning a web application with security in mind during the design phase. Although some best-practice guidelines emerged, no comprehensive security standards are available for developers. Thus, developers often apply their own favorite security mechanisms for data sanitization or validation to prohibit malicious input to a web application. In the context of our development of a new static code analysis tool for vulnerability detection, we studied commonly used input sanitization or validation mechanisms in 25 popular PHP applications. Our analysis of 2.5 million lines of code and over 26 thousand secured data flows provides a comprehensive overview of how developers utilize security mechanisms in practice regarding different markup contexts. In this paper, we discuss these security mechanisms in detail and reveal common pitfalls. For example, we found certain markup contexts and security mechanisms more frequently vulnerable than others. Our empirical study helps researchers, web developers, and tool developers to focus on error-prone markup contexts and security mechanisms in order to detect and mitigate vulnerabilities.},
	urldate = {2017-11-19},
	booktitle = {Proceedings of the 2015 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Dahse, Johannes and Holz, Thorsten},
	year = {2015},
	keywords = {input validation, Static analysis, PHP, input sanitization},
	pages = {60--70},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GE7UPAPY/Dahse and Holz - 2015 - Experience Report An Empirical Study of PHP Secur.pdf:application/pdf}
}

@inproceedings{doyle_empirical_2011,
	title = {An {Empirical} {Study} of the {Evolution} of {PHP} {Web} {Application} {Security}},
	doi = {10.1109/Metrisec.2011.18},
	abstract = {Web applications are increasingly subject to mass attacks, with vulnerabilities found easily in both open source and commercial applications as evinced by the fact that approximately half of reported vulnerabilities are found in web applications. In this paper, we perform an empirical investigation of the evolution of vulnerabilities in fourteen of the most widely used open source PHP web applications, finding that vulnerabilities densities declined from 28.12 to 19.96 vulnerabilities per thousand lines of code from 2006 to 2010. We also investigate whether complexity metrics or a security resources indicator (SRI) metric can be used to identify vulnerable web application showing that average cyclomatic complexity is an effective predictor of vulnerability for several applications, especially for those with low SRI scores.},
	booktitle = {2011 {Third} {International} {Workshop} on {Security} {Measurements} and {Metrics}},
	author = {Doyle, M. and Walden, J.},
	month = sep,
	year = {2011},
	keywords = {Software, Internet, Security, static analysis, Aggregates, average cyclomatic complexity, code complexity, complexity metrics, Complexity theory, Encyclopedias, mass attacks, PHP Web application security, security metrics, security of data, security resources indicator metric, Software measurement, software security, vulnerabilities densities},
	pages = {11--20},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/F3LKFMJN/6165758.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/LM7HENYF/Doyle and Walden - 2011 - An Empirical Study of the Evolution of PHP Web App.pdf:application/pdf}
}

@inproceedings{chang_type_2017,
	address = {New York, NY, USA},
	series = {{POPL} 2017},
	title = {Type {Systems} {As} {Macros}},
	isbn = {978-1-4503-4660-3},
	url = {http://doi.acm.org/10.1145/3009837.3009886},
	doi = {10.1145/3009837.3009886},
	abstract = {We present Turnstile, a metalanguage for creating typed embedded languages. To implement the type system, programmers write type checking rules resembling traditional judgment syntax. To implement the semantics, they incorporate elaborations into these rules. Turnstile critically depends on the idea of linguistic reuse. It exploits a macro system in a novel way to simultaneously type check and rewrite a surface program into a target language. Reusing a macro system also yields modular implementations whose rules may be mixed and matched to create other languages. Combined with typical compiler and runtime reuse, Turnstile produces performant typed embedded languages with little effort.},
	urldate = {2017-11-20},
	booktitle = {Proceedings of the 44th {ACM} {SIGPLAN} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Chang, Stephen and Knauth, Alex and Greenman, Ben},
	year = {2017},
	keywords = {type systems, macros, typed embedded DSLs},
	pages = {694--705},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DCBB25YS/Chang et al. - 2017 - Type Systems As Macros.pdf:application/pdf}
}

@inproceedings{omar_safely_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Safely {Composable} {Type}-{Specific} {Languages}},
	isbn = {978-3-662-44201-2 978-3-662-44202-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-662-44202-9_5},
	doi = {10.1007/978-3-662-44202-9_5},
	abstract = {Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.},
	language = {en},
	urldate = {2017-11-20},
	booktitle = {{ECOOP} 2014 {\textendash} {Object}-{Oriented} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Omar, Cyrus and Kurilova, Darya and Nistor, Ligia and Chung, Benjamin and Potanin, Alex and Aldrich, Jonathan},
	month = jul,
	year = {2014},
	pages = {105--130},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/T6RGPYTC/Omar et al. - 2014 - Safely Composable Type-Specific Languages.pdf:application/pdf}
}

@article{amin_essence_2016,
	title = {The {Essence} of {Dependent} {Object} {Types}},
	url = {https://infoscience.epfl.ch/record/215280},
	doi = {10.1007/978-3-319-30936-1_14},
	urldate = {2017-11-20},
	journal = {A List of Successes That Can Change the World: Essays Dedicated to Philip Wadler on the Occasion of His 60th Birthday},
	author = {Amin, Nada and Gr{\"u}tter, Karl Samuel and Odersky, Martin and Rompf, Tiark and Stucki, Sandro},
	year = {2016},
	pages = {249--272},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/QLQTLW5M/Amin et al. - 2016 - The Essence of Dependent Object Types.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/USEJRHLV/215280.html:text/html}
}

@article{ramson_active_2017,
	title = {Active {Expressions}: {Basic} {Building} {Blocks} for {Reactive} {Programming}},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {Active {Expressions}},
	url = {http://programming-journal.org/2017/1/12/},
	doi = {10.22152/programming-journal.org/2017/1/12},
	abstract = {Modern software development without reactive programming is hard to imagine. Reactive programming favors a wide class of contemporary software systems that respond to user input, network messages, and other events. While reactive programming is an active field of research, the implementation of reactive concepts remains challenging. In particular, change detection represents a hard but inevitable necessity when implementing reactive concepts. Typically, change detection mechanisms are not intended for reuse but are tightly coupled to the particular change resolution mechanism. As a result, developers often have to re-implement similar abstractions. A reusable primitive for change detection is still missing. To find a suitable primitive, we identify commonalities in existing reactive concepts. We discover a class of reactive concepts, state-based reactive concepts. All state-based reactive concepts share a common change detection mechanism: they detect changes in the evaluation result of an expression. On the basis of the identified common change detection mechanism, we propose active expressions as a reusable primitive. By abstracting the tedious implementation details of change detection, active expressions can ease the implementation of reactive programming concepts. We evaluate the design of active expressions by re-implementing a number of existing state-based reactive concepts using them. The resulting implementations highlight the expressiveness of active expressions. Active expressions enable the separation of essential from non-essential parts when reasoning about reactive programming concepts. By using active expressions as a primitive for change detection, developers of reactive language constructs and runtime support can now focus on the design of how application programmers should be able to react to change. Ultimately, we would like active expressions to encourage experiments with novel reactive programming concepts and with that to yield a wider variety of them to explore.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Ramson, Stefan and Hirschfeld, Robert},
	month = apr,
	year = {2017},
	pages = {12:1--12:49},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/PXXTCSYY/Ramson and Hirschfeld - 2017 - Active Expressions Basic Building Blocks for Reac.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/NJ5VWP29/12.html:text/html}
}

@article{thompson_pragmatics_2017,
	title = {The pragmatics of clone detection and elimination},
	volume = {1},
	issn = {2473-7321},
	url = {http://programming-journal.org/2017/1/8/},
	doi = {10.22152/programming-journal.org/2017/1/8},
	abstract = {The occurrence of similar code, or `code clones{\textquoteright}, can make program code difficult to read, modify and maintain. This paper describes industrial case studies of clone detection and elimination using a refactoring and clone detection tool. We discuss how the studies have informed the design of the tool; more importantly, we use the studies to illustrate the complex set of decisions that have to be taken when performing clone elimination in practice. The case studies were performed in collaboration with engineers from Ericsson AB, and used the refactoring tool Wrangler for Erlang. However, the conclusions we draw are largely language-independent, and set out the pragmatics of clone detection and elimination in real-world projects as well as design principles for clone detection decision-support tools.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Thompson, Simon and Li, Huiqing and Schumacher, Andreas},
	month = apr,
	year = {2017},
	pages = {8:1--8:34},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/IEX34JE4/Thompson et al. - 2017 - The pragmatics of clone detection and elimination.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/K64KETG4/8.html:text/html}
}

@article{mattis_edit_2017,
	title = {Edit {Transactions}: {Dynamically} {Scoped} {Change} {Sets} for {Controlled} {Updates} in {Live} {Programming}},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {Edit {Transactions}},
	url = {http://programming-journal.org/2017/1/13/},
	doi = {10.22152/programming-journal.org/2017/1/13},
	abstract = {Live programming environments enable programmers to edit a running program and obtain immediate feedback on each individual change. The liveness quality is valued by programmers to help work in small steps and continuously add or correct small functionality while maintaining the impression of a direct connection between each edit and its manifestation at run-time. Such immediacy may conflict with the desire to perform a combined set of intermediate steps, such as a refactoring, without immediately taking effect after each individual edit. This becomes important when an incomplete sequence of small-scale changes can easily break the running program. State-of-the-art solutions focus on retroactive recovery mechanisms, such as debugging or version control. In contrast, we propose a proactive approach: Multiple individual changes to the program are collected in an Edit Transaction, which can be made effective if deemed complete. Upon activation, the combined steps become visible together. Edit Transactions are capable of dynamic scoping, allowing a set of changes to be tested in isolation before being extended to the running application. This enables a live programming workflow with full control over change granularity, immediate feedback on tests, delayed effect on the running application, and coarse-grained undos. We present an implementation of Edit Transactions along with Edit-Transaction-aware tools in Squeak/Smalltalk. We asses this implementation by conducting a case study with and without the new tool support, comparing programming activities, errors, and detours for implementing new functionality in a running simulation. We conclude that workflows using Edit Transactions have the potential to increase confidence in a change, reduce potential for run-time errors, and eventually make live programming more predictable and engaging.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Mattis, Toni and Rein, Patrick and Hirschfeld, Robert},
	month = apr,
	year = {2017},
	pages = {13:1--13:32},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/GYEZWF28/Mattis et al. - 2017 - Edit Transactions Dynamically Scoped Change Sets .pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/TII9S82Q/13.html:text/html}
}

@article{kafer_what_2017,
	title = {What {Is} the {Best} {Way} {For} {Developers} to {Learn} {New} {Software} {Tools}? {An} {Empirical} {Comparison} {Between} a {Text} and a {Video} {Tutorial}},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {What {Is} the {Best} {Way} {For} {Developers} to {Learn} {New} {Software} {Tools}?},
	url = {http://programming-journal.org/2017/1/17/},
	doi = {10.22152/programming-journal.org/2017/1/17},
	abstract = {The better developers can learn software tools, the faster they can start using them and the more efficiently they can later work with them. Tutorials are supposed to help here. While in the early days of computing, mostly text tutorials were available, nowadays software developers can choose among a huge number of tutorials for almost any popular software tool. However, only little research was conducted to understand how text tutorials differ from other tutorials, which tutorial types are preferred and, especially, which tutorial types yield the best learning experience in terms of efficiency and effectiveness, especially for programmers. To evaluate these questions, we converted an existing video tutorial for a novel software tool into a content-equivalent text tutorial. We then conducted an experiment in three groups where 42 undergraduate students from a software engineering course were commissioned to operate the software tool after using a tutorial: the first group was provided only with the video tutorial, the second group only with the text tutorial and the third group with both. In this context, the differences in terms of efficiency were almost negligible: We could observe that participants using only the text tutorial completed the tutorial faster than the participants with the video tutorial. However, the participants using only the video tutorial applied the learned content faster, achieving roughly the same bottom line performance. We also found that if both tutorial types are offered, participants prefer video tutorials for learning new content but text tutorials for looking up {\textquotedblleft}missed{\textquotedblright} information. We mainly gathered our data through questionnaires and screen recordings and analyzed it with suitable statistical hypotheses tests. The data is available at [12]. Since producing tutorials requires effort, knowing with which type of tutorial learnability can be increased to which extent has an immense practical relevance. We conclude that in contexts similar to ours, while it would be ideal if software tool makers would offer both tutorial types, it seems more efficient to produce only text tutorials instead of a passive video tutorial {\textendash} provided you manage to motivate your learners to use them.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {K{\"a}fer, Verena and Kulesz, Daniel and Wagner, Stefan},
	month = apr,
	year = {2017},
	pages = {17:1--17:41},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/SUCDIWK5/K{\"a}fer et al. - 2017 - What Is the Best Way For Developers to Learn New S.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/QAKYLGTR/17.html:text/html}
}

@article{murphy_analysis_2017,
	title = {An {Analysis} of {Introductory} {Programming} {Courses} at {UK} {Universities}},
	volume = {1},
	issn = {2473-7321},
	url = {http://programming-journal.org/2017/1/18/},
	doi = {10.22152/programming-journal.org/2017/1/18},
	abstract = {Context: In the context of exploring the art, science and engineering of programming, the question of which programming languages should be taught first has been fiercely debated since computer science teaching started in universities. Failure to grasp programming readily almost certainly implies failure to progress in computer science. Inquiry: What first programming languages are being taught? There have been regular national-scale surveys in Australia and New Zealand, with the only US survey reporting on a small subset of universities. This the first such national survey of universities in the UK. Approach: We report the results of the first survey of introductory programming courses (N=80) taught at UK universities as part of their first year computer science (or related) degree programmes, conducted in the first half of 2016. We report on student numbers, programming paradigm, programming languages and environment/tools used, as well as the underpinning rationale for these choices. Knowledge: The results in this first UK survey indicate a dominance of Java at a time when universities are still generally teaching students who are new to programming (and computer science), despite the fact that Python is perceived, by the same respondents, to be both easier to teach as well as to learn. Grounding: We compare the results of this survey with a related survey conducted since 2010 (as well as earlier surveys from 2001 and 2003) in Australia and New Zealand. Importance: This survey provides a starting point for valuable pedagogic baseline data for the analysis of the art, science and engineering of programming, in the context of substantial computer science curriculum reform in UK schools, as well as increasing scrutiny of teaching excellence and graduate employability for UK universities.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Murphy, Ellen and Crick, Tom and Davenport, James H.},
	month = apr,
	year = {2017},
	pages = {18:1--18:23},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/3CL7JWTV/Murphy et al. - 2017 - An Analysis of Introductory Programming Courses at.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/VGE3QZLL/18.html:text/html}
}

@article{lorenz_application_2017,
	title = {Application {Embedding}: {A} {Language} {Approach} to {Declarative} {Web} {Programming}},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {Application {Embedding}},
	url = {http://programming-journal.org/2017/1/2/},
	doi = {10.22152/programming-journal.org/2017/1/2},
	abstract = {Since the early days of the Web, web application developers have aspired to develop much of their applications declaratively. However, one aspect of the application, namely its business-logic is constantly left imperative. In this work we present Application Embedding, a novel approach to application development which allows all aspects of an application, including its business-logic, to be programmed declaratively. We develop this approach in a two-step process. First, we draw a mapping between web applications and Domain-Specific Languages (DSLs). Second, we note that out of the two methods for implementing DSLs, namely as either internal or external, most traditional web applications correspond to external DSLs, while the the technique that corresponds to DSL embedding (implementing internal DSLs) is left mostly unexplored. By projecting the well-known technique of DSL embedding onto web applications, we derive a novel technique{\textemdash}Application Embedding. Application embedding offers a separation of code assets that encourages reuse of imperative code, while keeping all application-specific assets, including those specifying its business- logic, declarative. As validation, we implemented a simple, though nontrivial web application using the proposed separation of assets. This implementation includes an application-agnostic imperative host application named FishTank, intended to be applicable for a wide variety of web applications, and a declarative definition of the different aspects of the specific application, intended to be loaded on that host. Our method of separation of code assets facilitates a better separation of work, in comparison to traditional methods. By this separation, host application developers can focus mostly on the extra-functional aspects of a web application, namely on improving performance, scalability, and availability, while developers of an embedded application can focus on the functional aspects of their application, without worrying about extra- functional concerns. The reusability of the host application makes the effort put into a better implementation cost-effective, since it can benefit all applications built on top of it.},
	language = {en},
	number = {1},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Lorenz, David H. and Rosenan, Boaz},
	month = jan,
	year = {2017},
	pages = {2:1--2:38},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/WHWMJ5TW/Lorenz and Rosenan - 2017 - Application Embedding A Language Approach to Decl.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/ZAWX9QLR/2.html:text/html}
}

@article{pickering_profunctor_2017,
	title = {Profunctor {Optics}: {Modular} {Data} {Accessors}},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {Profunctor {Optics}},
	url = {http://programming-journal.org/2017/1/7/},
	doi = {10.22152/programming-journal.org/2017/1/7},
	abstract = {CONTEXT: Data accessors allow one to read and write components of a data structure, such as the fields of a record, the variants of a union, or the elements of a container. These data accessors are collectively known as optics; they are fundamental to programs that manipulate complex data. INQUIRY: Individual data accessors for simple data structures are easy to write, for example as pairs of {\textquotedblleft}getter{\textquotedblright} and {\textquotedblleft}setter{\textquotedblright} methods. However, it is not obvious how to combine data accessors, in such a way that data accessors for a compound data structure are composed out of smaller data accessors for the parts of that structure. Generally, one has to write a sequence of statements or declarations that navigate step by step through the data structure, accessing one level at a time - which is to say, data accessors are traditionally not first-class citizens, combinable in their own right. APPROACH: We present a framework for modular data access, in which individual data accessors for simple data structures may be freely combined to obtain more complex data accessors for compound data structures. Data accessors become first-class citizens. The framework is based around the notion of profunctors, a flexible generalization of functions. KNOWLEDGE: The language features required are higher-order functions ({\textquotedblleft}lambdas{\textquotedblright} or {\textquotedblleft}closures{\textquotedblright}), parametrized types ({\textquotedblleft}generics{\textquotedblright} or {\textquotedblleft}abstract types{\textquotedblright}), and some mechanism for separating interfaces from implementations ({\textquotedblleft}abstract classes{\textquotedblright} or {\textquotedblleft}modules{\textquotedblright}). We use Haskell as a vehicle in which to present our constructions, but languages such as Java, C\#, or Scala that provide the necessary features should work just as well. GROUNDING: We provide implementations of all our constructions, in the form of a literate program: the manuscript file for the paper is also the source code for the program, and the extracted code is available separately for evaluation. We also prove the essential properties demonstrating that our profunctor-based representations are precisely equivalent to the more familiar concrete representations. IMPORTANCE: Our results should pave the way to simpler ways of writing programs that access the components of compound data structures.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Pickering, Matthew and Gibbons, Jeremy and Wu, Nicolas},
	month = apr,
	year = {2017},
	pages = {7:1--7:51},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/N4STAZ4W/Pickering et al. - 2017 - Profunctor Optics Modular Data Accessors.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/87XF5QIL/7.html:text/html}
}

@article{petricek_miscomputation_2017,
	title = {Miscomputation in software: {Learning} to live with errors},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {Miscomputation in software},
	url = {http://programming-journal.org/2017/1/14/},
	doi = {10.22152/programming-journal.org/2017/1/14},
	abstract = {Computer programs do not always work as expected. In fact, ominous warnings about the desperate state of the software industry continue to be released with almost ritualistic regularity. In this paper, we look at the 60 years history of programming and at the different practical methods that software community developed to live with programming errors. We do so by observing a class of students discussing different approaches to programming errors. While learning about the different methods for dealing with errors, we uncover basic assumptions that proponents of different paradigms follow. We learn about the mathematical attempt to eliminate errors through formal methods, scientific method based on testing, a way of building reliable systems through engineering methods, as well as an artistic approach to live coding that accepts errors as a creative inspiration. This way, we can explore the differences and similarities among the different paradigms. By inviting proponents of different methods into a single discussion, we hope to open potential for new thinking about errors. When should we use which of the approaches? And what can software development learn from mathematics, science, engineering and art? When programming or studying programming, we are often enclosed in small communities and we take our basic assumptions for granted. Through the discussion in this paper, we attempt to map the large and rich space of programming ideas and provide reference points for exploring, perhaps foreign, ideas that can challenge some of our assumptions.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Petricek, Tomas},
	month = apr,
	year = {2017},
	pages = {14:1--14:24},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/V43DKU87/Petricek - 2017 - Miscomputation in software Learning to live with .pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/8464QWWL/14.html:text/html}
}

@article{cazzola_open_2017,
	title = {Open {Programming} {Language} {Interpreters}},
	volume = {1},
	issn = {2473-7321},
	url = {http://programming-journal.org/2017/1/5/},
	doi = {10.22152/programming-journal.org/2017/1/5},
	abstract = {Context: This paper presents the concept of open programming language interpreters and the implementation of a framework-level metaobject protocol (MOP) to support them. Inquiry: We address the problem of dynamic interpreter adaptation to tailor the interpreter{\textquoteright}s behavior on the task to be solved and to introduce new features to fulfill unforeseen requirements. Many languages provide a MOP that to some degree supports reflection. However, MOPs are typically language-specific, their reflective functionality is often restricted, and the adaptation and application logic are often mixed which hardens the understanding and maintenance of the source code. Our system overcomes these limitations. Approach: We designed and implemented a system to support open programming language interpreters. The prototype implementation is integrated in the Neverlang framework. The system exposes the structure, behavior and the runtime state of any Neverlang-based interpreter with the ability to modify it. Knowledge: Our system provides a complete control over interpreter{\textquoteright}s structure, behavior and its runtime state. The approach is applicable to every Neverlang-based interpreter. Adaptation code can potentially be reused across different language implementations. Grounding: Having a prototype implementation we focused on feasibility evaluation. The paper shows that our approach well addresses problems commonly found in the research literature. We have a demonstrative video and examples that illustrate our approach on dynamic software adaptation, aspect-oriented programming, debugging and context-aware interpreters Importance: To our knowledge, our paper presents the first reflective approach targeting a general framework for language development. Our system provides full reflective support for free to any Neverlang-based interpreter. We are not aware of any prior application of open implementations to programming language interpreters in the sense defined in this paper. Rather than substituting other approaches, we believe our system can be used as a complementary technique in situations where other approaches present serious limitations.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Cazzola, Walter and Shaqiri, Albert},
	month = apr,
	year = {2017},
	pages = {5:1--5:34},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/AV6WM6M6/Cazzola and Shaqiri - 2017 - Open Programming Language Interpreters.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/MKMJP9IJ/5.html:text/html}
}

@article{hartel_interconnected_2017,
	title = {Interconnected {Linguistic} {Architecture}},
	volume = {1},
	issn = {2473-7321},
	url = {http://programming-journal.org/2017/1/3/},
	doi = {10.22152/programming-journal.org/2017/1/3},
	abstract = {The context of the reported research is the documentation of software technologies such as object/relational mappers, web-application frameworks, or code generators. We assume that documentation should model a macroscopic view on usage scenarios of technologies in terms of involved artifacts, leveraged software languages, data flows, conformance relationships, and others. In previous work, we referred to such documentation also as {\textquoteleft}linguistic architecture{\textquoteright}. The corresponding models may also be referred to as {\textquoteleft}megamodels{\textquoteright} while adopting this term from the technological space of modeling/model-driven engineering. This work is an inquiry into making such documentation less abstract and more effective by means of connecting (mega)models, systems, and developer experience in several ways. To this end, we adopt an approach that is primarily based on prototyping (i.e., implementa- tion of a megamodeling infrastructure with all conceivable connections) and experimentation with showcases (i.e., documentation of concrete software technologies). The knowledge gained by this research is a notion of interconnected linguistic architecture on the grounds of connecting primary model elements, inferred model elements, static and runtime system artifacts, traceability links, system contexts, knowledge resources, plugged interpretations of model elements, and IDE views. A corresponding suite of aspects of interconnected linguistic architecture is systematically described. As to the grounding of this research, we describe a literature survey which tracks scattered occurrences and thus demonstrates the relevance of the identified aspects of interconnected linguistic architecture. Further, we describe the MegaL/Xtext+IDE infrastructure which realizes interconnected linguistic architecture. The importance of this work lies in providing more formal (ontologically rich, navigable, verifiable) documentation of software technologies helping developers to better understand how to use technologies in new systems (prescriptive mode) or how technologies are used in existing systems (descriptive mode).},
	language = {en},
	number = {1},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {H{\"a}rtel, Johannes and H{\"a}rtel, Lukas and L{\"a}mmel, Ralf and Varanovich, Andrei and Heinz, Marcel},
	month = jan,
	year = {2017},
	pages = {3:1--3:27},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/BUQUPMLR/H{\"a}rtel et al. - 2017 - Interconnected Linguistic Architecture.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/ENQ63Y52/3.html:text/html}
}

@article{lammel_relationship_2017,
	title = {Relationship {Maintenance} in {Software} {Language} {Repositories}},
	volume = {1},
	issn = {2473-7321},
	url = {http://programming-journal.org/2017/1/4/},
	doi = {10.22152/programming-journal.org/2017/1/4},
	abstract = {The context of this research is testing and building software systems and, specifically, software language repositories (SLRs), i.e., repositories with components for language processing (interpreters, translators, analyzers, transformers, pretty printers, etc.). SLRs are typically set up for developing and using metaprogramming systems, language workbenches, language definition frameworks, executable semantic frameworks, and modeling frameworks. This work is an inquiry into testing and building SLRs in a manner that the repository is seen as a collection of language-typed artifacts being related by the applications of language-typed functions or relations which serve language processing. The notion of language is used in a broad sense to include text-, tree-, graph-based languages as well as representations based on interchange formats and also proprietary formats for serialization. The overall approach underlying this research is one of language design driven by a complex case study, i.e., a specific SLR with a significant number of processed languages and language processors as well as a noteworthy heterogeneity in terms of representation types and implementation languages. The knowledge gained by our research is best understood as a declarative language design for regression testing and build management; we introduce a corresponding language Ueber with an executable semantics which maintains relationships between language-typed artifacts in an SLR. The grounding of the reported research is based on the comprehensive, formal, executable (logic programming-based) definition of the Ueber language and its systematic application to the management of the SLR YAS which consists of hundreds of language definition and processing components (such as interpreters and transformations) for more than thirty languages (not counting different representation types) with Prolog, Haskell, Java, and Python being used as implementation languages. The importance of this work follows from the significant costs implied by regression testing and build management and also from the complexity of SLRs which calls for means to help with understanding.},
	language = {en},
	number = {1},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {L{\"a}mmel, Ralf},
	month = jan,
	year = {2017},
	pages = {4:1--4:27},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/5FRKCSSL/L{\"a}mmel - 2017 - Relationship Maintenance in Software Language Repo.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/9ZCN5EKJ/4.html:text/html}
}

@article{arya_transition_2017,
	title = {Transition {Watchpoints}: {Teaching} {Old} {Debuggers} {New} {Tricks}},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {Transition {Watchpoints}},
	url = {http://programming-journal.org/2017/1/16/},
	doi = {10.22152/programming-journal.org/2017/1/16},
	abstract = {Reversible debuggers and process replay have been developed at least since 1970. This vision enables one to execute backwards in time under a debugger. Two important problems in practice are that, first, current reversible debuggers are slow when reversing over long time periods, and, second, after building one reversible debugger, it is difficult to transfer that achievement to a new programming environment. The user observes a bug when arriving at an error. Searching backwards for the correspond- ing fault may require many reverse steps. Ultimately, the user prefers to write an expression that will transition to false upon arriving at the fault. The solution is an expression-transition watchpoint facility based on top of snapshots and record/replay. Expression-transition watch- points are implemented as binary search through the timeline of a program execution, while using the snapshots as landmarks within that timeline. This allows for debugging of subtle bugs that appear only after minutes or more of program execution. When a bug occurs within seconds of program startup, repeated debugging sessions suffice. Reversible debugging is preferred for bugs seen only after minutes. This architecture allows for an efficient and easy-to-write snapshot-based reversibe debugger on top of a conventional debugger. The validity of this approach was tested by developing four personalities (for GDB, MATLAB, Perl, and Python), with each personality typically requiring just 100 lines of code.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Arya, Kapil and Denniston, Tyler and Rabkin, Ariel and Cooperman, Gene},
	month = apr,
	year = {2017},
	pages = {16:1--16:28},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/2RWGKDWF/Arya et al. - 2017 - Transition Watchpoints Teaching Old Debuggers New.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/887MFV47/16.html:text/html}
}

@article{iosif-lazar_effective_2017,
	title = {Effective {Analysis} of {C} {Programs} by {Rewriting} {Variability}},
	volume = {1},
	issn = {2473-7321},
	url = {http://programming-journal.org/2017/1/1/},
	doi = {10.22152/programming-journal.org/2017/1/1},
	abstract = {Context. Variability-intensive programs (program families) appear in many application areas and for many reasons today. Different family members, called variants, are derived by switching statically configurable options (features) on and off, while reuse of the common code is maximized. Inquiry. Verification of program families is challenging since the number of variants is exponential in the number of features. Existing single-program analysis and verification tools cannot be applied directly to program families, and designing and implementing the corresponding variability-aware versions is tedious and laborious. Approach. In this work, we propose a range of variability-related transformations for translating program families into single programs by replacing compile-time variability with run-time variability (non-determinism). The obtained transformed programs can be subsequently analyzed using the conventional off- the-shelf single-program analysis tools such as type checkers, symbolic executors, model checkers, and static analyzers. Knowledge. Our variability-related transformations are outcome-preserving, which means that the relation between the outcomes in the transformed single program and the union of outcomes of all variants derived from the original program family is equality. Grounding. We show our transformation rules and their correctness with respect to a minimal core imperative language IMP. Then, we discuss our experience of implementing and using the transformations for efficient and effective analysis and verification of real-world C program families. Importance. We report some interesting variability-related bugs that we discovered using various state-of-the-art single-program C verification tools, such as Frama-C, Clang, LLBMC.},
	language = {en},
	number = {1},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Iosif-Lazar, Alexandru Florin and Melo, Jean and Dimovski, Aleksandar S. and Brabrand, Claus and Wasowski, Andrzej},
	month = jan,
	year = {2017},
	pages = {1:1--1:25},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/5RG3G9PC/Iosif-Lazar et al. - 2017 - Effective Analysis of C Programs by Rewriting Vari.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/MQLYJP7F/1.html:text/html}
}

@article{serang_triot:_2017,
	title = {{TRIOT}: {Faster} tensor manipulation in {C}++11},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {{TRIOT}},
	url = {http://programming-journal.org/2017/1/6/},
	doi = {10.22152/programming-journal.org/2017/1/6},
	abstract = {Context: Multidimensional arrays are used by many different algorithms. As such, indexing and broadcasting complex operations over multidimensional arrays are ubiquitous tasks and can be performance limiting. Inquiry: Simultaneously indexing two or more multidimensional arrays with different shapes (e.g., copying data from one tensor to another larger, zero padded tensor in anticipation of a convolution) is difficult to do efficiently: Hard-coded nested for loops in C, Fortran, and Go cannot be applied when the dimension of a tensor is unknown at compile time. Likewise, boost::multi\_array cannot be used unless the dimensions of the array are known at compile time, and the style of implementation restricts the user from using the index tuple inside a vectorized operation (as would be required to compute an expected value of a multidimensional distribution). On the other hand, iteration methods that do not require the dimensionality or shape to be known at compile time (e.g., incrementing and applying carry operations to index tuples or remapping integer indices in the flat array), can be substantially slower than hard-coded nested for loops. Approach: Using a few tasks that broadcast operations over multidimensional arrays, several existing methods are compared: hard-coded nested for loops in C and Go, vectorized operations in Fortran, boost::multi\_array, numpy, tuple incrementing, and integer reindexing. Knowledge: A new approach to this problem, ``template-recursive iteration over tensors{\textquoteright}{\textquoteright} (TRIOT), is proposed. This new method, which is made possible by features of C++11, can be used when the dimensions of the tensors are unknown at compile time. Furthermore, the proposed method can access the index tuple inside the vectorized operations, permitting much more flexible operations. Grounding: Runtimes of all methods are compared, and demonstrate that the proposed TRIOT method is competitive with both hard-coded nested for loops in C and Go, as well as vectorized operations in Fortran, despite not knowing the dimensions at compile time. TRIOT outperforms boost::multi\_array, numpy, tuple incrementing, and integer reindexing. Importance: Manipulation of multidimensional arrays is a common task in software, especially in high-performance numerical methods. This paper proposes a novel way to leverage template recursion to iterate over and apply operations to multidimensional arrays, and then demonstrates the superior performance and flexibility of operations that can be achieved using this new approach.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Serang, Oliver and Heyl, Florian},
	month = apr,
	year = {2017},
	pages = {6:1--6:24},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/RQCBEQHA/Serang and Heyl - 2017 - TRIOT Faster tensor manipulation in C++11.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/449ZN2CU/6.html:text/html}
}

@article{ichikawa_user-defined_2017,
	title = {User-{Defined} {Operators} {Including} {Name} {Binding} for {New} {Language} {Constructs}},
	volume = {1},
	issn = {2473-7321},
	url = {http://programming-journal.org/2017/1/15/},
	doi = {10.22152/programming-journal.org/2017/1/15},
	abstract = {User-defined syntax extensions are useful to implement an embedded domain specific language (EDSL) with good code-readability. They allow EDSL authors to define domain-natural notation, which is often different from the host language syntax. Nowadays, there are several research works of powerful user-defined syntax extensions. One promising approach uses user-defined operators. A user-defined operator is a function with user-defined syntax. It can be regarded as a syntax extension implemented without macros. An advantage of user-defined operators is that an operator can be statically typed. The compiler can find type errors in the definition of an operator before the operator is used. In addition, the compiler can resolve syntactic ambiguities by using static types. However, user-defined operators are difficult to implement language constructs involving static name binding. Name binding is association between names and values (or memory locations). Our inquiry is whether we can design a system for user-defined operators involving a new custom name binding. This paper proposes a module system for user-defined operators named a dsl class. A dsl class is similar to a normal class in Java but it contains operators instead of methods. We use operators for implementing custom name binding. For example, we use a nullary operator for emulating a variable name. An instance of a dsl class, called a dsl object, reifies an environment that expresses name binding. Programmers can control a scope of instance operators by specifying where the dsl object is active. We extend the host type system so that it can express the activation of a dsl object. In our system, a bound name is propagated through a type parameter to a dsl object. This enables us to implement user-defined language constructs involving static name binding. A contribution of this paper is that we reveal we can integrate a system for managing names and their scopes with a module and type system of an object-oriented language like Java. This allows us to implement a proposed system by adopting eager disambiguation based on expected types so that the compilation time will be acceptable. Eager disambiguation, which prunes out semantically invalid abstract parsing trees (ASTs) while a parser is running, is needed because the parser may generate a huge number of potentially valid ASTs for the same source code. We have implemented ProteaJ2, which is a programming language based on Java and it supports our proposal. We describe a parsing method that adopts eager disambiguation for fast parsing and discuss its time complexity. To show the practicality of our proposal, we have conducted two micro benchmarks to see the performance of our compiler. We also show several use cases of dsl classes for demonstrating dsl classes can express various language constructs. Our ultimate goal is to let programmers add any kind of new language construct to a host language. To do this, programmers should be able to define new syntax, name binding, and type system within the host language. This paper shows programmers can define the former two: their own syntax and name binding.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Ichikawa, Kazuhiro and Chiba, Shigeru},
	month = apr,
	year = {2017},
	pages = {15:1--15:25},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/GQAUAFN3/Ichikawa and Chiba - 2017 - User-Defined Operators Including Name Binding for .pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/Q46UTBRL/15.html:text/html}
}

@article{erdweg_module-system_2017,
	title = {A {Module}-{System} {Discipline} for {Model}-{Driven} {Software} {Development}},
	volume = {1},
	issn = {2473-7321},
	url = {http://programming-journal.org/2017/1/9/},
	doi = {10.22152/programming-journal.org/2017/1/9},
	abstract = {Model-driven development is a pragmatic approach to software development that embraces domain-specific languages (DSLs), where models correspond to DSL programs. A distinguishing feature of model-driven development is that clients of a model can select from an open set of alternative semantics of the model by applying different model transformation. However, in existing model-driven frameworks, dependencies between models, model transformations, and generated code artifacts are either implicit or globally declared in build scripts, which impedes modular reasoning, separate compilation, and programmability in general. We propose the design of a new module system that incorporates models and model transformations as modules. A programmer can apply transformations in import statements, thus declaring a dependency on generated code artifacts. Our design enables modular reasoning and separate compilation by preventing hidden dependencies, and it supports mixing modeling artifacts with conventional code artifacts as well as higher-order transformations. We have formalized our design and the aforementioned properties and have validated it by an implementation and case studies that show that our module system successfully integrates model-driven development into conventional programming languages.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Erdweg, Sebastian and Ostermann, Klaus},
	month = apr,
	year = {2017},
	pages = {9:1--9:28},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/98IHWUK8/Erdweg and Ostermann - 2017 - A Module-System Discipline for Model-Driven Softwa.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/J6F3HT8R/9.html:text/html}
}

@article{hadas_language_2017,
	title = {Language {Oriented} {Modularity}: {From} {Theory} to {Practice}},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {Language {Oriented} {Modularity}},
	url = {http://programming-journal.org/2017/1/10/},
	doi = {10.22152/programming-journal.org/2017/1/10},
	abstract = {Language-oriented modularity (LOM) is a methodology that complements language-oriented programming (LOP) in providing on-demand language abstraction solutions during software development. It involves the implementation and immediate utilization of domain-specific languages (DSLs) that are also aspect-oriented (DSALs). However, while DSL development is affordable thanks to modern language workbenches, DSAL development lacks similar tool support. Consequently, LOM is often impractical and underutilized. The challenge we address is making the complexity of DSAL implementation comparable to that of DSLs and the effectiveness of programming with DSALs comparable to that of general-purpose aspect languages (GPALs). Today, despite being essentially both domain-specific and aspect-oriented, DSALs seem to be second-class. Aspect development tools (e.g., AJDT) do not work on DSAL code. DSL development tools like language workbenches (e.g., Spoofax) neither deal with the backend weaving nor handle the composition of DSALs. DSAL composition frameworks (e.g., Awesome) do not provide frontend development tools. DSAL code transformation approaches (e.g., XAspects) do not preserve the semantics of DSAL programs in the presence of other aspect languages. We extend AspectJ with a small set of annotations and interfaces that allows DSAL designers to define a semantic-preserving transformation to AspectJ and interface with AspectJ tools. Our transformation approach enables the use of standard language workbench to implement DSALs and use of standard aspect development tools to program with those DSALs. As a result, DSALs regain first-class status with respect to both DSLs and aspect languages. This, on the one hand, lowers the cost of developing DSALs to the level of DSLs and, on the other hand, raises the effectiveness of using a DSAL to the level of a GPAL. Consequently, LOM becomes cost-effective compared to the LOP baseline. We modified the ajc compiler to support our approach. Using two different language workbenches (Spoofax and Xtext) we then implemented several DSALs. AspectJ was supported out-of-the-box. We implemented Cool to demonstrate that the non-trivial composition of AspectJ and Cool can be accommodated using our approach. We applied LOM to crosscutting concerns in two open source projects (oVirt and muCommander), implementing in the process application-specific DSALs, thus providing a sense of the decrease in the cost of developing composable DSALs and the increase in the effectiveness of programming with them. Crosscutting concerns remain a problem in modern real-world projects (e.g., as observed in oVirt). DSALs are often the right tool for addressing these concerns. Our work makes LOM practical, thus facilitating use of DSAL solutions in the software development process.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Hadas, Arik and Lorenz, David H.},
	month = apr,
	year = {2017},
	pages = {10:1--10:37},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/IV7AWFJ5/Hadas and Lorenz - 2017 - Language Oriented Modularity From Theory to Pract.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/NS2WVXV9/10.html:text/html}
}

@article{vavrova_does_2017,
	title = {Does {Python} {Smell} {Like} {Java}? {Tool} {Support} for {Design} {Defect} {Discovery} in {Python}},
	volume = {1},
	issn = {2473-7321},
	shorttitle = {Does {Python} {Smell} {Like} {Java}?},
	url = {http://programming-journal.org/2017/1/11/},
	doi = {10.22152/programming-journal.org/2017/1/11},
	abstract = {The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we have programmed and let it loose on a huge test set obtained from open source code from thousands of GitHub projects. When it comes to knowledge, we have found that more than twice as many methods in Python can be considered too long (statistically extremely longer than their neighbours within the same project) than in Java, but long parameter lists are seven times less likely to be found in Python code than in Java code. We have also found that Functional Decomposition, the way it was defined for Java, is not found in the Python code at all, and Spaghetti Code and God Classes are extremely rare there as well. The grounding and the confidence in these results comes from the fact that we have performed our experiments on 32{\textquoteright}058{\textquoteright}823 lines of Python code, which is by far the largest test set for a freely available Python parser. We have also designed the experiment in such a way that it aligned with prior research on design defect detection in Java in order to ease the comparison if we treat our own actions as a replication. Thus, the importance of the work is both in the unique open Python grammar of highest quality, tested on millions of lines of code, and in the design defect detection tool which works on something else than Java.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {The Art, Science, and Engineering of Programming},
	author = {Vavrov{\'a}, Nicole and Zaytsev, Vadim},
	month = apr,
	year = {2017},
	pages = {11:1--11:29},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/2G2J6NEI/Vavrov{\'a} and Zaytsev - 2017 - Does Python Smell Like Java Tool Support for Desi.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/QBEKLYLP/11.html:text/html}
}

@inproceedings{camilli_event-based_2017,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Event-{Based} {Runtime} {Verification} of {Temporal} {Properties} {Using} {Time} {Basic} {Petri} {Nets}},
	isbn = {978-3-319-57287-1 978-3-319-57288-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-57288-8_8},
	doi = {10.1007/978-3-319-57288-8_8},
	abstract = {We introduce a formal framework to provide an efficient event-based monitoring technique, and we describe its current implementation as the MahaRAJA software tool. The framework enables the quantitative runtime verification of temporal properties extracted from occurring events on Java programs. The monitor continuously evaluates the conformance of the concrete implementation with respect to its formal specification given in terms of Time Basic Petri nets, a particular timed extension of Petri nets. The system under test is instrumented by using simple Java annotations on methods to link the implementation to its formal model. This allows a separation between implementation and specification that can be used for other purposes such as formal verification, simulation, and model-based testing. The tool has been successfully used to monitor at runtime and test a number of benchmarking case-studies. Experiments show that our approach introduces bounded overhead and effectively reduces the involvement of the monitor at run time by using negligible auxiliary memory. A comparison with a number of state-of-the-art runtime verification tools is also presented.},
	language = {en},
	urldate = {2017-11-21},
	booktitle = {{NASA} {Formal} {Methods}},
	publisher = {Springer, Cham},
	author = {Camilli, Matteo and Gargantini, Angelo and Scandurra, Patrizia and Bellettini, Carlo},
	month = may,
	year = {2017},
	pages = {115--130},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/3QSFCHII/Camilli et al. - 2017 - Event-Based Runtime Verification of Temporal Prope.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/E3KM7Q6Q/978-3-319-57288-8_8.html:text/html}
}

@inproceedings{shali_hybrid_2011,
	address = {New York, NY, USA},
	series = {{OOPSLA} '11},
	title = {Hybrid {Partial} {Evaluation}},
	isbn = {978-1-4503-0940-0},
	url = {http://doi.acm.org/10.1145/2048066.2048098},
	doi = {10.1145/2048066.2048098},
	abstract = {Hybrid partial evaluation (HPE) is a pragmatic approach to partial evaluation that borrows ideas from both online and offline partial evaluation. HPE performs offline-style specialization using an online approach without static binding time analysis. The goal of HPE is to provide a practical and predictable level of optimization for programmers, with an implementation strategy that fits well within existing compilers or interpreters. HPE requires the programmer to specify where partial evaluation should be applied. It provides no termination guarantee and reports errors in situations that violate simple binding time rules, or have incorrect use of side effects in compile-time code. We formalize HPE for a small imperative object-oriented language and describe Civet, a straightforward implementation of HPE as a relatively simple extension of a Java compiler. Code optimized by Civet performs as well as the output of a state-of-the-art offline partial evaluator.},
	urldate = {2017-11-21},
	booktitle = {Proceedings of the 2011 {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {Shali, Amin and Cook, William R.},
	year = {2011},
	keywords = {partial evaluation, object-oriented languages, hybrid},
	pages = {375--390},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/C48GKETB/Shali and Cook - 2011 - Hybrid Partial Evaluation.pdf:application/pdf}
}

@inproceedings{srinivasan_partial_2015,
	address = {New York, NY, USA},
	series = {{OOPSLA} 2015},
	title = {Partial {Evaluation} of {Machine} {Code}},
	isbn = {978-1-4503-3689-5},
	url = {http://doi.acm.org/10.1145/2814270.2814321},
	doi = {10.1145/2814270.2814321},
	abstract = {This paper presents an algorithm for off-line partial evaluation of machine code. The algorithm follows the classical two-phase approach of binding-time analysis (BTA) followed by specialization. However, machine-code partial evaluation presents a number of new challenges, and it was necessary to devise new techniques for use in each phase. - Our BTA algorithm makes use of an instruction-rewriting method that ``decouples'' multiple updates performed by a single instruction. This method counters the cascading imprecision that would otherwise occur with a more naive approach to BTA. - Our specializer specializes an explicit representation of the semantics of an instruction, and emits residual code via machine-code synthesis. Moreover, to create code that allows the stack and heap to be at different positions at run-time than at specialization-time, the specializer represents specialization-time addresses using symbolic constants, and uses a symbolic state for specialization. Our experiments show that our algorithm can be used to specialize binaries with respect to commonly used inputs to produce faster binaries, as well as to extract an executable component from a bloated binary.},
	urldate = {2017-11-21},
	booktitle = {Proceedings of the 2015 {ACM} {SIGPLAN} {International} {Conference} on {Object}-{Oriented} {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Srinivasan, Venkatesh and Reps, Thomas},
	year = {2015},
	keywords = {BTA, IA-32 instruction set, machine code, machine-code synthesis, Partial evaluation, specialization},
	pages = {860--879},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GH3SL7PM/Srinivasan and Reps - 2015 - Partial Evaluation of Machine Code.pdf:application/pdf}
}

@inproceedings{rompf_surgical_2014,
	address = {New York, NY, USA},
	series = {{PLDI} '14},
	title = {Surgical {Precision} {JIT} {Compilers}},
	isbn = {978-1-4503-2784-8},
	url = {http://doi.acm.org/10.1145/2594291.2594316},
	doi = {10.1145/2594291.2594316},
	abstract = {Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable. In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables "smart" libraries to supply domain-specific compiler optimizations or safety checks. We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction. In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Rompf, Tiark and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Chafi, Hassan and Olukotun, Kunle},
	year = {2014},
	keywords = {JIT compilation, program generation, staging},
	pages = {41--52},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/3APPKP6M/Rompf et al. - 2014 - Surgical Precision JIT Compilers.pdf:application/pdf}
}

@inproceedings{chari_building_2016,
	address = {New York, NY, USA},
	series = {{DLS} 2016},
	title = {Building {Efficient} and {Highly} {Run}-time {Adaptable} {Virtual} {Machines}},
	isbn = {978-1-4503-4445-6},
	url = {http://doi.acm.org/10.1145/2989225.2989234},
	doi = {10.1145/2989225.2989234},
	abstract = {Programming language virtual machines (VMs) realize language semantics, enforce security properties, and execute applications efficiently. Fully Reflective Execution Environments (EEs) are VMs that additionally expose their whole structure and behavior to applications. This enables develop- ers to observe and adapt VMs at run time. However, there is a belief that reflective EEs are not viable for practical usages because such flexibility would incur a high performance overhead. To refute this belief, we built a reflective EE on top of a highly optimizing dynamic compiler. We introduced a new optimization model that, based on the conjecture that variability of low-level (EE-level) reflective behavior is low in many scenarios, mitigates the most significant sources of the performance overheads related to the reflective capabilities in the EE. Our experiments indicate that reflective EEs can reach peak performance in the order of standard VMs. Concretely, that a) if reflective mechanisms are not used the execution overhead is negligible compared to standard VMs, b) VM operations can be redefined at language-level without incurring in significant overheads, c) for several software adaptation tasks, applying the reflection at the VM level is not only lightweight in terms of engineering effort, but also competitive in terms of performance in comparison to other ad-hoc solutions.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 12th {Symposium} on {Dynamic} {Languages}},
	publisher = {ACM},
	author = {Chari, Guido and Garbervetsky, Diego and Marr, Stefan},
	year = {2016},
	keywords = {Reflection, Metaobject Protocols, Performance, Virtual Machines},
	pages = {60--71},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/SMNL4MJX/Chari et al. - 2016 - Building Efficient and Highly Run-time Adaptable V.pdf:application/pdf}
}

@article{allen_program_1976,
	title = {A {Program} {Data} {Flow} {Analysis} {Procedure}},
	volume = {19},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/360018.360025},
	doi = {10.1145/360018.360025},
	abstract = {The global data relationships in a program can be exposed and codified by the static analysis methods described in this paper. A procedure is given which determines all the definitions which can possibly {\textquotedblleft}reach{\textquotedblright} each node of the control flow graph of the program and all the definitions that are {\textquotedblleft}live{\textquotedblright} on each edge of the graph. The procedure uses an {\textquotedblleft}interval{\textquotedblright} ordered edge listing data structure and handles reducible and irreducible graphs indistinguishably.},
	number = {3},
	urldate = {2017-11-22},
	journal = {Commun. ACM},
	author = {Allen, F. E. and Cocke, J.},
	month = mar,
	year = {1976},
	keywords = {program optimization, data flow analysis, algorithms, compilers, flow graphs},
	pages = {137--},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DU4MZC5H/Allen and Cocke - 1976 - A Program Data Flow Analysis Procedure.pdf:application/pdf}
}

@article{bacon_compiler_1994,
	title = {Compiler {Transformations} for {High}-performance {Computing}},
	volume = {26},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/197405.197406},
	doi = {10.1145/197405.197406},
	abstract = {In the last three decades a large number of compiler transformations for optimizing programs have been implemented. Most optimizations for uniprocessors reduce the number of instructions executed by the program using transformations based on the analysis of scalar quantities and data-flow techniques. In contrast, optimizations for high-performance superscalar, vector, and parallel processors maximize parallelism and memory locality with transformations that rely on tracking the properties of arrays using loop dependence analysis.This survey is a comprehensive overview of the important high-level program restructuring techniques for imperative languages, such as C and Fortran. Transformations for both sequential and various types of parallel architectures are covered in depth. We describe the purpose of each transformation, explain how to determine if it is legal, and give an example of its application.Programmers wishing to enhance the performance of their code can use this survey to improve their understanding of the optimizations that compilers can perform, or as a reference for techniques to be applied manually. Students can obtain an overview of optimizing compiler technology. Compiler writers can use this survey as a reference for most of the important optimizations developed to date, and as bibliographic reference for the details of each optimization. Readers are expected to be familiar with modern computer architecture and basic program compilation techniques.},
	number = {4},
	urldate = {2017-11-22},
	journal = {ACM Comput. Surv.},
	author = {Bacon, David F. and Graham, Susan L. and Sharp, Oliver J.},
	month = dec,
	year = {1994},
	keywords = {parallelism, compilation, dependence analysis, locality, multiprocessors, optimization, superscalar processors, vectorization},
	pages = {345--420},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/BSG4B7IS/Bacon et al. - 1994 - Compiler Transformations for High-performance Comp.pdf:application/pdf}
}

@inproceedings{baxter_understanding_2006,
	address = {New York, NY, USA},
	series = {{OOPSLA} '06},
	title = {Understanding the {Shape} of {Java} {Software}},
	isbn = {978-1-59593-348-5},
	url = {http://doi.acm.org/10.1145/1167473.1167507},
	doi = {10.1145/1167473.1167507},
	abstract = {Large amounts of Java software have been written since the language's escape into unsuspecting software ecology more than ten years ago. Surprisingly little is known about the structure of Java programs in the wild: about the way methods are grouped into classes and then into packages, the way packages relate to each other, or the way inheritance and composition are used to put these programs together. We present the results of the first in-depth study of the structure of Java programs. We have collected a number of Java programs and measured their key structural attributes. We have found evidence that some relationships follow power-laws, while others do not. We have also observed variations that seem related to some characteristic of the application itself. This study provides important information for researchers who can investigate how and why the structural relationships we find may have originated, what they portend, and how they can be managed.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 21st {Annual} {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programming} {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Baxter, Gareth and Frean, Marcus and Noble, James and Rickerby, Mark and Smith, Hayden and Visser, Matt and Melton, Hayden and Tempero, Ewan},
	year = {2006},
	keywords = {Java, object-oriented design, power-law distributions},
	pages = {397--412},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DZSNZZNH/Baxter et al. - 2006 - Understanding the Shape of Java Software.pdf:application/pdf}
}

@inproceedings{wurthinger_one_2013,
	address = {New York, NY, USA},
	series = {Onward! 2013},
	title = {One {VM} to {Rule} {Them} {All}},
	isbn = {978-1-4503-2472-4},
	url = {http://doi.acm.org/10.1145/2509578.2509581},
	doi = {10.1145/2509578.2509581},
	abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efficient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new high-performance language implementations can be obtained by writing little more than a stylized interpreter.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 2013 {ACM} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} \& {Software}},
	publisher = {ACM},
	author = {W{\"u}rthinger, Thomas and Wimmer, Christian and W{\"o}{\ss}, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
	year = {2013},
	keywords = {dynamic languages, javascript, java, virtual machine, optimization, language implementation},
	pages = {187--204},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GQZ2MS64/W{\"u}rthinger et al. - 2013 - One VM to Rule Them All.pdf:application/pdf}
}

@inproceedings{rompf_optimizing_2013,
	address = {New York, NY, USA},
	series = {{POPL} '13},
	title = {Optimizing {Data} {Structures} in {High}-level {Programs}: {New} {Directions} for {Extensible} {Compilers} {Based} on {Staging}},
	isbn = {978-1-4503-1832-7},
	shorttitle = {Optimizing {Data} {Structures} in {High}-level {Programs}},
	url = {http://doi.acm.org/10.1145/2429069.2429128},
	doi = {10.1145/2429069.2429128},
	abstract = {High level data structures are a cornerstone of modern programming and at the same time stand in the way of compiler optimizations. In order to reason about user- or library-defined data structures compilers need to be extensible. Common mechanisms to extend compilers fall into two categories. Frontend macros, staging or partial evaluation systems can be used to programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, some compilers allow extending the internal workings by adding new transformation passes at different points in the compile chain or adding new intermediate representation (IR) types. None of these mechanisms alone is sufficient to handle the challenges posed by high level data structures. This paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts. Instead of using staging merely as a front end, we implement internal compiler passes using staging as well. These internal passes delegate back to program execution to construct the transformed IR. Staging is known to simplify program generation, and in the same way it can simplify program transformation. Defining a transformation as a staged IR interpreter is simpler than implementing a low-level IR to IR transformer. With custom IR nodes, many optimizations that are expressed as rewritings from IR nodes to staged program fragments can be combined into a single pass, mitigating phase ordering problems. Speculative rewriting can preserve optimistic assumptions around loops. We demonstrate several powerful program optimizations using this architecture that are particularly geared towards data structures: a novel loop fusion and deforestation algorithm, array of struct to struct of array conversion, object flattening and code generation for heterogeneous parallel devices. We validate our approach using several non trivial case studies that exhibit order of magnitude speedups in experiments.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 40th {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Rompf, Tiark and Sujeeth, Arvind K. and Amin, Nada and Brown, Kevin J. and Jovanovic, Vojin and Lee, HyoukJoong and Jonnalagedda, Manohar and Olukotun, Kunle and Odersky, Martin},
	year = {2013},
	keywords = {code generation, staging, data structures, extensible compilers},
	pages = {497--510},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HVI9PWBB/Rompf et al. - 2013 - Optimizing Data Structures in High-level Programs.pdf:application/pdf}
}

@inproceedings{marr_zero-overhead_2015,
	address = {New York, NY, USA},
	series = {{PLDI} '15},
	title = {Zero-overhead {Metaprogramming}: {Reflection} and {Metaobject} {Protocols} {Fast} and {Without} {Compromises}},
	isbn = {978-1-4503-3468-6},
	shorttitle = {Zero-overhead {Metaprogramming}},
	url = {http://doi.acm.org/10.1145/2737924.2737963},
	doi = {10.1145/2737924.2737963},
	abstract = {Runtime metaprogramming enables many useful applications and is often a convenient solution to solve problems in a generic way, which makes it widely used in frameworks, middleware, and domain-specific languages. However, powerful metaobject protocols are rarely supported and even common concepts such as reflective method invocation or dynamic proxies are not optimized. Solutions proposed in literature either restrict the metaprogramming capabilities or require application or library developers to apply performance improving techniques. For overhead-free runtime metaprogramming, we demonstrate that dispatch chains, a generalized form of polymorphic inline caches common to self-optimizing interpreters, are a simple optimization at the language-implementation level. Our evaluation with self-optimizing interpreters shows that unrestricted metaobject protocols can be realized for the first time without runtime overhead, and that this optimization is applicable for just-in-time compilation of interpreters based on meta-tracing as well as partial evaluation. In this context, we also demonstrate that optimizing common reflective operations can lead to significant performance improvements for existing applications.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Marr, Stefan and Seaton, Chris and Ducasse, St{\'e}phane},
	year = {2015},
	keywords = {Reflection, Metaobject Protocols, Virtual Machines, Just-in-Time Compilation, Meta-tracing, Metaprogramming, Partial Evaluation, Proxies},
	pages = {545--554},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/M83GTHFZ/Marr et al. - 2015 - Zero-overhead Metaprogramming Reflection and Meta.pdf:application/pdf}
}

@inproceedings{leisa_shallow_2015,
	address = {New York, NY, USA},
	series = {{GPCE} 2015},
	title = {Shallow {Embedding} of {DSLs} via {Online} {Partial} {Evaluation}},
	isbn = {978-1-4503-3687-1},
	url = {http://doi.acm.org/10.1145/2814204.2814208},
	doi = {10.1145/2814204.2814208},
	abstract = {This paper investigates shallow embedding of DSLs by means of online partial evaluation. To this end, we present a novel online partial evaluator for continuation-passing style languages. We argue that it has, in contrast to prior work, a predictable termination policy that works well in practice. We present our approach formally using a continuation-passing variant of PCF and prove its termination properties. We evaluate our technique experimentally in the field of visual and high-performance computing and show that our evaluator produces highly specialized and efficient code for CPUs as well as GPUs that matches the performance of hand-tuned expert code.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 2015 {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {ACM},
	author = {Lei{\ss}a, Roland and Boesche, Klaas and Hack, Sebastian and Membarth, Richard and Slusallek, Philipp},
	year = {2015},
	keywords = {Partial Evaluation, Continuation-Passing Style, DSL Embedding},
	pages = {11--20},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/KPFVCPVW/Lei{\ss}a et al. - 2015 - Shallow Embedding of DSLs via Online Partial Evalu.pdf:application/pdf}
}

@inproceedings{asai_compiling_2014,
	address = {New York, NY, USA},
	series = {{GPCE} 2014},
	title = {Compiling a {Reflective} {Language} {Using} {MetaOCaml}},
	isbn = {978-1-4503-3161-6},
	url = {http://doi.acm.org/10.1145/2658761.2658775},
	doi = {10.1145/2658761.2658775},
	abstract = {A reflective language makes the language semantics open to user programs and allows them to access, extend, and modify it from within the same language framework. Because of its high flexibility and expressiveness, it can be an ideal platform for programming language research as well as practical applications in dynamic environments. However, efficient implementation of a reflective language is extremely difficult. Under the circumstance where the language semantics can change, a partial evaluator is required for compilation. This paper reports on the experience of using MetaOCaml as a compiler for a reflective language. With staging annotations, MetaOCaml achieves the same effect as using a partial evaluator. Unlike the standard partial evaluator, the run mechanism of MetaOCaml enables us to use the specialized (compiled) code in the current runtime environment. On the other hand, the lack of a binding-time analysis in MetaOCaml prohibits us from compiling a user program under modified compiled semantics.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {ACM},
	author = {Asai, Kenichi},
	year = {2014},
	keywords = {Reflection, partial evaluation, staging, binding-time analysis, metacircular interpreter, MetaOCaml},
	pages = {113--122},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/IQZ29BSV/Asai - 2014 - Compiling a Reflective Language Using MetaOCaml.pdf:application/pdf}
}

@article{dongarra_linpack_2003,
	title = {The {LINPACK} {Benchmark}: past, present and future},
	volume = {15},
	issn = {1532-0634},
	shorttitle = {The {LINPACK} {Benchmark}},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.728/abstract},
	doi = {10.1002/cpe.728},
	abstract = {This paper describes the LINPACK Benchmark and some of its variations commonly used to assess the performance of computer systems. Aside from the LINPACK Benchmark suite, the TOP500 and the HPL codes are presented. The latter is frequently used to obtained results for TOP500 submissions. Information is also given on how to interpret the results of the benchmark and how the results fit into the performance evaluation process. Copyright {\textcopyright} 2003 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {9},
	urldate = {2017-11-22},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Dongarra, Jack J. and Luszczek, Piotr and Petitet, Antoine},
	month = aug,
	year = {2003},
	keywords = {benchmarking, BLAS, high-performance computing, HPL, linear algebra, LINPACK, TOP500},
	pages = {803--820},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/RZYPRLTX/Dongarra et al. - 2003 - The LINPACK Benchmark past, present and future.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/DK99Z238/abstract.html:text/html}
}

@article{lee_study_2002,
	title = {A study of dynamic memory management in {C}++ programs},
	volume = {28},
	issn = {1477-8424},
	url = {http://www.sciencedirect.com/science/article/pii/S0096055102000152},
	doi = {10.1016/S0096-0551(02)00015-2},
	abstract = {Recently, the importance of dynamic memory management has been increased significantly as there is a growing number of development in object-oriented programs. Many studies show that dynamic memory management is one of the most expensive components in many software systems. It can consume up to 30\% of the program execution time. Especially, in C++ programs, it tends to have object creation and deletion prolifically. These objects tend to have short life-spans. This paper describes an integrated study of the C++'s memory allocation behavior, a memory tracing tool and memory managements based on the empirical study of C++ programs. First, this paper summarizes the hypothesis of situations that invoke the dynamic memory management explicitly and implicitly. They are: constructors, copy constructors, overloading assignment operator=, type conversions and application specific member functions. Second, a dynamic memory tracing tool, called mtrace++, is introduced to study the dynamic memory allocation behavior in C++ programs. Third, a dynamic memory allocation strategy, called O-Reuse, to reuse the allocated objects to speed up the object management. At the later part of this paper, an automatic dynamic memory management, called GC++, is discussed. GC++ collects unreferenced objects automatically with high speed of allocation/deallocation processes. The performance gains of O-Reuse and GC++ are come from the utilization of memory allocation/deallocation behavior.},
	number = {3},
	urldate = {2017-11-22},
	journal = {Computer Languages, Systems \& Structures},
	author = {Lee, Woo Hyong and Chang, Morris},
	month = oct,
	year = {2002},
	keywords = {Dynamic memory management, Garbage collection, Life-span},
	pages = {237--272},
	file = {ScienceDirect Full Text PDF:/Users/luigi/work/zotero/storage/6PITR68T/Lee and Chang - 2002 - A study of dynamic memory management in C++ progra.pdf:application/pdf;ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/IU5UBKMD/S0096055102000152.html:text/html}
}

@inproceedings{bhattacharya_assessing_2011,
	address = {New York, NY, USA},
	series = {{ICSE} '11},
	title = {Assessing {Programming} {Language} {Impact} on {Development} and {Maintenance}: {A} {Study} on {C} and {C}++},
	isbn = {978-1-4503-0445-0},
	shorttitle = {Assessing {Programming} {Language} {Impact} on {Development} and {Maintenance}},
	url = {http://doi.acm.org/10.1145/1985793.1985817},
	doi = {10.1145/1985793.1985817},
	abstract = {Billions of dollars are spent every year for building and maintaining software. To reduce these costs we must identify the key factors that lead to better software and more productive development. One such key factor, and the focus of our paper, is the choice of programming language. Existing studies that analyze the impact of choice of programming language suffer from several deficiencies with respect to methodology and the applications they consider. For example, they consider applications built by different teams in different languages, hence fail to control for developer competence, or they consider small-sized, infrequently-used, short-lived projects. We propose a novel methodology which controls for development process and developer competence, and quantifies how the choice of programming language impacts software quality and developer productivity. We conduct a study and statistical analysis on a set of long-lived, widely-used, open source projects - Firefox, Blender, VLC, and MySQL. The key novelties of our study are: (1) we only consider projects which have considerable portions of development in two languages, C and C++, and (2) a majority of developers in these projects contribute to both C and C++ code bases. We found that using C++ instead of C results in improved software quality and reduced maintenance effort, and that code bases are shifting from C to C++. Our methodology lays a solid foundation for future studies on comparative advantages of particular programming languages.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Bhattacharya, Pamela and Neamtiu, Iulian},
	year = {2011},
	keywords = {developer productivity, software quality, empirical studies, software evolution, high-level languages},
	pages = {171--180},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/MP3YYN8B/Bhattacharya and Neamtiu - 2011 - Assessing Programming Language Impact on Developme.pdf:application/pdf}
}

@article{hughes_why_1989,
	title = {Why {Functional} {Programming} {Matters}},
	volume = {32},
	issn = {0010-4620},
	url = {http://dx.doi.org/10.1093/comjnl/32.2.98},
	doi = {10.1093/comjnl/32.2.98},
	number = {2},
	urldate = {2017-11-22},
	journal = {Comput. J.},
	author = {Hughes, J.},
	month = apr,
	year = {1989},
	pages = {98--107},
	file = {whyfp90.pdf:/Users/luigi/work/zotero/storage/IVFUXLWS/whyfp90.pdf:application/pdf}
}

@article{ural_formal_1992,
	title = {Formal methods for test sequence generation},
	volume = {15},
	issn = {0140-3664},
	url = {http://www.sciencedirect.com/science/article/pii/014036649290092S},
	doi = {10.1016/0140-3664(92)90092-S},
	abstract = {Six formal methods and their enhancements for generating test sequences from FSM-based specifications are reviewed. These methods are: the Transition Tour (T) method; the Distinguishing Sequence (D) method; the Characterizing Set (W) method; the Unique Input/Output Sequence (UIO) method; the Single UIO (SUIO) method; and the Multiple UIO (MUIO) method. Improved variations of the D-, W-, UIO- and MUIO-methods are included in the discussions. These formal methods are compared in terms of the upper bounds on the length of resulting test sequences. A tool implementing these methods and their enhancements is presented.},
	number = {5},
	urldate = {2017-11-22},
	journal = {Computer Communications},
	author = {Ural, Hasan},
	month = jun,
	year = {1992},
	keywords = {finite state machines, formal methods, protocol testing, test sequence generation},
	pages = {311--325},
	file = {ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/PGLC6QYW/014036649290092S.html:text/html;ural.pdf:/Users/luigi/work/zotero/storage/ZMD6IS5N/ural.pdf:application/pdf}
}

@article{tofte_region-based_1997,
	title = {Region-{Based} {Memory} {Management}},
	volume = {132},
	issn = {0890-5401},
	url = {http://www.sciencedirect.com/science/article/pii/S0890540196926139},
	doi = {10.1006/inco.1996.2613},
	abstract = {This paper describes a memory management discipline for programs that perform dynamic memory allocation and de-allocation. At runtime, all values are put intoregions. The store consists of a stack of regions. All points of region allocation and de-allocation are inferred automatically, using a type and effect based program analysis. The scheme does not assume the presence of a garbage collector. The scheme was first presented in 1994 (M. Tofte and J.-P. Talpin,in{\textquotedblleft}Proceedings of the 21st ACM SIGPLAN{\textendash}SIGACT Symposium on Principles of Programming Languages,{\textquotedblright} pp. 188{\textendash}201); subsequently, it has been tested in The ML Kit with Regions, a region-based, garbage-collection free implementation of the Standard ML Core language, which includes recursive datatypes, higher-order functions and updatable references L. Birkedal, M. Tofte, and M. Vejlstrup, (1996),in{\textquotedblleft}Proceedings of the 23 rd ACM SIGPLAN{\textendash}SIGACT Symposium on Principles of Programming Languages,{\textquotedblright} pp. 171{\textendash}183. This paper defines a region-based dynamic semantics for a skeletal programming language extracted from Standard ML. We present the inference system which specifies where regions can be allocated and de-allocated and a detailed proof that the system is sound with respect to a standard semantics. We conclude by giving some advice on how to write programs that run well on a stack of regions, based on practical experience with the ML Kit.},
	number = {2},
	urldate = {2017-11-22},
	journal = {Information and Computation},
	author = {Tofte, Mads and Talpin, Jean-Pierre},
	month = feb,
	year = {1997},
	pages = {109--176},
	file = {ScienceDirect Full Text PDF:/Users/luigi/work/zotero/storage/K5ZTMDWS/Tofte and Talpin - 1997 - Region-Based Memory Management.pdf:application/pdf;ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/9DY227KU/S0890540196926139.html:text/html}
}

@inproceedings{dean_optimization_1995,
	address = {London, UK, UK},
	series = {{ECOOP} '95},
	title = {Optimization of {Object}-{Oriented} {Programs} {Using} {Static} {Class} {Hierarchy} {Analysis}},
	isbn = {978-3-540-60160-9},
	url = {http://dl.acm.org/citation.cfm?id=646153.679523},
	abstract = {Optimizing compilers for object-oriented languages apply static class analysis and other techniques to try to deduce precise information about the possible classes of the receivers of messages; if successful, dynamically-dispatched messages can be replaced with direct procedure calls and potentially further optimized through inline-expansion. By examining the complete inheritance graph of a program, which we call class hierarchy analysis, the compiler can improve the quality of static class information and thereby improve run-time performance. In this paper we present class hierarchy analysis and describe techniques for implementing this analysis effectively in both statically- and dynamically-typed languages and also in the presence of multi-methods. We also discuss how class hierarchy analysis can be supported in an interactive programming environment and, to some extent, in the presence of separate compilation. Finally, we assess the bottom-line performance improvement due to class hierarchy analysis alone and in combination with two other "competing" optimizations, profile-guided receiver class prediction and method specialization.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 9th {European} {Conference} on {Object}-{Oriented} {Programming}},
	publisher = {Springer-Verlag},
	author = {Dean, Jeffrey and Grove, David and Chambers, Craig},
	year = {1995},
	pages = {77--101},
	file = {dean-grove-chambers-ecoop95.pdf:/Users/luigi/work/zotero/storage/AAGL2DKU/dean-grove-chambers-ecoop95.pdf:application/pdf}
}

@inproceedings{gosling_java_1995,
	address = {New York, NY, USA},
	series = {{IR} '95},
	title = {Java {Intermediate} {Bytecodes}: {ACM} {SIGPLAN} {Workshop} on {Intermediate} {Representations} ({IR}'95)},
	isbn = {978-0-89791-754-4},
	shorttitle = {Java {Intermediate} {Bytecodes}},
	url = {http://doi.acm.org/10.1145/202529.202541},
	doi = {10.1145/202529.202541},
	abstract = {Java is a programming language loosely related to C++. Java originated in a project to produce a software development environment for small distributed embedded systems. Programs needed to be small, fast, {\textquotedblleft}safe{\textquotedblright} and portable. These needs led to a design that is rather different from standard practice. In particular, the form of compiled programs is machine independent bytecodes. But we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. This lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.},
	urldate = {2017-11-22},
	booktitle = {Papers from the 1995 {ACM} {SIGPLAN} {Workshop} on {Intermediate} {Representations}},
	publisher = {ACM},
	author = {Gosling, James},
	year = {1995},
	pages = {111--118},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/IT6IZUER/Gosling - 1995 - Java Intermediate Bytecodes ACM SIGPLAN Workshop .pdf:application/pdf}
}

@inproceedings{hauswirth_low-overhead_2004,
	address = {New York, NY, USA},
	series = {{ASPLOS} {XI}},
	title = {Low-overhead {Memory} {Leak} {Detection} {Using} {Adaptive} {Statistical} {Profiling}},
	isbn = {978-1-58113-804-7},
	url = {http://doi.acm.org/10.1145/1024393.1024412},
	doi = {10.1145/1024393.1024412},
	abstract = {Sampling has been successfully used to identify performance optimization opportunities. We would like to apply similar techniques to check program correctness. Unfortunately, sampling provides poor coverage of infrequently executed code, where bugs often lurk. We describe an adaptive profiling scheme that addresses this by sampling executions of code segments at a rate inversely proportional to their execution frequency. To validate our ideas, we have implemented SWAT, a novel memory leak detection tool. SWAT traces program allocations/ frees to construct a heap model and uses our adaptive profiling infrastructure to monitor loads/stores to these objects with low overhead. SWAT reports 'stale' objects that have not been accessed for a 'long' time as leaks. This allows it to find all leaks that manifest during the current program execution. Since SWAT has low runtime overhead ({\guilsinglleft}5\%), and low space overhead ({\guilsinglleft}10\% in most cases and often less than 5\%), it can be used to track leaks in production code that take days to manifest. In addition to identifying the allocations that leak memory, SWAT exposes where the program last accessed the leaked data, which facilitates debugging and fixing the leak. SWAT has been used by several product groups at Microsoft for the past 18 months and has proved effective at detecting leaks with a low false positive rate ({\guilsinglleft}10\%).},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 11th {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Hauswirth, Matthias and Chilimbi, Trishul M.},
	year = {2004},
	keywords = {low-overhead monitoring, memory leaks, runtime analysis},
	pages = {156--164},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/QR5GHZTZ/Hauswirth and Chilimbi - 2004 - Low-overhead Memory Leak Detection Using Adaptive .pdf:application/pdf}
}

@inproceedings{okur_how_2012,
	address = {New York, NY, USA},
	series = {{FSE} '12},
	title = {How {Do} {Developers} {Use} {Parallel} {Libraries}?},
	isbn = {978-1-4503-1614-9},
	url = {http://doi.acm.org/10.1145/2393596.2393660},
	doi = {10.1145/2393596.2393660},
	abstract = {Parallel programming is hard. The industry leaders hope to convert the hard problem of using parallelism into the easier problem of using a parallel library. Yet, we know little about how programmers adopt these libraries in practice. Without such knowledge, other programmers cannot educate themselves about the state of the practice, library designers are unaware of API misusage, researchers make wrong assumptions, and tool vendors do not support common usage of library constructs. We present the first study that analyzes the usage of parallel libraries in a large scale experiment. We analyzed 655 open-source applications that adopted Microsoft's new parallel libraries -- Task Parallel Library (TPL) and Parallel Language Integrated Query (PLINQ) -- comprising 17.6M lines of code written in C\#. These applications are developed by 1609 programmers. Using this data, we answer 8 research question and we uncover some interesting facts. For example, (i) for two of the fundamental parallel constructs, in at least 10\% of the cases developers misuse them so that the code runs sequentially instead of concurrently, (ii) developers make their parallel code unnecessarily complex, (iii) applications of different size have different adoption trends. The library designers confirmed that our finding are useful and will influence the future development of the libraries.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the {ACM} {SIGSOFT} 20th {International} {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Okur, Semih and Dig, Danny},
	year = {2012},
	keywords = {empirical study, C\#, multi-core, parallel libraries},
	pages = {54:1--54:11},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/K56C7BDL/Okur and Dig - 2012 - How Do Developers Use Parallel Libraries.pdf:application/pdf}
}

@inproceedings{hills_case_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Case} of {Visitor} versus {Interpreter} {Pattern}},
	isbn = {978-3-642-21951-1 978-3-642-21952-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-21952-8_17},
	doi = {10.1007/978-3-642-21952-8_17},
	abstract = {We compare the Visitor pattern with the Interpreter pattern, investigating a single case in point for the Java language. We have produced and compared two versions of an interpreter for a programming language. The first version makes use of the Visitor pattern. The second version was obtained by using an automated refactoring to transform uses of the Visitor pattern to uses of the Interpreter pattern. We compare these two nearly equivalent versions on their maintenance characteristics and execution efficiency. Using a tailored experimental research method we can highlight differences and the causes thereof. The contributions of this paper are that it isolates the choice between Visitor and Interpreter in a realistic software project and makes the difference experimentally observable.},
	language = {en},
	urldate = {2017-11-22},
	booktitle = {Objects, {Models}, {Components}, {Patterns}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Hills, Mark and Klint, Paul and Storm, Tijs van der and Vinju, Jurgen},
	month = jun,
	year = {2011},
	pages = {228--243},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/6I3GGBDE/Hills et al. - 2011 - A Case of Visitor versus Interpreter Pattern.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/QLW9VCEN/978-3-642-21952-8_17.html:text/html}
}

@book{klimov_approach_2008,
	title = {An {Approach} to {Supercompilation} for {Object}-oriented {Languages}: the {Java} {Supercompiler} {Case} {Study}},
	shorttitle = {An {Approach} to {Supercompilation} for {Object}-oriented {Languages}},
	abstract = {An extension of Turchin{\textquoteright}s supercompilation from functional to object-oriented languages as it is implemented in the current version of a Java supercompiler (JScp) is reviewed. There are two novelties: first, the construction of the specialized code of operations on objects is separated into two stages{\textemdash}residualization of all operations on objects during supercompilation proper and elimination of redundant code in post-processing; and second, limited configuration analysis, which processes each Java control statement one by one using width-first unfolding of a process graph, is used. The construction of JScp is based on the principle of user control of the process of supercompilation rather than building a black-box automatic supercompiler. The rationale for this decision is discussed.},
	author = {Klimov, Andrei V.},
	year = {2008},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/65CLZZTX/Klimov - 2008 - An Approach to Supercompilation for Object-oriente.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/EFRTY2QF/summary.html:text/html}
}

@inproceedings{cremet_core_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Core} {Calculus} for {Scala} {Type} {Checking}},
	isbn = {978-3-540-37791-7 978-3-540-37793-1},
	url = {https://link.springer.com/chapter/10.1007/11821069_1},
	doi = {10.1007/11821069_1},
	abstract = {We present a minimal core calculus that captures interesting constructs of the Scala programming language: nested classes, abstract types, mixin composition, and path dependent types. We show that the problems of type assignment and subtyping in this calculus are decidable.},
	language = {en},
	urldate = {2017-11-22},
	booktitle = {Mathematical {Foundations} of {Computer} {Science} 2006},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Cremet, Vincent and Garillot, Fran{\c c}ois and Lenglet, Sergue{\"i} and Odersky, Martin},
	month = aug,
	year = {2006},
	pages = {1--23},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/VL3QIB7S/Cremet et al. - 2006 - A Core Calculus for Scala Type Checking.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/8F2VDBVF/11821069_1.html:text/html}
}

@inproceedings{zhang_whole_2004,
	title = {Whole {Execution} {Traces}},
	doi = {10.1109/MICRO.2004.37},
	abstract = {Different types of program profiles (control flow, value, address, and dependence) have been collected and extensively studied by researchers to identify program characteristics that can then be exploited to develop more effective compilers and architectures. Due to the large amounts of profile data produced by realistic program runs, most work has focused on separately collecting and compressing different types of profiles. In this paper we present a unified representation of profiles called Whole Execution Trace (WET) which includes the complete information contained in each of the above types of traces. Thus WETs provide a basis for a next generation software tool that will enable mining of program profiles to identify program characteristics that require understanding of relationships among various types of profiles. The key features of our WET representation are: WET is constructed by labeling a static program representation with profile information such that relavent and related profile information can be directly accessed by analysis algorithms as they traverse the representation; a highly effective two tier strategy is used to significantly compress the WET; and compression techniques are designed such that they do not adversely affect the ability to rapidly traverse WET for extracting subsets of information corresponding to individual profile types as well as a combination of profile types (e.g., in form of dynamic slices of WETs). Our experimentation shows that on an average execution traces resulting from execution of 647 Million statements can be stored in 331 Megabytes of storage after compression. The compression factors range from 16 to 83. Moreover the rates at which different types of profiles can be individually or simultaneously extracted are high.},
	booktitle = {37th {International} {Symposium} on {Microarchitecture}, 2004. {MICRO}-37 2004},
	author = {Zhang, Xiangyu and Gupta, R.},
	month = dec,
	year = {2004},
	pages = {105--116},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/XPHZY82M/1550986.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/PM97DDLH/Zhang and Gupta - 2004 - Whole Execution Traces.pdf:application/pdf}
}

@inproceedings{feautrier_automatic_1996,
	title = {Automatic {Parallelization} in the {Polytope} {Model}},
	abstract = {. The aim of this paper is to explain the importance of polytope and polyhedra in automatic parallelization. We show that the semantics of parallel programs is best described geometrically, as properties of sets of integral points in n-dimensional spaces, where n is related to the maximum nesting depth of DO loops. The needed properties translate nicely to properties of polyhedra, for which many algorithms have been designed for the needs of optimization and operation research. We show how these ideas apply to scheduling, placement and parallel code generation. R'esum'e Le but de cet article est d'expliquer le role jou'e par les poly`edres et les polytopes en parall'elisation automatique. On montre que la s'emantique d'un programme parall`ele se d'ecrit naturellement sous forme g'eom'etrique, les propri'et'es du programme 'etant formalis'ees comme des propri'et'es d'ensemble de points dans un espace `a n dimensions. n est li'e `a la profondeur maximale d'imbrication des boucles DO. Les...},
	booktitle = {Laboratoire {PRiSM}, {Universit{\'e}} des {Versailles} {St}-{Quentin} en {Yvelines}, 45, avenue des {\'E}tats-{Unis}, {F}-78035 {Versailles} {Cedex}},
	publisher = {Springer-Verlag},
	author = {Feautrier, Paul},
	year = {1996},
	pages = {79--103},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/CGLFTXBS/Feautrier - 1996 - Automatic Parallelization in the Polytope Model.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/XZWPE2SL/summary.html:text/html}
}

@inproceedings{ammons_exploiting_1997,
	address = {New York, NY, USA},
	series = {{PLDI} '97},
	title = {Exploiting {Hardware} {Performance} {Counters} with {Flow} and {Context} {Sensitive} {Profiling}},
	isbn = {978-0-89791-907-4},
	url = {http://doi.acm.org/10.1145/258915.258924},
	doi = {10.1145/258915.258924},
	abstract = {A program profile attributes run-time costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis--flow and context sensitivity--to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that efficiently captures calling contexts for procedure-level measurements.Our measurements show that the SPEC95 benchmarks execute a small number (3--28) of hot paths that account for 9--98\% of their L1 data cache misses. Moreover, these hot paths are concentrated in a few routines, which have complex dynamic behavior.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 1997 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Ammons, Glenn and Ball, Thomas and Larus, James R.},
	year = {1997},
	pages = {85--96},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/D7QGCW74/Ammons et al. - 1997 - Exploiting Hardware Performance Counters with Flow.pdf:application/pdf}
}

@inproceedings{crew_astlog:_1997,
	address = {Berkeley, CA, USA},
	series = {{DSL}'97},
	title = {{ASTLOG}: {A} {Language} for {Examining} {Abstract} {Syntax} {Trees}},
	shorttitle = {{ASTLOG}},
	url = {http://dl.acm.org/citation.cfm?id=1267950.1267968},
	abstract = {We desired a facility for locating/analyzing syntactic artifacts in abstract syntax trees of C/C++ programs, similar to the facility grep or awk provides for locating artifacts at the lexical level. Prolog, with its implicit pattern-matching and backtracking capabilities, is a natural choice for such an application. We have developed a Prolog variant that avoids the overhead of translating the source syntactic structures into the form of a Prolog database; this is crucial to obtaining acceptable performance on large programs. An interpreter for this language has been implemented and used to find various kinds of syntactic bugs and other questionable constructs in real programs like Microsoft SQL server (450Klines) and Microsoft Word (2Mlines) in time comparable to the runtime of the actual compiler. The model in which terms are matched against an implicit current object, rather than simply proven against a database of facts, leads to a distinct "inside-out functional" programming style that is quite unlike typical Prolog, but one that is, in fact, well-suited to the examination of trees. Also, various second-order Prolog set-predicates may be implemented via manipulation of the current object, thus retaining an important feature without entailing that the database be dynamically extensible as the usual implementation does.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the {Conference} on {Domain}-{Specific} {Languages} on {Conference} on {Domain}-{Specific} {Languages} ({DSL}), 1997},
	publisher = {USENIX Association},
	author = {Crew, Roger F.},
	year = {1997},
	pages = {18--18},
	file = {download.pdf:/Users/luigi/work/zotero/storage/CALGNDHI/download.pdf:application/pdf}
}

@article{savage_eraser:_1997,
	title = {Eraser: {A} {Dynamic} {Data} {Race} {Detector} for {Multithreaded} {Programs}},
	volume = {15},
	issn = {0734-2071},
	shorttitle = {Eraser},
	url = {http://doi.acm.org/10.1145/265924.265927},
	doi = {10.1145/265924.265927},
	abstract = {Multithreaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This article describes a new tool, called Eraser, for dynamically detecting data races in lock-based multithreaded programs. Eraser uses binary rewriting techniques to monitor every shared-monory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multithreaded Web search engine, that demonstrate the effectiveness of this approach.},
	number = {4},
	urldate = {2017-11-22},
	journal = {ACM Trans. Comput. Syst.},
	author = {Savage, Stefan and Burrows, Michael and Nelson, Greg and Sobalvarro, Patrick and Anderson, Thomas},
	month = nov,
	year = {1997},
	keywords = {binary code modification, multithreaded programming, race detection},
	pages = {391--411},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/CMMTXIF6/Savage et al. - 1997 - Eraser A Dynamic Data Race Detector for Multithre.pdf:application/pdf}
}

@inproceedings{shavit_software_1995,
	address = {New York, NY, USA},
	series = {{PODC} '95},
	title = {Software {Transactional} {Memory}},
	isbn = {978-0-89791-710-0},
	url = {http://doi.acm.org/10.1145/224964.224987},
	doi = {10.1145/224964.224987},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the {Fourteenth} {Annual} {ACM} {Symposium} on {Principles} of {Distributed} {Computing}},
	publisher = {ACM},
	author = {Shavit, Nir and Touitou, Dan},
	year = {1995},
	pages = {204--213},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NHMYJL73/Shavit and Touitou - 1995 - Software Transactional Memory.pdf:application/pdf}
}

@techreport{volanschi_declarative_1997,
	type = {report},
	title = {Declarative {Specialization} of {Object}-{Oriented} {Programs}},
	url = {https://hal.inria.fr/inria-00073572/document},
	abstract = {Designing and implementing generic software components is encouraged by languages such as object-oriented ones and commonly advocated in most application areas. Generic software components have many advantages among which the most important is reusability. However, it comes at a price: genericity often incurs a loss of efficiency. This paper presents an approach aimed at reconciling genericity and efficiency. To do so, we introduce declarations to the Java language to enable a programmer to specify how generic programs should be specialized for a particular usage pattern. Our approach has been implemented as a compiler from our extended language into standard Java.},
	language = {en},
	urldate = {2017-11-22},
	institution = {INRIA},
	author = {Volanschi, Eugen-Nicolae and Consel, Charles and Muller, Gilles and Cowan, Crispin},
	year = {1997},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/A6CYMXET/Volanschi et al. - 1997 - Declarative Specialization of Object-Oriented Prog.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/7ZW6V9S3/inria-00073572.html:text/html}
}

@techreport{ducasse_coca:_1998,
	type = {report},
	title = {Coca: {A} {Debugger} for {C} {Based} on {Fine} {Grained} {Control} {Flow} and {Data} {Events}},
	shorttitle = {Coca},
	url = {https://hal.inria.fr/inria-00073198/document},
	abstract = {We present Coca, an automated debugger for C, where the breakpoint mechanism is based on events related to language constructs. Events have semantics whereas source lines used by most debuggers do not have any. A trace is a sequence of events. It can be seen as an ordered relation in a database. Users can specify precisely which events they want to see by specifying values for event attributes. At each event, visible variables can be queried. The trace query language is Prolog with a handful of primitives. The trace query mechanism searches through the execution traces using both control flow and data whereas debuggers usually search according to either control flow or data. As opposed to fully {\guillemotleft}relational{\guillemotright} debuggers which use plain database querying mechanisms, Coca trace querying mechanism does not require any storage. The analysis is done on the fly, synchronously with the traced execution. Coca is therefore more powerful than {\guillemotleft}source line{\guillemotright} debuggers and more efficient than relational debuggers.},
	language = {en},
	urldate = {2017-11-22},
	institution = {INRIA},
	author = {Ducass{\'e}, Mireille},
	year = {1998},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/GSCD3HPN/Ducass{\'e} - 1998 - Coca A Debugger for C Based on Fine Grained Contr.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/VM9ZNKUB/en.html:text/html}
}

@inproceedings{augustsson_cayenne_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Cayenne {\textemdash} {A} {Language} with {Dependent} {Types}},
	isbn = {978-3-540-66241-9 978-3-540-48506-3},
	url = {https://link.springer.com/chapter/10.1007/10704973_6},
	doi = {10.1007/10704973_6},
	abstract = {Cayenne is a Haskell-like language. The main difference between Haskell and Cayenne is that Cayenne has dependent types, i.e., the result type of a function may depend on the argument value, and types of record components (which can be types or values) may depend on other components. Cayenne also combines the syntactic categories for value expressions and type expressions; thus reducing the number of language concepts.Having dependent types and combined type and value expressions makes the language very powerful. It is powerful enough that a special module concept is unnecessary; ordinary records suffice. It is also powerful enough to encode predicate logic at the type level, allowing types to be used as specifications of programs. However, this power comes at a cost: type checking of Cayenne is undecidable. While this may appear to be a steep price to pay, it seems to work well in practice.},
	language = {en},
	urldate = {2017-11-22},
	booktitle = {Advanced {Functional} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Augustsson, Lennart},
	month = sep,
	year = {1998},
	pages = {240--267},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/RYDPIAUF/Augustsson - 1998 - Cayenne {\textemdash} A Language with Dependent Types.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/R7Q2B7VA/10704973_6.html:text/html}
}

@inproceedings{pugh_fixing_1999,
	address = {New York, NY, USA},
	series = {{JAVA} '99},
	title = {Fixing the {Java} {Memory} {Model}},
	isbn = {978-1-58113-161-1},
	url = {http://doi.acm.org/10.1145/304065.304106},
	doi = {10.1145/304065.304106},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the {ACM} 1999 {Conference} on {Java} {Grande}},
	publisher = {ACM},
	author = {Pugh, William},
	year = {1999},
	pages = {89--98},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/7MW7IE9U/Pugh - 1999 - Fixing the Java Memory Model.pdf:application/pdf}
}

@inproceedings{xi_dependent_1999,
	address = {New York, NY, USA},
	series = {{POPL} '99},
	title = {Dependent {Types} in {Practical} {Programming}},
	isbn = {978-1-58113-095-9},
	url = {http://doi.acm.org/10.1145/292540.292560},
	doi = {10.1145/292540.292560},
	abstract = {We present an approach to enriching the type system of ML with a restricted form of dependent types, where type index objects are drawn from a constraint domain C, leading to the DML(C) language schema. This allows specification and inference of significantly more precise type information, facilitating program error detection and compiler optimization. A major complication resulting from introducing dependent types is that pure type inference for the enriched system is no longer possible, but we show that type-checking a sufficiently annotated program in DML(C) can be reduced to constraint satisfaction in the constraint domain C. We exhibit the unobtrusiveness of our approach through practical examples and prove that DML(C) is conservative over ML. The main contribution of the paper lies in our language design, including the formulation of type-checking rules which makes the approach practical. To our knowledge, no previous type system for a general purpose programming language such as ML has combined dependent types with features including datatype declarations, higher-order functions, general recursions, let-polymorphism, mutable references, and exceptions. In addition, we have finished a prototype implementation of DML(C) for an integer constraint domain C, where constraints are linear inequalities (Xi and Pfenning 1998).},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 26th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Xi, Hongwei and Pfenning, Frank},
	year = {1999},
	pages = {214--227},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/5VXUPQVI/Xi and Pfenning - 1999 - Dependent Types in Practical Programming.pdf:application/pdf}
}

@inproceedings{chiba_load-time_2000,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Load-{Time} {Structural} {Reflection} in {Java}},
	isbn = {978-3-540-67660-7 978-3-540-45102-0},
	url = {https://link.springer.com/chapter/10.1007/3-540-45102-1_16},
	doi = {10.1007/3-540-45102-1_16},
	abstract = {The standard reflection API of Java provides the ability to introspect a program but not to alter program behavior. This paper presents an extension to the reflection API for addressing this limitation. Unlike other extensions enabling behavioral reflection, our extension called Javassist enables structural reflection in Java. For using a standard Java virtual machine (JVM) and avoiding a performance problem, Javassist allows structural reflection only before a class is loaded into the JVM. However, Javassist still covers various applications including a language extension emulating behavioral reflection. This paper also presents the design principles of Javassist, which distinguish Javassist from related work.},
	language = {en},
	urldate = {2017-11-22},
	booktitle = {{ECOOP} 2000 {\textemdash} {Object}-{Oriented} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Chiba, Shigeru},
	month = jun,
	year = {2000},
	pages = {313--336},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/M8LRDSEZ/Chiba - 2000 - Load-Time Structural Reflection in Java.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/9DVSWD3X/3-540-45102-1_16.html:text/html}
}

@inproceedings{guitart_fernandez_java_2000,
	title = {Java instrumentation suite: accurate analysis of {Java} threaded applications},
	copyright = {Open Access},
	shorttitle = {Java instrumentation suite},
	url = {http://upcommons.upc.edu/handle/2117/28290},
	language = {eng},
	urldate = {2017-11-22},
	author = {Guitart Fern{\'a}ndez, Jordi and Torres Vi{\~n}als, Jordi and Ayguad{\'e} Parra, Eduard and Oliver, Jose and Mancho, Labarta and Jos{\'e}, Jes{\'u}s},
	year = {2000},
	keywords = {{\`A}rees tem{\`a}tiques de la UPC::Inform{\`a}tica::Llenguatges de programaci{\'o}, Java (Computer program language), Java (Llenguatge de programaci{\'o})},
	pages = {15--25},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/CYB58AVX/Guitart Fern{\'a}ndez et al. - 2000 - Java instrumentation suite accurate analysis of J.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/6S25UF3T/28290.html:text/html}
}

@inproceedings{alpern_efficient_2001,
	address = {New York, NY, USA},
	series = {{OOPSLA} '01},
	title = {Efficient {Implementation} of {Java} {Interfaces}: {Invokeinterface} {Considered} {Harmless}},
	isbn = {978-1-58113-335-6},
	shorttitle = {Efficient {Implementation} of {Java} {Interfaces}},
	url = {http://doi.acm.org/10.1145/504282.504291},
	doi = {10.1145/504282.504291},
	abstract = {Single superclass inheritance enables simple and efficient table-driven virtual method dispathc. However, virtual method table dispatch does not handle multiple inheritance and interfaces. This complication has led to a widespread misimpression that interface method dispatch is inherently inefficient. This paper argues that with proper implementation techniques, Java interfaces need not be a source of significant performance degradation.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Alpern, Bowen and Cocchi, Anthony and Fink, Stephen and Grove, David},
	year = {2001},
	pages = {108--124},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/AXHNCVKU/Alpern et al. - 2001 - Efficient Implementation of Java Interfaces Invok.pdf:application/pdf}
}

@inproceedings{ingalls_smalltalk-76_1978,
	address = {New York, NY, USA},
	series = {{POPL} '78},
	title = {The {Smalltalk}-76 {Programming} {System} {Design} and {Implementation}},
	url = {http://doi.acm.org/10.1145/512760.512762},
	doi = {10.1145/512760.512762},
	abstract = {This paper describes a programming system based on the metaphor of communicating objects. Experience with a running system shows that this model provides flexibility, modularity and compactness. A compiled representation for the language is presented, along with an interpreter suitable for microcoding. The object-oriented model provides naturally efficient addressing; a corresponding virtual memory is described which offers dense utilization of resident space.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 5th {ACM} {SIGACT}-{SIGPLAN} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Ingalls, Daniel H. H.},
	year = {1978},
	pages = {9--16},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/4GQ7A9IJ/Ingalls - 1978 - The Smalltalk-76 Programming System Design and Imp.pdf:application/pdf}
}

@article{igarashi_featherweight_2001,
	title = {Featherweight {Java}: {A} {Minimal} {Core} {Calculus} for {Java} and {GJ}},
	volume = {23},
	issn = {0164-0925},
	shorttitle = {Featherweight {Java}},
	url = {http://doi.acm.org/10.1145/503502.503505},
	doi = {10.1145/503502.503505},
	abstract = {Several recent studies have introduced lightweight versions of Java: reduced languages in which complex features like threads and reflection are dropped to enable rigorous arguments about key properties such as type safety. We carry this process a step further, omitting almost all features of the full language (including interfaces and even assignment) to obtain a small calculus, Featherweight Java, for which rigorous proofs are not only possible but easy. Featherweight Java bears a similar relation to Java as the lambda-calculus does to languages such as ML and Haskell. It offers a similar computational "feel," providing classes, methods, fields, inheritance, and dynamic typecasts with a semantics closely following Java's. A proof of type safety for Featherweight Java thus illustrates many of the interesting features of a safety proof for the full language, while remaining pleasingly compact. The minimal syntax, typing rules, and operational semantics of Featherweight Java make it a handy tool for studying the consequences of extensions and variations. As an illustration of its utility in this regard, we extend Featherweight Java with generic classes in the style of GJ (Bracha, Odersky, Stoutamire, and Wadler) and give a detailed proof of type safety. The extended system formalizes for the first time some of the key features of GJ.},
	number = {3},
	urldate = {2017-11-22},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Igarashi, Atsushi and Pierce, Benjamin C. and Wadler, Philip},
	month = may,
	year = {2001},
	keywords = {Java, Compilation, generic classes, language design, language semantics},
	pages = {396--450},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/LNEIATY6/Igarashi et al. - 2001 - Featherweight Java A Minimal Core Calculus for Ja.pdf:application/pdf}
}

@inproceedings{beebee_implementation_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Implementation} of {Scoped} {Memory} for {Real}-{Time} {Java}},
	isbn = {978-3-540-42673-8 978-3-540-45449-6},
	url = {https://link.springer.com/chapter/10.1007/3-540-45449-7_21},
	doi = {10.1007/3-540-45449-7_21},
	abstract = {This paper presents our experience implementing the memory management extensions in the Real-Time Specification for Java. These extensions are designed to given real-time programmers the control they need to obtain predictable memory system behavior while preserving Java{\textquoteright}s safe memory model.We describe our implementation of certain dynamic checks required by the Real-Time Java extensions. In particular, we describe how to perform these checks in a way that avoids harmful interactions between the garbage collector and the memory management system. We also found that extensive debugging support was necessary during the development of Real-Time Java programs. We therefore used a static analysis and a dynamic debugging package during the development of our benchmark applications.},
	language = {en},
	urldate = {2017-11-22},
	booktitle = {Embedded {Software}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Beebee, William S. and Rinard, Martin},
	month = oct,
	year = {2001},
	pages = {289--305},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/83VL2CWC/Beebee and Rinard - 2001 - An Implementation of Scoped Memory for Real-Time J.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/96K25Z6E/3-540-45449-7_21.html:text/html}
}

@inproceedings{grimmer_dynamically_2015,
	address = {New York, NY, USA},
	series = {{MODULARITY} 2015},
	title = {Dynamically {Composing} {Languages} in a {Modular} {Way}: {Supporting} {C} {Extensions} for {Dynamic} {Languages}},
	isbn = {978-1-4503-3249-1},
	shorttitle = {Dynamically {Composing} {Languages} in a {Modular} {Way}},
	url = {http://doi.acm.org/10.1145/2724525.2728790},
	doi = {10.1145/2724525.2728790},
	abstract = {Many dynamic languages such as Ruby, Python and Perl offer some kind of functionality for writing parts of applications in a lower-level language such as C. These C extension modules are usually written against the API of an interpreter, which provides access to the higher-level language's internal data structures. Alternative implementations of the high-level languages often do not support such C extensions because implementing the same API as in the original implementations is complicated and limits performance. In this paper we describe a novel approach for modular composition of languages that allows dynamic languages to support C extensions through interpretation. We propose a flexible and reusable cross-language mechanism that allows composing multiple language interpreters, which run on the same VM and share the same form of intermediate representation - in this case abstract syntax trees. This mechanism allows us to efficiently exchange runtime data across different interpreters and also enables the dynamic compiler of the host VM to inline and optimize programs across multiple language boundaries. We evaluate our approach by composing a Ruby interpreter with a C interpreter. We run existing Ruby C extensions and show how our system executes combined Ruby and C modules on average over 3x faster than the conventional implementation of Ruby with native C extensions, and on average over 20x faster than an existing alternate Ruby implementation on the JVM (JRuby) calling compiled C extensions through a bridge interface. We demonstrate that cross-language inlining, which is not possible with native code, is performance-critical by showing how speedup is reduced by around 50\% when it is disabled.},
	urldate = {2017-11-22},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Modularity}},
	publisher = {ACM},
	author = {Grimmer, Matthias and Seaton, Chris and W{\"u}rthinger, Thomas and M{\"o}ssenb{\"o}ck, Hanspeter},
	year = {2015},
	keywords = {Optimization, C, Cross-language, Language Interoperability, Native Extension, Ruby, Virtual Machine},
	pages = {1--13},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/RUJXVNWF/Grimmer et al. - 2015 - Dynamically Composing Languages in a Modular Way .pdf:application/pdf}
}

@inproceedings{foster_flow-sensitive_2002,
	address = {New York, NY, USA},
	series = {{PLDI} '02},
	title = {Flow-sensitive {Type} {Qualifiers}},
	isbn = {978-1-58113-463-6},
	url = {http://doi.acm.org/10.1145/512529.512531},
	doi = {10.1145/512529.512531},
	abstract = {We present a system for extending standard type systems with flow-sensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flow-sensitively---the underlying standard types are unchanged, which allows us to obtain an efficient constraint-based inference algorithm that integrates flow-insensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flow-sensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.},
	urldate = {2017-11-23},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2002 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Foster, Jeffrey S. and Terauchi, Tachio and Aiken, Alex},
	year = {2002},
	keywords = {alias analysis, constraints, effect inference, flow-sensitivity, linux kernel, locking, restrict, type qualifiers, types},
	pages = {1--12},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GV3ZW85U/Foster et al. - 2002 - Flow-sensitive Type Qualifiers.pdf:application/pdf}
}

@inproceedings{grossman_region-based_2002,
	address = {New York, NY, USA},
	series = {{PLDI} '02},
	title = {Region-based {Memory} {Management} in {Cyclone}},
	isbn = {978-1-58113-463-6},
	url = {http://doi.acm.org/10.1145/512529.512563},
	doi = {10.1145/512529.512563},
	abstract = {Cyclone is a type-safe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing type-safety. In this paper, we focus on the region-based memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region effects reduces this burden. As a result, we integrate C idioms in a region-based framework. In our experience, porting legacy C to Cyclone has required altering about 8\% of the code; of the changes, only 6\% (of the 8\%) were region annotations.},
	urldate = {2017-11-23},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2002 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Grossman, Dan and Morrisett, Greg and Jim, Trevor and Hicks, Michael and Wang, Yanling and Cheney, James},
	year = {2002},
	pages = {282--293},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DUQYG33H/Grossman et al. - 2002 - Region-based Memory Management in Cyclone.pdf:application/pdf}
}

@article{soloway_empirical_1984,
	title = {Empirical {Studies} of {Programming} {Knowledge}},
	volume = {SE-10},
	issn = {0098-5589},
	doi = {10.1109/TSE.1984.5010283},
	abstract = {We suggest that expert programmers have and use two types of programming knowledge: 1) programming plans, which are generic program fragments that represent stereotypic action sequences in programming, and 2) rules of programming discourse, which capture the conventions in programming and govern the composition of the plans into programs. We report here on two empirical studies that attempt to evaluate the above hypothesis. Results from these studies do in fact support our claim.},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {Soloway, E. and Ehrlich, K.},
	month = sep,
	year = {1984},
	keywords = {Programming profession, Circuits, Cognitive models of programming, Functional programming, novice/expert differences, Physics, program conprehension, Psychology, software psychology, Text processing},
	pages = {595--609},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/UVYQFJAB/5010283.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/VEG4H8ZW/Soloway and Ehrlich - 1984 - Empirical Studies of Programming Knowledge.pdf:application/pdf}
}

@inproceedings{budd_theoretical_1980,
	address = {New York, NY, USA},
	series = {{POPL} '80},
	title = {Theoretical and {Empirical} {Studies} on {Using} {Program} {Mutation} to {Test} the {Functional} {Correctness} of {Programs}},
	isbn = {978-0-89791-011-8},
	url = {http://doi.acm.org/10.1145/567446.567468},
	doi = {10.1145/567446.567468},
	urldate = {2017-11-25},
	booktitle = {Proceedings of the 7th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Budd, Timothy A. and DeMillo, Richard A. and Lipton, Richard J. and Sayward, Frederick G.},
	year = {1980},
	pages = {220--233},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/KAUFK5G4/Budd et al. - 1980 - Theoretical and Empirical Studies on Using Program.pdf:application/pdf}
}

@article{prechelt_empirical_2000,
	title = {An empirical comparison of seven programming languages},
	volume = {33},
	issn = {0018-9162},
	doi = {10.1109/2.876288},
	abstract = {Often heated, debates regarding different programming languages' effectiveness remain inconclusive because of scarce data and a lack of direct comparisons. The author addresses that challenge, comparatively analyzing 80 implementations of the phone-code program in seven different languages (C, C++, Java, Perl, Python, Rexx and Tcl). Further, for each language, the author analyzes several separate implementations by different programmers. The comparison investigates several aspects of each language, including program length, programming effort, runtime efficiency, memory consumption, and reliability. The author uses comparisons to present insight into program language performance},
	number = {10},
	journal = {Computer},
	author = {Prechelt, L.},
	month = oct,
	year = {2000},
	keywords = {Java, Programming profession, Runtime, Computer languages, Production, Program processors, Statistics, C language, authoring languages, C++, C++ language, memory consumption, Perl, phone-code program, program language performance, program length, programming, programming language comparison, Python, Read-write memory, Rexx, runtime efficiency, software reliability, Sun, Tcl, Workstations},
	pages = {23--29},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/Q7MZKNJJ/876288.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/4MSJ8NRW/Prechelt - 2000 - An empirical comparison of seven programming langu.pdf:application/pdf}
}

@inproceedings{haas_bringing_2017,
	address = {New York, NY, USA},
	series = {{PLDI} 2017},
	title = {Bringing the {Web} {Up} to {Speed} with {WebAssembly}},
	isbn = {978-1-4503-4988-8},
	url = {http://doi.acm.org/10.1145/3062341.3062363},
	doi = {10.1145/3062341.3062363},
	abstract = {The maturation of the Web platform has given rise to sophisticated and demanding Web applications such as interactive 3D visualization, audio and video software, and games. With that, efficiency and security of code on the Web has become more important than ever. Yet JavaScript as the only built-in language of the Web is not well-equipped to meet these requirements, especially as a compilation target.   Engineers from the four major browser vendors have risen to the challenge and collaboratively designed a portable low-level bytecode called WebAssembly. It offers compact representation, efficient validation and compilation, and safe low to no-overhead execution. Rather than committing to a specific programming model, WebAssembly is an abstraction over modern hardware, making it language-, hardware-, and platform-independent, with use cases beyond just the Web. WebAssembly has been designed with a formal semantics from the start. We describe the motivation, design and formal semantics of WebAssembly and provide some preliminary experience with implementations.},
	urldate = {2017-11-25},
	booktitle = {Proceedings of the 38th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Haas, Andreas and Rossberg, Andreas and Schuff, Derek L. and Titzer, Ben L. and Holman, Michael and Gohman, Dan and Wagner, Luke and Zakai, Alon and Bastien, JF},
	year = {2017},
	keywords = {virtual machines, type systems, programming languages, assembly languages, just-in-time compilers},
	pages = {185--200},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/EF3PQK7Y/Haas et al. - 2017 - Bringing the Web Up to Speed with WebAssembly.pdf:application/pdf}
}

@inproceedings{bracha_strongtalk:_1993,
	address = {New York, NY, USA},
	series = {{OOPSLA} '93},
	title = {Strongtalk: {Typechecking} {Smalltalk} in a {Production} {Environment}},
	isbn = {978-0-89791-587-8},
	shorttitle = {Strongtalk},
	url = {http://doi.acm.org/10.1145/165854.165893},
	doi = {10.1145/165854.165893},
	urldate = {2017-11-25},
	booktitle = {Proceedings of the {Eighth} {Annual} {Conference} on {Object}-oriented {Programming} {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Bracha, Gilad and Griswold, David},
	year = {1993},
	pages = {215--230},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/H8G9RBI3/Bracha and Griswold - 1993 - Strongtalk Typechecking Smalltalk in a Production.pdf:application/pdf}
}

@inproceedings{livshits_reflection_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Reflection {Analysis} for {Java}},
	isbn = {978-3-540-29735-2 978-3-540-32247-4},
	url = {https://link.springer.com/chapter/10.1007/11575467_11},
	doi = {10.1007/11575467_11},
	abstract = {Reflection has always been a thorn in the side of Java static analysis tools. Without a full treatment of reflection, static analysis tools are both incomplete because some parts of the program may not be included in the application call graph, and unsound because the static analysis does not take into account reflective features of Java that allow writes to object fields and method invocations. However, accurately analyzing reflection has always been difficult, leading to most static analysis tools treating reflection in an unsound manner or just ignoring it entirely. This is unsatisfactory as many modern Java applications make significant use of reflection.In this paper we propose a static analysis algorithm that uses points-to information to approximate the targets of reflective calls as part of call graph construction. Because reflective calls may rely on input to the application, in addition to performing reflection resolution, our algorithm also discovers all places in the program where user-provided specifications are necessary to fully resolve reflective targets. As an alternative to user-provided specifications, we also propose a reflection resolution approach based on type cast information that reduces the need for user input, but typically results in a less precise call graph.We have implemented the reflection resolution algorithms described in this paper and applied them to a set of six large, widely-used benchmark applications consisting of more than 600,000 lines of code combined. Experiments show that our technique is effective for resolving most reflective calls without any user input. Certain reflective calls, however, cannot be resolved at compile time precisely. Relying on a user-provided specification to obtain a conservative call graph results in graphs that contain 1.43 to 6.58 times more methods that the original. In one case, a conservative call graph has 7,047 more methods than a call graph that does not interpret reflective calls. In contrast, ignoring reflection leads to missing substantial portions of the application call graph.},
	language = {en},
	urldate = {2017-11-26},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Livshits, Benjamin and Whaley, John and Lam, Monica S.},
	month = nov,
	year = {2005},
	pages = {139--160},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/NXVAYMZH/Livshits et al. - 2005 - Reflection Analysis for Java.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/RJE4DL2I/11575467_11.html:text/html}
}

@inproceedings{banados_schwerter_theory_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {A {Theory} of {Gradual} {Effect} {Systems}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628149},
	doi = {10.1145/2628136.2628149},
	abstract = {Effect systems have the potential to help software developers, but their practical adoption has been very limited. We conjecture that this limited adoption is due in part to the difficulty of transitioning from a system where effects are implicit and unrestricted to a system with a static effect discipline, which must settle for conservative checking in order to be decidable. To address this hindrance, we develop a theory of gradual effect checking, which makes it possible to incrementally annotate and statically check effects, while still rejecting statically inconsistent programs. We extend the generic type-and-effect framework of Marino and Millstein with a notion of unknown effects, which turns out to be significantly more subtle than unknown types in traditional gradual typing. We appeal to abstract interpretation to develop and validate the concepts of gradual effect checking. We also demonstrate how an effect system formulated in Marino and Millstein's framework can be automatically extended to support gradual checking.},
	urldate = {2017-11-27},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Ba{\~n}ados Schwerter, Felipe and Garcia, Ronald and Tanter, {\'E}ric},
	year = {2014},
	keywords = {abstract interpretation, gradual typing, type-and-effect systems},
	pages = {283--295},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DQ423LAM/Ba{\~n}ados Schwerter et al. - 2014 - A Theory of Gradual Effect Systems.pdf:application/pdf}
}

@inproceedings{li_self-inferencing_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Self-inferencing {Reflection} {Resolution} for {Java}},
	isbn = {978-3-662-44201-2 978-3-662-44202-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-662-44202-9_2},
	doi = {10.1007/978-3-662-44202-9_2},
	abstract = {Reflection has always been an obstacle both for sound and for effective under-approximate pointer analysis for Java applications. In pointer analysis tools, reflection is either ignored or handled partially, resulting in missed, important behaviors. In this paper, we present our findings on reflection usage in Java benchmarks and applications. Guided by these findings, we introduce a static reflection analysis, called Elf, by exploiting a self-inferencing property inherent in many reflective calls. Given a reflective call, the basic idea behind Elf is to automatically infer its targets (methods or fields) based on the dynamic types of the arguments of its target calls and the downcasts (if any) on their returned values, if its targets cannot be already obtained from the Class, Method or Field objects on which the reflective call is made. We evaluate Elf against Doop{\textquoteright}s state-of-the-art reflection analysis performed in the same context-sensitive Andersen{\textquoteright}s pointer analysis using all 11 DaCapo benchmarks and two applications. Elf can make a disciplined tradeoff among soundness, precision and scalability while also discovering usually more reflective targets. Elf is useful for any pointer analysis, particularly under-approximate techniques deployed for such clients as bug detection, program understanding and speculative compiler optimization.},
	language = {en},
	urldate = {2017-11-27},
	booktitle = {{ECOOP} 2014 {\textendash} {Object}-{Oriented} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Li, Yue and Tan, Tian and Sui, Yulei and Xue, Jingling},
	month = jul,
	year = {2014},
	pages = {27--53},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/EMWPLUKU/Li et al. - 2014 - Self-inferencing Reflection Resolution for Java.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/4XPRPCYV/978-3-662-44202-9_2.html:text/html}
}

@inproceedings{li_effective_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Effective {Soundness}-{Guided} {Reflection} {Analysis}},
	isbn = {978-3-662-48287-2 978-3-662-48288-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-662-48288-9_10},
	doi = {10.1007/978-3-662-48288-9_10},
	abstract = {We introduce Solar, the first reflection analysis that allows its soundness to be reasoned about when some assumptions are met and produces significantly improved under-approximations otherwise. In both settings, Solar has three novel aspects: (1) lazy heap modeling for reflective allocation sites, (2) collective inference for improving the inferences on related reflective calls, and (3) automatic identification of {\textquotedblleft}problematic{\textquotedblright} reflective calls that may threaten its soundness, precision and scalability, thereby enabling their improvement via lightweight annotations. We evaluate Solar against two state-of-the-art solutions, Doop and Elf, with the three treated as under-approximate reflection analyses, using 11 large Java benchmarks and applications. Solar is significantly more sound while achieving nearly the same precision and running only several-fold more slowly, subject to only 7 annotations in 3 programs.},
	language = {en},
	urldate = {2017-11-27},
	booktitle = {Static {Analysis}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Li, Yue and Tan, Tian and Xue, Jingling},
	month = sep,
	year = {2015},
	pages = {162--180},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/RRKJYQEP/Li et al. - 2015 - Effective Soundness-Guided Reflection Analysis.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/27HKMVWG/978-3-662-48288-9_10.html:text/html}
}

@phdthesis{livshits_improving_2006,
	address = {Stanford, California},
	title = {Improving {Software} {Security} with {Precise} {Static} and {Runtime} {Analysis}},
	school = {Stanford University},
	author = {Livshits, Benjamin},
	year = {2006},
	file = {thesis.pdf:/Users/luigi/work/zotero/storage/SAMQZDXG/thesis.pdf:application/pdf}
}

@inproceedings{hirzel_pointer_2004,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pointer {Analysis} in the {Presence} of {Dynamic} {Class} {Loading}},
	isbn = {978-3-540-22159-3 978-3-540-24851-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-24851-4_5},
	doi = {10.1007/978-3-540-24851-4_5},
	abstract = {Many optimizations need precise pointer analyses to be effective. Unfortunately, some Java features, such as dynamic class loading, reflection, and native methods, make pointer analyses difficult to develop. Hence, prior pointer analyses for Java either ignore these features or are overly conservative. This paper presents the first non-trivial pointer analysis that deals with all Java language features. This paper identifies all problems in performing Andersen{\textquoteright}s pointer analysis for the full Java language, presents solutions to those problems, and uses a full implementation of the solutions in Jikes RVM for validation and performance evaluation. The results from this work should be transferable to other analyses and to other languages.},
	language = {en},
	urldate = {2017-11-27},
	booktitle = {{ECOOP} 2004 {\textendash} {Object}-{Oriented} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Hirzel, Martin and Diwan, Amer and Hind, Michael},
	month = jun,
	year = {2004},
	pages = {96--122},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/XKESHMCS/Hirzel et al. - 2004 - Pointer Analysis in the Presence of Dynamic Class .pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/BXGQ42JQ/10.html:text/html}
}

@incollection{jones_reflection_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Reflection} on {Types}},
	isbn = {978-3-319-30935-4 978-3-319-30936-1},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-30936-1_16},
	abstract = {The ability to perform type tests at runtime blurs the line between statically-typed and dynamically-checked languages. Recent developments in Haskell{\textquoteright}s type system allow even programs that use reflection to themselves be statically typed, using a type-indexed runtime representation of types called {\textbackslash}????????????\{??????????????\}{\textbackslash}textit\{TypeRep\}{\textbackslash}textsf \{{\textbackslash}textit\{TypeRep\}\}. As a result we can build dynamic types as an ordinary, statically-typed library, on top of {\textbackslash}????????????\{??????????????\}{\textbackslash}textit\{TypeRep\}{\textbackslash}textsf \{{\textbackslash}textit\{TypeRep\}\} in an open-world context.},
	language = {en},
	urldate = {2017-11-28},
	booktitle = {A {List} of {Successes} {That} {Can} {Change} the {World}},
	publisher = {Springer, Cham},
	author = {Jones, Simon Peyton and Weirich, Stephanie and Eisenberg, Richard A. and Vytiniotis, Dimitrios},
	year = {2016},
	doi = {10.1007/978-3-319-30936-1_16},
	pages = {292--317},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/YSXWDV9B/Jones et al. - 2016 - A Reflection on Types.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/M6ZD5RH6/978-3-319-30936-1_16.html:text/html}
}

@inproceedings{zhang_toward_2014,
	address = {New York, NY, USA},
	series = {{POPL} '14},
	title = {Toward {General} {Diagnosis} of {Static} {Errors}},
	isbn = {978-1-4503-2544-8},
	url = {http://doi.acm.org/10.1145/2535838.2535870},
	doi = {10.1145/2535838.2535870},
	abstract = {We introduce a general way to locate programmer mistakes that are detected by static analyses such as type checking. The program analysis is expressed in a constraint language in which mistakes result in unsatisfiable constraints. Given an unsatisfiable system of constraints, both satisfiable and unsatisfiable constraints are analyzed, to identify the program expressions most likely to be the cause of unsatisfiability. The likelihood of different error explanations is evaluated under the assumption that the programmer's code is mostly correct, so the simplest explanations are chosen, following Bayesian principles. For analyses that rely on programmer-stated assumptions, the diagnosis also identifies assumptions likely to have been omitted. The new error diagnosis approach has been implemented for two very different program analyses: type inference in OCaml and information flow checking in Jif. The effectiveness of the approach is evaluated using previously collected programs containing errors. The results show that when compared to existing compilers and other tools, the general technique identifies the location of programmer errors significantly more accurately.},
	urldate = {2017-11-28},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Zhang, Danfeng and Myers, Andrew C.},
	year = {2014},
	keywords = {type inference, information flow, error diagnosis, static program analysis},
	pages = {569--581},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/C9XSKHFR/Zhang and Myers - 2014 - Toward General Diagnosis of Static Errors.pdf:application/pdf}
}

@article{wagner_perspective:_1996,
	title = {{PERSPECTIVE}: {COMPLEX} {ADAPTATIONS} {AND} {THE} {EVOLUTION} {OF} {EVOLVABILITY}},
	volume = {50},
	issn = {1558-5646},
	shorttitle = {{PERSPECTIVE}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1558-5646.1996.tb02339.x/full},
	doi = {10.1111/j.1558-5646.1996.tb02339.x},
	abstract = {The problem of complex adaptations is studied in two largely disconnected research traditions: evolutionary biology and evolutionary computer science. This paper summarizes the results from both areas...},
	language = {en},
	number = {3},
	urldate = {2017-11-29},
	journal = {Evolution},
	author = {Wagner, G{\"u}nter P. and Altenberg, Lee},
	month = jun,
	year = {1996},
	pages = {967--976},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/K36L5BDG/Wagner and Altenberg - 1996 - PERSPECTIVE COMPLEX ADAPTATIONS AND THE EVOLUTION.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/I3N7D59I/abstract.html:text/html}
}

@inproceedings{de_roover_soul_2011,
	address = {New York, NY, USA},
	series = {{PPPJ} '11},
	title = {The {SOUL} {Tool} {Suite} for {Querying} {Programs} in {Symbiosis} with {Eclipse}},
	isbn = {978-1-4503-0935-6},
	url = {http://doi.acm.org/10.1145/2093157.2093168},
	doi = {10.1145/2093157.2093168},
	abstract = {Program queries can answer important software engineering questions that range from "which expressions are cast to this type?" over "does my program attempt to read from a closed file?" to "does my code follow the prescribed design?". In this paper, we present a comprehensive tool suite for querying Java programs. It consists of the logic program query language Soul, the Cava library of predicates for quantifying over an Eclipse workspace and the Eclipse plugin Barista for launching queries and inspecting their results. Barista allows other Eclipse plugins to peruse program query results which is facilitated by the symbiosis of Soul with Java -- setting Soul apart from other program query languages. This symbiosis enables the Cava library to forego the predominant transcription to logic facts of the queried program. Instead, the library queries the actual AST nodes used by Eclipse itself, making it trivial for any Eclipse plugin to find the AST nodes that correspond to a query result. Moreover, such plugins do not have to worry about having queried stale program information. We illustrate the extensibility of our suite by implementing a tool for co-evolving source code and annotations using program queries.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Principles} and {Practice} of {Programming} in {Java}},
	publisher = {ACM},
	author = {De Roover, Coen and Noguera, Carlos and Kellens, Andy and Jonckers, Vivane},
	year = {2011},
	keywords = {program analysis, integrated development environments, logic programming, program queries, software engineering tools},
	pages = {71--80},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/FT47EZRD/De Roover et al. - 2011 - The SOUL Tool Suite for Querying Programs in Symbi.pdf:application/pdf}
}

@inproceedings{wadler_theorems_1989,
	address = {New York, NY, USA},
	series = {{FPCA} '89},
	title = {Theorems for {Free}!},
	isbn = {978-0-89791-328-7},
	url = {http://doi.acm.org/10.1145/99370.99404},
	doi = {10.1145/99370.99404},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the {Fourth} {International} {Conference} on {Functional} {Programming} {Languages} and {Computer} {Architecture}},
	publisher = {ACM},
	author = {Wadler, Philip},
	year = {1989},
	pages = {347--359},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TSZ592RZ/Wadler - 1989 - Theorems for Free!.pdf:application/pdf}
}

@article{wadler_critique_1987,
	title = {A {Critique} of {Abelson} and {Sussman} or {Why} {Calculating} is {Better} {Than} {Scheming}},
	volume = {22},
	issn = {0362-1340},
	url = {http://doi.acm.org/10.1145/24697.24706},
	doi = {10.1145/24697.24706},
	number = {3},
	urldate = {2017-11-29},
	journal = {SIGPLAN Not.},
	author = {Wadler, P},
	month = mar,
	year = {1987},
	pages = {83--94},
	file = {wadler87.pdf:/Users/luigi/work/zotero/storage/QV5AZYG3/wadler87.pdf:application/pdf}
}

@inproceedings{wadler_comprehending_1990,
	address = {New York, NY, USA},
	series = {{LFP} '90},
	title = {Comprehending {Monads}},
	isbn = {978-0-89791-368-3},
	url = {http://doi.acm.org/10.1145/91556.91592},
	doi = {10.1145/91556.91592},
	abstract = {Category theorists invented monads in the 1960's to concisely express certain aspects of universal algebra. Functional programmers invented list comprehensions in the 1970's to concisely express certain programs involving lists. This paper shows how list comprehensions may be generalised to an arbitrary monad, and how the resulting programming feature can concisely express in a pure functional language some programs that manipulate state, handle exceptions, parse text, or invoke continuations. A new solution to the old problem of destructive array update is also presented. No knowledge of category theory is assumed.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the 1990 {ACM} {Conference} on {LISP} and {Functional} {Programming}},
	publisher = {ACM},
	author = {Wadler, Philip},
	year = {1990},
	pages = {61--78},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/B39RNP65/Wadler - 1990 - Comprehending Monads.pdf:application/pdf}
}

@inproceedings{whaley_cloning-based_2004,
	address = {New York, NY, USA},
	series = {{PLDI} '04},
	title = {Cloning-based {Context}-sensitive {Pointer} {Alias} {Analysis} {Using} {Binary} {Decision} {Diagrams}},
	isbn = {978-1-58113-807-8},
	url = {http://doi.acm.org/10.1145/996841.996859},
	doi = {10.1145/996841.996859},
	abstract = {This paper presents the first scalable context-sensitive, inclusion-based pointer alias analysis for Java programs. Our approach to context sensitivity is to create a clone of a method for every context of interest, and run a context-insensitive algorithm over the expanded call graph to get context-sensitive results. For precision, we generate a clone for every acyclic path through a program's call graph, treating methods in a strongly connected component as a single node. Normally, this formulation is hopelessly intractable as a call graph often has 10 14 acyclic paths or more. We show that these exponential relations can be computed efficiently using binary decision diagrams (BDDs). Key to the scalability of the technique is a context numbering scheme that exposes the commonalities across contexts. We applied our algorithm to the most popular applications available on Sourceforge, and found that the largest programs, with hundreds of thousands of Java bytecodes, can be analyzed in under 20 minutes.This paper shows that pointer analysis, and many other queries and algorithms, can be described succinctly and declaratively using Datalog, a logic programming language. We have developed a system called bddbddb that automatically translates Datalog programs into highly efficient BDD implementations. We used this approach to develop a variety of context-sensitive algorithms including side effect analysis, type analysis, and escape analysis.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2004 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Whaley, John and Lam, Monica S.},
	year = {2004},
	keywords = {Java, program analysis, binary decision diagrams, scalable, Datalog, logic programming, cloning, context-sensitive, inclusion-based, pointer analysis},
	pages = {131--144},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GEEUFRY3/Whaley and Lam - 2004 - Cloning-based Context-sensitive Pointer Alias Anal.pdf:application/pdf}
}

@inproceedings{stancu_comparing_2014,
	address = {New York, NY, USA},
	series = {{PPPJ} '14},
	title = {Comparing {Points}-to {Static} {Analysis} with {Runtime} {Recorded} {Profiling} {Data}},
	isbn = {978-1-4503-2926-2},
	url = {http://doi.acm.org/10.1145/2647508.2647524},
	doi = {10.1145/2647508.2647524},
	abstract = {We present an empirical study that sheds new light on static analysis results precision by comparing them with runtime collected data. Our motivation is finding additional sources of information that can guide static analysis for increased application performance. This is the first step in formulating an adaptive approach to static analysis that uses dynamic information to increase results precision of frequently executed code. The adaptive approach allows static analysis to (i) scale to real world applications (ii) identify important optimization opportunities. Our preliminary results show that runtime profiling is 10\% more accurate in optimizing frequently executed virtual calls and 73\% more accurate in optimizing frequently executed type checks.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Principles} and {Practices} of {Programming} on the {Java} {Platform}: {Virtual} {Machines}, {Languages}, and {Tools}},
	publisher = {ACM},
	author = {Stancu, Codru{\c t} and Wimmer, Christian and Brunthaler, Stefan and Larsen, Per and Franz, Michael},
	year = {2014},
	keywords = {static program analysis, feedback directed optimizations, performance optimization, runtime profiling},
	pages = {157--168},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/9T6KEK9K/Stancu et al. - 2014 - Comparing Points-to Static Analysis with Runtime R.pdf:application/pdf}
}

@inproceedings{hackett_under-performing_2014,
	address = {New York, NY, USA},
	series = {{IFL} '13},
	title = {The {Under}-{Performing} {Unfold}: {A} {New} {Approach} to {Optimising} {Corecursive} {Programs}},
	isbn = {978-1-4503-2988-0},
	shorttitle = {The {Under}-{Performing} {Unfold}},
	url = {http://doi.acm.org/10.1145/2620678.2620679},
	doi = {10.1145/2620678.2620679},
	abstract = {This paper presents a new approach to optimising corecursive programs by factorisation. In particular, we focus on programs written using the corecursion operator unfold. We use and expand upon the proof techniques of guarded coinduction and unfold fusion, capturing a pattern of generalising coinductive hypotheses by means of abstraction and representation functions. The pattern we observe is simple, has not been observed before, and is widely applicable. We develop a general program factorisation theorem from this pattern, demonstrating its utility with a range of practical examples.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the 25th {Symposium} on {Implementation} and {Application} of {Functional} {Languages}},
	publisher = {ACM},
	author = {Hackett, Jennifer and Hutton, Graham and Jaskelioff, Mauro},
	year = {2014},
	keywords = {coinduction, factorisation, fusion, unfolds},
	pages = {1:4321--1:4332},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HEAQWJ9Z/Hackett et al. - 2014 - The Under-Performing Unfold A New Approach to Opt.pdf:application/pdf}
}

@techreport{bolingbroke_call-by-need_2013,
	title = {Call-by-need supercompilation},
	url = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html},
	number = {UCAM-CL-TR-835},
	urldate = {2017-11-29},
	institution = {University of Cambridge, Computer Laboratory},
	author = {Bolingbroke, Maximilian C.},
	year = {2013},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/IQYQHP27/Bolingbroke - 2013 - Call-by-need supercompilation.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/SQ7MSG7G/UCAM-CL-TR-835.html:text/html}
}

@article{strom_typestate:_1986,
	title = {Typestate: {A} programming language concept for enhancing software reliability},
	volume = {SE-12},
	issn = {0098-5589},
	shorttitle = {Typestate},
	doi = {10.1109/TSE.1986.6312929},
	abstract = {The authors introduce a new programming language concept, called typestate, which is a refinement of the concept of type. Whereas the type of a data object determines the set of operations over permitted on the object, typestate determines the subset of these operations which is permitted in a particular context. Typestate tracking is a program analysis technique which enhances program reliability by detecting at compile-time syntactically legal but semantically undefined execution sequences. These include reading a variable before it has been initialized and dereferencing a pointer after the dynamic object has been deallocated. The authors define typestate, give examples of its application, and show how typestate checking may be embedded into a compiler. They discuss the consequences of typestate checking for software reliability and software structure, and summarize their experience in using a high-level language incorporating typestate checking.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Strom, R. E. and Yemini, S.},
	month = jan,
	year = {1986},
	keywords = {Computer languages, program compilers, program verification, type checking, programming language, Program processors, Context, compiler, data structures, software reliability, compile-time, data object, dynamic object, high-level language, Law, Program analysis, program analysis technique, security, Software reliability, typestate, undefined execution sequences},
	pages = {157--171},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/N6TFWZK6/6312929.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/6U5MU7PP/Strom and Yemini - 1986 - Typestate A programming language concept for enha.pdf:application/pdf}
}

@inproceedings{albert_verification_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Verification of {Java} {Bytecode} {Using} {Analysis} and {Transformation} of {Logic} {Programs}},
	isbn = {978-3-540-69608-7 978-3-540-69611-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-69611-7_8},
	doi = {10.1007/978-3-540-69611-7_8},
	abstract = {State of the art analyzers in the Logic Programming (LP) paradigm are nowadays mature and sophisticated. They allow inferring a wide variety of global properties including termination, bounds on resource consumption, etc. The aim of this work is to automatically transfer the power of such analysis tools for LP to the analysis and verification of Java bytecode (jvml). In order to achieve our goal, we rely on well-known techniques for meta-programming and program specialization. More precisely, we propose to partially evaluate a jvml interpreter implemented in LP together with (an LP representation of) a jvml program and then analyze the residual program. Interestingly, at least for the examples we have studied, our approach produces very simple LP representations of the original jvml programs. This can be seen as a decompilation from jvml to high-level LP source. By reasoning about such residual programs, we can automatically prove in the CiaoPP system some non-trivial properties of jvml programs such as termination, run-time error freeness and infer bounds on its resource consumption. We are not aware of any other system which is able to verify such advanced properties of Java bytecode.},
	language = {en},
	urldate = {2017-11-29},
	booktitle = {Practical {Aspects} of {Declarative} {Languages}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Albert, Elvira and G{\'o}mez-Zamalloa, Miguel and Hubert, Laurent and Puebla, Germ{\'a}n},
	month = jan,
	year = {2007},
	pages = {124--139},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/6YRT9HT4/Albert et al. - 2007 - Verification of Java Bytecode Using Analysis and T.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/X3HTWJE7/978-3-540-69611-7_8.html:text/html}
}

@inproceedings{atkey_coqjvm:_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CoqJVM}: {An} {Executable} {Specification} of the {Java} {Virtual} {Machine} {Using} {Dependent} {Types}},
	isbn = {978-3-540-68084-0 978-3-540-68103-8},
	shorttitle = {{CoqJVM}},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-68103-8_2},
	doi = {10.1007/978-3-540-68103-8_2},
	abstract = {We describe an executable specification of the Java Virtual Machine (JVM) within the Coq proof assistant. The principal features of the development are that it is executable, meaning that it can be tested against a real JVM to gain confidence in the correctness of the specification; and that it has been written with heavy use of dependent types, this is both to structure the model in a useful way, and to constrain the model to prevent spurious partiality. We describe the structure of the formalisation and the way in which we have used dependent types.},
	language = {en},
	urldate = {2017-11-29},
	booktitle = {Types for {Proofs} and {Programs}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Atkey, Robert},
	month = may,
	year = {2007},
	pages = {18--32},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/WFLFUDWD/Atkey - 2007 - CoqJVM An Executable Specification of the Java Vi.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/ZSEV5ZB3/978-3-540-68103-8_2.html:text/html}
}

@inproceedings{bettini_generic_2014,
	address = {New York, NY, USA},
	series = {{PPPJ} '14},
	title = {Generic {Traits} for the {Java} {Platform}},
	isbn = {978-1-4503-2926-2},
	url = {http://doi.acm.org/10.1145/2647508.2647518},
	doi = {10.1145/2647508.2647518},
	abstract = {A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. Traits were proposed as a mechanism for fine-grained code reuse to overcome many limitations of class-based inheritance. In this paper we present the extended version of Xtraitj, a trait-based programming language that features complete compatibility and interoperability with the Java platform. Xtraitj provides a full Eclipse IDE that aims to support an incremental adoption of traits in existing Java projects. This new version fully supports Java generics: traits can have type parameters just like in Java, so that they can completely interoperate with any existing Java library. Furthermore, Xtraitj now supports Java annotations, so that it can integrate with frameworks like JUnit 4.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Principles} and {Practices} of {Programming} on the {Java} {Platform}: {Virtual} {Machines}, {Languages}, and {Tools}},
	publisher = {ACM},
	author = {Bettini, Lorenzo and Damiani, Ferruccio},
	year = {2014},
	keywords = {Java, implementation, eclipse, IDE, trait},
	pages = {5--16},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/C54M5RVS/Bettini and Damiani - 2014 - Generic Traits for the Java Platform.pdf:application/pdf}
}

@inproceedings{bierhoff_practical_2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Practical {API} {Protocol} {Checking} with {Access} {Permissions}},
	isbn = {978-3-642-03012-3 978-3-642-03013-0},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-03013-0_10},
	doi = {10.1007/978-3-642-03013-0_10},
	abstract = {Reusable APIs often define usage protocols. We previously developed a sound modular type system that checks compliance with typestate-based protocols while affording a great deal of aliasing flexibility. We also developed Plural, a prototype tool that embodies our approach as an automated static analysis and includes several extensions we found useful in practice. This paper evaluates our approach along the following dimensions: (1) We report on experience in specifying relevant usage rules for a large Java standard API with our approach. We also specify several other Java APIs and identify recurring patterns. (2) We summarize two case studies in verifying third-party open-source code bases with few false positives using our tool. We discuss how tool shortcomings can be addressed either with code refactorings or extensions to the tool itself. These results indicate that our approach can be used to specify and enforce real API protocols in practice.},
	language = {en},
	urldate = {2017-11-29},
	booktitle = {{ECOOP} 2009 {\textendash} {Object}-{Oriented} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Bierhoff, Kevin and Beckman, Nels E. and Aldrich, Jonathan},
	month = jul,
	year = {2009},
	pages = {195--219},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/S96P2VJI/Bierhoff et al. - 2009 - Practical API Protocol Checking with Access Permis.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/ZIHL2H8D/978-3-642-03013-0_10.html:text/html}
}

@inproceedings{bockisch_instance_2014,
	address = {New York, NY, USA},
	series = {{PPPJ} '14},
	title = {Instance {Pointcuts}: {Selecting} {Object} {Sets} {Based} on {Their} {Usage} {History}},
	isbn = {978-1-4503-2926-2},
	shorttitle = {Instance {Pointcuts}},
	url = {http://doi.acm.org/10.1145/2647508.2647526},
	doi = {10.1145/2647508.2647526},
	abstract = {At runtime, how objects have to be handled frequently depends on how they were used before. But with current programming-language support, selecting objects according to their previous usage patterns often results in scattered and tangled code. In this study, we propose a new kind of pointcut, called Instance Pointcuts, for maintaining sets that contain objects with a specified usage history. Instance pointcut specifications can be reused, by refining their selection criteria, e.g., by restricting the scope of an existing instance pointcut; and they can be composed, e.g., by set operations. These features make instance pointcuts easy to evolve according to new requirements. Our approach improves modularity by providing a fine-grained mechanism and a declarative syntax to create and maintain usage-specific object sets.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Principles} and {Practices} of {Programming} on the {Java} {Platform}: {Virtual} {Machines}, {Languages}, and {Tools}},
	publisher = {ACM},
	author = {Bockisch, Christoph and Hatun, Kardelen and Aksit, Mehmet},
	year = {2014},
	keywords = {aspect-oriented programming, programming languages, instance pointcuts, objects},
	pages = {27--38},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NFSCKHFJ/Bockisch et al. - 2014 - Instance Pointcuts Selecting Object Sets Based on.pdf:application/pdf}
}

@inproceedings{bockisch_overview_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Overview} of {ALIA}4J},
	isbn = {978-3-642-21951-1 978-3-642-21952-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-21952-8_11},
	doi = {10.1007/978-3-642-21952-8_11},
	abstract = {New programming languages that allow to reduce the complexity of software solutions are frequently developed, often as extensions of existing languages. Many implementations thus resort to transforming the extension{\textquoteright}s source code to the imperative intermediate representation of the parent language. But approaches like compiler frameworks only allow for re-use of code transformations for syntactically-related languages; they do not allow for re-use across language families. In this paper, we present the ALIA4J approach to bring such re-use to language families with advanced dispatching mechanisms like pointcut-advice or predicate dispatching. ALIA4J introduces a meta-model of dispatching as a rich, extensible intermediate language. By implementing language constructs from four languages as refinements of this meta-model, we show that a significant amount of them can be re-used across language families. Another building block of ALIA4J is a framework for execution environments that automatically derives an execution model of the program{\textquoteright}s dispatching from representations in our intermediate language. This model enables different execution strategies for dispatching; we have validated this by implementing three execution environments whose strategies range from interpretation to optimizing code generation.},
	language = {en},
	urldate = {2017-11-29},
	booktitle = {Objects, {Models}, {Components}, {Patterns}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Bockisch, Christoph and Sewe, Andreas and Mezini, Mira and Ak{\c s}it, Mehmet},
	month = jun,
	year = {2011},
	pages = {131--146},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/8PUC7525/Bockisch et al. - 2011 - An Overview of ALIA4J.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/7ZC5DGEE/978-3-642-21952-8_11.html:text/html}
}

@inproceedings{bonetta_tigerquoll:_2013,
	address = {New York, NY, USA},
	series = {{PPoPP} '13},
	title = {{TigerQuoll}: {Parallel} {Event}-based {JavaScript}},
	isbn = {978-1-4503-1922-5},
	shorttitle = {{TigerQuoll}},
	url = {http://doi.acm.org/10.1145/2442516.2442541},
	doi = {10.1145/2442516.2442541},
	abstract = {JavaScript, the most popular language on the Web, is rapidly moving to the server-side, becoming even more pervasive. Still, JavaScript lacks support for shared memory parallelism, making it challenging for developers to exploit multicores present in both servers and clients. In this paper we present TigerQuoll, a novel API and runtime for parallel programming in JavaScript. TigerQuoll features an event-based API and a parallel runtime allowing applications to exploit a mutable shared memory space. The programming model of TigerQuoll features automatic consistency and concurrency management, such that developers do not have to deal with shared-data synchronization. TigerQuoll supports an innovative transaction model that allows for eventual consistency to speed up high-contention workloads. Experiments show that TigerQuoll applications scale well, allowing one to implement common parallelism patterns in JavaScript.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {ACM},
	author = {Bonetta, Daniele and Binder, Walter and Pautasso, Cesare},
	year = {2013},
	keywords = {javascript, parallelism, event-based programming, eventual transactions},
	pages = {251--260},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/3PZ4GA48/Bonetta et al. - 2013 - TigerQuoll Parallel Event-based JavaScript.pdf:application/pdf}
}

@inproceedings{burtscher_static_2002,
	address = {New York, NY, USA},
	series = {{PLDI} '02},
	title = {Static {Load} {Classification} for {Improving} the {Value} {Predictability} of {Data}-cache {Misses}},
	isbn = {978-1-58113-463-6},
	url = {http://doi.acm.org/10.1145/512529.512556},
	doi = {10.1145/512529.512556},
	abstract = {While caches are effective at avoiding most main-memory accesses, the few remaining memory references are still expensive. Even one cache miss per one hundred accesses can double a program's execution time. To better tolerate the data-cache miss latency, architects have proposed various speculation mechanisms, including load-value prediction. A load-value predictor guesses the result of a load so that the dependent instructions can immediately proceed without having to wait for the memory access to complete. To use the prediction resources most effectively, speculation should be restricted to loads that are likely to miss in the cache and that are likely to be predicted correctly. Prior work has considered hardware- and profile-based methods to make these decisions. Our work focuses on making these decisions at compile time. We show that a simple compiler classification is effective at separating the loads that should be speculated from the loads that should not. We present results for a number of C and Java programs and demonstrate that our results are consistent across programming languages and across program inputs.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2002 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Burtscher, Martin and Diwan, Amer and Hauswirth, Matthias},
	year = {2002},
	keywords = {load-value prediction, type-based analysis},
	pages = {222--233},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/ANPREV2W/Burtscher et al. - 2002 - Static Load Classification for Improving the Value.pdf:application/pdf}
}

@inproceedings{cabri_implementing_2014,
	address = {New York, NY, USA},
	series = {{PPPJ} '14},
	title = {Implementing {Agent} {Roles} in {Java}},
	isbn = {978-1-4503-2926-2},
	url = {http://doi.acm.org/10.1145/2655183.2655184},
	doi = {10.1145/2655183.2655184},
	abstract = {Roles represent a powerful means to enable software agents to act in open environments. They can be implemented in different ways, and in this talk I will show two directions exploiting Java. The former one is quite traditional and exploits composition; the latter one adds the capabilities of roles to agents' classes in form of injected bytecode. I will compare the two approaches trying to generalize the considerations.},
	urldate = {2017-11-29},
	booktitle = {Proceedings of the 2014 {International} {Conference} on {Principles} and {Practices} of {Programming} on the {Java} {Platform}: {Virtual} {Machines}, {Languages}, and {Tools}},
	publisher = {ACM},
	author = {Cabri, Giacomo},
	year = {2014},
	keywords = {Java, agents, roles},
	pages = {1--3},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/QAPJVLUX/Cabri - 2014 - Implementing Agent Roles in Java.pdf:application/pdf}
}

@article{chugh_dependent_2011,
	title = {Dependent {Types} for {JavaScript}},
	url = {http://arxiv.org/abs/1112.4106},
	abstract = {We present Dependent JavaScript (DJS), a statically-typed dialect of the imperative, object-oriented, dynamic language. DJS supports the particularly challenging features such as run-time type-tests, higher-order functions, extensible objects, prototype inheritance, and arrays through a combination of nested refinement types, strong updates to the heap, and heap unrolling to precisely track prototype hierarchies. With our implementation of DJS, we demonstrate that the type system is expressive enough to reason about a variety of tricky idioms found in small examples drawn from several sources, including the popular book JavaScript: The Good Parts and the SunSpider benchmark suite.},
	urldate = {2017-11-29},
	journal = {arXiv:1112.4106 [cs]},
	author = {Chugh, Ravi and Herman, David and Jhala, Ranjit},
	month = dec,
	year = {2011},
	note = {arXiv: 1112.4106},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv\:1112.4106 PDF:/Users/luigi/work/zotero/storage/T5HQIQIK/Chugh et al. - 2011 - Dependent Types for JavaScript.pdf:application/pdf;arXiv.org Snapshot:/Users/luigi/work/zotero/storage/GU5TLHF4/1112.html:text/html}
}

@inproceedings{mitschke_i3ql:_2014,
	address = {New York, NY, USA},
	series = {{OOPSLA} '14},
	title = {I3QL: {Language}-integrated {Live} {Data} {Views}},
	isbn = {978-1-4503-2585-1},
	shorttitle = {I3QL},
	url = {http://doi.acm.org/10.1145/2660193.2660242},
	doi = {10.1145/2660193.2660242},
	abstract = {An incremental computation updates its result based on a change to its input, which is often an order of magnitude faster than a recomputation from scratch. In particular, incrementalization can make expensive computations feasible for settings that require short feedback cycles, such as interactive systems, IDEs, or (soft) real-time systems. This paper presents i3QL, a general-purpose programming language for specifying incremental computations. i3QL provides a declarative SQL-like syntax and is based on incremental versions of operators from relational algebra, enriched with support for general recursion. We integrated i3QL into Scala as a library, which enables programmers to use regular Scala code for non-incremental subcomputations of an i3QL query and to easily integrate incremental computations into larger software projects. To improve performance, i3QL optimizes user-defined queries by applying algebraic laws and partial evaluation. We describe the design and implementation of i3QL and its optimizations, demonstrate its applicability, and evaluate its performance.},
	urldate = {2017-11-30},
	booktitle = {Proceedings of the 2014 {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} \& {Applications}},
	publisher = {ACM},
	author = {Mitschke, Ralf and Erdweg, Sebastian and K{\"o}hler, Mirko and Mezini, Mira and Salvaneschi, Guido},
	year = {2014},
	keywords = {incremental computation, reactive programming, scala},
	pages = {417--432},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/PQLWILUE/Mitschke et al. - 2014 - I3QL Language-integrated Live Data Views.pdf:application/pdf}
}

@inproceedings{iu_queryll:_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Queryll: {Java} {Database} {Queries} {Through} {Bytecode} {Rewriting}},
	isbn = {978-3-540-49023-4 978-3-540-68256-1},
	shorttitle = {Queryll},
	url = {https://link.springer.com/chapter/10.1007/11925071_11},
	doi = {10.1007/11925071_11},
	abstract = {When interfacing Java with other systems such as databases, programmers must often program in special interface languages like SQL. Code written in these languages often needs to be embedded in strings where they cannot be error-checked at compile-time, or the Java compiler needs to be altered to directly recognize code written in these languages. We have taken a different approach to adding database query facilities to Java. Bytecode rewriting allows us to add query facilities to Java whose correctness can be checked at compile-time but which don{\textquoteright}t require any changes to the Java language, Java compilers, Java VMs, or IDEs. Like traditional object-relational mapping tools, we provide Java libraries for accessing individual database entries as objects and navigating among them. To express a query though, a programmer simply writes code that takes a Collection representing the entire contents of a database, iterates over each entry like they would with a normal Collection, and choose the entries of interest. The query is fully valid Java code that, if executed, will read through an entire database and copy entries into Java objects where they will be inspected. Executing queries in this way is obviously inefficient, but we have a special bytecode rewriting tool that can decompile Java class files, identify queries in the bytecode, and rewrite the code to use SQL instead. The rewritten bytecode can then be run using any standard Java VM. Since queries use standard Java set manipulation syntax, Java programmers do not need to learn any new syntax. Our system is able to handle complex queries that make use of all the basic relational operations and exhibits performance comparable to that of hand-written SQL.},
	language = {en},
	urldate = {2017-11-30},
	booktitle = {Middleware 2006},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Iu, Ming-Yee and Zwaenepoel, Willy},
	month = nov,
	year = {2006},
	pages = {201--218},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/MQXSWHN6/Iu and Zwaenepoel - 2006 - Queryll Java Database Queries Through Bytecode Re.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/3NTEUZ4X/11925071_11.html:text/html}
}

@inproceedings{buse_synthesizing_2012,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '12},
	title = {Synthesizing {API} {Usage} {Examples}},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337316},
	abstract = {Key program interfaces are sometimes documented with usage examples: concrete code snippets that characterize common use cases for a particular data type. While such documentation is known to be of great utility, it is burdensome to create and can be incomplete, out of date, or not representative of actual practice. We present an automatic technique for mining and synthesizing succinct and representative human-readable documentation of program interfaces. Our algorithm is based on a combination of path sensitive dataflow analysis, clustering, and pattern abstraction. It produces output in the form of well-typed program snippets which document initialization, method calls, assignments, looping constructs, and exception handling. In a human study involving over 150 participants, 82\% of our generated examples were found to be at least as good at human-written instances and 94\% were strictly preferred to state of the art code search.},
	urldate = {2017-12-02},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Buse, Raymond P. L. and Weimer, Westley},
	year = {2012},
	pages = {782--792},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NU9P3XHT/Buse and Weimer - 2012 - Synthesizing API Usage Examples.pdf:application/pdf}
}

@misc{noauthor_contextual_nodate,
	title = {Contextual {Code} {Completion} {Using} {Machine} {Learning} - {Semantic} {Scholar}},
	url = {/paper/Contextual-Code-Completion-Using-Machine-Learning-Das-Shah/3d426d5d686db3dfa5cad88dbbf0bcf443828cf6},
	abstract = {Large projects such as kernels, drivers and libraries follow a code style, and have recurring patterns. In this project, we explore learning based code recommendation, to use the project context and give meaningful suggestions. Using word vectors to model code tokens, and neural network based learning techniques, we are able to capture interesting patterns, and predict code that that cannot be predicted by a simple grammar and syntax based approach as in conventional IDEs. We achieve a total prediction accuracy of 56.0\% on Linux kernel, a C project, and 40.6\% on Twisted, a Python networking library.},
	urldate = {2017-12-02},
	file = {6d5d686db3dfa5cad88dbbf0bcf443828cf6.pdf:/Users/luigi/work/zotero/storage/U68IUF7I/6d5d686db3dfa5cad88dbbf0bcf443828cf6.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/SHN4R6TU/3d426d5d686db3dfa5cad88dbbf0bcf443828cf6.html:text/html}
}

@misc{noauthor_automatic_nodate,
	title = {Automatic {Code} {Completion} - {Semantic} {Scholar}},
	url = {/paper/Automatic-Code-Completion-Ginzberg-Kostas/acaa0541c7ba79049a6e39c791b9da4b740b7f4a},
	abstract = {Code completion software is an important tool for many developers, but it traditionally fails to model any long term program dependencies such as scoped variables, instead settling for suggestions based on static libraries. In this paper we present a deep learning approach to code completion for non-terminals (program structural components) and terminals (program text) that takes advantage of running dependencies to improve predictions. We develop an LSTM model and augment it with several approaches to Attention in order to better capture the relative value of the input, hidden state, and context. After evaluating on a large dataset of JavaScript programs, we demonstrate that our Gated LSTM model significantly improves over a Vanilla LSTM baseline, achieving an accuracy of 77\% on the non-terminal prediction problem and 46\% accuracy on the terminal prediction problem.},
	urldate = {2017-12-02},
	file = {0541c7ba79049a6e39c791b9da4b740b7f4a.pdf:/Users/luigi/work/zotero/storage/IAF4JSIF/0541c7ba79049a6e39c791b9da4b740b7f4a.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/7TZPBTCW/acaa0541c7ba79049a6e39c791b9da4b740b7f4a.html:text/html}
}

@inproceedings{vazou_abstract_2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Abstract {Refinement} {Types}},
	isbn = {978-3-642-37035-9 978-3-642-37036-6},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-37036-6_13},
	doi = {10.1007/978-3-642-37036-6_13},
	abstract = {We present abstract refinement types which enable quantification over the refinements of data- and function-types. Our key insight is that we can avail of quantification while preserving SMT-based decidability, simply by encoding refinement parameters as uninterpreted propositions within the refinement logic. We illustrate how this mechanism yields a variety of sophisticated means for reasoning about programs, including: parametric refinements for reasoning with type classes, index-dependent refinements for reasoning about key-value maps, recursive refinements for reasoning about recursive data types, and inductive refinements for reasoning about higher-order traversal routines. We have implemented our approach in a refinement type checker for Haskell and present experiments using our tool to verify correctness invariants of various programs.},
	language = {en},
	urldate = {2017-12-02},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Vazou, Niki and Rondon, Patrick M. and Jhala, Ranjit},
	month = mar,
	year = {2013},
	pages = {209--228},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/UKL7BZP6/Vazou et al. - 2013 - Abstract Refinement Types.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/3L5UD35L/978-3-642-37036-6_13.html:text/html}
}

@article{caso_enabledness-based_2013,
	title = {Enabledness-based {Program} {Abstractions} for {Behavior} {Validation}},
	volume = {22},
	issn = {1049-331X},
	url = {http://doi.acm.org/10.1145/2491509.2491519},
	doi = {10.1145/2491509.2491519},
	abstract = {Code artifacts that have nontrivial requirements with respect to the ordering in which their methods or procedures ought to be called are common and appear, for instance, in the form of API implementations and objects. This work addresses the problem of validating if API implementations provide their intended behavior when descriptions of this behavior are informal, partial, or nonexistent. The proposed approach addresses this problem by generating abstract behavior models which resemble typestates. These models are statically computed and encode all admissible sequences of method calls. The level of abstraction at which such models are constructed has shown to be useful for validating code artifacts and identifying findings which led to the discovery of bugs, adjustment of the requirements expected by the engineer to the requirements implicit in the code, and the improvement of available documentation.},
	number = {3},
	urldate = {2017-12-02},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Caso, Guido De and Braberman, Victor and Garbervetsky, Diego and Uchitel, Sebastian},
	month = jul,
	year = {2013},
	keywords = {enabledness abstractions, Source-code validation},
	pages = {25:1--25:46},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NLDUHA8T/Caso et al. - 2013 - Enabledness-based Program Abstractions for Behavio.pdf:application/pdf}
}

@inproceedings{wadler_how_1989,
	address = {New York, NY, USA},
	series = {{POPL} '89},
	title = {How to {Make} {Ad}-hoc {Polymorphism} {Less} {Ad} {Hoc}},
	isbn = {978-0-89791-294-5},
	url = {http://doi.acm.org/10.1145/75277.75283},
	doi = {10.1145/75277.75283},
	abstract = {This paper presents type classes, a new approach to ad-hoc polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the {\textquotedblleft}eqtype variables{\textquotedblright} of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in object-oriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules.},
	urldate = {2017-12-02},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Wadler, P. and Blott, S.},
	year = {1989},
	pages = {60--76},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/KMUC358R/Wadler and Blott - 1989 - How to Make Ad-hoc Polymorphism Less Ad Hoc.pdf:application/pdf}
}

@article{sussman_scheme:_1975,
	title = {{SCHEME}: {An} {Interpreter} for {Extended} {Lambda} {Calculus}},
	shorttitle = {{SCHEME}},
	url = {http://dspace.mit.edu/handle/1721.1/5794},
	abstract = {Inspired by ACTORS [Greif and Hewitt] [Smith and Hewitt], we have implemented an interpreter for a LISP-like language, SCHEME, based on the lambda calculus [Church], but extended for side effects, multiprocessing, and process synchronization. The purpose of this implementation is tutorial. We wish to: (1) alleviate the confusion caused by Micro-PLANNER, CONNIVER, etc. by clarifying the embedding of non-recursive control structures in a recursive host language like LISP. (2) explain how to use these control structures, independent of such issues as pattern matching and data base manipulation. (3) have a simple concrete experimental domain for certain issues of programming semantics and style.},
	language = {en\_US},
	urldate = {2017-12-02},
	author = {Sussman, Gerald J. and Steele, Guy L.},
	month = dec,
	year = {1975},
	file = {AIM-349.pdf:/Users/luigi/work/zotero/storage/PJ7NT6PG/AIM-349.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/J5WJR4IZ/5794.html:text/html}
}

@article{sestoft_deriving_1997,
	title = {Deriving a {Lazy} {Abstract} {Machine}},
	volume = {7},
	issn = {0956-7968},
	url = {http://dx.doi.org/10.1017/S0956796897002712},
	doi = {10.1017/S0956796897002712},
	abstract = {We derive a simple abstract machine for lazy evaluation of the lambda calculus, starting from Launchbury's natural semantics. Lazy evaluation here means non-strict evaluation with sharing of argument evaluation, i.e. call-by-need. The machine we derive is a lazy version of Krivine's abstract machine, which was originally designed for call-by-name evaluation. We extend it with datatype constructors and base values, so the final machine implements all dynamic aspects of a lazy functional language.},
	number = {3},
	urldate = {2017-12-02},
	journal = {J. Funct. Program.},
	author = {Sestoft, Peter},
	month = may,
	year = {1997},
	pages = {231--264},
	file = {amlazy5.pdf:/Users/luigi/work/zotero/storage/QXX2BSXD/amlazy5.pdf:application/pdf}
}

@inproceedings{ansel_petabricks:_2009,
	address = {New York, NY, USA},
	series = {{PLDI} '09},
	title = {{PetaBricks}: {A} {Language} and {Compiler} for {Algorithmic} {Choice}},
	isbn = {978-1-60558-392-1},
	shorttitle = {{PetaBricks}},
	url = {http://doi.acm.org/10.1145/1542476.1542481},
	doi = {10.1145/1542476.1542481},
	abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse-grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near-optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.},
	urldate = {2017-12-02},
	booktitle = {Proceedings of the 30th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Ansel, Jason and Chan, Cy and Wong, Yee Lok and Olszewski, Marek and Zhao, Qin and Edelman, Alan and Amarasinghe, Saman},
	year = {2009},
	keywords = {language, compiler, adaptive, algorithmic choice, autotuning, implicitly parallel},
	pages = {38--49},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/NT64HMNV/Ansel et al. - 2009 - PetaBricks A Language and Compiler for Algorithmi.pdf:application/pdf}
}

@inproceedings{whaley_using_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Datalog} with {Binary} {Decision} {Diagrams} for {Program} {Analysis}},
	isbn = {978-3-540-29735-2 978-3-540-32247-4},
	url = {https://link.springer.com/chapter/10.1007/11575467_8},
	doi = {10.1007/11575467_8},
	abstract = {Many problems in program analysis can be expressed naturally and concisely in a declarative language like Datalog. This makes it easy to specify new analyses or extend or compose existing analyses. However, previous implementations of declarative languages perform poorly compared with traditional implementations. This paper describes bddbddb, a BDD-Based Deductive DataBase, which implements the declarative language Datalog with stratified negation, totally-ordered finite domains and comparison operators. bddbddb uses binary decision diagrams (BDDs) to efficiently represent large relations. BDD operations take time proportional to the size of the data structure, not the number of tuples in a relation, which leads to fast execution times. bddbddb is an effective tool for implementing a large class of program analyses. We show that a context-insensitive points-to analysis implemented with bddbddb is about twice as fast as a carefully hand-tuned version. The use of BDDs also allows us to solve heretofore unsolved problems, like context-sensitive pointer analysis for large programs.},
	language = {en},
	urldate = {2017-12-02},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Whaley, John and Avots, Dzintars and Carbin, Michael and Lam, Monica S.},
	month = nov,
	year = {2005},
	pages = {97--118},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/FJMFWB6Z/Whaley et al. - 2005 - Using Datalog with Binary Decision Diagrams for Pr.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/F3F7AKAC/11575467_8.html:text/html}
}

@article{mcbride_applicative_2008,
	title = {Applicative {Programming} with {Effects}},
	volume = {18},
	doi = {10.1017/S0956796807006326},
	abstract = {In this article, we introduce Applicative functors {\textendash} an abstract characterisation of an applicative style of effectful programming, weaker than Monads and hence more widespread. Indeed, it is the ubiquity of this programming pattern that drew us to the abstraction. We retrace our steps in this article, introducing the applicative pattern by diverse examples, then abstracting it to define the Applicative type class and introducing a bracket notation that interprets the normal application syntax in the idiom of an Applicative functor. Furthermore, we develop the properties of applicative functors and the generic operations they support. We close by identifying the categorical structure of applicative functors and examining their relationship both with Monads and with Arrow.},
	journal = {Journal of Functional Programming},
	author = {McBride, Conor and Paterson, Ross},
	month = jan,
	year = {2008},
	pages = {1--13},
	file = {Idiom.pdf:/Users/luigi/work/zotero/storage/ITMTG5S7/Idiom.pdf:application/pdf}
}

@inproceedings{steuwer_generating_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Generating {Performance} {Portable} {Code} {Using} {Rewrite} {Rules}: {From} {High}-level {Functional} {Expressions} to {High}-performance {OpenCL} {Code}},
	isbn = {978-1-4503-3669-7},
	shorttitle = {Generating {Performance} {Portable} {Code} {Using} {Rewrite} {Rules}},
	url = {http://doi.acm.org/10.1145/2784731.2784754},
	doi = {10.1145/2784731.2784754},
	abstract = {Computers have become increasingly complex with the emergence of heterogeneous hardware combining multicore CPUs and GPUs. These parallel systems exhibit tremendous computational power at the cost of increased programming effort resulting in a tension between performance and code portability. Typically, code is either tuned in a low-level imperative language using hardware-specific optimizations to achieve maximum performance or is written in a high-level, possibly functional, language to achieve portability at the expense of performance. We propose a novel approach aiming to combine high-level programming, code portability, and high-performance. Starting from a high-level functional expression we apply a simple set of rewrite rules to transform it into a low-level functional representation, close to the OpenCL programming model, from which OpenCL code is generated. Our rewrite rules define a space of possible implementations which we automatically explore to generate hardware-specific OpenCL implementations. We formalize our system with a core dependently-typed lambda-calculus along with a denotational semantics which we use to prove the correctness of the rewrite rules. We test our design in practice by implementing a compiler which generates high performance imperative OpenCL code. Our experiments show that we can automatically derive hardware-specific implementations from simple functional high-level algorithmic expressions offering performance on a par with highly tuned code for multicore CPUs and GPUs written by experts.},
	urldate = {2017-12-02},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Steuwer, Michel and Fensch, Christian and Lindley, Sam and Dubach, Christophe},
	year = {2015},
	keywords = {GPU, code generation, OpenCL, Algorithmic patterns, performance portability, rewrite rules},
	pages = {205--217},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/FFJASUBM/Steuwer et al. - 2015 - Generating Performance Portable Code Using Rewrite.pdf:application/pdf}
}

@inproceedings{pradel_automatic_2009,
	address = {Washington, DC, USA},
	series = {{ASE} '09},
	title = {Automatic {Generation} of {Object} {Usage} {Specifications} from {Large} {Method} {Traces}},
	isbn = {978-0-7695-3891-4},
	url = {http://dx.doi.org/10.1109/ASE.2009.60},
	doi = {10.1109/ASE.2009.60},
	abstract = {Formal specifications are used to identify programming errors, verify the correctness of programs, and as documentation. Unfortunately, producing them is error-prone and time-consuming, so they are rarely used in practice. Inferring specifications from a running application is a promising solution. However, to be practical, such an approach requires special techniques to treat large amounts of runtime data. We present a scalable dynamic analysis that infers specifications of correct method call sequences on multiple related objects. It preprocesses method traces to identify small sets of related objects and method calls which can be analyzed separately. We implemented our approach and applied the analysis to eleven real-world applications and more than 240 million runtime events. The experiments show the scalability of our approach. Moreover, the generated specifications describe correct and typical behavior, and match existing API usage documentation.},
	urldate = {2017-12-02},
	booktitle = {Proceedings of the 2009 {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Pradel, Michael and Gross, Thomas R.},
	year = {2009},
	keywords = {formal specifications, dynamic analysis, Specification inference, temporal properties},
	pages = {371--382},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/MJJ2NJF7/Pradel and Gross - 2009 - Automatic Generation of Object Usage Specification.pdf:application/pdf}
}

@inproceedings{tabareau_aspectual_2014,
	title = {Aspectual {Session} {Types}},
	url = {https://hal.inria.fr/hal-00872791/document},
	doi = {10.1145/2577080.2577085},
	abstract = {Multiparty session types allow the definition of distributed processes with strong communication safety properties. A global type is a choreographic specification of the interactions between peers, which is then projected locally in each peer. Well-typed processes behave accordingly to the global protocol specification. Multiparty session types are however monolithic entities that are not amenable to modular extensions. Also, session types impose conservative requirements to prevent any race condition, which prohibit the uni- form application of extensions at different points in a protocol. In this paper, we describe a means to support modular extensions with aspectual session types, a static pointcut/advice mechanism at the session type level. To support the modular definition of crosscut- ting concerns, we augment the expressivity of session types to al- low harmless race conditions. We formally prove that well-formed aspectual session types entail communication safety. As a result, aspectual session types make multiparty session types more flexible, modular, and extensible.},
	language = {en},
	urldate = {2017-12-02},
	author = {Tabareau, Nicolas and S{\"u}dholt, Mario and Tanter, {\'E}ric},
	month = apr,
	year = {2014},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/ABIG8C2I/Tabareau et al. - 2014 - Aspectual Session Types.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/5WCTI5GS/en.html:text/html}
}

@article{anand_orchestrated_2013,
	title = {An {Orchestrated} {Survey} of {Methodologies} for {Automated} {Software} {Test} {Case} {Generation}},
	volume = {86},
	issn = {0164-1212},
	url = {http://dx.doi.org/10.1016/j.jss.2013.02.061},
	doi = {10.1016/j.jss.2013.02.061},
	abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
	number = {8},
	urldate = {2017-12-02},
	journal = {J. Syst. Softw.},
	author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and Mcminn, Phil},
	month = aug,
	year = {2013},
	keywords = {Software testing, Adaptive random testing, Combinatorial testing, Model-based testing, Orchestrated survey, Search-based software testing, Symbolic execution, Test automation, Test case generation},
	pages = {1978--2001},
	file = {ASTJSS.pdf:/Users/luigi/work/zotero/storage/JTBIH8N9/ASTJSS.pdf:application/pdf}
}

@inproceedings{chakravarty_associated_2005,
	address = {New York, NY, USA},
	series = {{ICFP} '05},
	title = {Associated {Type} {Synonyms}},
	isbn = {978-1-59593-064-4},
	url = {http://doi.acm.org/10.1145/1086365.1086397},
	doi = {10.1145/1086365.1086397},
	abstract = {Haskell programmers often use a multi-parameter type class in which one or more type parameters are functionally dependent on the first. Although such functional dependencies have proved quite popular in practice, they express the programmer's intent somewhat indirectly. Developing earlier work on associated data types, we propose to add functionally dependent types as type synonyms to type-class bodies. These associated type synonyms constitute an interesting new alternative to explicit functional dependencies.},
	urldate = {2017-12-02},
	booktitle = {Proceedings of the {Tenth} {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Chakravarty, Manuel M. T. and Keller, Gabriele and Jones, Simon Peyton},
	year = {2005},
	keywords = {type inference, associated types, generic programming, type classes, type functions},
	pages = {241--253},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/6VUM9SPU/Chakravarty et al. - 2005 - Associated Type Synonyms.pdf:application/pdf}
}

@article{bacon_kava:_2003,
	title = {Kava: a {Java} dialect with a uniform object model for lightweight classes},
	volume = {15},
	issn = {1532-0634},
	shorttitle = {Kava},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.653/abstract},
	doi = {10.1002/cpe.653},
	abstract = {Object-oriented programming languages have always distinguished between {\textquoteleft}primitive{\textquoteright} and {\textquoteleft}user-defined{\textquoteright} data types, and in the case of languages like C++ and Java the primitives are not even treated as objects, further fragmenting the programming model. The distinction is especially problematic when a particular programming community requires primitive-level support for a new data type, as for complex, intervals, fixed-point numbers, and so on. We present Kava, a design for a backward-compatible version of Java that solves the problem of programmable lightweight objects in a much more aggressive and uniform manner than previous proposals. In Kava, there are no primitive types; instead, object-oriented programming is provided down to the level of single bits, and types such as int can be explicitly programmed within the language. While the language maintains a uniform object reference semantics, efficiency is obtained by making heavy use of unboxing and semantic expansion. We describe Kava as a dialect of the Java language, show how it can be used to define various primitive types, describe how it can be translated into Java, and compare it to other approaches to lightweight objects. Copyright {\textcopyright} 2003 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3-5},
	urldate = {2017-12-02},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Bacon, David F.},
	month = mar,
	year = {2003},
	keywords = {Java, lightweight classes, object inlining, object models, unboxing},
	pages = {185--206},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/9DLREEXU/Bacon - 2003 - Kava a Java dialect with a uniform object model f.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/SCQKGLMI/abstract.html:text/html}
}

@inproceedings{mitropoulos_vulnerability_2014,
	title = {The {Vulnerability} {Dataset} of a {Large} {Software} {Ecosystem}},
	doi = {10.1109/BADGERS.2014.8},
	abstract = {Security bugs are critical programming errors that can lead to serious vulnerabilities in software. Examining their behaviour and characteristics within a software ecosystem can provide the research community with data regarding their evolution, persistence and others. We present a dataset that we produced by applying static analysis to the Maven Central Repository (approximately 265GB of data) in order to detect potential security bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics' results that FindBugs reports for every project version (a JAR) included in the ecosystem. For every version in our data repository, we also store specific metadata, such as the JAR's size, its dependencies and others. Our dataset can be used to produce interesting research results involving security bugs, as we show in specific examples.},
	booktitle = {2014 {Third} {International} {Workshop} on {Building} {Analysis} {Datasets} and {Gathering} {Experience} {Returns} for {Security} ({BADGERS})},
	author = {Mitropoulos, D. and Gousios, G. and Papadopoulos, P. and Karakoidas, V. and Louridas, P. and Spinellis, D.},
	month = sep,
	year = {2014},
	keywords = {Java, Java bytecode, Software, program debugging, Security, Static Analysis, Computer bugs, Ecosystems, static analysis, Software Evolution, security of data, Correlation, critical programming errors, data repository, FindBugs, large software ecosystem, Maven Central Repository, Maven Repository, metadata, Metadata, research community, security bugs, Security Bugs, Software Ecosystem, Software Security, vulnerability dataset},
	pages = {69--74},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/ZJPC4GDR/7446036.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/2GXECKR7/Mitropoulos et al. - 2014 - The Vulnerability Dataset of a Large Software Ecos.pdf:application/pdf}
}

@article{barendregt_introduction_1991,
	title = {Introduction to generalized type systems},
	volume = {1},
	issn = {0956-7968, 1469-7653},
	url = {https://www.cambridge.org/core/journals/journal-of-functional-programming/article/introduction-to-generalized-type-systems/869991BA6A99180BF96A616894C6D710},
	doi = {10.1017/S0956796800020025},
	abstract = {Abstract
Programming languages often come with type systems. Some of these are simple, others are sophisticated. As a stylistic representation of types in programming languages several versions of typed lambda calculus are studied. During the last 20 years many of these systems have appeared, so there is some need of classification. Working towards a taxonomy, Barendregt (1991) gives a fine-structure of the theory of constructions (Coquand and Huet 1988) in the form of a canonical cube of eight type systems ordered by inclusion. Berardi (1988) and Terlouw (1988) have independently generalized the method of constructing systems in the $\lambda$-cube. Moreover, Berardi (1988, 1990) showed that the generalized type systems are flexible enough to describe many logical systems. In that way the well-known propositions-as-types interpretation obtains a nice canonical form.},
	number = {2},
	urldate = {2017-12-02},
	journal = {Journal of Functional Programming},
	author = {Barendregt, Henk},
	month = apr,
	year = {1991},
	pages = {125--154},
	file = {barendregt.pdf:/Users/luigi/work/zotero/storage/I5YK4QH6/barendregt.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/RAKPCYFW/869991BA6A99180BF96A616894C6D710.html:text/html}
}

@inproceedings{bastoul_code_2004,
	address = {Washington, DC, USA},
	series = {{PACT} '04},
	title = {Code {Generation} in the {Polyhedral} {Model} {Is} {Easier} {Than} {You} {Think}},
	isbn = {978-0-7695-2229-6},
	url = {https://doi.org/10.1109/PACT.2004.11},
	doi = {10.1109/PACT.2004.11},
	abstract = {Many advances in automatic parallelization and optimization have been achieved through the polyhedral model. It has been extensively shown that this computational model provides convenient abstractions to reason about and apply program transformations. Nevertheless, the complexity of code generation has long been a deterrent for using polyhedral representation in optimizing compilers. First, code generators have a hard time coping with generated code size and control overhead that may spoil theoretical benefits achieved by the transformations. Second, this step is usually time consuming, hampering the integration of the polyhedral framework in production compilers or feedback-directed, iterative optimization schemes. Moreover, current code generation algorithms only cover a restrictive set of possible transformation functions. This paper discusses a general transformation framework able to deal with non-unimodular, non-invertible, non-integral or even non-uniform functions. It presents several improvements to a state-of-the-art code generation algorithm. Two directions are explored: generated code size and code generator efficiency. Experimental evidence proves the ability of the improved method to handle real-life problems.},
	urldate = {2017-12-02},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {IEEE Computer Society},
	author = {Bastoul, Cedric},
	year = {2004},
	pages = {7--16},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/BKIYUL87/Bastoul - 2004 - Code Generation in the Polyhedral Model Is Easier .pdf:application/pdf}
}

@inproceedings{gueheneuc_no_2002,
	title = {No {Java} without caffeine: {A} tool for dynamic analysis of {Java} programs},
	shorttitle = {No {Java} without caffeine},
	doi = {10.1109/ASE.2002.1115000},
	abstract = {To understand the behavior of a program, a maintainer reads some code, asks a question about this code, conjectures an answer, and searches the code and the documentation for confirmation of her conjecture. However, the confirmation of the conjecture can be error-prone and time-consuming because the maintainer has only static information at her disposal. She would benefit from dynamic information. In this paper, we present Caffeine, an assistant that helps the maintainer in checking her conjecture about the behavior of a Java program. Our assistant is a dynamic analysis tool that uses the Java platform debug architecture to generate a trace, i.e., an execution history, and a Prolog engine to perform queries over the trace. We present a usage scenario based on the n-queens problem, and two real-life examples based on the Singleton design pattern and on the composition relationship.},
	booktitle = {Proceedings 17th {IEEE} {International} {Conference} on {Automated} {Software} {Engineering},},
	author = {Gueheneuc, Y. G. and Douence, R. and Jussien, N.},
	year = {2002},
	keywords = {Java, reverse engineering, Software maintenance, History, Performance analysis, dynamic analysis, Electronic mail, program debugging, software tools, Documentation, Law, Caffeine, Engines, execution history, Java platform debug architecture, Java programs, Legal factors, n-queens problem, program behaviour understanding, Prolog engine, real-life examples, Singleton design pattern, Software algorithms, static information, usage scenario},
	pages = {117--126},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/YT29I2A5/1115000.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/UJT5THED/Gueheneuc et al. - 2002 - No Java without caffeine A tool for dynamic analy.pdf:application/pdf}
}

@inproceedings{teyton_mining_2012,
	title = {Mining {Library} {Migration} {Graphs}},
	doi = {10.1109/WCRE.2012.38},
	abstract = {Software systems intensively depend on external libraries, chosen at conception time. However, relevance of any library irremediably changes during projects and/or library life cycle. As a consequence, projects developers must periodically reconsider the libraries they depend on, and must think about library migration. When they want to migrate their libraries, they then have to identify candidate libraries that offer similar facilities and thus can substitute to each other. They also have to compare candidates to choose the one that best fits their needs. Finding a relevant library replacement is a well known tedious and time-consuming task. In this paper, we propose an approach that identifies sets of similar libraries and that produces what we call library migration graphs that show how existing projects have performed migrations among them. These graphs, constructed from the observation of a large number of software projects, ease the discovery and selection of library replacements.},
	booktitle = {2012 19th {Working} {Conference} on {Reverse} {Engineering}},
	author = {Teyton, C. and Falleri, J. R. and Blanc, X.},
	month = oct,
	year = {2012},
	keywords = {Data mining, Libraries, software maintenance, software libraries, data mining, Software, software evolution, Google, Software algorithms, dependencies management, external software library, graph theory, library life cycle, library migration graph mining, library replacement, Manuals, project life cycle, Search engines, software project, software system},
	pages = {289--298},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/3UIEKISZ/6385124.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/4EA6KSEF/Teyton et al. - 2012 - Mining Library Migration Graphs.pdf:application/pdf}
}

@inproceedings{xu_mining_2013,
	title = {Mining {Test} {Oracles} for {Test} {Inputs} {Generated} from {Java} {Bytecode}},
	doi = {10.1109/COMPSAC.2013.8},
	abstract = {Search-based test generation can automatically produce a large volume of test inputs. However, it is difficult to define the test oracle for each of the test inputs. This paper presents a mining approach to building a decision tree model according to the test inputs generated from Java bytecode. It converts Java bytecode into the Jimple representation, extracts predicates from the control flow graph of the Jimple code, and uses these predicates as attributes for organizing training data to build a decision tree. Our case studies show that the mining approach generated accurate behavioral models and that test oracles derived from these models were able to kill 94.67\% of the mutants with injected faults.},
	booktitle = {2013 {IEEE} 37th {Annual} {Computer} {Software} and {Applications} {Conference}},
	author = {Xu, W. and Ding, T. and Wang, H. and Xu, D.},
	month = jul,
	year = {2013},
	keywords = {Java, Data mining, Software testing, Buildings, data mining, Java bytecode, mining, program testing, Accuracy, control flow graph, decision tree, decision tree model, decision trees, Decision trees, Input variables, Jimple, Jimple representation, mining approach, search-based test generation, test oracle, test oracles, Training data},
	pages = {27--32},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/FS48L778/6649795.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/ALL76Y3V/Xu et al. - 2013 - Mining Test Oracles for Test Inputs Generated from.pdf:application/pdf}
}

@inproceedings{berger_hoard:_2000,
	address = {New York, NY, USA},
	series = {{ASPLOS} {IX}},
	title = {Hoard: {A} {Scalable} {Memory} {Allocator} for {Multithreaded} {Applications}},
	isbn = {978-1-58113-317-2},
	shorttitle = {Hoard},
	url = {http://doi.acm.org/10.1145/378993.379232},
	doi = {10.1145/378993.379232},
	abstract = {Parallel, multithreaded C and C++ programs such as web servers, database managers, news servers, and scientific applications are becoming increasingly prevalent. For these applications, the memory allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. Previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. Worse, many allocators exhibit a dramatic increase in memory consumption when confronted with a producer-consumer pattern of object allocation and freeing. This increase in memory consumption can range from a factor of P (the number of processors) to unbounded memory consumption.This paper introduces Hoard, a fast, highly scalable allocator that largely avoids false sharing and is memory efficient. Hoard is the first allocator to simultaneously solve the above problems. Hoard combines one global heap and per-processor heaps with a novel discipline that provably bounds memory consumption and has very low synchronization costs in the common case. Our results on eleven programs demonstrate that Hoard yields low average fragmentation and improves overall program performance over the standard Solaris allocator by up to a factor of 60 on 14 processors, and up to a factor of 18 over the next best allocator we tested.},
	urldate = {2017-12-02},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Berger, Emery D. and McKinley, Kathryn S. and Blumofe, Robert D. and Wilson, Paul R.},
	year = {2000},
	pages = {117--128},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/JW3GKEPN/Berger et al. - 2000 - Hoard A Scalable Memory Allocator for Multithread.pdf:application/pdf}
}

@inproceedings{schultz_data_2001,
	title = {Data mining methods for detection of new malicious executables},
	doi = {10.1109/SECPRI.2001.924286},
	abstract = {A serious security threat today is malicious executables, especially new, unseen malicious executables often arriving as email attachments. These new malicious executables are created at the rate of thousands every year and pose a serious security threat. Current anti-virus systems attempt to detect these new malicious programs with heuristics generated by hand. This approach is costly and oftentimes ineffective. We present a data mining framework that detects new, previously unseen malicious executables accurately and automatically. The data mining framework automatically found patterns in our data set and used these patterns to detect a set of new malicious binaries. Comparing our detection methods with a traditional signature-based method, our method more than doubles the current detection rates for new malicious executables},
	booktitle = {Proceedings 2001 {IEEE} {Symposium} on {Security} and {Privacy}. {S} {P} 2001},
	author = {Schultz, M. G. and Eskin, E. and Zadok, F. and Stolfo, S. J.},
	year = {2001},
	keywords = {Protection, Data mining, Computer science, data mining, Testing, security of data, Training data, anti-virus systems, Computer security, data security, Data security, data set, electronic mail, email attachments, Face detection, heuristics, Information security, malicious binaries, malicious executable detection, pattern recognition, Permission, security threat, signature-based method},
	pages = {38--49},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/NLVSTM8V/924286.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/RYHDILPM/Schultz et al. - 2001 - Data mining methods for detection of new malicious.pdf:application/pdf}
}

@incollection{sorensen_introduction_1999,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Introduction to {Supercompilation}},
	isbn = {978-3-540-66710-0 978-3-540-47018-2},
	url = {https://link.springer.com/chapter/10.1007/3-540-47018-2_10},
	abstract = {This paper gives an introduction to Turchin{\textquoteright}s supercompiler, a program transformer for functional programs which performs optimizations beyond partial evaluation and deforestation. More precisely, the paper presents positive supercompilation.},
	language = {en},
	urldate = {2017-12-02},
	booktitle = {Partial {Evaluation}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {S{\o}rensen, Morten Heine B. and Gl{\"u}ck, Robert},
	year = {1999},
	doi = {10.1007/3-540-47018-2_10},
	pages = {246--270},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/AG6L54ZU/S{\o}rensen and Gl{\"u}ck - 1999 - Introduction to Supercompilation.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/JKLKDM28/3-540-47018-2_10.html:text/html}
}

@inproceedings{zhong_mapo:_2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MAPO}: {Mining} and {Recommending} {API} {Usage} {Patterns}},
	isbn = {978-3-642-03012-3 978-3-642-03013-0},
	shorttitle = {{MAPO}},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-03013-0_15},
	doi = {10.1007/978-3-642-03013-0_15},
	abstract = {To improve software productivity, when constructing new software systems, programmers often reuse existing libraries or frameworks by invoking methods provided in their APIs. Those API methods, however, are often complex and not well documented. To get familiar with how those API methods are used, programmers often exploit a source code search tool to search for code snippets that use the API methods of interest. However, the returned code snippets are often large in number, and the huge number of snippets places a barrier for programmers to locate useful ones. In order to help programmers overcome this barrier, we have developed an API usage mining framework and its supporting tool called MAPO (Mining API usage Pattern from Open source repositories) for mining API usage patterns automatically. A mined pattern describes that in a certain usage scenario, some API methods are frequently called together and their usages follow some sequential rules. MAPO further recommends the mined API usage patterns and their associated code snippets upon programmers{\textquoteright} requests. Our experimental results show that with these patterns MAPO helps programmers locate useful code snippets more effectively than two state-of-the-art code search tools. To investigate whether MAPO can assist programmers in programming tasks, we further conducted an empirical study. The results show that using MAPO, programmers produce code with fewer bugs when facing relatively complex API usages, comparing with using the two state-of-the-art code search tools.},
	language = {en},
	urldate = {2017-12-02},
	booktitle = {{ECOOP} 2009 {\textendash} {Object}-{Oriented} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Zhong, Hao and Xie, Tao and Zhang, Lu and Pei, Jian and Mei, Hong},
	month = jul,
	year = {2009},
	pages = {318--343},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/X4SELQIY/Zhong et al. - 2009 - MAPO Mining and Recommending API Usage Patterns.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/7WG7CTBD/978-3-642-03013-0_15.html:text/html}
}

@article{hafiz_growing_2016,
	title = {Growing a language: {An} empirical study on how (and why) developers use some recently-introduced and/or recently-evolving {JavaScript} features},
	volume = {121},
	issn = {0164-1212},
	shorttitle = {Growing a language},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121216300309},
	doi = {10.1016/j.jss.2016.04.045},
	abstract = {We describe an empirical study to understand how different language features in JavaScript are used by developers, with the goal of using this information to assist future extensions of JavaScript. We inspected more than one million unique scripts (over 80 MLOC) from various sources: JavaScript programs in the wild collected by a spider, (supposedly) better JavaScript programs collected from the top 100 URLs from the Alexa list, JavaScript programs with new language features used in Firefox Add-ons, widely used JavaScript libraries, and Node.js applications. Our corpus is larger and more diversified than those in prior studies. We also performed two explanatory studies to understand the reasons behind some of the language feature choices. One study was conducted on 107 JavaScript developers; the other was conducted on 45 developers of Node.js applications. Our study shows that there is a widespread confusion about newly introduced JavaScript features, a continuing misuse of existing problematic features, and a surprising lack of adoption of object-oriented features. It also hints at why developers choose to use language features this way. This information is valuable to the language designers and the stakeholders, e.g., IDE and tool builders, all of whom are responsible for growing a language.},
	number = {Supplement C},
	urldate = {2017-12-02},
	journal = {Journal of Systems and Software},
	author = {Hafiz, Munawar and Hasan, Samir and King, Zachary and Wirfs-Brock, Allen},
	month = nov,
	year = {2016},
	keywords = {Empirical study, JavaScript, Language evolution},
	pages = {191--208},
	file = {ScienceDirect Full Text PDF:/Users/luigi/work/zotero/storage/ZDTE8LPZ/Hafiz et al. - 2016 - Growing a language An empirical study on how (and.pdf:application/pdf;ScienceDirect Snapshot:/Users/luigi/work/zotero/storage/W6ZKT6RC/S0164121216300309.html:text/html}
}

@inproceedings{nguyen_api_2016,
	address = {New York, NY, USA},
	series = {{FSE} 2016},
	title = {{API} {Code} {Recommendation} {Using} {Statistical} {Learning} from {Fine}-grained {Changes}},
	isbn = {978-1-4503-4218-6},
	url = {http://doi.acm.org/10.1145/2950290.2950333},
	doi = {10.1145/2950290.2950333},
	abstract = {Learning and remembering how to use APIs is difficult. While code-completion tools can recommend API methods, browsing a long list of API method names and their documentation is tedious. Moreover, users can easily be overwhelmed with too much information. We present a novel API recommendation approach that taps into the predictive power of repetitive code changes to provide relevant API recommendations for developers. Our approach and tool, APIREC, is based on statistical learning from fine-grained code changes and from the context in which those changes were made. Our empirical evaluation shows that APIREC correctly recommends an API call in the first position 59\% of the time, and it recommends the correct API call in the top five positions 77\% of the time. This is a significant improvement over the state-of-the-art approaches by 30-160\% for top-1 accuracy, and 10-30\% for top-5 accuracy, respectively. Our result shows that APIREC performs well even with a one-time, minimal training dataset of 50 publicly available projects.},
	urldate = {2017-12-04},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Nguyen, Anh Tuan and Hilton, Michael and Codoban, Mihai and Nguyen, Hoan Anh and Mast, Lily and Rademacher, Eli and Nguyen, Tien N. and Dig, Danny},
	year = {2016},
	keywords = {API Recommendation, Fine-grained Code Changes, Statistical Learning},
	pages = {511--522},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/5XSUP35V/Nguyen et al. - 2016 - API Code Recommendation Using Statistical Learning.pdf:application/pdf}
}

@inproceedings{hills_evolution_2015,
	title = {Evolution of dynamic feature usage in {PHP}},
	doi = {10.1109/SANER.2015.7081870},
	abstract = {PHP includes a number of dynamic features that, if used, make it challenging for both programmers and tools to reason about programs. In this paper we examine how usage of these features has changed over time, looking at usage trends for three categories of dynamic features across the release histories of two popular open-source PHP systems, WordPress and MediaWiki. Our initial results suggest that, while features such as eval are being removed over time, more constrained dynamic features such as variable properties are becoming more common. We believe the results of this analysis provide useful insights for researchers and tool developers into the evolving use of dynamic features in real PHP programs.},
	booktitle = {2015 {IEEE} 22nd {International} {Conference} on {Software} {Analysis}, {Evolution}, and {Reengineering} ({SANER})},
	author = {Hills, M.},
	month = mar,
	year = {2015},
	keywords = {Runtime, Maintenance engineering, History, program diagnostics, Software, programming languages, Web sites, Market research, public domain software, Arrays, dynamic feature usage, Feature extraction, MediaWiki, open-source PHP system, PHP program, reason about program, reasoning about programs, WordPress},
	pages = {525--529},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/ML3VNEX6/7081870.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/K3TFK2FF/Hills - 2015 - Evolution of dynamic feature usage in PHP.pdf:application/pdf}
}

@article{landman_empirical_2016,
	title = {Empirical analysis of the relationship between {CC} and {SLOC} in a large corpus of {Java} methods and {C} functions},
	volume = {28},
	issn = {2047-7481},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/smr.1760/full},
	doi = {10.1002/smr.1760},
	abstract = {Measuring the internal quality of source code is one of the traditional goals of making software development into an engineering discipline. Cyclomatic complexity (CC) is an often used source code quality...},
	language = {en},
	number = {7},
	urldate = {2017-12-06},
	journal = {Journal of Software: Evolution and Process},
	author = {Landman, Davy and Serebrenik, Alexander and Bouwers, Eric and Vinju, Jurgen J.},
	month = jul,
	year = {2016},
	pages = {589--618},
	file = {Landman2014-ccsloc-icsme2014-preprint.pdf:/Users/luigi/work/zotero/storage/FSE2WT5T/Landman2014-ccsloc-icsme2014-preprint.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/4MHVVA37/full.html:text/html}
}

@inproceedings{vinju_how_2006,
	address = {Dagstuhl, Germany},
	series = {Dagstuhl {Seminar} {Proceedings}},
	title = {How to make a bridge between transformation and analysis technologies?},
	url = {http://drops.dagstuhl.de/opus/volltexte/2006/426},
	urldate = {2017-12-06},
	booktitle = {Transformation {Techniques} in {Software} {Engineering}},
	publisher = {Internationales Begegnungs- und Forschungszentrum f{\"u}r Informatik (IBFI), Schloss Dagstuhl, Germany},
	author = {Vinju, Jurgen and Cordy, James R.},
	editor = {Cordy, James R. and L{\"a}mmel, Ralf and Winter, Andreas},
	year = {2006},
	keywords = {analysis, fact extraction, middleware, source code representations, Transformation},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/HYN44RLV/Vinju and Cordy - 2006 - How to make a bridge between transformation and an.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/KLJZF749/426.html:text/html}
}

@inproceedings{nagappan_diversity_2013,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2013},
	title = {Diversity in {Software} {Engineering} {Research}},
	isbn = {978-1-4503-2237-9},
	url = {http://doi.acm.org/10.1145/2491411.2491415},
	doi = {10.1145/2491411.2491415},
	abstract = {One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space).},
	urldate = {2017-12-06},
	booktitle = {Proceedings of the 2013 9th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Nagappan, Meiyappan and Zimmermann, Thomas and Bird, Christian},
	year = {2013},
	keywords = {Coverage, Diversity, Representativeness, Sampling},
	pages = {466--476},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/ASKS9ZL8/Nagappan et al. - 2013 - Diversity in Software Engineering Research.pdf:application/pdf}
}

@inproceedings{meyerovich_empirical_2013,
	address = {New York, NY, USA},
	series = {{OOPSLA} '13},
	title = {Empirical {Analysis} of {Programming} {Language} {Adoption}},
	isbn = {978-1-4503-2374-1},
	url = {http://doi.acm.org/10.1145/2509136.2509515},
	doi = {10.1145/2509136.2509515},
	abstract = {Some programming languages become widely popular while others fail to grow beyond their niche or disappear altogether. This paper uses survey methodology to identify the factors that lead to language adoption. We analyze large datasets, including over 200,000 SourceForge projects, 590,000 projects tracked by Ohloh, and multiple surveys of 1,000-13,000 programmers. We report several prominent findings. First, language adoption follows a power law; a small number of languages account for most language use, but the programming market supports many languages with niche user bases. Second, intrinsic features have only secondary importance in adoption. Open source libraries, existing code, and experience strongly influence developers when selecting a language for a project. Language features such as performance, reliability, and simple semantics do not. Third, developers will steadily learn and forget languages. The overall number of languages developers are familiar with is independent of age. Finally, when considering intrinsic aspects of languages, developers prioritize expressivity over correctness. They perceive static types as primarily helping with the latter, hence partly explaining the popularity of dynamic languages.},
	urldate = {2017-12-23},
	booktitle = {Proceedings of the 2013 {ACM} {SIGPLAN} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} \& {Applications}},
	publisher = {ACM},
	author = {Meyerovich, Leo A. and Rabkin, Ariel S.},
	year = {2013},
	keywords = {programming language adoption, survey research},
	pages = {1--18},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/WD7I8GW9/Meyerovich and Rabkin - 2013 - Empirical Analysis of Programming Language Adoptio.pdf:application/pdf}
}

@inproceedings{pradel_good_2015,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {The {Good}, the {Bad}, and the {Ugly}: {An} {Empirical} {Study} of {Implicit} {Type} {Conversions} in {JavaScript}},
	volume = {37},
	isbn = {978-3-939897-86-6},
	shorttitle = {The {Good}, the {Bad}, and the {Ugly}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2015/5236},
	doi = {10.4230/LIPIcs.ECOOP.2015.519},
	urldate = {2018-01-05},
	booktitle = {29th {European} {Conference} on {Object}-{Oriented} {Programming} ({ECOOP} 2015)},
	publisher = {Schloss Dagstuhl{\textendash}Leibniz-Zentrum fuer Informatik},
	author = {Pradel, Michael and Sen, Koushik},
	editor = {Boyland, John Tang},
	year = {2015},
	keywords = {JavaScript, Dynamically typed languages, Type coercions, Types},
	pages = {519--541},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/ZNSRZC2G/Pradel and Sen - 2015 - The Good, the Bad, and the Ugly An Empirical Stud.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/3XQHDIDI/5236.html:text/html}
}

@inproceedings{shatnawi_analyzing_2017,
	title = {Analyzing {Program} {Dependencies} in {Java} {EE} {Applications}},
	doi = {10.1109/MSR.2017.6},
	abstract = {Program dependency artifacts such as call graphs help support a number of software engineering tasks such as software mining, program understanding, debugging, feature location, software maintenance and evolution. Java Enterprise Edition (JEE) applications represent a significant part of the recent legacy applications, and we are interested in modernizing them. This modernization involves, among other things, analyzing dependencies between their various components/tiers. JEE applications tend to be multilanguage, rely on JEE container services, and make extensive use of late binding techniques-all of which makes finding such dependencies difficult. In this paper, we describe some of these difficulties and how we addressed them to build a dependency call graph. We developed our tool called DeJEE (Dependencies in JEE) as an Eclipse plug-in. We applied DeJEE on two open-source JEE applications: Java PetStore and JSP Blog. The results show that DeJEE is able to identify different types of JEE dependencies.},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Shatnawi, A. and Mili, H. and Boussaidi, G. El and Boubaker, A. and Gu{\'e}h{\'e}neuc, Y. G. and Moha, N. and Privat, J. and Abdellatif, M.},
	month = may,
	year = {2017},
	keywords = {Java, Object oriented modeling, Data mining, program diagnostics, software maintenance, Software, Tools, code analysis, container services, Containers, DeJEE, dependencies in JEE, dependency call graph, Eclipse plug-in, Java EE application, Java EE applications, Java Enterprise Edition applications, Java PetStore, JEE applications, JEE container services, JSP Blog, late binding techniques, legacy applications, modernization, multilanguage, open-source JEE applications, Program dependency, program dependency analysis, program dependency artifacts, server pages, Servers, software engineering tasks},
	pages = {64--74},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/RJFSIQK8/7962356.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/XRWXMDP5/Shatnawi et al. - 2017 - Analyzing Program Dependencies in Java EE Applicat.pdf:application/pdf}
}

@inproceedings{saini_dataset_2014,
	address = {New York, NY, USA},
	series = {{MSR} 2014},
	title = {A {Dataset} for {Maven} {Artifacts} and {Bug} {Patterns} {Found} in {Them}},
	isbn = {978-1-4503-2863-0},
	url = {http://doi.acm.org/10.1145/2597073.2597134},
	doi = {10.1145/2597073.2597134},
	abstract = {In this paper, we present data downloaded from Maven, one of the most popular component repositories. The data includes the binaries of 186,392 components, along with source code for 161,025. We identify and organize these components into groups where each group contains all the versions of a library. In order to asses the quality of these components, we make available report generated by the FindBugs tool on 64,574 components. The information is also made available in the form of a database which stores total number, type, and priority of bug patterns found in each component, along with its defect density. We also describe how this dataset can be useful in software engineering research.},
	urldate = {2018-01-13},
	booktitle = {Proceedings of the 11th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Saini, Vaibhav and Sajnani, Hitesh and Ossher, Joel and Lopes, Cristina V.},
	year = {2014},
	keywords = {FindBugs, Empirical Research, Empirical Software Engineering, Maven, Software Quality},
	pages = {416--419},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/ZFL7UWCY/Saini et al. - 2014 - A Dataset for Maven Artifacts and Bug Patterns Fou.pdf:application/pdf}
}

@inproceedings{shields_dynamic_1998,
	address = {New York, NY, USA},
	series = {{POPL} '98},
	title = {Dynamic {Typing} {As} {Staged} {Type} {Inference}},
	isbn = {978-0-89791-979-1},
	url = {http://doi.acm.org/10.1145/268946.268970},
	doi = {10.1145/268946.268970},
	abstract = {Dynamic typing extends statically typed languages with a universal datatype, simplifying programs which must manipulate other programs as data, such as distributed, persistent, interpretive and generic programs. Current approaches, however, limit the use of polymorphism in dynamic values, and can be syntactically awkward.We introduce a new approach to dynamic typing, based on staged computation, which allows a single type-reconstruction algorithm to execute partly at compile time and partly at run-time. This approach seamlessly extends a single type system to accommodate types that are only known at run-time, while still supporting both type inference and polymorphism. The system is significantly more expressive than other approaches. Furthermore it can be implemented efficiently; most of the type inference is done at compile-time, leaving only some residual unification for run-time.We demonstrate our approach by examples in a small polymorphic functional language, and present its type system, type reconstruction algorithm, and operational semantics. Our proposal could also be readily adapted to many other programming languages.},
	urldate = {2018-01-14},
	booktitle = {Proceedings of the 25th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Shields, Mark and Sheard, Tim and Peyton Jones, Simon},
	year = {1998},
	pages = {289--302},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/DQ7NC5TI/Shields et al. - 1998 - Dynamic Typing As Staged Type Inference.pdf:application/pdf}
}

@inproceedings{lester_information_2013,
	title = {Information {Flow} {Analysis} for a {Dynamically} {Typed} {Language} with {Staged} {Metaprogramming}},
	doi = {10.1109/CSF.2013.21},
	abstract = {Web applications written in JavaScript are regularly used for dealing with sensitive or personal data. Consequently, reasoning about their security properties has become an important problem, which is made very difficult by the highly dynamic nature of the language, particularly its support for runtime code generation. As a first step towards dealing with this, we propose to investigate security analyses for languages with more principled forms of dynamic code generation. To this end, we present a static information flow analysis for a dynamically typed functional language with prototype-based inheritance and staged metaprogramming. We prove its soundness, implement it and test it on various examples designed to show its relevance to proving security properties, such as noninterference, in JavaScript. To our knowledge, this is the first fully static information flow analysis for a language with staged metaprogramming, and the first formal soundness proof of a CFA-based information flow analysis for a functional programming language.},
	booktitle = {2013 {IEEE} 26th {Computer} {Security} {Foundations} {Symposium}},
	author = {Lester, M. and Ong, L. and Schaefer, M.},
	month = jun,
	year = {2013},
	keywords = {Java, program compilers, JavaScript, Security, Semantics, static analysis, Context, information flow, Educational institutions, data flow analysis, dynamically typed languages, CFA, CFA-based information flow analysis, Cognition, dynamic code generation, dynamically typed functional language, formal soundness proof, functional languages, functional programming language, metacomputing, noninterference, prototype-based inheritance, security analyses, security properties, Stability analysis, staged metaprogramming, static information flow analysis, Syntactics, theorem proving, type theory, Web applications},
	pages = {209--223},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/VXZC2UDT/6595830.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/ZTTV453F/Lester et al. - 2013 - Information Flow Analysis for a Dynamically Typed .pdf:application/pdf}
}

@article{blackburn_truth_2016,
	title = {The {Truth}, {The} {Whole} {Truth}, and {Nothing} {But} the {Truth}: {A} {Pragmatic} {Guide} to {Assessing} {Empirical} {Evaluations}},
	volume = {38},
	issn = {0164-0925},
	shorttitle = {The {Truth}, {The} {Whole} {Truth}, and {Nothing} {But} the {Truth}},
	url = {http://doi.acm.org/10.1145/2983574},
	doi = {10.1145/2983574},
	abstract = {An unsound claim can misdirect a field, encouraging the pursuit of unworthy ideas and the abandonment of promising ideas. An inadequate description of a claim can make it difficult to reason about the claim, for example, to determine whether the claim is sound. Many practitioners will acknowledge the threat of unsound claims or inadequate descriptions of claims to their field. We believe that this situation is exacerbated, and even encouraged, by the lack of a systematic approach to exploring, exposing, and addressing the source of unsound claims and poor exposition. This article proposes a framework that identifies three sins of reasoning that lead to unsound claims and two sins of exposition that lead to poorly described claims and evaluations. Sins of exposition obfuscate the objective of determining whether or not a claim is sound, while sins of reasoning lead directly to unsound claims. Our framework provides practitioners with a principled way of critiquing the integrity of their own work and the work of others. We hope that this will help individuals conduct better science and encourage a cultural shift in our research community to identify and promulgate sound claims.},
	number = {4},
	urldate = {2018-01-15},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Blackburn, Stephen M. and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F. and Amaral, Jos{\'e} Nelson and Brecht, Tim and Bulej, Lubom{\'i}r and Click, Cliff and Eeckhout, Lieven and Fischmeister, Sebastian and Frampton, Daniel and Hendren, Laurie J. and Hind, Michael and Hosking, Antony L. and Jones, Richard E. and Kalibera, Tomas and Keynes, Nathan and Nystrom, Nathaniel and Zeller, Andreas},
	month = oct,
	year = {2016},
	keywords = {Experimental evaluation, experimentation, observation study},
	pages = {15:1--15:20},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/3X6UYR4P/Blackburn et al. - 2016 - The Truth, The Whole Truth, and Nothing But the Tr.pdf:application/pdf}
}

@inproceedings{holmes_strathcona_2005,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE}-13},
	title = {Strathcona {Example} {Recommendation} {Tool}},
	isbn = {978-1-59593-014-9},
	url = {http://doi.acm.org/10.1145/1081706.1081744},
	doi = {10.1145/1081706.1081744},
	abstract = {Using the application programming interfaces (API) of large software systems requires developers to understand details about the interfaces that are often not explicitly defined. However, documentation about the API is often incomplete or out of date. Existing systems that make use of the API provide a form of implicit information on how to use that code. Manually searching through existing projects to find relevant source code is tedious and time consuming. We have created the Strathcona Example.Recommendation Tool to assist developers in finding relevant fragments of code, or examples, of an API's use. These examples can be used by developers to provide insight on how they are supposed to interact with the API.},
	urldate = {2018-01-15},
	booktitle = {Proceedings of the 10th {European} {Software} {Engineering} {Conference} {Held} {Jointly} with 13th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Holmes, Reid and Walker, Robert J. and Murphy, Gail C.},
	year = {2005},
	keywords = {examples, recommender, software structure},
	pages = {237--240},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/SPEKSBBV/Holmes et al. - 2005 - Strathcona Example Recommendation Tool.pdf:application/pdf}
}

@inproceedings{xie_mapo:_2006,
	address = {New York, NY, USA},
	series = {{MSR} '06},
	title = {{MAPO}: {Mining} {API} {Usages} from {Open} {Source} {Repositories}},
	isbn = {978-1-59593-397-3},
	shorttitle = {{MAPO}},
	url = {http://doi.acm.org/10.1145/1137983.1137997},
	doi = {10.1145/1137983.1137997},
	abstract = {To improve software productivity, when constructing new software systems, developers often reuse existing class libraries or frameworks by invoking their APIs. Those APIs, however, are often complex and not well documented, posing barriers for developers to use them in new client code. To get familiar with how those APIs are used, developers may search the Web using a general search engine to find relevant documents or code examples. Developers can also use a source code search engine to search open source repositories for source files that use the same APIs. Nevertheless, the number of returned source files is often large. It is difficult for developers to learn API usages from a large number of returned results. In order to help developers understand API usages and write API client code more effectively, we have developed an API usage mining framework and its supporting tool called MAPO (for {\textless}u{\textgreater}M{\textless}/u{\textgreater}ining {\textless}u{\textgreater}AP{\textless}/u{\textgreater}I usages from {\textless}u{\textgreater}O{\textless}/u{\textgreater}pen source repositories). Given a query that describes a method, class, or package for an API, MAPO leverages the existing source code search engines to gather relevant source files and conducts data mining. The mining leads to a short list of frequent API usages for developers to inspect. MAPO currently consists of five components: a code search engine, a source code analyzer, a sequence preprocessor, a frequent sequence miner, and a frequent sequence post processor. We have examined the effectiveness of MAPO using a set of various queries. The preliminary results show that the framework is practical for providing informative and succinct API usage patterns.},
	urldate = {2018-01-15},
	booktitle = {Proceedings of the 2006 {International} {Workshop} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Xie, Tao and Pei, Jian},
	year = {2006},
	keywords = {mining software repositories, application programming interfaces, program comprehension},
	pages = {54--57},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/TC9HGMHB/Xie and Pei - 2006 - MAPO Mining API Usages from Open Source Repositor.pdf:application/pdf}
}

@inproceedings{xie_loopster:_2017,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2017},
	title = {Loopster: {Static} {Loop} {Termination} {Analysis}},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Loopster},
	url = {http://doi.acm.org/10.1145/3106237.3106260},
	doi = {10.1145/3106237.3106260},
	abstract = {Loop termination is an important problem for proving the correctness of a system and ensuring that the system always reacts. Existing loop termination analysis techniques mainly depend on the synthesis of ranking functions, which is often expensive. In this paper, we present a novel approach, named Loopster, which performs an efficient static analysis to decide the termination for loops based on path termination analysis and path dependency reasoning. Loopster adopts a divide-and-conquer approach: (1) we extract individual paths from a target multi-path loop and analyze the termination of each path, (2) analyze the dependencies between each two paths, and then (3) determine the overall termination of the target loop based on the relations among paths. We evaluate Loopster by applying it on the loop termination competition benchmark and three real-world projects. The results show that Loopster is effective in a majority of loops with better accuracy and 20 {\texttimes}+ performance improvement compared to the state-of-the-art tools.},
	urldate = {2018-01-15},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Xie, Xiaofei and Chen, Bihuan and Zou, Liang and Lin, Shang-Wei and Liu, Yang and Li, Xiaohong},
	year = {2017},
	keywords = {Loop Termination, Path Dependency Automaton, Reachability},
	pages = {84--94},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HANYZ59V/Xie et al. - 2017 - Loopster Static Loop Termination Analysis.pdf:application/pdf}
}

@inproceedings{gopstein_understanding_2017,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2017},
	title = {Understanding {Misunderstandings} in {Source} {Code}},
	isbn = {978-1-4503-5105-8},
	url = {http://doi.acm.org/10.1145/3106237.3106264},
	doi = {10.1145/3106237.3106264},
	abstract = {Humans often mistake the meaning of source code, and so misjudge a program's true behavior. These mistakes can be caused by extremely small, isolated patterns in code, which can lead to significant runtime errors. These patterns are used in large, popular software projects and even recommended in style guides. To identify code patterns that may confuse programmers we extracted a preliminary set of `atoms of confusion' from known confusing code. We show empirically in an experiment with 73 participants that these code patterns can lead to a significantly increased rate of misunderstanding versus equivalent code without the patterns. We then go on to take larger confusing programs and measure (in an experiment with 43 participants) the impact, in terms of programmer confusion, of removing these confusing patterns. All of our instruments, analysis code, and data are publicly available online for replication, experimentation, and feedback.},
	urldate = {2018-01-15},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Gopstein, Dan and Iannacone, Jake and Yan, Yu and DeLong, Lois and Zhuang, Yanyan and Yeh, Martin K.-C. and Cappos, Justin},
	year = {2017},
	keywords = {Program Understanding, Programming Languages},
	pages = {129--139},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/UWHGYDEZ/Gopstein et al. - 2017 - Understanding Misunderstandings in Source Code.pdf:application/pdf}
}

@article{bezemer_empirical_2017,
	title = {An empirical study of unspecified dependencies in make-based build systems},
	volume = {22},
	issn = {1382-3256, 1573-7616},
	url = {https://link.springer.com/article/10.1007/s10664-017-9510-8},
	doi = {10.1007/s10664-017-9510-8},
	abstract = {Software developers rely on a build system to compile their source code changes and produce deliverables for testing and deployment. Since the full build of large software systems can take hours, the incremental build is a cornerstone of modern build systems. Incremental builds should only recompile deliverables whose dependencies have been changed by a developer. However, in many organizations, such dependencies still are identified by build rules that are specified and maintained (mostly) manually, typically using technologies like make. Incomplete rules lead to unspecified dependencies that can prevent certain deliverables from being rebuilt, yielding incomplete results, which leave sources and deliverables out-of-sync. In this paper, we present a case study on unspecified dependencies in the make-based build systems of the glib, openldap, linux and qt open source projects. To uncover unspecified dependencies in make-based build systems, we use an approach that combines a conceptual model of the dependencies specified in the build system with a concrete model of the files and processes that are actually exercised during the build. Our approach provides an overview of the dependencies that are used throughout the build system and reveals unspecified dependencies that are not yet expressed in the build system rules. During our analysis, we find that unspecified dependencies are common. We identify 6 common causes in more than 1.2 million unspecified dependencies.},
	language = {en},
	number = {6},
	urldate = {2018-01-15},
	journal = {Empirical Software Engineering},
	author = {Bezemer, Cor-Paul and McIntosh, Shane and Adams, Bram and German, Daniel M. and Hassan, Ahmed E.},
	month = dec,
	year = {2017},
	pages = {3117--3148},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/YM4DLHL8/Bezemer et al. - 2017 - An empirical study of unspecified dependencies in .pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/PNGDR36N/10.html:text/html}
}

@article{madsen_model_2017,
	title = {A {Model} for {Reasoning} {About} {JavaScript} {Promises}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133910},
	doi = {10.1145/3133910},
	abstract = {In JavaScript programs, asynchrony arises in situations such as web-based user-interfaces, communicating with servers through HTTP requests, and non-blocking I/O. Event-based programming is the most popular approach for managing asynchrony, but suffers from problems such as lost events and event races, and results in code that is hard to understand and debug. Recently, ECMAScript 6 has added support for promises, an alternative mechanism for managing asynchrony that enables programmers to chain asynchronous computations while supporting proper error handling. However, promises are complex and error-prone in their own right, so programmers would benefit from techniques that can reason about the correctness of promise-based code. Since the ECMAScript 6 specification is informal and intended for implementers of JavaScript engines, it does not provide a suitable basis for formal reasoning. This paper presents {\^I}{\guillemotright}p, a core calculus that captures the essence of ECMAScript 6 promises. Based on {\^I}{\guillemotright}p, we introduce the promise graph, a program representation that can assist programmers with debugging of promise-based code. We then report on a case study in which we investigate how the promise graph can be helpful for debugging errors related to promises in code fragments posted to the StackOverflow website.},
	number = {OOPSLA},
	urldate = {2018-01-16},
	journal = {Proc. ACM Program. Lang.},
	author = {Madsen, Magnus and Lhot{\'a}k, Ond{\v r}ej and Tip, Frank},
	month = oct,
	year = {2017},
	keywords = {JavaScript, EcmaScript 6, Formal Semantics, Promise Graph, Promises},
	pages = {86:1--86:24},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/IG859BB8/Madsen et al. - 2017 - A Model for Reasoning About JavaScript Promises.pdf:application/pdf}
}

@article{ringer_solver-aided_2017,
	title = {A {Solver}-aided {Language} for {Test} {Input} {Generation}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133915},
	doi = {10.1145/3133915},
	abstract = {Developing a small but useful set of inputs for tests is challenging. We show that a domain-specific language backed by a constraint solver can help the programmer with this process. The solver can generate a set of test inputs and guarantee that each input is different from other inputs in a way that is useful for testing. This paper presents Iorek: a tool that empowers the programmer with the ability to express to any SMT solver what it means for inputs to be different. The core of Iorek is a rich language for constraining the set of inputs, which includes a novel bounded enumeration mechanism that makes it easy to define and encode a flexible notion of difference over a recursive structure. We demonstrate the flexibility of this mechanism for generating strings. We use Iorek to test real services and find that it is effective at finding bugs. We also build Iorek into a random testing tool and show that it increases coverage.},
	number = {OOPSLA},
	urldate = {2018-01-16},
	journal = {Proc. ACM Program. Lang.},
	author = {Ringer, Talia and Grossman, Dan and Schwartz-Narbonne, Daniel and Tasiran, Serdar},
	month = oct,
	year = {2017},
	keywords = {generators, solver-aided languages, test input generation},
	pages = {91:1--91:24},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/FJN4967I/Ringer et al. - 2017 - A Solver-aided Language for Test Input Generation.pdf:application/pdf}
}

@article{rapoport_simple_2017,
	title = {A {Simple} {Soundness} {Proof} for {Dependent} {Object} {Types}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133870},
	doi = {10.1145/3133870},
	abstract = {Dependent Object Types (DOT) is intended to be a core calculus for modelling Scala. Its distinguishing feature is abstract type members, fields in objects that hold types rather than values. Proving soundness of DOT has been surprisingly challenging, and existing proofs are complicated, and reason about multiple concepts at the same time (e.g. types, values, evaluation). To serve as a core calculus for Scala, DOT should be easy to experiment with and extend, and therefore its soundness proof needs to be easy to modify.   This paper presents a simple and modular proof strategy for reasoning in DOT. The strategy separates reasoning about types from other concerns. It is centred around a theorem that connects the full DOT type system to a restricted variant in which the challenges and paradoxes caused by abstract type members are eliminated. Almost all reasoning in the proof is done in the intuitive world of this restricted type system. Once we have the necessary results about types, we observe that the other aspects of DOT are mostly standard and can be incorporated into a soundness proof using familiar techniques known from other calculi.},
	number = {OOPSLA},
	urldate = {2018-01-16},
	journal = {Proc. ACM Program. Lang.},
	author = {Rapoport, Marianna and Kabir, Ifaz and He, Paul and Lhot{\'a}k, Ond{\v r}ej},
	month = oct,
	year = {2017},
	keywords = {Scala, dependent object types, DOT calculus, type safety},
	pages = {46:1--46:27},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/GEXTI2XG/Rapoport et al. - 2017 - A Simple Soundness Proof for Dependent Object Type.pdf:application/pdf}
}

@article{jeong_data-driven_2017,
	title = {Data-driven {Context}-sensitivity for {Points}-to {Analysis}},
	volume = {1},
	issn = {2475-1421},
	url = {http://doi.acm.org/10.1145/3133924},
	doi = {10.1145/3133924},
	abstract = {We present a new data-driven approach to achieve highly cost-effective context-sensitive points-to analysis for Java. While context-sensitivity has greater impact on the analysis precision and performance than any other precision-improving techniques, it is difficult to accurately identify the methods that would benefit the most from context-sensitivity and decide how much context-sensitivity should be used for them. Manually designing such rules is a nontrivial and laborious task that often delivers suboptimal results in practice. To overcome these challenges, we propose an automated and data-driven approach that learns to effectively apply context-sensitivity from codebases. In our approach, points-to analysis is equipped with a parameterized and heuristic rules, in disjunctive form of properties on program elements, that decide when and how much to apply context-sensitivity. We present a greedy algorithm that efficiently learns the parameter of the heuristic rules. We implemented our approach in the Doop framework and evaluated using three types of context-sensitive analyses: conventional object-sensitivity, selective hybrid object-sensitivity, and type-sensitivity. In all cases, experimental results show that our approach significantly outperforms existing techniques.},
	number = {OOPSLA},
	urldate = {2018-01-16},
	journal = {Proc. ACM Program. Lang.},
	author = {Jeong, Sehun and Jeon, Minseok and Cha, Sungdeok and Oh, Hakjoo},
	month = oct,
	year = {2017},
	keywords = {Context-sensitivity, Data-driven program analysis, Points-to analysis},
	pages = {100:1--100:28},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/4T776QN4/Jeong et al. - 2017 - Data-driven Context-sensitivity for Points-to Anal.pdf:application/pdf}
}

@article{grech_p/taint:_2017,
	title = {P/{Taint}: {Unified} {Points}-to and {Taint} {Analysis}},
	volume = {1},
	issn = {2475-1421},
	shorttitle = {P/{Taint}},
	url = {http://doi.acm.org/10.1145/3133926},
	doi = {10.1145/3133926},
	abstract = {Static information-flow analysis (especially taint-analysis) is a key technique in software security, computing where sensitive or untrusted data can propagate in a program. Points-to analysis is a fundamental static program analysis, computing what abstract objects a program expression may point to. In this work, we propose a deep unification of information-flow and points-to analysis. We observe that information-flow analysis is not a mere high-level client of points-to information, but it is indeed identical to points-to analysis on artificial abstract objects that represent different information sources. The very same algorithm can compute, simultaneously, two interlinked but separate results (points-to and information-flow values) with changes only to its initial conditions. The benefits of such a unification are manifold. We can use existing points-to analysis implementations, with virtually no modification (only minor additions of extra logic for sanitization) to compute information flow concepts, such as value tainting. The algorithmic enhancements of points-to analysis (e.g., different flavors of context sensitivity) can be applied transparently to information-flow analysis. Heavy engineering work on points-to analysis (e.g., handling of the reflection API for Java) applies to information-flow analysis without extra effort. We demonstrate the benefits in a realistic implementation that leverages the Doop points-to analysis framework (including its context-sensitivity and reflection analysis features) to provide an information-flow analysis with excellent precision (over 91\%) and recall (over 99\%) for standard Java information-flow benchmarks. The analysis comfortably scales to large, real-world Android applications, analyzing the Facebook Messenger app with more than 55K classes in under 7 hours.},
	number = {OOPSLA},
	urldate = {2018-01-16},
	journal = {Proc. ACM Program. Lang.},
	author = {Grech, Neville and Smaragdakis, Yannis},
	month = oct,
	year = {2017},
	keywords = {Android, Pointer Analysis, Taint Analysis},
	pages = {102:1--102:28},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HCPX6ZSP/Grech and Smaragdakis - 2017 - PTaint Unified Points-to and Taint Analysis.pdf:application/pdf}
}

@inproceedings{lam_context-sensitive_2005,
	address = {New York, NY, USA},
	series = {{PODS} '05},
	title = {Context-sensitive {Program} {Analysis} {As} {Database} {Queries}},
	isbn = {978-1-59593-062-0},
	url = {http://doi.acm.org/10.1145/1065167.1065169},
	doi = {10.1145/1065167.1065169},
	abstract = {Program analysis has been increasingly used in software
engineering tasks such as auditing programs for security
vulnerabilities and finding errors in general. Such tools often
require analyses much more sophisticated than those traditionally
used in compiler optimizations. In particular, context-sensitive
pointer alias information is a prerequisite for any sound and
precise analysis that reasons about uses of heap objects in a
program. Context-sensitive analysis is challenging because there
are over 1014 contexts in a typical large program, even
after recursive cycles are collapsed. Moreover, pointers cannot be
resolved in general without analyzing the entire program.

This paper presents a new framework, based on the concept of
deductive databases, for context-sensitive program analysis. In
this framework, all program information is stored as relations;
data access and analyses are written as Datalog queries. To handle
the large number of contexts in a program, the database represents
relations with binary decision diagrams (BDDs). The system we have
developed, called bddbddb, automatically translates database
queries into highly optimized BDD programs.

Our preliminary experiences suggest that a large class of
analyses involving heap objects can be described succinctly in
Datalog and implemented efficiently with BDDs. To make developing
application-specific analyses easy for programmers, we have also
created a language called PQL that makes a subset of Datalog
queries more intuitive to define. We have used the language to find
many security holes in Web applications.},
	urldate = {2018-01-16},
	booktitle = {Proceedings of the {Twenty}-fourth {ACM} {SIGMOD}-{SIGACT}-{SIGART} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {ACM},
	author = {Lam, Monica S. and Whaley, John and Livshits, V. Benjamin and Martin, Michael C. and Avots, Dzintars and Carbin, Michael and Unkel, Christopher},
	year = {2005},
	pages = {1--12},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/IQ5AK8FR/Lam et al. - 2005 - Context-sensitive Program Analysis As Database Que.pdf:application/pdf}
}

@phdthesis{proksch_enriched_2017,
	address = {Darmstadt},
	type = {Ph.{D}. {Thesis}},
	title = {Enriched {Event} {Streams}: {A} {General} {Platform} {For} {Empirical} {Studies} {On} {In}-{IDE} {Activities} {Of} {Software} {Developers}},
	copyright = {CC-BY-SA 4.0 International - Creative Commons Attribution Share-alike, 4.0},
	shorttitle = {Enriched {Event} {Streams}},
	url = {http://tuprints.ulb.tu-darmstadt.de/6971/},
	abstract = {Current studies on software development either focus on the change history of source code from version-control systems or on an analysis of simplistic in-IDE events without context information. Each of these approaches contains valuable information that is unavailable in the other case. This work proposes enriched event streams, a solution that combines the best of both worlds and provides a holistic view on the in-IDE software development process. Enriched event streams not only capture developer activities in the IDE, but also specialized context information, such as source-code snapshots for change events. To enable the storage of such code snapshots in an analyzable format, we introduce a new intermediate representation called Simplified Syntax Trees (SSTs) and build CARET, a platform that offers reusable components to conveniently work with enriched event streams. We implement FeedBaG++, an instrumentation for Visual Studio that collects enriched event streams with code snapshots in the form of SSTs and share a dataset of enriched event streams captured in an ongoing field study from 81 users and representing 15K hours of active development. We complement this with a dataset of 69M lines of released source code extracted from 360 GitHub repositories. To demonstrate the usefulness of our platform, we use it to conduct studies on the in-IDE development process that are both concerned with source-code evolution and the analysis of developer interactions. In addition, we build recommendation systems for software engineering and analyze and improve current evaluation techniques.},
	language = {en},
	urldate = {2018-01-17},
	school = {Technische Universit{\"a}t},
	author = {Proksch, Sebastian},
	month = may,
	year = {2017},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/4QWSWZGS/Proksch - 2017 - Enriched Event Streams A General Platform For Emp.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/ZYERRRPX/6971.html:text/html}
}

@inproceedings{dietrich_construction_2017,
	address = {New York, NY, USA},
	series = {{SOAP} 2017},
	title = {On the {Construction} of {Soundness} {Oracles}},
	isbn = {978-1-4503-5072-3},
	url = {http://doi.acm.org/10.1145/3088515.3088520},
	doi = {10.1145/3088515.3088520},
	abstract = {One of the inherent advantages of static analysis is that it can create and reason about models of an entire program. However, mainstream languages such as Java use numerous dynamic language features designed to boost programmer productivity, but these features are notoriously difficult to capture by static analysis, leading to unsoundness in practice. While existing research has focused on providing sound handling for selected language features (mostly reflection) based on anecdotal evidence and case studies, there is little empirical work to investigate the extent to which particular features cause unsoundness of static analysis in practice. In this paper, we (1) discuss language features that may cause unsoundness and (2) discuss a methodology that can be used to check the (un)soundness of a particular static analysis, call-graph construction, based on soundness oracles. These oracles can also be used for hybrid analyses.},
	urldate = {2018-01-17},
	booktitle = {Proceedings of the 6th {ACM} {SIGPLAN} {International} {Workshop} on {State} {Of} the {Art} in {Program} {Analysis}},
	publisher = {ACM},
	author = {Dietrich, Jens and Sui, Li and Rasheed, Shawn and Tahir, Amjed},
	year = {2017},
	keywords = {Static analysis, Dynamic analysis, Soundness},
	pages = {37--42},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/VQ66MXC9/Dietrich et al. - 2017 - On the Construction of Soundness Oracles.pdf:application/pdf}
}

@inproceedings{umemori_design_2003,
	title = {Design and implementation of bytecode-based {Java} slicing system},
	doi = {10.1109/SCAM.2003.1238037},
	abstract = {A program slice is a set of statements that affect the value of a variable v in a statement s. In order to calculate a program slice, we must know the dependence relations between statements in the program. Program slicing techniques are roughly divided into two categories, static slicing and dynamic slicing, and we have proposed DC slicing technique which uses both static and dynamic information. We propose a method of constructing a DC slicing system for Java programs. Java programs have many elements which are dynamically determined at the time of execution, so the DC slicing technique is effective in the analysis of Java programs. To construct the system, we have extended a Java virtual machine for extraction of dynamic information. We have applied the system to several sample programs to evaluate our approach.},
	booktitle = {Proceedings {Third} {IEEE} {International} {Workshop} on {Source} {Code} {Analysis} and {Manipulation}},
	author = {Umemori, F. and Konda, K. and Yokomori, R. and Inoue, K.},
	month = sep,
	year = {2003},
	keywords = {Java, Debugging, Data mining, Java virtual machine, Performance analysis, virtual machines, Programming, Costs, Virtual machining, Testing, static information, graph theory, bytecode-based Java slicing system, DC slicing technique, dynamic information, dynamic slicing, dynamically determined elements, Information science, Java program, Merging, program slicing, program statement, sample program, static slicing},
	pages = {108--117},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/9IJCK7BI/1238037.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/JAIIEDYK/Umemori et al. - 2003 - Design and implementation of bytecode-based Java s.pdf:application/pdf}
}

@inproceedings{rosa_accurate_2017,
	address = {New York, NY, USA},
	series = {{GPCE} 2017},
	title = {Accurate {Reification} of {Complete} {Supertype} {Information} for {Dynamic} {Analysis} on the {JVM}},
	isbn = {978-1-4503-5524-7},
	url = {http://doi.acm.org/10.1145/3136040.3136061},
	doi = {10.1145/3136040.3136061},
	abstract = {Reflective supertype information (RSI) is useful for many instrumentation-based dynamic analyses on the Java Virtual Machine (JVM). On the one hand, while such information can be obtained when performing the instrumentation within the same JVM process executing the instrumented program, in-process instrumentation severely limits the code coverage of the analysis. On the other hand, performing the instrumentation in a separate process can achieve full code coverage, but complete RSI is generally not available, often requiring expensive runtime checks in the instrumented program. Providing accurate and complete RSI in the instrumentation process is challenging because of dynamic class loading and classloader namespaces. In this paper, we present a novel technique to accurately reify complete RSI in a separate instrumentation process. We implement our technique in the dynamic analysis framework DiSL and evaluate it on a task profiler, achieving speedups of up to 45\% for an analysis with full code coverage.},
	urldate = {2018-01-18},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {ACM},
	author = {Ros{\`a}, Andrea and Rosales, Eduardo and Binder, Walter},
	year = {2017},
	keywords = {bytecode instrumentation, Dynamic analysis, Java Virtual Machine, reflective information},
	pages = {104--116},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/PDUHLTFF/Ros{\`a} et al. - 2017 - Accurate Reification of Complete Supertype Informa.pdf:application/pdf}
}

@inproceedings{zaytsev_parser_2017,
	address = {New York, NY, USA},
	series = {{GPCE} 2017},
	title = {Parser {Generation} by {Example} for {Legacy} {Pattern} {Languages}},
	isbn = {978-1-4503-5524-7},
	url = {http://doi.acm.org/10.1145/3136040.3136058},
	doi = {10.1145/3136040.3136058},
	abstract = {Most modern software languages enjoy relatively free and relaxed concrete syntax, with significant flexibility of formatting of the program/model/sheet text. Yet, in the dark legacy corners of software engineering there are still languages with a strict fixed column-based structure {\textemdash} the compromises of times long gone, attempting to combine some human readability with some ease of machine processing. In this paper, we consider an industrial case study for retirement of a legacy domain-specific language, completed under extreme circumstances: absolute lack of documentation, varying line structure, hierarchical blocks within one file, scalability demands for millions of lines of code, performance demands for manipulating tens of thousands multi-megabyte files, etc. However, the regularity of the language allowed to infer its structure from the available examples, automatically, and produce highly efficient parsers for it.},
	urldate = {2018-01-18},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {ACM},
	author = {Zaytsev, Vadim},
	year = {2017},
	keywords = {parser generation, engineering by example, grammar inference, language acquisition, legacy software, pattern languages},
	pages = {212--218},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/XMAJJYFK/Zaytsev - 2017 - Parser Generation by Example for Legacy Pattern La.pdf:application/pdf}
}

@inproceedings{carlson_type_2017,
	address = {New York, NY, USA},
	series = {{GPCE} 2017},
	title = {Type {Qualifiers} {As} {Composable} {Language} {Extensions}},
	isbn = {978-1-4503-5524-7},
	url = {http://doi.acm.org/10.1145/3136040.3136055},
	doi = {10.1145/3136040.3136055},
	abstract = {This paper reformulates type qualifiers as language extensions that can be automatically and reliably composed. Type qualifiers annotate type expressions to introduce new subtyping relations and are powerful enough to detect many kinds of errors. Type qualifiers, as illustrated in our ableC extensible language framework for C, can introduce rich forms of concrete syntax, can generate dynamic checks on data when static checks are infeasible or not appropriate, and inject code that affects the program's behavior, for example for conversions of data or logging.   ableC language extensions to C are implemented as attribute grammar fragments and provide an expressive mechanism for type qualifier implementations to check for additional errors, e.g. dereferences to pointers not qualified by a "nonnull" qualifier, and report custom error messages. Our approach distinguishes language extension users from developers and provides modular analyses to developers to ensure that when users select a set of extensions to use, they will automatically compose to form a working compiler.},
	urldate = {2018-01-18},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN} {International} {Conference} on {Generative} {Programming}: {Concepts} and {Experiences}},
	publisher = {ACM},
	author = {Carlson, Travis and Van Wyk, Eric},
	year = {2017},
	keywords = {type systems, type qualifiers, dimensional analysis, extensible languages, pluggable types},
	pages = {91--103},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/KFS8YDPJ/Carlson and Van Wyk - 2017 - Type Qualifiers As Composable Language Extensions.pdf:application/pdf}
}

@inproceedings{wurthinger_practical_2017,
	address = {New York, NY, USA},
	series = {{PLDI} 2017},
	title = {Practical {Partial} {Evaluation} for {High}-performance {Dynamic} {Language} {Runtimes}},
	isbn = {978-1-4503-4988-8},
	url = {http://doi.acm.org/10.1145/3062341.3062381},
	doi = {10.1145/3062341.3062381},
	abstract = {Most high-performance dynamic language virtual machines duplicate language semantics in the interpreter, compiler, and runtime system. This violates the principle to not repeat yourself. In contrast, we define languages solely by writing an interpreter. The interpreter performs specializations, e.g., augments the interpreted program with type information and profiling information. Compiled code is derived automatically using partial evaluation while incorporating these specializations. This makes partial evaluation practical in the context of dynamic languages: It reduces the size of the compiled code while still compiling all parts of an operation that are relevant for a particular program. When a speculation fails, execution transfers back to the interpreter, the program re-specializes in the interpreter, and later partial evaluation again transforms the new state of the interpreter to compiled code. We evaluate our approach by comparing our implementations of JavaScript, Ruby, and R with best-in-class specialized production implementations. Our general-purpose compilation system is competitive with production systems even when they have been heavily optimized for the one language they support. For our set of benchmarks, our speedup relative to the V8 JavaScript VM is 0.83x, relative to JRuby is 3.8x, and relative to GNU R is 5x.},
	urldate = {2018-01-19},
	booktitle = {Proceedings of the 38th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {W{\"u}rthinger, Thomas and Wimmer, Christian and Humer, Christian and W{\"o}{\ss}, Andreas and Stadler, Lukas and Seaton, Chris and Duboscq, Gilles and Simon, Doug and Grimmer, Matthias},
	year = {2017},
	keywords = {dynamic languages, partial evaluation, virtual machine, optimization, language implementation},
	pages = {662--676},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/YVGBKN8X/W{\"u}rthinger et al. - 2017 - Practical Partial Evaluation for High-performance .pdf:application/pdf}
}

@inproceedings{gallaba_dont_2015,
	title = {Don't {Call} {Us}, {We}'ll {Call} {You}: {Characterizing} {Callbacks} in {Javascript}},
	shorttitle = {Don't {Call} {Us}, {We}'ll {Call} {You}},
	doi = {10.1109/ESEM.2015.7321196},
	abstract = {JavaScript is a popular language for developing web applications and is increasingly used for both client-side and server-side application logic. The JavaScript runtime is inherently event-driven and callbacks are a key language feature. Unfortunately, callbacks induce a non-linear control flow and can be deferred to execute asynchronously, declared anonymously, and may be nested to arbitrary levels. All of these features make callbacks difficult to understand and maintain. We perform an empirical study to characterize JavaScript callback usage across a representative corpus of 138 JavaScript programs, with over 5 million lines of JavaScript code. We find that on average, every 10th function definition takes a callback argument, and that over 43\% of all callback-accepting function callsites are anonymous. Furthermore, the majority of callbacks are nested, more than half of all callbacks are asynchronous, and asynchronous callbacks, on average, appear more frequently in client-side code (72\%) than server-side (55\%). We also study three well-known solutions designed to help with the complexities associated with callbacks, including the error-first callback convention, Async.js library, and Promises. Our results inform the design of future JavaScript analysis and code comprehension tools.},
	booktitle = {2015 {ACM}/{IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement} ({ESEM})},
	author = {Gallaba, K. and Mesbah, A. and Beschastnikh, I.},
	month = oct,
	year = {2015},
	keywords = {Java, Protocols, Libraries, program diagnostics, Complexity theory, Web applications, Async.js library, Best practices, Browsers, callback argument, callback-accepting function, client-side application logic, code comprehension tool, error-first callback convention, Games, JavaScript analysis, JavaScript callback usage, JavaScript code, JavaScript language, JavaScript runtime feature, Reactive power, server-side application logic},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/BCE9TETT/7321196.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/DM8Z9KNP/Gallaba et al. - 2015 - Don't Call Us, We'll Call You Characterizing Call.pdf:application/pdf}
}

@article{bull_benchmark_2000,
	title = {A benchmark suite for high performance {Java}},
	volume = {12},
	issn = {1096-9128},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/1096-9128(200005)12:6<375::AID-CPE480>3.0.CO;2-M/abstract},
	doi = {10.1002/1096-9128(200005)12:6<375::AID-CPE480>3.0.CO;2-M},
	abstract = {Increasing interest is being shown in the use of Java for large scale or Grande applications. This new use of Java places specific demands on the Java execution environments that could be tested and compared using a standard benchmark suite. We describe the design and implementation of such a suite, paying particular attention to Java-specific issues. Sample results are presented for a number of implementations of the Java Virtual Machine (JVM). Copyright {\textcopyright} 2000 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2018-01-23},
	journal = {Concurrency: Practice and Experience},
	author = {Bull, J. M. and Smith, L. A. and Westhead, M. D. and Henty, D. S. and Davey, R. A.},
	month = may,
	year = {2000},
	keywords = {Java, JVM, benchmarking, high performance},
	pages = {375--388},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/CX69ELJS/Bull et al. - 2000 - A benchmark suite for high performance Java.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/LWN2NES3/abstract.html:text/html}
}

@article{fitzgerald_marmot:_2000,
	title = {Marmot: an optimizing compiler for {Java}},
	volume = {30},
	issn = {1097-024X},
	shorttitle = {Marmot},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-024X(200003)30:3<199::AID-SPE296>3.0.CO;2-2/abstract},
	doi = {10.1002/(SICI)1097-024X(200003)30:3<199::AID-SPE296>3.0.CO;2-2},
	abstract = {The Marmot system is a research platform for studying the implementation of high level programming languages. It currently comprises an optimizing native-code compiler, runtime system, and libraries for a large subset of Java. Marmot integrates well-known representation, optimization, code generation, and runtime techniques with a few Java-specific features to achieve competitive performance. This paper contains a description of the Marmot system design, along with highlights of our experience applying and adapting traditional implementation techniques to Java. A detailed performance evaluation assesses both Marmot's overall performance relative to other Java and C++ implementations, and the relative costs of various Java language features in Marmot-compiled code. Our experience with Marmot has demonstrated that well-known compilation techniques can produce very good performance for static Java applications {\textendash} comparable or superior to other Java systems, and approaching that of C++ in some cases. Copyright {\textcopyright} 2000 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	urldate = {2018-01-23},
	journal = {Software: Practice and Experience},
	author = {Fitzgerald, Robert and Knoblock, Todd B. and Ruf, Erik and Steensgaard, Bjarne and Tarditi, David},
	month = mar,
	year = {2000},
	keywords = {Java, compilers, optimization, language translation},
	pages = {199--232},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/ENLD6QS7/Fitzgerald et al. - 2000 - Marmot an optimizing compiler for Java.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/63TCAEZU/abstract.html:text/html}
}

@inproceedings{calciati_how_2017,
	title = {How {Do} {Apps} {Evolve} in {Their} {Permission} {Requests}? {A} {Preliminary} {Study}},
	shorttitle = {How {Do} {Apps} {Evolve} in {Their} {Permission} {Requests}?},
	doi = {10.1109/MSR.2017.64},
	abstract = {We present a preliminary study to understand how apps evolve in their permission requests across different releases. We analyze over 14K releases of 227 Android apps, and we see how permission requests change and how they are used. We find that apps tend to request more permissions in their evolution, and many of the newly requested permissions are initially overprivileged. Our qualitative analysis, however, shows that the results that popular tools report on overprivileged apps may be biased by incomplete information or by other factors. Finally, we observe that when apps no longer request a permission, it does not necessarily mean that the new release offers less in terms of functionalities.},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Calciati, P. and Gorla, A.},
	month = may,
	year = {2017},
	keywords = {Software, Market research, Tools, Android (operating system), Androids, Google, Humanoid robots, Android application, Cameras, overprivileged application, permission requests, qualitative analysis},
	pages = {37--41},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/HC68J3RD/7962353.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/CU6RVINP/Calciati and Gorla - 2017 - How Do Apps Evolve in Their Permission Requests A.pdf:application/pdf}
}

@inproceedings{corbellini_mining_2017,
	title = {Mining {Social} {Web} {Service} {Repositories} for {Social} {Relationships} to {Aid} {Service} {Discovery}},
	doi = {10.1109/MSR.2017.16},
	abstract = {The Service Oriented Computing (SOC) paradigm promotes building new applications by discovering and then invoking services, i.e., software components accessible through the Internet. Discovering services means inspecting registries where textual descriptions of services functional capabilities are stored. To automate this, existing approaches index descriptions and associate users' queries to relevant services. However, the massive adoption of Web-exposed API development practices, specially in large service ecosystems such as the IoT, is leading to evergrowing registries which challenge the accuracy and speed of such approaches. The recent notion of Social Web Services (SWS), where registries not only store service information but also sociallike relationships between users and services opens the door to new discovery schemes. We investigate an approach to discover SWSs that operates on graphs with user-service relationships and employs lightweight topological metrics to assess service similarity. Then, "socially" similar services, which are determined exploiting explicit relationships and mining implicit relationships in the graph, are clustered via exemplar-based clustering to ultimately aid discovery. Experiments performed with the ProgrammableWeb.com registry, which is at present the largest SWS repository with over 15k services and 140k user-service relationships, show that pure topology-based clustering may represent a promising complement to content-based approaches, which in fact are more time-consuming due to text processing operations.},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Corbellini, A. and Godoy, D. and Mateos, C. and Zunino, A. and Lizarralde, I.},
	month = may,
	year = {2017},
	keywords = {Buildings, data mining, object-oriented programming, application program interfaces, Indexes, Measurement, Internet, Web services, Clustering algorithms, content-based approaches, Entropy, exemplar-based clustering, Exemplar-based clustering, index descriptions, IoT, large service ecosystems, pattern clustering, ProgrammableWeb.com registry, service discovery, Service discovery, service functional capabilities, service oriented computing, service-oriented architecture, Social network services, social networking (online), Social recommender systems, social relationships, Social Web Service, social Web service repositories mining, software components, text processing operations, topology-based clustering, user-service relationships, Web-exposed API development},
	pages = {75--79},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/BZC4M79E/7962357.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/XDUQTJS2/Corbellini et al. - 2017 - Mining Social Web Service Repositories for Social .pdf:application/pdf}
}

@inproceedings{patil_concept-based_2017,
	title = {Concept-{Based} {Classification} of {Software} {Defect} {Reports}},
	doi = {10.1109/MSR.2017.20},
	abstract = {Automatic identification of the defect type from the textual description of a software defect can significantly speed-up as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects. In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the "semantic similarity" between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Patil, S.},
	month = may,
	year = {2017},
	keywords = {data mining, Software, software engineering, Internet, Semantics, Encyclopedias, Training data, concept-based classification, Electronic publishing, ESA, explicit semantic analysis, Explicit Semantic Analysis, labeled training data, learning (artificial intelligence), Mining Software Respositories, semantic similarity, software defect classification, Software Defect Classification, software defect management life-cycle, software defect reports, software management, supervised learning approach, Text Data Mining},
	pages = {182--186},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/KXYTBSH7/7962367.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/QQDY87RC/Patil - 2017 - Concept-Based Classification of Software Defect Re.pdf:application/pdf}
}

@inproceedings{mantyla_bootstrapping_2017,
	address = {Piscataway, NJ, USA},
	series = {{MSR} '17},
	title = {Bootstrapping a {Lexicon} for {Emotional} {Arousal} in {Software} {Engineering}},
	isbn = {978-1-5386-1544-7},
	url = {https://doi.org/10.1109/MSR.2017.47},
	doi = {10.1109/MSR.2017.47},
	abstract = {Emotional arousal increases activation and performance but may also lead to burnout in software development. We present the first version of a Software Engineering Arousal lexicon (SEA) that is specifically designed to address the problem of emotional arousal in the software developer ecosystem. SEA is built using a bootstrapping approach that combines word embedding model trained on issue-tracking data and manual scoring of items in the lexicon. We show that our lexicon is able to differentiate between issue priorities, which are a source of emotional activation and then act as a proxy for arousal. The best performance is obtained by combining SEA (428 words) with a previously created general purpose lexicon by Warriner et al. (13,915 words) and it achieves Cohen's d effect sizes up to 0.5.},
	urldate = {2018-01-23},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {IEEE Press},
	author = {M{\"a}ntyl{\"a}, Mika V. and Novielli, Nicole and Lanubile, Filippo and Claes, Ma{\"e}lick and Kuutila, Miikka},
	year = {2017},
	keywords = {empirical software engineering, emotional arousal, issue report, lexicon, sentiment analysis},
	pages = {198--202},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/HHNVQNW9/M{\"a}ntyl{\"a} et al. - 2017 - Bootstrapping a Lexicon for Emotional Arousal in S.pdf:application/pdf}
}

@inproceedings{claes_abnormal_2017,
	title = {Abnormal {Working} {Hours}: {Effect} of {Rapid} {Releases} and {Implications} to {Work} {Content}},
	shorttitle = {Abnormal {Working} {Hours}},
	doi = {10.1109/MSR.2017.3},
	abstract = {During the past years, overload at work leading to psychological diseases, such as burnouts, have drawn more public attention. This paper is a preliminary step toward an analysis of the work patterns and possible indicators of overload and time pressure on software developers with mining software repositories approach. We explore the working pattern of developers in the context of Mozilla Firefox, a large and long-lived open source project. To that end we investigate the impact of the move from traditional to rapid release cycle on work pattern. Moreover we compare Mozilla Firefox work pattern with another Mozilla product, Firefox OS, which has a different release cycle than Firefox. We find that both projects exhibit healthy working patterns, i.e. lower activity during the weekends and outside of office hours. Firefox experiences proportionally more activity on weekends than Firefox OS (Cohen's d = 0.94). We find that switching to rapid releases has reduced weekend work (Cohen's d = 1.43) and working during the night (Cohen's d = 0.45). This result holds even when we limit the analyzes on the hired resources, i.e. considering only individuals with Mozilla foundation email address, although, the effect sizes are smaller for weekends (Cohen's d = 0.64) and nights (Cohen's d = 0.23). Moreover, we use dissimilarity word clouds and find that work during the weekend is more technical while work during the week expresses more positive sentiment with words like "good" and "nice". Our results suggest that moving to rapid releases have positive impact on the work health and work-life-balance of software engineers. However, caution is needed as our results are based on a limited set of quantitative data from a single organization.},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Claes, M. and M{\"a}ntyl{\"a}, M. and Kuutila, M. and Adams, B.},
	month = may,
	year = {2017},
	keywords = {Data mining, Software, software engineering, Electronic mail, public domain software, Computer bugs, software developers, empirical software engineering, human factors, Psychology, abnormal working hours, bugzilla, burnouts, diseases, Diseases, dissimilarity word clouds, firefox, Firefox OS, mozilla, Mozilla Firefox work pattern, open source project, psychological diseases, psychology, rapid releases, release, software engineer work health, software engineer work-life-balance, software repositories approach, Switches, time pressure, weekend, work content, working pattern},
	pages = {243--247},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/YKYA5S8L/7962374.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/P4DJSA59/Claes et al. - 2017 - Abnormal Working Hours Effect of Rapid Releases a.pdf:application/pdf}
}

@inproceedings{cartaxo_using_2017,
	title = {Using {Q} {A} {Websites} as a {Method} for {Assessing} {Systematic} {Reviews}},
	doi = {10.1109/MSR.2017.5},
	abstract = {Questions and Answers (Q\&A) websites maintain a long history of needs, problems, and challenges that software developers face. In contrast to Q\&A websites, which are strongly tied to practitioners' needs, there are systematic reviews (SRs), which, according to recent studies, lack a connection with software engineering practice. In this paper, we investigate this claim by assessing to what extent systematic reviews help to solve questions posted on Q\&A websites. To achieve this goal, we propose and evaluate a coverage method. We applied this method to a set of more than 600 questions related to agile software development. Results suggest that 12\% of the related questions were covered. When considering specific agile methods, the majority of them have coverage below 50\% or were not covered at all. We also identified 27 recurrent questions.},
	booktitle = {2017 {IEEE}/{ACM} 14th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	author = {Cartaxo, B. and Pinto, G. and Ribeiro, D. and Kamei, F. and Santos, R. E. S. and Silva, F. Q. B. da and Soares, S.},
	month = may,
	year = {2017},
	keywords = {Programming, Software, software engineering, Software engineering, Web sites, Systematics, software developers, agile software development, Encoding, Focusing, Q\&A Web sites, questions and answers Web sites, Scrum (Software development), software prototyping, software reviews, systematic review assessment},
	pages = {238--242},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/SNW887GQ/7962373.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/3R9CC42Q/Cartaxo et al. - 2017 - Using Q A Websites as a Method for Assessing Syste.pdf:application/pdf}
}

@inproceedings{orlov_genetic_2009,
	address = {New York, NY, USA},
	series = {{GECCO} '09},
	title = {Genetic {Programming} in the {Wild}: {Evolving} {Unrestricted} {Bytecode}},
	isbn = {978-1-60558-325-9},
	shorttitle = {Genetic {Programming} in the {Wild}},
	url = {http://doi.acm.org/10.1145/1569901.1570042},
	doi = {10.1145/1569901.1570042},
	abstract = {We describe a methodology for evolving Java bytecode, enabling the evolution of extant, unrestricted Java programs, or programs in other languages that compile to Java bytecode. Bytecode is evolved directly, without any intermediate genomic representation. Our approach is based upon the notion of compatible crossover, which produces correct programs by performing operand stack-, local variables-, and control flow-based compatibility checks on source and destination bytecode sections. This is in contrast to existing work that uses restricted subsets of the Java bytecode instruction set as a representation language for individuals in genetic programming. Given the huge universe of unrestricted Java bytecode, as is programs, our work enables the applications of evolution within this realm. We experimentally validate our methodology by both extensively testing the correctness of compatible crossover on arbitrary bytecode, and by running evolution on a program that exploits the richness of the Java virtual machine architecture and type system.},
	urldate = {2018-01-23},
	booktitle = {Proceedings of the 11th {Annual} {Conference} on {Genetic} and {Evolutionary} {Computation}},
	publisher = {ACM},
	author = {Orlov, Michael and Sipper, Moshe},
	year = {2009},
	keywords = {software evolution, java bytecode},
	pages = {1043--1050},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/KH84DVYT/Orlov and Sipper - 2009 - Genetic Programming in the Wild Evolving Unrestri.pdf:application/pdf}
}

@inproceedings{tronicek_refactoringng:_2012,
	address = {New York, NY, USA},
	series = {{SAC} '12},
	title = {{RefactoringNG}: {A} {Flexible} {Java} {Refactoring} {Tool}},
	isbn = {978-1-4503-0857-1},
	shorttitle = {{RefactoringNG}},
	url = {http://doi.acm.org/10.1145/2245276.2231959},
	doi = {10.1145/2245276.2231959},
	abstract = {The Java programming language and the Java API evolve and this evolution certainly will continue in future. Upgrade to a new version of programming language or API is nowadays usually done manually. We describe a new flexible refactoring tool for the Java programming language that can upgrade the code almost automatically. The tool performs refactoring rules described in the special language based on the abstract syntax trees. Each rule consists of two abstract syntax trees: the pattern and the rewrite. First, we search for the pattern and then replace each pattern occurrence with the rewrite. Searching and replacement is performed on the abstract syntax trees that are built and fully attributed by the Java compiler. Complete syntactic and semantic information about the source code and flexibility in refactoring rules give the tool competitive advantage over most similar tools.},
	urldate = {2018-01-23},
	booktitle = {Proceedings of the 27th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {ACM},
	author = {Tron{\'i}{\v c}ek, Zden{\v e}k},
	year = {2012},
	keywords = {Java, refactoring, API evolution, software evolution},
	pages = {1165--1170},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/5H3AMBT9/Tron{\'i}{\v c}ek - 2012 - RefactoringNG A Flexible Java Refactoring Tool.pdf:application/pdf}
}

@inproceedings{ernst_family_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Family {Polymorphism}},
	isbn = {978-3-540-42206-8 978-3-540-45337-6},
	url = {https://link.springer.com/chapter/10.1007/3-540-45337-7_17},
	doi = {10.1007/3-540-45337-7_17},
	abstract = {This paper takes polymorphism to the multi-object level. Traditional inheritance, polymorphism, and late binding interact nicely to provide both flexibility and safety {\textemdash} when a method is invoked on an object via a polymorphic reference, late binding ensures that we get the appropriate implementation of that method for the actual object. We are granted the flexibility of using different kinds of objects and different method implementations, and we are guaranteed the safety of the combination. Nested classes, polymorphism, and late binding of nested classes interact similarly to provide both safety and flexibility at the level of multi-object systems. We are granted the flexibility of using different families of kinds of objects, and we are guaranteed the safety of the combination. This paper highlights the inability of traditional polymorphism to handle multiple objects, and presents family polymorphism as a way to overcome this problem. Family polymorphism has been implemented in the programming language gbeta, a generalized version of Beta, and the source code of this implementation is available under GPL.1},
	language = {en},
	urldate = {2018-01-23},
	booktitle = {{ECOOP} 2001 {\textemdash} {Object}-{Oriented} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Ernst, Erik},
	month = jun,
	year = {2001},
	pages = {303--326},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/NIALHHAA/Ernst - 2001 - Family Polymorphism.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/JNEHCZHY/3-540-45337-7_17.html:text/html}
}

@article{wadler_propositions_2015,
	title = {Propositions {As} {Types}},
	volume = {58},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2699407},
	doi = {10.1145/2699407},
	abstract = {Connecting mathematical logic and computation, it ensures that some aspects of programming are absolute.},
	number = {12},
	urldate = {2018-01-27},
	journal = {Commun. ACM},
	author = {Wadler, Philip},
	month = nov,
	year = {2015},
	pages = {75--84},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/SALIDGBQ/Wadler - 2015 - Propositions As Types.pdf:application/pdf}
}

@inproceedings{ploeg_practical_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Practical {Principled} {FRP}: {Forget} the {Past}, {Change} the {Future}, {FRPNow}!},
	isbn = {978-1-4503-3669-7},
	shorttitle = {Practical {Principled} {FRP}},
	url = {http://doi.acm.org/10.1145/2784731.2784752},
	doi = {10.1145/2784731.2784752},
	abstract = {We present a new interface for practical Functional Reactive Programming (FRP) that (1) is close in spirit to the original FRP ideas, (2) does not have the original space-leak problems, without using arrows or advanced types, and (3) provides a simple and expressive way for performing IO actions from FRP code. We also provide a denotational semantics for this new interface, and a technique (using Kripke logical relations) for reasoning about which FRP functions may "forget their past", i.e. which functions do not have an inherent space-leak. Finally, we show how we have implemented this interface as a Haskell library called FRPNow.},
	urldate = {2018-01-27},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Ploeg, Atze van der and Claessen, Koen},
	year = {2015},
	keywords = {Functional Reactive Programming, Kripke Logical Relations, Purely Functional IO, Space-leak},
	pages = {302--314},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/Y5IZU8LK/Ploeg and Claessen - 2015 - Practical Principled FRP Forget the Past, Change .pdf:application/pdf}
}

@inproceedings{najd_everything_2016,
	address = {New York, NY, USA},
	series = {{PEPM} '16},
	title = {Everything {Old} is {New} {Again}: {Quoted} {Domain}-specific {Languages}},
	isbn = {978-1-4503-4097-7},
	shorttitle = {Everything {Old} is {New} {Again}},
	url = {http://doi.acm.org/10.1145/2847538.2847541},
	doi = {10.1145/2847538.2847541},
	abstract = {We describe a new approach to implementing Domain-Specific Languages(DSLs), called Quoted DSLs (QDSLs), that is inspired by two old ideas:quasi-quotation, from McCarthy's Lisp of 1960, and the subformula principle of normal proofs, from Gentzen's natural deduction of 1935. QDSLs reuse facilities provided for the host language, since host and quoted terms share the same syntax, type system, and normalisation rules. QDSL terms are normalised to a canonical form, inspired by the subformula principle, which guarantees that one can use higher-order types in the source while guaranteeing first-order types in the target, and enables using types to guide fusion. We test our ideas by re-implementing Feldspar, which was originally implemented as an Embedded DSL (EDSL), as a QDSL; and we compare the QDSL and EDSL variants. The two variants produce identical code.},
	urldate = {2018-01-27},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} {Workshop} on {Partial} {Evaluation} and {Program} {Manipulation}},
	publisher = {ACM},
	author = {Najd, Shayan and Lindley, Sam and Svenningsson, Josef and Wadler, Philip},
	year = {2016},
	keywords = {domain-specific language, DSL, EDSL, embedded language, normalisation, QDSL, quotation, subformula principle},
	pages = {25--36},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/6QX3S8NS/Najd et al. - 2016 - Everything Old is New Again Quoted Domain-specifi.pdf:application/pdf}
}

@inproceedings{norell_dependently_2009,
	address = {New York, NY, USA},
	series = {{TLDI} '09},
	title = {Dependently {Typed} {Programming} in {Agda}},
	isbn = {978-1-60558-420-1},
	url = {http://doi.acm.org/10.1145/1481861.1481862},
	doi = {10.1145/1481861.1481862},
	abstract = {Dependently typed languages have for a long time been used to describe proofs about programs. Traditionally, dependent types are used mostly for stating and proving the properties of the programs and not in defining the programs themselves. An impressive example is the certified compiler by Leroy (2006) implemented and proved correct in Coq (Bertot and Cast{\'e}ran 2004). Recently there has been an increased interest in dependently typed programming, where the aim is to write programs that use the dependent type system to a much higher degree. In this way a lot of the properties that were previously proved separately can be integrated in the type of the program, in many cases adding little or no complexity to the definition of the program. New languages, such as Epigram (McBride and McKinna 2004), are being designed, and existing languages are being extended with new features to accomodate these ideas, for instance the work on dependently typed programming in Coq by Sozeau (2007). This talk gives an overview of the Agda programming language (Norell 2007), whose main focus is on dependently typed programming. Agda provides a rich set of inductive types with a powerful mechanism for pattern matching, allowing dependently typed programs to be written with minimal fuss. To read about programming in Agda, see the lecture notes from the Advanced Functional Programming summer school (Norell 2008) and the work by Oury and Swierstra (2008). In the talk a number of examples of interesting dependently typed programs chosen from the domain of programming language implementation are presented as they are implemented in Agda.},
	urldate = {2018-01-27},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Types} in {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Norell, Ulf},
	year = {2009},
	keywords = {dependent types, programming},
	pages = {1--2},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/44HEQ3PL/Norell - 2009 - Dependently Typed Programming in Agda.pdf:application/pdf}
}

@inproceedings{norell_dependently_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Dependently {Typed} {Programming} in {Agda}},
	isbn = {978-3-642-04651-3 978-3-642-04652-0},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-04652-0_5},
	doi = {10.1007/978-3-642-04652-0_5},
	abstract = {In Hindley-Milner style languages, such as Haskell and ML, there is a clear separation between types and values. In a dependently typed language the line is more blurry - types can contain (depend on) arbitrary values and appear as arguments and results of ordinary functions.},
	language = {en},
	urldate = {2018-01-27},
	booktitle = {Advanced {Functional} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Norell, Ulf},
	month = may,
	year = {2008},
	pages = {230--266},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/ABTIMZRN/Norell - 2008 - Dependently Typed Programming in Agda.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/UFIZYIBP/978-3-642-04652-0_5.html:text/html}
}

@article{gedik_elastic_2014,
	title = {Elastic {Scaling} for {Data} {Stream} {Processing}},
	volume = {25},
	issn = {1045-9219},
	doi = {10.1109/TPDS.2013.295},
	abstract = {This article addresses the profitability problem associated with auto-parallelization of general-purpose distributed data stream processing applications. Auto-parallelization involves locating regions in the application's data flow graph that can be replicated at run-time to apply data partitioning, in order to achieve scale. In order to make auto-parallelization effective in practice, the profitability question needs to be answered: How many parallel channels provide the best throughput? The answer to this question changes depending on the workload dynamics and resource availability at run-time. In this article, we propose an elastic auto-parallelization solution that can dynamically adjust the number of channels used to achieve high throughput without unnecessarily wasting resources. Most importantly, our solution can handle partitioned stateful operators via run-time state migration, which is fully transparent to the application developers. We provide an implementation and evaluation of the system on an industrial-strength data stream processing platform to validate our solution.},
	number = {6},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Gedik, B. and Schneider, S. and Hirzel, M. and Wu, K. L.},
	month = jun,
	year = {2014},
	keywords = {Runtime, Parallel processing, Indexes, Measurement, data analysis, application data flow graph, auto-parallelization, Availability, data flow graphs, data partitioning, Data stream processing, elastic scaling, elasticity, general-purpose distributed data stream processing applications, industrial-strength data stream processing platform, parallel channels, parallel processing, parallelization, profitability, profitability problem, resource availability, run-time state migration, Safety, Throughput, workload dynamics},
	pages = {1447--1463},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/ZSGTXRAX/6678504.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/EIKRYV75/Gedik et al. - 2014 - Elastic Scaling for Data Stream Processing.pdf:application/pdf}
}

@book{rossberg_beyond_2002,
	title = {Beyond {Type} {Classes}},
	abstract = {We discuss type classes in the context of the Chameleon language, a Haskell-style language where overloading resolution is expressed in terms of the meta-language of Constraint Handling Rules (CHRs). In a first step, we show how to encode Haskell's single-parameter type classes into Chameleon. The encoding works by providing an approrpriate set of CHRs which mimic the Haskell conditions. We also consider constructor classes, multi-parameter type classes and functional dependencies. Chameleon provides a testbed to experiment with new overloading features. We show how some novel features such as universal quantification in context can naturally be expressed in Chameleon.},
	author = {Rossberg, Andreas and Sulzmann, Martin},
	year = {2002},
	file = {Citeseer - Full Text PDF:/Users/luigi/work/zotero/storage/F4UY4QYZ/Rossberg and Sulzmann - 2002 - Beyond Type Classes.pdf:application/pdf;Citeseer - Snapshot:/Users/luigi/work/zotero/storage/UJZ2NRN9/summary.html:text/html}
}

@inproceedings{rompf_functional_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Functional {Pearl}: {A} {SQL} to {C} {Compiler} in 500 {Lines} of {Code}},
	isbn = {978-1-4503-3669-7},
	shorttitle = {Functional {Pearl}},
	url = {http://doi.acm.org/10.1145/2784731.2784760},
	doi = {10.1145/2784731.2784760},
	abstract = {We present the design and implementation of a SQL query processor that outperforms existing database systems and is written in just about 500 lines of Scala code -- a convincing case study that high-level functional programming can handily beat C for systems-level programming where the last drop of performance matters. The key enabler is a shift in perspective towards generative programming. The core of the query engine is an interpreter for relational algebra operations, written in Scala. Using the open-source LMS Framework (Lightweight Modular Staging), we turn this interpreter into a query compiler with very low effort. To do so, we capitalize on an old and widely known result from partial evaluation known as Futamura projections, which state that a program that can specialize an interpreter to any given input program is equivalent to a compiler. In this pearl, we discuss LMS programming patterns such as mixed-stage data structures (e.g. data records with static schema and dynamic field components) and techniques to generate low-level C code, including specialized data structures and data loading primitives.},
	urldate = {2018-01-27},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Rompf, Tiark and Amin, Nada},
	year = {2015},
	keywords = {Futamura Projections, Generative Programming, Query Compilation, SQL, Staging},
	pages = {2--9},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/QMGANW9U/Rompf and Amin - 2015 - Functional Pearl A SQL to C Compiler in 500 Lines.pdf:application/pdf}
}

@inproceedings{nagaraj_approximating_2015,
	address = {Washington, DC, USA},
	series = {{CGO} '15},
	title = {Approximating {Flow}-sensitive {Pointer} {Analysis} {Using} {Frequent} {Itemset} {Mining}},
	isbn = {978-1-4799-8161-8},
	url = {http://dl.acm.org/citation.cfm?id=2738600.2738629},
	abstract = {Pointer alias analysis is a well researched problem in the area of compilers and program verification. Many recent works in this area have focused on flow-sensitivity due to the additional precision it offers. However, a flow-sensitive analysis is computationally expensive, thus, preventing its use in larger programs. In this work, we observe that a number of object sets, consisting of tens to hundreds of objects appear together and frequently in many points-to sets. By approximating each of these object sets by a single object, we can speedup computation of points-to sets. Although the proposed approach incurs a slight loss in precision, it is shown to be safe. We use a well known data mining technique called frequent itemset mining to find these frequently occurring objects. We compare our approximation to a fully flow-sensitive pointer analysis on a set of ten benchmarks. We measure precision loss using two common client analysis queries and report an average precision loss of 0.25\% on one measure and 1.40\% on the other. The proposed approach results in a speedup of upto 12.9x (and an average speedup of 6.2x) in computing the points-to sets.},
	urldate = {2018-02-05},
	booktitle = {Proceedings of the 13th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {IEEE Computer Society},
	author = {Nagaraj, Vaivaswatha and Govindarajan, R.},
	year = {2015},
	pages = {225--234},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/LNJF5GH2/Nagaraj and Govindarajan - 2015 - Approximating Flow-sensitive Pointer Analysis Usin.pdf:application/pdf}
}

@inproceedings{bayne_always-available_2011,
	address = {New York, NY, USA},
	series = {{ICSE} '11},
	title = {Always-available {Static} and {Dynamic} {Feedback}},
	isbn = {978-1-4503-0445-0},
	url = {http://doi.acm.org/10.1145/1985793.1985864},
	doi = {10.1145/1985793.1985864},
	abstract = {Developers who write code in a statically typed language are denied the ability to obtain dynamic feedback by executing their code during periods when it fails the static type checker. They are further confined to the static typing discipline during times in the development process where it does not yield the highest productivity. If they opt instead to use a dynamic language, they forgo the many benefits of static typing, including machine-checked documentation, improved correctness and reliability, tool support (such as for refactoring), and better runtime performance. We present a novel approach to giving developers the benefits of both static and dynamic typing, throughout the development process, and without the burden of manually separating their program into statically and dynamically-typed parts. Our approach, which is intended for temporary use during the development process, relaxes the static type system and provides a semantics for many type-incorrect programs. It defers type errors to run time, or suppresses them if they do not affect runtime semantics. We implemented our approach in a publicly available tool, DuctileJ, for the Java language. In case studies, DuctileJ conferred benefits both during prototyping and during the evolution of existing code.},
	urldate = {2018-02-08},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Bayne, Michael and Cook, Richard and Ernst, Michael D.},
	year = {2011},
	keywords = {refactoring, gradual typing, dynamic typing, hybrid typing, productivity, prototyping, static typing, type error},
	pages = {521--530},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/M4FXS9SQ/Bayne et al. - 2011 - Always-available Static and Dynamic Feedback.pdf:application/pdf}
}

@inproceedings{brown_blackbox:_2014,
	address = {New York, NY, USA},
	series = {{SIGCSE} '14},
	title = {Blackbox: {A} {Large} {Scale} {Repository} of {Novice} {Programmers}' {Activity}},
	isbn = {978-1-4503-2605-6},
	shorttitle = {Blackbox},
	url = {http://doi.acm.org/10.1145/2538862.2538924},
	doi = {10.1145/2538862.2538924},
	abstract = {Automatically observing and recording the programming behaviour of novices is an established computing education research technique. However, prior studies have been conducted at a single institution on a small or medium scale, without the possibility of data re-use. Now, the widespread availability of always-on Internet access allows for data collection at a much larger, global scale. In this paper we report on the Blackbox project, begun in June 2013. Blackbox is a perpetual data collection project that collects data from worldwide users of the BlueJ IDE -- a programming environment designed for novice programmers. Over one hundred thousand users have already opted-in to Blackbox. The collected data is anonymous and is available to other researchers for use in their own studies, thus benefitting the larger research community. In this paper, we describe the data available via Blackbox, show some examples of analyses that can be performed using the collected data, and discuss some of the analysis challenges that lie ahead.},
	urldate = {2018-02-12},
	booktitle = {Proceedings of the 45th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Brown, Neil Christopher Charles and K{\"o}lling, Michael and McCall, Davin and Utting, Ian},
	year = {2014},
	keywords = {blackbox, BlueJ, data collection, programming education},
	pages = {223--228},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/BEFNXL65/Brown et al. - 2014 - Blackbox A Large Scale Repository of Novice Progr.pdf:application/pdf}
}

@inproceedings{begel_analyze_2014,
	address = {New York, NY, USA},
	series = {{ICSE} 2014},
	title = {Analyze {This}! 145 {Questions} for {Data} {Scientists} in {Software} {Engineering}},
	isbn = {978-1-4503-2756-5},
	url = {http://doi.acm.org/10.1145/2568225.2568233},
	doi = {10.1145/2568225.2568233},
	abstract = {In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.},
	urldate = {2018-02-12},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Begel, Andrew and Zimmermann, Thomas},
	year = {2014},
	keywords = {Analytics, Data Science, Software Engineering},
	pages = {12--23},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/RJZVMR5U/Begel and Zimmermann - 2014 - Analyze This! 145 Questions for Data Scientists in.pdf:application/pdf}
}

@article{myers_programmers_2016,
	title = {Programmers {Are} {Users} {Too}: {Human}-{Centered} {Methods} for {Improving} {Programming} {Tools}},
	volume = {49},
	issn = {0018-9162},
	shorttitle = {Programmers {Are} {Users} {Too}},
	doi = {10.1109/MC.2016.200},
	abstract = {Human-centered methods can help researchers better understand and meet programmers' needs. Because programming is a human activity, many of these methods can be used without change. However, some programmer needs require new methods, which can also be applied to domains other than software engineering. This article features five Web extras. The video at https://youtu.be/4PH9-qi-yTQ demonstrates Azurite, an Eclipse plug-in with a selective undo feature that lets programmers more easily backtrack their code. The video at https://youtu.be/gOSlR62-rd8 describes Graphite, an Eclipse plug-in offering active code completion, a simple but powerful technique that integrates useful code-generation tools directly into the editor. The video at https://youtu.be/zyrqcYxqDtI describes HANDS, a new programming system that emphasizes usability by building on children's and beginning programmers' natural problem-solving tendencies. The video extra at https://youtu.be/80EctbI7PFc describes Whyline, a debugging tool that lets developers ask questions about their program's output and behavior. The video at https://youtu.be/3L4MK2dG\_6k demonstrates the prototype for Whyline, a debugging tool that lets developers pose questions about their program's output.},
	number = {7},
	journal = {Computer},
	author = {Myers, B. A. and Ko, A. J. and LaToza, T. D. and Yoon, Y.},
	month = jul,
	year = {2016},
	keywords = {Java, Data mining, data mining, Programming, software development, software engineering, Software engineering, program debugging, software tools, software psychology, Eclipse plug-in, A/B testing, active code completion, Azurite, contextual inquiry, debugging tool, end-user software engineering, evaluation studies, exploratory lab studies, Graphite, HANDS, HCI, human activity, Human computer interaction, human-centered computing, Human-centered computing, human-centered methods, human-computer interaction, Human-computer interaction, log analysis, Natural language processing, natural problem-solving tendencies, natural-programming elicitation, programming tools, rapid prototyping, studies of program constructs, think-aloud usability evaluation, undo feature, user centred design, user interfaces, Whyline},
	pages = {44--52},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/S5NXT9GC/7503516.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/EVAECBD9/Myers et al. - 2016 - Programmers Are Users Too Human-Centered Methods .pdf:application/pdf}
}

@inproceedings{zhang_context-sensitive_2017,
	address = {New York, NY, USA},
	series = {{POPL} 2017},
	title = {Context-sensitive {Data}-dependence {Analysis} via {Linear} {Conjunctive} {Language} {Reachability}},
	isbn = {978-1-4503-4660-3},
	url = {http://doi.acm.org/10.1145/3009837.3009848},
	doi = {10.1145/3009837.3009848},
	abstract = {Many program analysis problems can be formulated as graph reachability problems. In the literature, context-free language (CFL) reachability has been the most popular formulation and can be computed in subcubic time. The context-sensitive data-dependence analysis is a fundamental abstraction that can express a broad range of program analysis problems. It essentially describes an interleaved matched-parenthesis language reachability problem. The language is not context-free, and the problem is well-known to be undecidable. In practice, many program analyses adopt CFL-reachability to exactly model the matched parentheses for either context-sensitivity or structure-transmitted data-dependence, but not both. Thus, the CFL-reachability formulation for context-sensitive data-dependence analysis is inherently an approximation. To support more precise and scalable analyses, this paper introduces linear conjunctive language (LCL) reachability, a new, expressive class of graph reachability. LCL not only contains the interleaved matched-parenthesis language, but is also closed under all set-theoretic operations. Given a graph with n nodes and m edges, we propose an O(mn) time approximation algorithm for solving all-pairs LCL-reachability, which is asymptotically better than known CFL-reachability algorithms. Our formulation and algorithm offer a new perspective on attacking the aforementioned undecidable problem {\^a}?? the LCL-reachability formulation is exact, while the LCL-reachability algorithm yields a sound approximation. We have applied the LCL-reachability framework to two existing client analyses. The experimental results show that the LCL-reachability framework is both more precise and scalable than the traditional CFL-reachability framework. This paper opens up the opportunity to exploit LCL-reachability in program analysis.},
	urldate = {2018-02-12},
	booktitle = {Proceedings of the 44th {ACM} {SIGPLAN} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Zhang, Qirun and Su, Zhendong},
	year = {2017},
	keywords = {program analysis, Context-free language reachability, linear conjunctive grammar, trellis automata},
	pages = {344--358},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/9LBZX4PJ/Zhang and Su - 2017 - Context-sensitive Data-dependence Analysis via Lin.pdf:application/pdf}
}

@inproceedings{bartman_srcql:_2017,
	title = {{srcQL}: {A} syntax-aware query language for source code},
	shorttitle = {{srcQL}},
	doi = {10.1109/SANER.2017.7884655},
	abstract = {A tool and domain specific language for querying source code is introduced and demonstrated. The tool, srcQL, allows for the querying of source code using the syntax of the language to identify patterns within source code documents. srcQL is built upon srcML, a widely used XML representation of source code, to identify the syntactic contexts being queried. srcML inserts XML tags into the source code to mark syntactic constructs. srcQL uses a combination of XPath on srcML, regular expressions, and syntactic patterns within a query. The syntactic patterns are snippets of source code that supports the use of logical variables which are unified during the query process. This allows for very complex patterns to be easily formulated and queried. The tool is implemented (in C++) and a number of queries are presented to demonstrate the approach. srcQL currently supports C++ and scales to large systems.},
	booktitle = {2017 {IEEE} 24th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Bartman, B. and Newman, C. D. and Collard, M. L. and Maletic, J. I.},
	month = feb,
	year = {2017},
	keywords = {computational linguistics, domain specific language, XML, source code (software), Context, Pattern matching, query languages, Syntactics, Reactive power, C++ languages, Database languages, language syntax, query process, source code documents, source code querying, Source code querying, srcML, srcQL, syntactic contexts, syntactic patterns, syntactic search, syntax-aware query language, XML representation, XML tags, XPath},
	pages = {467--471},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/KGJRY66L/7884655.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/XSGRTIDR/Bartman et al. - 2017 - srcQL A syntax-aware query language for source co.pdf:application/pdf}
}

@inproceedings{collard_srcml_2016,
	title = {{srcML} 1.0: {Explore}, {Analyze}, and {Manipulate} {Source} {Code}},
	shorttitle = {{srcML} 1.0},
	doi = {10.1109/ICSME.2016.36},
	abstract = {Summary form only given. This technology briefing is intended for those interested in constructing custom software analysis and manipulation tools to support research or commercial applications. srcML (srcML.org) is an infrastructure consisting of an XML representation for C/C++/C\#/Java source code along with efficient parsing technology to convert source code to-and-from the srcML format. The briefing describes srcML, the toolkit, and the application of XPath and XSLT to query and modify source code. Additionally, a short tutorial of how to use srcML and XML tools to construct custom analysis and manipulation tools will be conducted.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Collard, M. L. and Maletic, J. I.},
	month = oct,
	year = {2016},
	keywords = {Software maintenance, program transformation, program diagnostics, source code, software engineering, XML, source code (software), static program analysis, srcML, XML representation, Conferences, software analysis tool, software manipulation tool, srcML format, Tutorials},
	pages = {649--649},
	file = {ICSME16-srcML.pdf:/Users/luigi/work/zotero/storage/VQFII8LF/ICSME16-srcML.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/XNQ3PFUB/7816536.html:text/html}
}

@inproceedings{newman_srctype:_2016,
	title = {{srcType}: {A} {Tool} for {Efficient} {Static} {Type} {Resolution}},
	shorttitle = {{srcType}},
	doi = {10.1109/ICSME.2016.38},
	abstract = {An efficient, static type resolution tool is presented. The tool is implemented on top of srcML, an XML representation of source code and abstract syntax. The approach computes the type of every identifier (i.e., function names and variable names) within the provided body of code. The result is a dictionary that can be used to lookup the type of each name. Type information includes metadata such as constness, class membership, aliasing, line number, file, and namespace. The approach is highly scalable and can generate a dictionary for Linux (13 MLOC) in less than 7 minutes. The tool is open source under a GPL license and available for download at srcML.org.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Newman, C. D. and Maletic, J. I. and Collard, M. L.},
	month = oct,
	year = {2016},
	keywords = {Runtime, computational linguistics, Software maintenance, program diagnostics, XML, static analysis tool, public domain software, software tools, source code (software), Context, metadata, Metadata, srcML, Conferences, abstract syntax, dictionaries, Dictionaries, GPL license, Linux, Linux dictionary, meta data, open source tool, source code representation, srcType, static type resolution, static type resolution tool, type information},
	pages = {604--606},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/783WUYZ7/7816517.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/UN3JHJAR/Newman et al. - 2016 - srcType A Tool for Efficient Static Type Resoluti.pdf:application/pdf}
}

@inproceedings{guzman_sentiment_2014,
	address = {New York, NY, USA},
	series = {{MSR} 2014},
	title = {Sentiment {Analysis} of {Commit} {Comments} in {GitHub}: {An} {Empirical} {Study}},
	isbn = {978-1-4503-2863-0},
	shorttitle = {Sentiment {Analysis} of {Commit} {Comments} in {GitHub}},
	url = {http://doi.acm.org/10.1145/2597073.2597118},
	doi = {10.1145/2597073.2597118},
	abstract = {Emotions have a high impact in productivity, task quality, creativity, group rapport and job satisfaction. In this work we use lexical sentiment analysis to study emotions expressed in commit comments of different open source projects and analyze their relationship with different factors such as used programming language, time and day of the week in which the commit was made, team distribution and project approval. Our results show that projects developed in Java tend to have more negative commit comments, and that projects that have more distributed teams tend to have a higher positive polarity in their emotional content. Additionally, we found that commit comments written on Mondays tend to a more negative emotion. While our results need to be confirmed by a more representative sample they are an initial step into the study of emotions and related factors in open source projects.},
	urldate = {2018-02-19},
	booktitle = {Proceedings of the 11th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Guzman, Emitza and Az{\'o}car, David and Li, Yang},
	year = {2014},
	keywords = {Human Factors in Software Engineering, Sentiment Analysis},
	pages = {352--355},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/UDYDBBTJ/Guzman et al. - 2014 - Sentiment Analysis of Commit Comments in GitHub A.pdf:application/pdf}
}

@inproceedings{lin_rust_2016,
	address = {New York, NY, USA},
	series = {{ISMM} 2016},
	title = {Rust {As} a {Language} for {High} {Performance} {GC} {Implementation}},
	isbn = {978-1-4503-4317-6},
	url = {http://doi.acm.org/10.1145/2926697.2926707},
	doi = {10.1145/2926697.2926707},
	abstract = {High performance garbage collectors build upon performance-critical low-level code, typically exhibit multiple levels of concurrency, and are prone to subtle bugs. Implementing, debugging and maintaining such collectors can therefore be extremely challenging. The choice of implementation language is a crucial consideration when building a collector. Typically, the drive for performance and the need for efficient support of low-level memory operations leads to the use of low-level languages like C or C++, which offer little by way of safety and software engineering benefits. This risks undermining the robustness and flexibility of the collector design. Rust's ownership model, lifetime specification, and reference borrowing deliver safety guarantees through a powerful static checker with little runtime overhead. These features make Rust a compelling candidate for a collector implementation language, but they come with restrictions that threaten expressiveness and efficiency. We describe our experience implementing an Immix garbage collector in Rust and C. We discuss the benefits of Rust, the obstacles encountered, and how we overcame them. We show that our Immix implementation has almost identical performance on micro benchmarks, compared to its implementation in C, and outperforms the popular BDW collector on the gcbench micro benchmark. We find that Rust's safety features do not create significant barriers to implementing a high performance collector. Though memory managers are usually considered low-level, our high performance implementation relies on very little unsafe code, with the vast majority of the implementation benefiting from Rust's safety. We see our experience as a compelling proof-of-concept of Rust as an implementation language for high performance garbage collection.},
	urldate = {2018-02-26},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} {International} {Symposium} on {Memory} {Management}},
	publisher = {ACM},
	author = {Lin, Yi and Blackburn, Stephen M. and Hosking, Antony L. and Norrish, Michael},
	year = {2016},
	keywords = {garbage collection, memory management, Rust},
	pages = {89--98},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/WQTA3E67/Lin et al. - 2016 - Rust As a Language for High Performance GC Impleme.pdf:application/pdf}
}

@inproceedings{zhang_abstraction_2014,
	address = {New York, NY, USA},
	series = {{PLDI} '14},
	title = {On {Abstraction} {Refinement} for {Program} {Analyses} in {Datalog}},
	isbn = {978-1-4503-2784-8},
	url = {http://doi.acm.org/10.1145/2594291.2594327},
	doi = {10.1145/2594291.2594327},
	abstract = {A central task for a program analysis concerns how to efficiently find a program abstraction that keeps only information relevant for proving properties of interest. We present a new approach for finding such abstractions for program analyses written in Datalog. Our approach is based on counterexample-guided abstraction refinement: when a Datalog analysis run fails using an abstraction, it seeks to generalize the cause of the failure to other abstractions, and pick a new abstraction that avoids a similar failure. Our solution uses a boolean satisfiability formulation that is general, complete, and optimal: it is independent of the Datalog solver, it generalizes the failure of an abstraction to as many other abstractions as possible, and it identifies the cheapest refined abstraction to try next. We show the performance of our approach on a pointer analysis and a typestate analysis, on eight real-world Java benchmark programs.},
	urldate = {2018-02-26},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Zhang, Xin and Mangal, Ravi and Grigore, Radu and Naik, Mayur and Yang, Hongseok},
	year = {2014},
	pages = {239--248},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/CZST8AZI/Zhang et al. - 2014 - On Abstraction Refinement for Program Analyses in .pdf:application/pdf}
}

@article{kanvar_heap_2016,
	title = {Heap {Abstractions} for {Static} {Analysis}},
	volume = {49},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/2931098},
	doi = {10.1145/2931098},
	abstract = {Heap data is potentially unbounded and seemingly arbitrary. Hence, unlike stack and static data, heap data cannot be abstracted in terms of a fixed set of program variables. This makes it an interesting topic of study and there is an abundance of literature employing heap abstractions. Although most studies have addressed similar concerns, insights gained in one description of heap abstraction may not directly carry over to some other description. In our search of a unified theme, we view heap abstraction as consisting of two steps: (a) heap modelling, which is the process of representing a heap memory (i.e., an unbounded set of concrete locations) as a heap model (i.e., an unbounded set of abstract locations), and (b) summarization, which is the process of bounding the heap model by merging multiple abstract locations into summary locations. We classify the heap models as storeless, store based, and hybrid. We describe various summarization techniques based on k-limiting, allocation sites, patterns, variables, other generic instrumentation predicates, and higher-order logics. This approach allows us to compare the insights of a large number of seemingly dissimilar heap abstractions and also paves the way for creating new abstractions by mix and match of models and summarization techniques.},
	number = {2},
	urldate = {2018-02-26},
	journal = {ACM Comput. Surv.},
	author = {Kanvar, Vini and Khedker, Uday P.},
	month = jun,
	year = {2016},
	keywords = {heap, static analysis, Abstraction, pointers, shape analysis, store based, storeless, summarization},
	pages = {29:1--29:47},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/R6GQT9CW/Kanvar and Khedker - 2016 - Heap Abstractions for Static Analysis.pdf:application/pdf}
}

@book{pierce_types_2002,
	edition = {1st},
	title = {Types and {Programming} {Languages}},
	isbn = {978-0-262-16209-8},
	abstract = {A type system is a syntactic method for automatically checking the absence of certain erroneous behaviors by classifying program phrases according to the kinds of values they compute. The study of type systems -- and of programming languages from a type-theoretic perspective -- has important applications in software engineering, language design, high-performance compilers, and security.This text provides a comprehensive introduction both to type systems in computer science and to the basic theory of programming languages. The approach is pragmatic and operational; each new concept is motivated by programming examples and the more theoretical sections are driven by the needs of implementations. Each chapter is accompanied by numerous exercises and solutions, as well as a running implementation, available via the Web. Dependencies between chapters are explicitly identified, allowing readers to choose a variety of paths through the material.The core topics include the untyped lambda-calculus, simple type systems, type reconstruction, universal and existential polymorphism, subtyping, bounded quantification, recursive types, kinds, and type operators. Extended case studies develop a variety of approaches to modeling the features of object-oriented languages.},
	publisher = {The MIT Press},
	author = {Pierce, Benjamin C.},
	year = {2002},
	keywords = {tapl},
	file = {Pierce - 2002 - Types and Programming Languages.pdf:/Users/luigi/work/zotero/storage/7PFLWK8V/Pierce - 2002 - Types and Programming Languages.pdf:application/pdf}
}

@article{strachey_fundamental_2000,
	title = {Fundamental {Concepts} in {Programming} {Languages}},
	volume = {13},
	issn = {1388-3690, 1573-0557},
	url = {https://link.springer.com/article/10.1023/A:1010000313106},
	doi = {10.1023/A:1010000313106},
	abstract = {This paper forms the substance of a course of lectures given at the International Summer School in Computer Programming at Copenhagen in August, 1967. The lectures were originally given from notes and the paper was written after the course was finished. In spite of this, and only partly because of the shortage of time, the paper still retains many of the shortcomings of a lecture course. The chief of these are an uncertainty of aim{\textemdash}it is never quite clear what sort of audience there will be for such lectures{\textemdash}and an associated switching from formal to informal modes of presentation which may well be less acceptable in print than it is natural in the lecture room. For these (and other) faults, I apologise to the reader.There are numerous references throughout the course to CPL [1{\textendash}3]. This is a programming language which has been under development since 1962 at Cambridge and London and Oxford. It has served as a vehicle for research into both programming languages and the design of compilers. Partial implementations exist at Cambridge and London. The language is still evolving so that there is no definitive manual available yet. We hope to reach another resting point in its evolution quite soon and to produce a compiler and reference manuals for this version. The compiler will probably be written in such a way that it is relatively easyto transfer it to another machine, and in the first instance we hope to establish it on three or four machines more or less at the same time.The lack of a precise formulation for CPL should not cause much difficulty in this course, as we are primarily concerned with the ideas and concepts involved rather than with their precise representation in a programming language.},
	language = {en},
	number = {1-2},
	urldate = {2018-03-02},
	journal = {Higher-Order and Symbolic Computation},
	author = {Strachey, Christopher},
	month = apr,
	year = {2000},
	pages = {11--49},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/8TGV69IU/Strachey - 2000 - Fundamental Concepts in Programming Languages.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/32ITHINJ/A1010000313106.html:text/html}
}

@inproceedings{altidor_taming_2011,
	address = {New York, NY, USA},
	series = {{PLDI} '11},
	title = {Taming the {Wildcards}: {Combining} {Definition}- and {Use}-site {Variance}},
	isbn = {978-1-4503-0663-8},
	shorttitle = {Taming the {Wildcards}},
	url = {http://doi.acm.org/10.1145/1993498.1993569},
	doi = {10.1145/1993498.1993569},
	abstract = {Variance allows the safe integration of parametric and subtype polymorphism. Two flavors of variance, definition-site versus use-site variance, have been studied and have had their merits hotly debated. Definition-site variance (as in Scala and C\#) offers simple type-instantiation rules, but causes fractured definitions of naturally invariant classes; Use-site variance (as in Java) offers simplicity in class definitions, yet complex type-instantiation rules that elude most programmers. We present a unifying framework for reasoning about variance. Our framework is quite simple and entirely denotational, that is, it evokes directly the definition of variance with a small core calculus that does not depend on specific type systems. This general framework can have multiple applications to combine the best of both worlds: for instance, it can be used to add use-site variance annotations to the Scala type system. We show one such application in detail: we extend the Java type system with a mechanism that modularly infers the definition-site variance of type parameters, while allowing use-site variance annotations on any type-instantiation. Applying our technique to six Java generic libraries (including the Java core library) shows that 20-58 (depending on the library) of generic definitions are inferred to have single-variance; 8-63\% of method signatures can be relaxed through this inference, and up to 91\% of existing wildcard annotations are unnecessary and can be elided.},
	urldate = {2018-03-07},
	booktitle = {Proceedings of the 32Nd {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Altidor, John and Huang, Shan Shan and Smaragdakis, Yannis},
	year = {2011},
	keywords = {definition-site variance, language extensions, use-site variance, variance, wildcards},
	pages = {602--613},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/N3CWKXQ9/Altidor et al. - 2011 - Taming the Wildcards Combining Definition- and Us.pdf:application/pdf}
}

@inproceedings{dolan_polymorphism_2017,
	address = {New York, NY, USA},
	series = {{POPL} 2017},
	title = {Polymorphism, {Subtyping}, and {Type} {Inference} in {MLsub}},
	isbn = {978-1-4503-4660-3},
	url = {http://doi.acm.org/10.1145/3009837.3009882},
	doi = {10.1145/3009837.3009882},
	abstract = {We present a type system combining subtyping and ML-style parametric polymorphism. Unlike previous work, our system supports type inference and has compact principal types. We demonstrate this system in the minimal language MLsub, which types a strict superset of core ML programs.   This is made possible by keeping a strict separation between the types used to describe inputs and those used to describe outputs, and extending the classical unification algorithm to handle subtyping constraints between these input and output types. Principal types are kept compact by type simplification, which exploits deep connections between subtyping and the algebra of regular languages. An implementation is available online.},
	urldate = {2018-03-22},
	booktitle = {Proceedings of the 44th {ACM} {SIGPLAN} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Dolan, Stephen and Mycroft, Alan},
	year = {2017},
	keywords = {Algebra, Polymorphism, Subtyping, Type Inference},
	pages = {60--72},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/2NERMPEG/Dolan and Mycroft - 2017 - Polymorphism, Subtyping, and Type Inference in MLs.pdf:application/pdf}
}

@inproceedings{vaziri_declarative_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Declarative {Object} {Identity} {Using} {Relation} {Types}},
	isbn = {978-3-540-73588-5 978-3-540-73589-2},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-73589-2_4},
	doi = {10.1007/978-3-540-73589-2_4},
	abstract = {Object-oriented languages define the identity of an object to be an address-based object identifier. The programmer may customize the notion of object identity by overriding the equals() and hashCode() methods following a specified contract. This customization often introduces latent errors, since the contract is unenforced and at times impossible to satisfy, and its implementation requires tedious and error-prone boilerplate code. Relation types are a programming model in which object identity is defined declaratively, obviating the need for equals() and hashCode() methods. This entails a stricter contract: identity never changes during an execution. We formalize the model as an adaptation of Featherweight Java, and implement it by extending Java with relation types. Experiments on a set of Java programs show that the majority of classes that override equals() can be refactored into relation types, and that most of the remainder are buggy or fragile.},
	language = {en},
	urldate = {2018-03-25},
	booktitle = {{ECOOP} 2007 {\textendash} {Object}-{Oriented} {Programming}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Vaziri, Mandana and Tip, Frank and Fink, Stephen and Dolby, Julian},
	month = jul,
	year = {2007},
	pages = {54--78},
	file = {Full Text PDF:/Users/luigi/work/zotero/storage/3SMW9WTT/Vaziri et al. - 2007 - Declarative Object Identity Using Relation Types.pdf:application/pdf;Snapshot:/Users/luigi/work/zotero/storage/HVIIFBLL/978-3-540-73589-2_4.html:text/html}
}

@inproceedings{kechagia_undocumented_2014,
	address = {New York, NY, USA},
	series = {{MSR} 2014},
	title = {Undocumented and {Unchecked}: {Exceptions} {That} {Spell} {Trouble}},
	isbn = {978-1-4503-2863-0},
	shorttitle = {Undocumented and {Unchecked}},
	url = {http://doi.acm.org/10.1145/2597073.2597089},
	doi = {10.1145/2597073.2597089},
	abstract = {Modern programs rely on large application programming interfaces (APIs). The Android framework comprises 231 core APIs, and is used by countless developers. We examine a sample of 4,900 distinct crash stack traces from 1,800 different Android applications, looking for Android API methods with undocumented exceptions that are part of application crashes. For the purposes of this study, we take as a reference the version 15 of the Android API, which matches our stack traces. Our results show that a significant number of crashes (19\%) might have been avoided if these methods had the corresponding exceptions documented as part of their interface.},
	urldate = {2018-04-04},
	booktitle = {Proceedings of the 11th {Working} {Conference} on {Mining} {Software} {Repositories}},
	publisher = {ACM},
	author = {Kechagia, Maria and Spinellis, Diomidis},
	year = {2014},
	keywords = {stack traces, exceptions, APIs, mobile applications},
	pages = {312--315},
	file = {ACM Full Text PDF:/Users/luigi/work/zotero/storage/2QCM2NHZ/Kechagia and Spinellis - 2014 - Undocumented and Unchecked Exceptions That Spell .pdf:application/pdf}
}

@inproceedings{saied_visualization_2015,
	title = {Visualization based {API} usage patterns refining},
	doi = {10.1109/VISSOFT.2015.7332428},
	abstract = {Learning to use existing or new software libraries is a difficult task for software developers, which would impede their productivity. Most of existing work provided different techniques to mine API usage patterns from client programs, in order to help developers to understand and use existing libraries. However, considering only client programs to identify API usage patterns, is a strong constraint as collecting several similar client programs for an API is not a trivial task. And even if these clients are available, all the usage scenarios of the API of interest may not be covered by those clients. In this paper, we propose a visualization based approach for the refinement of Client-based Usage Patterns. We first visualize the patterns structure. Then we enrich the patterns with API methods that are semantically related to them, and thus may contribute together to the implementation of a particular functionality for potential client programs.},
	booktitle = {2015 {IEEE} 3rd {Working} {Conference} on {Software} {Visualization} ({VISSOFT})},
	author = {Saied, M. A. and Benomar, O. and Sahraoui, H.},
	month = sep,
	year = {2015},
	keywords = {Libraries, application program interfaces, Software, Semantics, application programming interfaces, client programs, Documentation, API usage patterns refinement, data visualisation, Layout, Matrix decomposition, patterns structure visualization, Visualization},
	pages = {155--159},
	file = {IEEE Xplore Abstract Record:/Users/luigi/work/zotero/storage/647D5HGF/7332428.html:text/html;IEEE Xplore Full Text PDF:/Users/luigi/work/zotero/storage/QV6CWF22/Saied et al. - 2015 - Visualization based API usage patterns refining.pdf:application/pdf}
}