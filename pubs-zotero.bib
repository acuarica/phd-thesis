
@inproceedings{mastrangeloJNIFJavaNative2014,
  address = {New York, NY, USA},
  series = {{{PPPJ}} '14},
  title = {{{JNIF}}: {{Java Native Instrumentation Framework}}},
  isbn = {978-1-4503-2926-2},
  shorttitle = {{{JNIF}}},
  doi = {10.1145/2647508.2647516},
  abstract = {The development of instrumentation-based dynamic analyses for Java bytecode is enabled by various bytecode rewriting frameworks. Those frameworks are all implemented in Java. This complicates their use for developing full-coverage analyses that not only observe application code, but that also observe the execution of the complete Java class library. Moreover, it makes it hard to avoid perturbation due to the Java code of the instrumentation tool interfering with the Java code of the observed program. So far, workarounds for these problems required either statically instrumenting the runtime library or running a separate JVM as an instrumentation server. This paper solves this problem. It introduces JNIF, the first complete bytecode rewriting framework implemented in native code. JNIF can be used in a JVMTI agent to create isolated, full-coverage, in-process dynamic instrumentation tools. JNIF is written in C++ and has an object-oriented design familiar to users of Java-based rewriting libraries. JNIF is able to decode, analyze, edit, and encode Java class files. This includes the generation of stack maps required by split-time verifiers of modern JVMs. Our performance evaluation shows that JNIF is often faster than the most performant competitive approach based on ASM.},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform}}: {{Virtual Machines}}, {{Languages}}, and {{Tools}}},
  publisher = {{ACM}},
  author = {Mastrangelo, Luis and Hauswirth, Matthias},
  year = {2014},
  keywords = {bytecode instrumentation,Java,program analysis},
  pages = {194--199},
  file = {/Users/luigi/work/zotero/storage/8IXP8DSD/Mastrangelo and Hauswirth - 2014 - JNIF Java Native Instrumentation Framework.pdf}
}

@inproceedings{coelhoUnveilingExceptionHandling2015,
  address = {Piscataway, NJ, USA},
  series = {{{MSR}} '15},
  title = {Unveiling {{Exception Handling Bug Hazards}} in {{Android Based}} on {{GitHub}} and {{Google Code Issues}}},
  isbn = {978-0-7695-5594-2},
  abstract = {This paper reports on a study mining the exception stack traces included in 159,048 issues reported on Android projects hosted in GitHub (482 projects) and Google Code (157 projects). The goal of this study is to investigate whether stack trace information can reveal bug hazards related to exception handling code that may lead to a decrease in application robustness. Overall 6,005 exception stack traces were extracted, and subjected to source code and bytecode analysis. The outcomes of this study include the identification of the following bug hazards: (i) unexpected cross-type exception wrappings (for instance, trying to handle an instance of OutOfMemoryError "hidden" in a checked exception) which can make the exception-related code more complex and negatively impact the application robustness; (ii) undocumented runtime exceptions thrown by both the Android platform and third party libraries; and (iii) undocumented checked exceptions thrown by the Android Platform. Such undocumented exceptions make it difficult, and most of the times infeasible for the client code to protect against "unforeseen" situations that may happen while calling third-party code. This study provides further insights on such bug hazards and the robustness threats they impose to Android apps as well as to other systems based on the Java exception model.},
  booktitle = {Proceedings of the 12th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{IEEE Press}},
  author = {Coelho, Roberta and Almeida, Lucas and Gousios, Georgios and {van Deursen}, Arie},
  year = {2015},
  pages = {134--145},
  file = {/Users/luigi/work/zotero/storage/TGLYE77G/Coelho et al. - 2015 - Unveiling Exception Handling Bug Hazards in Androi.pdf}
}

@article{knuthEmpiricalStudyFORTRAN1971,
  title = {An Empirical Study of {{FORTRAN}} Programs},
  volume = {1},
  issn = {1097-024X},
  doi = {10.1002/spe.4380010203},
  abstract = {A sample of programs, written in FORTRAN by a wide variety of people for a wide variety of applications, was chosen `at random' in an attempt to discover quantitatively `what programmers really do'. Statistical results of this survey are presented here, together with some of their apparent implications for future work in compiler design. The principal conclusion which may be drawn is the importance of a program `profile', namely a table of frequency counts which record how often each statement is performed in a typical run; there are strong indications that profile-keeping should become a standard practice in all computer systems, for casual users as well as system programmers. This paper is the report of a three month study undertaken by the author and about a dozen students and representatives of the software industry during the summer of 1970. It is hoped that a reader who studies this report will obtain a fairly clear conception of how FORTRAN is being used, and what compilers can do about it.},
  language = {en},
  number = {2},
  journal = {Software: Practice and Experience},
  author = {Knuth, Donald E.},
  month = apr,
  year = {1971},
  keywords = {Compiler,Efficiency,FORTRAN,Optimization},
  pages = {105-133},
  file = {/Users/luigi/work/zotero/storage/VDMLM4DI/empirical-fortran.pdf;/Users/luigi/work/zotero/storage/WPWCCP4B/abstract.html}
}

@article{fletcherAppropriateLanguageSystem1972,
  title = {On the {{Appropriate Language}} for {{System Programming}}},
  volume = {7},
  issn = {0362-1340},
  doi = {10.1145/953360.953361},
  number = {7},
  journal = {SIGPLAN Not.},
  author = {Fletcher, J. G. and Badger, C. S. and Boer, G. L. and Marshall, G. G.},
  month = jul,
  year = {1972},
  keywords = {implementation,language,system},
  pages = {28--30},
  file = {/Users/luigi/work/zotero/storage/Q34Z72E7/Fletcher et al. - 1972 - On the Appropriate Language for System Programming.pdf}
}

@inproceedings{kildallUnifiedApproachGlobal1973,
  address = {New York, NY, USA},
  series = {{{POPL}} '73},
  title = {A {{Unified Approach}} to {{Global Program Optimization}}},
  doi = {10.1145/512927.512945},
  abstract = {A technique is presented for global analysis of program structure in order to perform compile time optimization of object code generated for expressions. The global expression optimization presented includes constant propagation, common subexpression elimination, elimination of redundant register load operations, and live expression analysis. A general purpose program flow analysis algorithm is developed which depends upon the existence of an "optimizing function." The algorithm is defined formally using a directed graph model of program flow structure, and is shown to be correct. Several optimizing functions are defined which, when used in conjunction with the flow analysis algorithm, provide the various forms of code optimization. The flow analysis algorithm is sufficiently general that additional functions can easily be defined for other forms of global code optimization.},
  booktitle = {Proceedings of the 1st {{Annual ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Kildall, Gary A.},
  year = {1973},
  pages = {194--206},
  file = {/Users/luigi/work/zotero/storage/ELKWJNV8/Kildall - 1973 - A Unified Approach to Global Program Optimization.pdf}
}

@inproceedings{fraileyShouldHighLevel1975,
  address = {New York, NY, USA},
  series = {{{ACM}} '75},
  title = {Should {{High Level Languages Be Used}} to {{Write Systems Software}}?},
  doi = {10.1145/800181.810317},
  abstract = {Most of us write our programs in whatever language is most convenient for the problem at hand. Often this means, not so much that the language is well suited to the problem, but simply that it's the best suited of the choices available. Particularly with microprocessors and many minicomputers, we don't have a very wide choice of available software. Perhaps we have only an assembler or only a Basic interpreter. Those who have a choice, or who are responsible for developing compilers and other basic systems software, must determine how much money to spend and where to spend it, becoming embroiled in such questions as what high level languages, if any, should be used or how important it is to develop a good assembler versus a good high level language compiler for our systems work.},
  booktitle = {Proceedings of the 1975 {{Annual Conference}}},
  publisher = {{ACM}},
  author = {Frailey, Dennis J.},
  year = {1975},
  pages = {205--},
  file = {/Users/luigi/work/zotero/storage/K22D4IMH/Frailey - 1975 - Should High Level Languages Be Used to Write Syste.pdf}
}

@inproceedings{fletcherNoHighLevel1975,
  address = {New York, NY, USA},
  series = {{{ACM}} '75},
  title = {No! {{High Level Languages Should Not Be Used}} to {{Write Systems Software}}},
  doi = {10.1145/800181.810319},
  abstract = {The views expressed here derive from the experience of the author and his colleagues in designing and implementing the Octopus computer network at the Lawrence Livermore Laboratory. This network serves five major time-shared computers (CDC 7600's and STAR-100's), connecting them to over 800 interactive terminals, about 200 television monitor displays, printers that operate at up to 18,000 lines/minute, and more than a trillion bits of storage. The software for the network has been written entirely in assembly language (for PDP-8's, 10's, and 11's, MODCOMP II's, and TI 980's) and from scratch, basing none of it on manufacturers' or other commercial software. The same persons who create the design also do the programming and debugging. In most cases one or two persons program a computer; four persons were used on the largest system (the PDP-10's). Our experience does not accord with much of what we read in the computing literature, leading us to conclude that it is written by persons unaware the real problems of systems work. We have had little or no trouble with deadlocks, security loopholes, and other logical flaws that are belabored at length in the literature. Most of our effort has gone into devising ways for the system to survive in the presence of intermittent and random failures of hardware components and for it to maintain high data transfer rates among multiply-interconnected devices and computers of varying speeds, matters that are seldom discussed in the literature at all. It is certainly not the case that the difficulties encountered with operating systems are the same as those encountered with other large programs, such as compilers.},
  booktitle = {Proceedings of the 1975 {{Annual Conference}}},
  publisher = {{ACM}},
  author = {Fletcher, John G.},
  year = {1975},
  pages = {209--211},
  file = {/Users/luigi/work/zotero/storage/WRJQZAFW/Fletcher - 1975 - No! High Level Languages Should Not Be Used to Wri.pdf}
}

@inproceedings{horningYesHighLevel1975,
  address = {New York, NY, USA},
  series = {{{ACM}} '75},
  title = {Yes! {{High Level Languages Should Be Used}} to {{Write Systems Software}}},
  doi = {10.1145/800181.810318},
  abstract = {It has frequently been remarked that it is easier recognize ``high level'' languages than to define the concept. For the purposes of this debate, however, I think that we agree that a language is high level to the extent that it discourages (forbids) the specification of machine details (register numbers, absolute addresses, op codes, word-packing, etc.) as a routine part of program composition and low level to the extent that it encourages (requires) such specification. (Note that, by this definition, assembly languages occupy a position intermediate between machine languages and compiled languages.) Thus, I take the point at issue to be: ``To what extent is it desirable for the system programmer to specify machine details?''},
  booktitle = {Proceedings of the 1975 {{Annual Conference}}},
  publisher = {{ACM}},
  author = {Horning, James J.},
  year = {1975},
  pages = {206--208},
  file = {/Users/luigi/work/zotero/storage/AYFN2GJM/Horning - 1975 - Yes! High Level Languages Should Be Used to Write .pdf}
}

@article{hammondBASICEvaluationProcessing1977,
  title = {{{BASIC}} - an Evaluation of Processing Methods and a Study of Some Programs},
  volume = {7},
  issn = {1097-024X},
  doi = {10.1002/spe.4380070605},
  abstract = {The relative merits of compiling and interpreting BASIC are examined, and these methods are compared with the technique called throw-away compiling. The comparison reveals that a throw-away compiler has much to recommend it, and some reasons for its superior performance are explained. The BASIC programs used for the performance tests are analysed, both statically and dynamically, and certain features are picked out for comment.},
  language = {en},
  number = {6},
  journal = {Software: Practice and Experience},
  author = {Hammond, John},
  month = nov,
  year = {1977},
  keywords = {Compiler,Efficiency,BASIC,Interpreter,Throw-away compiler},
  pages = {697-711},
  file = {/Users/luigi/work/zotero/storage/MH8LMVNV/abstract.html}
}

@article{lamportTimeClocksOrdering1978,
  title = {Time, {{Clocks}}, and the {{Ordering}} of {{Events}} in a {{Distributed System}}},
  volume = {21},
  issn = {0001-0782},
  doi = {10.1145/359545.359563},
  abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
  number = {7},
  journal = {Commun. ACM},
  author = {Lamport, Leslie},
  month = jul,
  year = {1978},
  keywords = {clock synchronization,computer networks,distributed systems,multiprocess systems},
  pages = {558--565},
  file = {/Users/luigi/work/zotero/storage/NRN7SRHY/Lamport - 1978 - Time, Clocks, and the Ordering of Events in a Dist.pdf}
}

@article{herlihyWaitfreeSynchronization1991,
  title = {Wait-Free {{Synchronization}}},
  volume = {13},
  issn = {0164-0925},
  doi = {10.1145/114005.102808},
  abstract = {A wait-free implementation of a concurrent data object is one that guarantees that any process can complete any operation in a finite number of steps, regardless of the execution speeds of the other processes. The problem of constructing a wait-free implementation of one data object from another lies at the heart of much recent work in concurrent algorithms, concurrent data structures, and multiprocessor architectures. First, we introduce a simple and general technique, based on reduction to a concensus protocol, for proving statements of the form, ``there is no wait-free implementation of X by Y.'' We derive a hierarchy of objects such that no object at one level has a wait-free implementation in terms of objects at lower levels. In particular, we show that    atomic read/write registers, which have been the focus of much recent attention, are at the bottom of the hierarchy: thay cannot be used to construct wait-free implementations of many simple and familiar data types. Moreover, classical synchronization primitives such astest\&set and fetch\&add, while more powerful than read and write, are also computationally weak, as are the standard message-passing primitives. Second, nevertheless, we show that there do exist simple universal objects from which one can construct a wait-free implementation of any sequential object.},
  number = {1},
  journal = {ACM Trans. Program. Lang. Syst.},
  author = {Herlihy, Maurice},
  month = jan,
  year = {1991},
  keywords = {linearization,wait-free synchronization},
  pages = {124--149},
  file = {/Users/luigi/work/zotero/storage/7ZNQEGFS/Herlihy - 1991 - Wait-free Synchronization.pdf}
}

@article{liskovBehavioralNotionSubtyping1994,
  title = {A {{Behavioral Notion}} of {{Subtyping}}},
  volume = {16},
  issn = {0164-0925},
  doi = {10.1145/197320.197383},
  abstract = {The use of hierarchy is an important component of object-oriented design. Hierarchy allows the use of type families, in which higher level supertypes capture the behavior that all of their subtypes have in common. For this methodology to be effective, it is necessary to have a clear understanding of how subtypes and supertypes are related. This paper takes the position that the relationship should ensure that any property proved about supertype objects also holds for its subtype objects. It presents two ways of defining the subtype relation, each of which meets this criterion, and each of which is easy for programmers to use. The subtype relation is based on the specifications of the sub- and supertypes; the paper presents a way of specifying types that makes it convenient to define  the subtype relation. The paper also discusses the ramifications of this notion of subtyping on the design of type families.},
  number = {6},
  journal = {ACM Trans. Program. Lang. Syst.},
  author = {Liskov, Barbara H. and Wing, Jeannette M.},
  month = nov,
  year = {1994},
  keywords = {formal specifications,Larch,subtyping},
  pages = {1811--1841},
  file = {/Users/luigi/work/zotero/storage/CTJTTACC/Liskov and Wing - 1994 - A Behavioral Notion of Subtyping.pdf;/Users/luigi/work/zotero/storage/WFCG2IFK/Liskov and Wing - 1994 - A behavioral notion of subtyping.pdf}
}

@article{bershadSPINExtensibleMicrokernel1995,
  title = {{{SPIN}}\textemdash{}an {{Extensible Microkernel}} for {{Application}}-Specific {{Operating System Services}}},
  volume = {29},
  issn = {0163-5980},
  doi = {10.1145/202453.202472},
  abstract = {Application domains such as multimedia, databases, and parallel computing, require operating system services with high performance and high functionality. Existing operating systems provide fixed interfaces and implementations to system services and resources. This makes them inappropriate for applications whose resource demands and usage patterns are poorly matched by the services provided. The SPIN operating system enables system services to be defined in an application-specific fashion through an extensible microkernel. It offers applications fine-grained control over a machine's logical and physical resources through run-time adaptation of the system to application requirements.},
  number = {1},
  journal = {SIGOPS Oper. Syst. Rev.},
  author = {Bershad, Brian N. and Chambers, Craig and Eggers, Susan and Maeda, Chris and McNamee, Dylan and Pardyak, Przemys\textbackslash{}law and Savage, Stefan and Sirer, Emin G\"un},
  month = jan,
  year = {1995},
  pages = {74--77},
  file = {/Users/luigi/work/zotero/storage/TVGKU4UV/Bershad et al. - 1995 - SPIN—an Extensible Microkernel for Application-spe.pdf}
}

@inproceedings{neculaProofcarryingCode1997,
  address = {New York, NY, USA},
  series = {{{POPL}} '97},
  title = {Proof-Carrying {{Code}}},
  isbn = {978-0-89791-853-4},
  doi = {10.1145/263699.263712},
  abstract = {This paper describes proof-carrying code (PCC), a mechanism by which a host system can determine with certainty that it is safe to execute a program supplied (possibly in binary form) by an untrusted source. For this to be possible, the untrusted code producer must supply with the code a safety proof that attests to the code's adherence to a previously defined safety policy. The host can then easily and quickly validate the proof without using cryptography and without consulting any external agents.In order to gain preliminary experience with PCC, we have performed several case studies. We show in this paper how proof-carrying code might be used to develop safe assembly-language extensions of ML programs. In the context of this case study, we present and prove the adequacy of concrete representations for the safety policy, the safety proofs, and the proof validation. Finally, we briefly discuss how we use proof-carrying code to develop network packet filters that are faster than similar filters developed using other techniques and are formally guaranteed to be safe with respect to a given operating system safety policy.},
  booktitle = {Proceedings of the 24th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Necula, George C.},
  year = {1997},
  pages = {106--119},
  file = {/Users/luigi/work/zotero/storage/JFCFZE54/Necula - 1997 - Proof-carrying Code.pdf}
}

@article{ritchieSystemsProgrammingJava1997,
  title = {Systems Programming in {{Java}}},
  volume = {17},
  issn = {0272-1732},
  doi = {10.1109/40.591652},
  abstract = {The Java programming language has been widely accepted as a general purpose language for developing portable applications, toolkits, and applets. With so much activity in industry and academia in these user-level areas, is it surprising that Java is also an equally capable systems programming language? This article describes our experiences at JavaSoft with using Java as a systems-level programming language during the development of JavaOS. The author discusses the motivations for using Java and shows code examples to demonstrate various system-level primitives, including an Ethernet device driver},
  number = {3},
  journal = {IEEE Micro},
  author = {Ritchie, S.},
  month = may,
  year = {1997},
  keywords = {Java,Debugging,Ethernet networks,general purpose language,high level languages,Java programming language,JavaOS,JavaSoft,Mice,Object oriented modeling,Operating systems,operating systems (computers),portable applications,Programming profession,Protection,Protocols,Runtime,system-level primitives,systems analysis,systems programming},
  pages = {30-35},
  file = {/Users/luigi/work/zotero/storage/NI5IITLU/Ritchie - 1997 - Systems programming in Java.pdf;/Users/luigi/work/zotero/storage/BZU7XIFI/591652.html}
}

@inproceedings{vallee-raiSootJavaBytecode1999,
  address = {Mississauga, Ontario, Canada},
  series = {{{CASCON}} '99},
  title = {Soot - a {{Java Bytecode Optimization Framework}}},
  abstract = {This paper presents Soot, a framework for optimizing Java bytecode. The framework is implemented in Java and supports three intermediate representations for representing Java bytecode: Baf, a streamlined representation of bytecode which is simple to manipulate; Jimple, a typed 3-address intermediate representation suitable for optimization; and Grimp, an aggregated version of Jimple suitable for decompilation. We describe the motivation for each representation, and the salient points in translating from one representation to another.In order to demonstrate the usefulness of the framework, we have implemented intraprocedural and whole program optimizations. To show that whole program bytecode optimization can give performance improvements, we provide experimental results for 12 large benchmarks, including 8 SPECjvm98 benchmarks running on JDK 1.2 for GNU/Linuxtm. These results show up to 8\% improvement when the optimized bytecode is run using the interpreter and up to 21\% when run using the JIT compiler.},
  booktitle = {Proceedings of the 1999 {{Conference}} of the {{Centre}} for {{Advanced Studies}} on {{Collaborative Research}}},
  publisher = {{IBM Press}},
  author = {{Vall\'ee-Rai}, Raja and Co, Phong and Gagnon, Etienne and Hendren, Laurie and Lam, Patrick and Sundaresan, Vijay},
  year = {1999},
  pages = {13--},
  file = {/Users/luigi/work/zotero/storage/DXB3947K/Vallée-Rai et al. - 1999 - Soot - a Java Bytecode Optimization Framework.pdf}
}

@inproceedings{siebertEliminatingExternalFragmentation2000,
  address = {New York, NY, USA},
  series = {{{CASES}} '00},
  title = {Eliminating {{External Fragmentation}} in a {{Non}}-Moving {{Garbage Collector}} for {{Java}}},
  isbn = {978-1-58113-338-7},
  doi = {10.1145/354880.354883},
  booktitle = {Proceedings of the 2000 {{International Conference}} on {{Compilers}}, {{Architecture}}, and {{Synthesis}} for {{Embedded Systems}}},
  publisher = {{ACM}},
  author = {Siebert, Fridtjof},
  year = {2000},
  pages = {9--17},
  file = {/Users/luigi/work/zotero/storage/FBLQV5QS/Siebert - 2000 - Eliminating External Fragmentation in a Non-moving.pdf}
}

@inproceedings{leaJavaForkJoin2000,
  address = {New York, NY, USA},
  series = {{{JAVA}} '00},
  title = {A {{Java Fork}}/{{Join Framework}}},
  isbn = {978-1-58113-288-5},
  doi = {10.1145/337449.337465},
  booktitle = {Proceedings of the {{ACM}} 2000 {{Conference}} on {{Java Grande}}},
  publisher = {{ACM}},
  author = {Lea, Doug},
  year = {2000},
  pages = {36--43},
  file = {/Users/luigi/work/zotero/storage/GFMTENEJ/Lea - 2000 - A Java ForkJoin Framework.pdf}
}

@inproceedings{moonenGeneratingRobustParsers2001,
  title = {Generating Robust Parsers Using Island Grammars},
  doi = {10.1109/WCRE.2001.957806},
  abstract = {Source model extraction, the automated extraction of information from system artifacts, is a common phase in reverse engineering tools. One of the major challenges of this phase is creating extractors that can deal with irregularities in the artifacts that are typical for the reverse engineering domain (for example, syntactic errors, incomplete source code, language dialects and embedded languages). The paper proposes a solution in the form of island grammars, a special kind of grammar that combines the detailed specification possibilities of grammars with the liberal behavior of lexical approaches. We show how island grammars can be used to generate robust parsers that combine the accuracy of syntactical analysis with the speed, flexibility and tolerance usually only found in lexical analysis. We conclude with a discussion of the development of MANGROVE, a generator for source model extractors based on island grammars and describe its application to a number of case studies},
  booktitle = {Proceedings {{Eighth Working Conference}} on {{Reverse Engineering}}},
  author = {Moonen, L.},
  year = {2001},
  keywords = {program analysis,Application software,automated information extraction,case studies,computational linguistics,Computer languages,Data mining,detailed specification,embedded languages,fuzzy parsing,grammars,incomplete source code,island grammars,language dialects,lexical approaches,Libraries,Maintenance engineering,MANGROVE,Mars,parser generation,partial parsing,program compilers,reverse engineering,Reverse engineering,reverse engineering domain,reverse engineering tools,robust parser generation,robust parsers,Robustness,Software maintenance,source model extraction,source model extractors,syntactic errors,syntactical analysis,system artifacts,Transaction databases},
  pages = {13-22},
  file = {/Users/luigi/work/zotero/storage/4H8D6X2H/Moonen - 2001 - Generating robust parsers using island grammars.pdf;/Users/luigi/work/zotero/storage/B8PQQ46Q/957806.html}
}

@inproceedings{leroyJavaBytecodeVerification2001,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Java {{Bytecode Verification}}: {{An Overview}}},
  isbn = {978-3-540-42345-4 978-3-540-44585-2},
  shorttitle = {Java {{Bytecode Verification}}},
  doi = {10.1007/3-540-44585-4_26},
  abstract = {Bytecode verification is a crucial security component for Java applets, on the Web and on embedded devices such as smart cards. This paper describes the main bytecode verification algorithms and surveys the variety of formal methods that have been applied to bytecode verification in order to establish its correctness.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Leroy, Xavier},
  month = jul,
  year = {2001},
  pages = {265-285},
  file = {/Users/luigi/work/zotero/storage/NDKZZIJ2/Leroy - 2001 - Java Bytecode Verification An Overview.pdf;/Users/luigi/work/zotero/storage/Y296DJRG/10.html}
}

@inproceedings{brunetonASMCodeManipulation2002,
  title = {{{ASM}}: {{A}} Code Manipulation Tool to Implement Adaptable Systems},
  shorttitle = {{{ASM}}},
  abstract = {ABSTRACT. ASM is a Java class manipulation tool designed to dynamically generate and manipulate Java classes, which are useful techniques to implement adaptable systems. ASM is based on a new approach, compared to equivalent existing tools, which consists in using the \&quot;visitor\&quot; design pattern without explicitly representing the visited tree with objects. This new approach gives much better performances than those of existing tools, for most of practical needs. R\'ESUM\'E. ASM est un outil de manipulation de classes Java con{\c c}u pour la g\'en\'eration et la manipulation dynamiques de code, qui sont des techniques tr\`es utiles pour la r\'ealisation de syst\`emes adaptables. ASM est bas\'e sur une approche originale, par rapport aux outils existants \'equivalents, qui consiste \`a utiliser le patron de conception \guillemotleft{} visiteur \guillemotright{} sans repr\'esenter explicitement l'arborescence visit\'ee sous forme d'objets. Cette nouvelle approche permet d'obtenir des performances bien sup\'erieures \`a celles des outils existants, pour la plupart des besoins courants.},
  booktitle = {In {{Adaptable}} and Extensible Component Systems},
  author = {Bruneton, Eric and Lenglet, Romain and Coupaye, Thierry},
  year = {2002},
  file = {/Users/luigi/work/zotero/storage/X2Q7NC9L/Bruneton et al. - 2002 - ASM A code manipulation tool to implement adaptab.pdf;/Users/luigi/work/zotero/storage/KL9Z4HDH/summary.html}
}

@book{jimCycloneSafeDialect,
  title = {Cyclone: {{A}} Safe Dialect of {{C}}},
  shorttitle = {Cyclone},
  abstract = {Cyclone is a safe dialect of C. It has been designed from the ground up to prevent the buffer overflows, format string attacks, and memory management errors that are common in C programs, while retaining C's syntax and semantics. This paper examines safety violations enabled by C's design, and shows how Cyclone avoids them, without giving up C's hallmark control over low-level details such as data representation and memory management.},
  author = {Jim, Trevor and Morrisett, Greg and Grossman, Dan and Hicks, Michael and Cheney, James and Wang, Yanling},
  file = {/Users/luigi/work/zotero/storage/7E5RTS32/Jim et al. - Cyclone A safe dialect of C.pdf;/Users/luigi/work/zotero/storage/BIBZRDU6/summary.html}
}

@article{barnettVerificationObjectOrientedPrograms2003,
  title = {Verification of {{Object}}-{{Oriented Programs With Invariants}}},
  volume = {3, No. 6},
  abstract = {An object invariant defines what it means for an object's data to be in a consistent state. Object invariants are central to the design and correctness of object-oriented programs. This paper defines a programming methodology for using object invariants. The methodology, which enriches a program's state space to express when each object invariant holds, deals \ldots{}},
  journal = {Journal of Object Technology, Special issue: ECOOP 2003 workshop on FTfJP},
  author = {Barnett, Mike and DeLine, Rob and Fahndrich, Manuel and Leino, Rustan and Schulte, Wolfram},
  month = jul,
  year = {2003},
  file = {/Users/luigi/work/zotero/storage/7Z5SLUBY/article2.pdf;/Users/luigi/work/zotero/storage/YLL6KZ28/verification-of-object-oriented-programs-with-invariants-2.html}
}

@inproceedings{nystromPolyglotExtensibleCompiler2003,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Polyglot: {{An Extensible Compiler Framework}} for {{Java}}},
  isbn = {978-3-540-00904-7 978-3-540-36579-2},
  shorttitle = {Polyglot},
  doi = {10.1007/3-540-36579-6_11},
  abstract = {Polyglot is an extensible compiler framework that supports the easy creation of compilers for languages similar to Java, while avoiding code duplication. The Polyglot framework is useful for domain-specific languages, exploration of language design, and for simplified versions of Java for pedagogical use. We have used Polyglot to implement several major and minor modifications to Java; the cost of implementing language extensions scales well with the degree to which the language differs from Java. This paper focuses on the design choices in Polyglot that are important for making the framework usable and highly extensible. Polyglot source code is available.},
  language = {en},
  booktitle = {Compiler {{Construction}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Nystrom, Nathaniel and Clarkson, Michael R. and Myers, Andrew C.},
  month = apr,
  year = {2003},
  pages = {138-152},
  file = {/Users/luigi/work/zotero/storage/7R2QK48C/Nystrom et al. - 2003 - Polyglot An Extensible Compiler Framework for Jav.pdf;/Users/luigi/work/zotero/storage/SAJRSBBV/10.html}
}

@inproceedings{chenHeapCompressionMemoryconstrained2003,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '03},
  title = {Heap {{Compression}} for {{Memory}}-Constrained {{Java Environments}}},
  isbn = {978-1-58113-712-5},
  doi = {10.1145/949305.949330},
  abstract = {Java is becoming the main software platform for consumer and embedded devices such as mobile phones, PDAs, TV set-top boxes, and in-vehicle systems. Since many of these systems are memory constrained, it is extremely important to keep the memory footprint of Java applications under control.The goal of this work is to enable the execution of Java applications using a smaller heap footprint than that possible using current embedded JVMs. We propose a set of memory management strategies to reduce heap footprint of embedded Java applications that execute under severe memory constraints. Our first contribution is a new garbage collector, referred to as the Mark-Compact-Compress (MCC) collector, that allows an application to run with a heap smaller than its footprint. An important characteristic of this collector is that it compresses objects when heap compaction is not sufficient for creating space for the current allocation request. In addition to employing compression, we also consider a heap management strategy and associated garbage collector, called MCL (Mark-Compact-Lazy Allocate), based on lazy allocation of object portions. This new collector operates like the conventional Mark-Compact (MC) collector, but takes advantage of the observation that many Java applications create large objects, of which only a small portion is actually used. In addition, we also combine MCC and MCL, and present MCCL (Mark-Compact-Compress-Lazy Al-locate), which outperforms both MCC and MCL.We have implemented these collectors using KVM, and performed extensive experiments using a set of ten embedded Java applications. We have found our new garbage collection strategies to be useful in two main aspects. First, they reduce the minimum heap size necessary to execute an application without out-of-memory exception. Second, our strategies reduce the heap occupancy. That is, at a given time, they reduce the heap memory requirement of the application being executed. We have also conducted experiments with a more aggressive object compression strategy and discussed its main advantages.},
  booktitle = {Proceedings of the 18th {{Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programing}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Chen, G. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M. J. and Mathiske, B. and Wolczko, M.},
  year = {2003},
  keywords = {garbage collection,heap,Java virtual machine,memory compression},
  pages = {282--301},
  file = {/Users/luigi/work/zotero/storage/WHRRN5EQ/Chen et al. - 2003 - Heap Compression for Memory-constrained Java Envir.pdf}
}

@inproceedings{baconRealtimeGarbageCollector2003,
  address = {New York, NY, USA},
  series = {{{POPL}} '03},
  title = {A {{Real}}-Time {{Garbage Collector}} with {{Low Overhead}} and {{Consistent Utilization}}},
  isbn = {978-1-58113-628-9},
  doi = {10.1145/604131.604155},
  abstract = {Now that the use of garbage collection in languages like Java is becoming widely accepted due to the safety and software engineering benefits it provides, there is significant interest in applying garbage collection to hard real-time systems. Past approaches have generally suffered from one of two major flaws: either they were not provably real-time, or they imposed large space overheads to meet the real-time bounds. We present a mostly non-moving, dynamically defragmenting collector that overcomes both of these limitations: by avoiding copying in most cases, space requirements are kept low; and by fully incrementalizing the collector we are able to meet real-time bounds. We implemented our algorithm in the Jikes RVM and show that at real-time resolution we are able to obtain mutator utilization rates of 45\% with only 1.6--2.5 times the actual space required by the application, a factor of 4 improvement in utilization over the best previously published results. Defragmentation causes no more than 4\% of the traced data to be copied.},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Bacon, David F. and Cheng, Perry and Rajan, V. T.},
  year = {2003},
  keywords = {defragmentation,read barrier,real-time scheduling,utilization},
  pages = {285--298},
  file = {/Users/luigi/work/zotero/storage/I37SF3CL/Bacon et al. - 2003 - A Real-time Garbage Collector with Low Overhead an.pdf}
}

@article{leroyJavaBytecodeVerification2003,
  title = {Java {{Bytecode Verification}}: {{Algorithms}} and {{Formalizations}}},
  volume = {30},
  issn = {0168-7433, 1573-0670},
  shorttitle = {Java {{Bytecode Verification}}},
  doi = {10.1023/A:1025055424017},
  abstract = {Bytecode verification is a crucial security component for Java applets, on the Web and on embedded devices such as smart cards. This paper reviews the various bytecode verification algorithms that have been proposed, recasts them in a common framework of dataflow analysis, and surveys the use of proof assistants to specify bytecode verification and prove its correctness.},
  language = {en},
  number = {3-4},
  journal = {Journal of Automated Reasoning},
  author = {Leroy, Xavier},
  month = may,
  year = {2003},
  pages = {235-269},
  file = {/Users/luigi/work/zotero/storage/BDMRQBEU/Leroy - 2003 - Java Bytecode Verification Algorithms and Formali.pdf;/Users/luigi/work/zotero/storage/AVBMVBXU/10.html}
}

@article{huntSingularityDesignMotivation2004,
  title = {Singularity {{Design Motivation}}},
  abstract = {Singularity is a cross-discipline research project in Microsoft Research building a managed code operating system. This technical report describes the motivation and priorities for Singularity. Other technical reports describe the abstractions and implementations of Singularity features.},
  journal = {Microsoft Research},
  author = {Hunt, Galen and Larus, Jim},
  month = nov,
  year = {2004},
  file = {/Users/luigi/work/zotero/storage/V4FM6DPL/Hunt and Larus - 2004 - Singularity Design Motivation.pdf;/Users/luigi/work/zotero/storage/9CHFMXKS/singularity-design-motivation.html}
}

@inproceedings{barnettSpecProgrammingSystem2004,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {The {{Spec}}\# {{Programming System}}: {{An Overview}}},
  isbn = {978-3-540-24287-1 978-3-540-30569-9},
  shorttitle = {The {{Spec}}\# {{Programming System}}},
  doi = {10.1007/978-3-540-30569-9_3},
  abstract = {The Spec\# programming system is a new attempt at a more cost effective way to develop and maintain high-quality software. This paper describes the goals and architecture of the Spec\# programming system, consisting of the object-oriented Spec\# programming language, the Spec\# compiler, and the Boogie static program verifier. The language includes constructs for writing specifications that capture programmer intentions about how methods and data are to be used, the compiler emits run-time checks to enforce these specifications, and the verifier can check the consistency between a program and its specifications.},
  language = {en},
  booktitle = {Construction and {{Analysis}} of {{Safe}}, {{Secure}}, and {{Interoperable Smart Devices}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Barnett, Mike and Leino, K. Rustan M. and Schulte, Wolfram},
  month = mar,
  year = {2004},
  pages = {49-69},
  file = {/Users/luigi/work/zotero/storage/5FZ9KSD7/Barnett et al. - 2004 - The Spec# Programming System An Overview.pdf;/Users/luigi/work/zotero/storage/CB4CFN9F/10.html}
}

@inproceedings{lisitsaVerificationSupercompilation2005,
  title = {Towards Verification via Supercompilation},
  volume = {2},
  doi = {10.1109/COMPSAC.2005.159},
  abstract = {Supercompilation, or supervised compilation is a technique for program specialization, optimization and, more generally, program transformation. We present an idea to use supercompilation for verification of parameterized programs and protocols, present a case study and report on our initial experiments.},
  booktitle = {29th {{Annual International Computer Software}} and {{Applications Conference}} ({{COMPSAC}}'05)},
  author = {Lisitsa, A. and Nemytykh, A.},
  month = jul,
  year = {2005},
  keywords = {Protocols,Computer languages,program compilers,Computer applications,Computer science,Concrete,formal verification,History,Power system modeling,program optimization,program specialization,program transformation,Resource description framework,Software testing,supercompilation,System testing},
  pages = {9-10 Vol. 1},
  file = {/Users/luigi/work/zotero/storage/XPALV5Q5/Lisitsa and Nemytykh - 2005 - Towards verification via supercompilation.pdf;/Users/luigi/work/zotero/storage/Z4WLQ6QE/1508067.html}
}

@inproceedings{hallgrenPrincipledApproachOperating2005,
  address = {New York, NY, USA},
  series = {{{ICFP}} '05},
  title = {A {{Principled Approach}} to {{Operating System Construction}} in {{Haskell}}},
  isbn = {978-1-59593-064-4},
  doi = {10.1145/1086365.1086380},
  abstract = {We describe a monadic interface to low-level hardware features that is a suitable basis for building operating systems in Haskell. The interface includes primitives for controlling memory management hardware, user-mode process execution, and low-level device I/O. The interface enforces memory safety in nearly all circumstances. Its behavior is specified in part by formal assertions written in a programming logic called P-Logic. The interface has been implemented on bare IA32 hardware using the Glasgow Haskell Compiler (GHC) runtime system. We show how a variety of simple O/S kernels can be constructed on top of the interface, including a simple separation kernel and a demonstration system in which the kernel, window system, and all device drivers are written in Haskell.},
  booktitle = {Proceedings of the {{Tenth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Hallgren, Thomas and Jones, Mark P. and Leslie, Rebekah and Tolmach, Andrew},
  year = {2005},
  keywords = {hardware interface,Haskell,monads,operating systems,programming logic,verification},
  pages = {116--128},
  file = {/Users/luigi/work/zotero/storage/NHIE7IWP/Hallgren et al. - 2005 - A Principled Approach to Operating System Construc.pdf}
}

@article{alpernJikesResearchVirtual2005,
  title = {The {{Jikes Research Virtual Machine}} Project: {{Building}} an Open-Source Research Community},
  volume = {44},
  issn = {0018-8670},
  shorttitle = {The {{Jikes Research Virtual Machine}} Project},
  doi = {10.1147/sj.442.0399},
  abstract = {This paper describes the evolution of the Jikes\texttrademark{} Research Virtual Machine project from an IBM internal research project, called Jalape\~no, into an open-source project. After summarizing the original goals of the project, we discuss the motivation for releasing it as an open-source project and the activities performed to ensure the success of the project. Throughout, we highlight the unique challenges of developing and maintaining an open-source project designed specifically to support a research community.},
  number = {2},
  journal = {IBM Systems Journal},
  author = {Alpern, B. and Augart, S. and Blackburn, S. M. and Butrico, M. and Cocchi, A. and Cheng, P. and Dolby, J. and Fink, S. and Grove, D. and Hind, M. and McKinley, K. S. and Mergen, M. and Moss, J. E. B. and Ngo, T. and Sarkar, V. and Trapp, M.},
  year = {2005},
  pages = {399-417},
  file = {/Users/luigi/work/zotero/storage/EYDPZ64K/Alpern et al. - 2005 - The Jikes Research Virtual Machine project Buildi.pdf;/Users/luigi/work/zotero/storage/QPE9ZXNA/5386722.html}
}

@article{huntOverviewSingularityProject2005,
  title = {An {{Overview}} of the {{Singularity Project}}},
  abstract = {Singularity is a research project in Microsoft Research that started with the question: what would a software platform look like if it was designed from scratch with the primary goal of dependability? Singularity is working to answer this question by building on advances in programming languages and tools to develop a new system architecture and \ldots{}},
  journal = {Microsoft Research},
  author = {Hunt, Galen and Abadi, Martin and Barham, Paul and Fahndrich, Manuel and Hawblitzel, Chris and Hodson, Orion and Levi, Steven and Wobber, Ted and Zill, Brian and Larus, Jim},
  month = oct,
  year = {2005},
  file = {/Users/luigi/work/zotero/storage/964VBIY4/Hunt et al. - 2005 - An Overview of the Singularity Project.pdf;/Users/luigi/work/zotero/storage/HKNJXFFK/an-overview-of-the-singularity-project.html}
}

@inproceedings{hindmanAtomicitySourcetosourceTranslation2006,
  address = {New York, NY, USA},
  series = {{{MSPC}} '06},
  title = {Atomicity via {{Source}}-to-Source {{Translation}}},
  isbn = {978-1-59593-578-6},
  doi = {10.1145/1178597.1178611},
  abstract = {We present an implementation and evaluation of atomicity (also known as software transactions) for a dialect of Java. Our implementation is fundamentally different from prior work in three respects: (1) It is entirely a source-to-source translation, producing Java source code that can be compiled by any Java compiler and run on any Java Virtual Machine. (2) It can enforce "strong" atomicity without assuming special hardware or a uniprocessor. (3) The implementation uses locks rather than optimistic concurrency, but it cannot deadlock and requires inter-thread communication only when there is data contention.},
  booktitle = {Proceedings of the 2006 {{Workshop}} on {{Memory System Performance}} and {{Correctness}}},
  publisher = {{ACM}},
  author = {Hindman, Benjamin and Grossman, Dan},
  year = {2006},
  keywords = {Java,atomicity,concurrent programming,transactional memory},
  pages = {82--91},
  file = {/Users/luigi/work/zotero/storage/AQRRWIM6/Hindman and Grossman - 2006 - Atomicity via source-to-source translation.pdf;/Users/luigi/work/zotero/storage/CD9LTTAT/Hindman and Grossman - 2006 - Atomicity via Source-to-source Translation.pdf;/Users/luigi/work/zotero/storage/JDFYI5HZ/Hindman and Grossman - 2006 - Atomicity via Source-to-source Translation.pdf}
}

@inproceedings{yamauchiWritingSolarisDevice2006,
  address = {New York, NY, USA},
  series = {{{PLOS}} '06},
  title = {Writing {{Solaris Device Drivers}} in {{Java}}},
  isbn = {978-1-59593-577-9},
  doi = {10.1145/1215995.1215998},
  abstract = {Operating system kernels have been traditionally written in C for flexibility and efficiency. They, however, often suffer from bugs and security vulnerabilities because C is inherently error-prone and unsafe. While there have been attempts to experimentally construct a complete operating system in a type safe language such as Java for higher safety and reliability, such type safe operating systems are not mainstream as yet. In this paper, we present an experimental implementation of the Java Virtual Machine that runs inside the kernel of the Solaris operating system. Our approach is to extend the existing operating system, rather than creating a new operating system from scratch, in order to reap the benefits of a type safe language in the kernel without expensive development and transition cost for a new operating system architecture. We implemented our system by porting an existing small, portable JVM, Squawk, into the Solaris kernel. Our first application of this system is to allow device drivers to be written in Java. A simple device driver was ported from C to Java. Characteristics of the Java device driver and our device driver interface are described.},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Programming Languages}} and {{Operating Systems}}: {{Linguistic Support}} for {{Modern Operating Systems}}},
  publisher = {{ACM}},
  author = {Yamauchi, Hiroshi and Wolczko, Mario},
  year = {2006},
  keywords = {operating systems,device drivers,type-safe languages},
  file = {/Users/luigi/work/zotero/storage/DSMYR96U/Yamauchi and Wolczko - 2006 - Writing Solaris Device Drivers in Java.pdf;/Users/luigi/work/zotero/storage/LPT7G8NB/Yamauchi and Wolczko - 2006 - Writing Solaris Device Drivers in Java.pdf}
}

@article{cachopoVersionedBoxesBasis2006,
  series = {Special Issue on Synchronization and Concurrency in Object-Oriented Languages},
  title = {Versioned Boxes as the Basis for Memory Transactions},
  volume = {63},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2006.05.009},
  abstract = {In this paper, we propose the use of Versioned Boxes, which keep a history of values, as the basis for language-level memory transactions. Unlike previous work on software transactional memory, in our proposal read-only transactions never conflict with any other concurrent transaction. This may improve significantly the concurrency on applications which have longer transactions and a high read/write ratio. Furthermore, we discuss how we can reduce transaction conflicts by delaying computations and re-executing only parts of a transaction in case of a conflict. We propose two language-level abstractions to support these strategies: the per-transaction boxes and the restartable transactions. Finally, we lay out the basis for a more generic model, which better supports fine-grained restartable transactions. The goal of this new model is to generalize the previous two abstractions to reduce conflicts.},
  number = {2},
  journal = {Science of Computer Programming},
  author = {Cachopo, Jo\~ao and {Rito-Silva}, Ant\'onio},
  month = dec,
  year = {2006},
  keywords = {Conflict reduction,Multi-version concurrency control,Software transactional memory,Transactions},
  pages = {172-185},
  file = {/Users/luigi/work/zotero/storage/77L9QTGT/Cachopo and Rito-Silva - 2006 - Versioned boxes as the basis for memory transactio.pdf;/Users/luigi/work/zotero/storage/HLH3QHIB/S0167642306001171.html}
}

@article{fahndrichLanguageSupportFast2006,
  title = {Language {{Support}} for {{Fast}} and {{Reliable Message}}-Based {{Communication}} in {{Singularity OS}}},
  abstract = {Message-based communication offers the potential benefits of providing stronger specification and cleaner separation between components. Compared with shared-memory interactions, message passing has the potential disadvantages of more expensive data exchange (no direct sharing) and more complicated programming. In this paper we report on the language, verification, and run-time system features that make messages practical as \ldots{}},
  journal = {Microsoft Research},
  author = {Fahndrich, Manuel and Aiken, Mark and Hawblitzel, Chris and Hodson, Orion and Hunt, Galen and Larus, Jim and Levi, Steven},
  month = apr,
  year = {2006},
  file = {/Users/luigi/work/zotero/storage/UKFJ96R8/Fahndrich et al. - 2006 - Language Support for Fast and Reliable Message-bas.pdf;/Users/luigi/work/zotero/storage/B3NEXJB9/language-support-for-fast-and-reliable-message-based-communication-in-singularity-os.html}
}

@article{collbergEmpiricalStudyJava2007,
  title = {An Empirical Study of {{Java}} Bytecode Programs},
  volume = {37},
  issn = {1097-024X},
  doi = {10.1002/spe.776},
  abstract = {We present a study of the static structure of real Java bytecode programs. A total of 1132 Java jar-files were collected from the Internet and analyzed. In addition to simple counts (number of methods per class, number of bytecode instructions per method, etc.), structural metrics such as the complexity of control-flow and inheritance graphs were computed. We believe this study will be valuable in the design of future programming languages and virtual machine instruction sets, as well as in the efficient implementation of compilers and other language processors. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {6},
  journal = {Software: Practice and Experience},
  author = {Collberg, Christian and Myles, Ginger and Stepp, Michael},
  month = may,
  year = {2007},
  keywords = {Java,bytecode,measure,software complexity metrics},
  pages = {581-641},
  file = {/Users/luigi/work/zotero/storage/P2N3ATDN/Collberg et al. - 2007 - An empirical study of Java bytecode programs.pdf;/Users/luigi/work/zotero/storage/S9IRIET3/abstract.html}
}

@inproceedings{gaySafeManualMemory2007,
  address = {New York, NY, USA},
  series = {{{ISMM}} '07},
  title = {Safe {{Manual Memory Management}}},
  isbn = {978-1-59593-893-0},
  doi = {10.1145/1296907.1296911},
  abstract = {We present HeapSafe, a tool that uses reference counting to dynamically verify the soundness of manual memory management of C programs. HeapSafe relies on asimple extension to the usual malloc/free memory management API: delayed free scopes during which otherwise dangling references can exist. Porting programs for use with HeapSafe typically requires little effort (on average 0.6\% oflines change), adds an average 11\% time overhead (84\% in the worst case), and increases space usage by an average of 13\%. These results are based on portingover half a million lines of C code, including perl where we found sixpreviously unknown bugs.Many existing C programs continue to use unchecked manual memorymanagement. One reason is that programmers fear that moving to garbage collection is too big a risk. We believe that HeapSafe is a practical way toprovide safe memory management for such programs. Since HeapSafe checks existing memory management rather than changing it, programmers need not worrythat HeapSafe will introduce new bugs; and, since HeapSafe does not managememory itself, programmers can choose to deploy their programs without HeapSafe if performance is critical (a simple header file allows HeapSafe programs to compile and run with a regular C compiler). In contrast, we foundthat garbage collection, although faster, had much higher space overhead, and occasionally caused a space-usage explosion that made the program unusable.},
  booktitle = {Proceedings of the 6th {{International Symposium}} on {{Memory Management}}},
  publisher = {{ACM}},
  author = {Gay, David and Ennals, Rob and Brewer, Eric},
  year = {2007},
  keywords = {C,memory management,reference counting,safety},
  pages = {2--14},
  file = {/Users/luigi/work/zotero/storage/HZI238P2/Gay et al. - 2007 - Safe Manual Memory Management.pdf}
}

@inproceedings{kinneerSofyaSupportingRapid2007,
  title = {Sofya: {{Supporting Rapid Development}} of {{Dynamic Program Analyses}} for {{Java}}},
  shorttitle = {Sofya},
  doi = {10.1109/ICSECOMPANION.2007.68},
  abstract = {Dynamic analysis is an increasingly important means of supporting software validation and maintenance. To date, developers of dynamic analyses have used low-level instrumentation and debug interfaces to realize their analyses. Many dynamic analyses, however, share multiple common high-level requirements, e.g., capture of program data state as well as events, and efficient and accurate event capture in the presence of threading. We present SOFYA - an infra-structure designed to provide high-level, efficient, concurrency-aw are support for building analyses that reason about rich observations of program data and events. It provides a layered, modular architecture, which has been successfully used to rapidly develop and evaluate a variety of demanding dynamic program analyses. In this paper, we describe the SOFYA framework, the challenges it addresses, and survey several such analyses.},
  booktitle = {29th {{International Conference}} on {{Software Engineering}} - {{Companion}}, 2007. {{ICSE}} 2007 {{Companion}}},
  author = {Kinneer, A. and Dwyer, M. B. and Rothermel, G.},
  month = may,
  year = {2007},
  keywords = {Java,Software maintenance,Computer science,Buildings,Concurrent computing,debug interfaces,dynamic program analyses,Instruments,modular architecture,Monitoring,Payloads,Performance analysis,program data observations,program diagnostics,program verification,software maintenance,software validation,SOFYA framework,Yarn},
  pages = {51-52},
  file = {/Users/luigi/work/zotero/storage/V6JL39T3/Kinneer et al. - 2007 - Sofya Supporting Rapid Development of Dynamic Pro.pdf;/Users/luigi/work/zotero/storage/ISXRMTMN/4222676.html}
}

@book{kuleshovUsingASMFramework2007,
  title = {Using the {{ASM}} Framework to Implement Common {{Java}} Bytecode Transformation Patterns},
  author = {Kuleshov, Eugene},
  year = {2007},
  file = {/Users/luigi/work/zotero/storage/8ECNLPNX/Kuleshov - 2007 - Using the ASM framework to implement common Java b.pdf;/Users/luigi/work/zotero/storage/AS2T2AXP/summary.html}
}

@inproceedings{mitchellSupercompilerCoreHaskell2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Supercompiler}} for {{Core Haskell}}},
  isbn = {978-3-540-85372-5 978-3-540-85373-2},
  doi = {10.1007/978-3-540-85373-2_9},
  abstract = {Haskell is a functional language, with features such as higher order functions and lazy evaluation, which allow succinct programs. These high-level features present many challenges for optimising compilers. We report practical experiments using novel variants of supercompilation, with special attention to let bindings and the generalisation technique.},
  language = {en},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Mitchell, Neil and Runciman, Colin},
  month = sep,
  year = {2007},
  pages = {147-164},
  file = {/Users/luigi/work/zotero/storage/3T9SUZ5H/Mitchell and Runciman - 2007 - A Supercompiler for Core Haskell.pdf;/Users/luigi/work/zotero/storage/JYRJPCMS/10.html}
}

@inproceedings{binderReengineeringStandardJava2007,
  title = {Reengineering {{Standard Java Runtime Systems}} through {{Dynamic Bytecode Instrumentation}}},
  doi = {10.1109/SCAM.2007.20},
  abstract = {Java bytecode instrumentation is a widely used technique, especially for profiling purposes. In order to ensure the instrumentation of all classes in the system, including dynamically generated or downloaded code, instrumentation has to be performed at runtime. The standard JDK offers some mechanisms for dynamic instrumentation, which however either require the use of native code or impose severe restrictions on the instrumentation of certain core classes of the JDK. These limitations prevent several instrumentation techniques that are important for efficient, calling context-sensitive profiling. In this paper we present a generic bytecode instrumentation framework that goes beyond these restrictions and enables the customized, dynamic instrumentation of all classes in pure Java. Our framework addresses important issues, such as bootstrapping an instrumented JDK, as well as avoiding measurement perturbations due to dynamic instrumentation or execution of instrumentation code. We validated and evaluated our framework using an instrumentation for exact profiling which generates complete calling context trees of various platform-independent dynamic metrics.},
  booktitle = {Seventh {{IEEE International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}} 2007)},
  author = {Binder, W. and Hulaas, J. and Moret, P.},
  month = sep,
  year = {2007},
  keywords = {Java,Runtime,Libraries,Instruments,Performance analysis,aspect-oriented programming,bootstrapping,Code standards,Communication standards,computer bootstrapping,context-sensitive profiling,dynamic bytecode instrumentation,dynamic metrics,Dynamic programming,Informatics,Java dynamic bytecode instrumentation,JVM,Manipulator dynamics,profiling,program transformations,software libraries,standard Java runtime system reengineering,systems re-engineering,virtual machines},
  pages = {91-100},
  file = {/Users/luigi/work/zotero/storage/LQNG6DB6/Binder et al. - 2007 - Reengineering Standard Java Runtime Systems throug.pdf;/Users/luigi/work/zotero/storage/XBF7PL7V/4362901.html}
}

@article{errenTenSimpleRules2007,
  title = {Ten {{Simple Rules}} for {{Doing Your Best Research}}, {{According}} to {{Hamming}}},
  volume = {3},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.0030213},
  number = {10},
  journal = {PLOS Computational Biology},
  author = {Erren, Thomas C. and Cullen, Paul and Erren, Michael and Bourne, Philip E.},
  year = {26-Oct-2007},
  keywords = {Computer and information sciences,Creativity,Distillation,Fathers,Information theory,Physicists,Scientists,Telecommunications},
  pages = {e213},
  file = {/Users/luigi/work/zotero/storage/RFK49KQ7/Erren et al. - 2007 - Ten Simple Rules for Doing Your Best Research, Acc.pdf;/Users/luigi/work/zotero/storage/UPHAN9SH/article.html}
}

@article{lhotakEvaluatingBenefitsContextsensitive2008,
  title = {Evaluating the {{Benefits}} of {{Context}}-Sensitive {{Points}}-to {{Analysis Using}} a {{BDD}}-Based {{Implementation}}},
  volume = {18},
  issn = {1049-331X},
  doi = {10.1145/1391984.1391987},
  abstract = {We present Paddle, a framework of BDD-based context-sensitive points-to and call graph analyses for Java, as well as client analyses that use their results. Paddle supports several variations of context-sensitive analyses, including call site strings and object sensitivity, and context-sensitively specializes both pointer variables and the heap abstraction. We empirically evaluate the precision of these context-sensitive analyses on significant Java programs. We find that that object-sensitive analyses are more precise than comparable variations of the other approaches, and that specializing the heap abstraction improves precision more than extending the length of context strings.},
  number = {1},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  author = {Lhot\'ak, Ond{\v r}ej and Hendren, Laurie},
  month = oct,
  year = {2008},
  keywords = {Java,binary decision diagrams,call graph construction,cast safety analysis,context sensitivity,Interprocedural program analysis,points-to analysis},
  pages = {3:1--3:53},
  file = {/Users/luigi/work/zotero/storage/96WDZGXD/Lhoták and Hendren - 2008 - Evaluating the Benefits of Context-sensitive Point.pdf}
}

@inproceedings{holknerEvaluatingDynamicBehaviour2009,
  address = {Darlinghurst, Australia, Australia},
  series = {{{ACSC}} '09},
  title = {Evaluating the {{Dynamic Behaviour}} of {{Python Applications}}},
  isbn = {978-1-920682-72-9},
  abstract = {The Python programming language is typical among dynamic languages in that programs written in it are not susceptible to static analysis. This makes efficient static program compilation difficult, as well as limiting the amount of early error detection that can be performed. Prior research in this area tends to make assumptions about the nature of programs written in Python, restricting the expressiveness of the language. One may question why programmers are drawn to these languages at all, if only to use them in a static-friendly style. In this paper we present our results after measuring the dynamic behaviour of 24 production-stage open source Python programs. The programs tested included arcade games, GUI applications and non-interactive batch programs. We found that while most dynamic activity occurs during program startup, dynamic activity after startup cannot be discounted entirely.},
  booktitle = {Proceedings of the {{Thirty}}-{{Second Australasian Conference}} on {{Computer Science}} - {{Volume}} 91},
  publisher = {{Australian Computer Society, Inc.}},
  author = {Holkner, Alex and Harland, James},
  year = {2009},
  keywords = {dynamic languages,Python and compilers},
  pages = {19--28},
  file = {/Users/luigi/work/zotero/storage/EGCHB39A/Holkner and Harland - 2009 - Evaluating the Dynamic Behaviour of Python Applica.pdf}
}

@inproceedings{framptonDemystifyingMagicHighlevel2009,
  address = {New York, NY, USA},
  series = {{{VEE}} '09},
  title = {Demystifying {{Magic}}: {{High}}-Level {{Low}}-Level {{Programming}}},
  isbn = {978-1-60558-375-4},
  shorttitle = {Demystifying {{Magic}}},
  doi = {10.1145/1508293.1508305},
  abstract = {The power of high-level languages lies in their abstraction over hardware and software complexity, leading to greater security, better reliability, and lower development costs. However, opaque abstractions are often show-stoppers for systems programmers, forcing them to either break the abstraction, or more often, simply give up and use a different language. This paper addresses the challenge of opening up a high-level language to allow practical low-level programming without forsaking integrity or performance. The contribution of this paper is three-fold: 1) we draw together common threads in a diverse literature, 2) we identify a framework for extending high-level languages for low-level programming, and 3) we show the power of this approach through concrete case studies. Our framework leverages just three core ideas: extending semantics via intrinsic methods, extending types via unboxing and architectural-width primitives, and controlling semantics via scoped semantic regimes. We develop these ideas through the context of a rich literature and substantial practical experience. We show that they provide the power necessary to implement substantial artifacts such as a high-performance virtual machine, while preserving the software engineering benefits of the host language. The time has come for high-level low-level programming to be taken more seriously: 1) more projects now use high-level languages for systems programming, 2) increasing architectural heterogeneity and parallelism heighten the need for abstraction, and 3) a new generation of high-level languages are under development and ripe to be influenced.},
  booktitle = {Proceedings of the 2009 {{ACM SIGPLAN}}/{{SIGOPS International Conference}} on {{Virtual Execution Environments}}},
  publisher = {{ACM}},
  author = {Frampton, Daniel and Blackburn, Stephen M. and Cheng, Perry and Garner, Robin J. and Grove, David and Moss, J. Eliot B. and Salishev, Sergey I.},
  year = {2009},
  keywords = {systems programming,debugging,intrinsics,jikes rvm,magic,mmtk,virtualization,vmmagic},
  pages = {81--90},
  file = {/Users/luigi/work/zotero/storage/HFNEJJ25/Frampton et al. - 2009 - Demystifying Magic High-level Low-level Programmi.pdf}
}

@inproceedings{cheRodiniaBenchmarkSuite2009,
  title = {Rodinia: {{A}} Benchmark Suite for Heterogeneous Computing},
  shorttitle = {Rodinia},
  doi = {10.1109/IISWC.2009.5306797},
  abstract = {This paper presents and characterizes Rodinia, a benchmark suite for heterogeneous computing. To help architects study emerging platforms such as GPUs (Graphics Processing Units), Rodinia includes applications and kernels which target multi-core CPU and GPU platforms. The choice of applications is inspired by Berkeley's dwarf taxonomy. Our characterization shows that the Rodinia benchmarks cover a wide range of parallel communication patterns, synchronization techniques and power consumption, and has led to some important architectural insight, such as the growing importance of memory-bandwidth limitations and the consequent importance of data layout.},
  booktitle = {2009 {{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}})},
  author = {Che, S. and Boyer, M. and Meng, J. and Tarjan, D. and Sheaffer, J. W. and Lee, S. H. and Skadron, K.},
  month = oct,
  year = {2009},
  keywords = {Application software,Yarn,Benchmark testing,Berkeleys dwarf taxonomy,Central Processing Unit,Computer architecture,data layout,Energy consumption,heterogeneous computing,Kernel,memory-bandwidth limitation,Microprocessors,multicore CPU platform,multicore GPU platform,Multicore processing,parallel communication pattern,Parallel processing,parallel program,parallel programming,power consumption,Rodinia-a benchmark suite,synchronization technique},
  pages = {44-54},
  file = {/Users/luigi/work/zotero/storage/5VHPSFJX/Che et al. - 2009 - Rodinia A benchmark suite for heterogeneous compu.pdf;/Users/luigi/work/zotero/storage/G6CVUCGG/5306797.html}
}

@inproceedings{qiMaskedTypesSound2009,
  address = {New York, NY, USA},
  series = {{{POPL}} '09},
  title = {Masked {{Types}} for {{Sound Object Initialization}}},
  isbn = {978-1-60558-379-2},
  doi = {10.1145/1480881.1480890},
  abstract = {This paper presents a type-based solution to the long-standing problem of object initialization. Constructors, the conventional mechanism for object initialization, have semantics that are surprising to programmers and that lead to bugs. They also contribute to the problem of null-pointer exceptions, which make software less reliable. Masked types are a new type-state mechanism that explicitly tracks the initialization state of objects and prevents reading from uninitialized fields. In the resulting language, constructors are ordinary methods that operate on uninitialized objects, and no special default value (null) is needed in the language. Initialization of cyclic data structures is achieved with the use of conditionally masked types. Masked types are modular and compatible with data abstraction. The type system is presented in a simplified object calculus and is proved to soundly prevent reading from uninitialized fields. Masked types have been implemented as an extension to Java, in which compilation simply erases extra type information. Experience using the extended language suggests that masked types work well on real code.},
  booktitle = {Proceedings of the 36th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Qi, Xin and Myers, Andrew C.},
  year = {2009},
  keywords = {conditional masks,cyclic data structures,data abstraction,invariants,null pointer exceptions},
  pages = {53--65},
  file = {/Users/luigi/work/zotero/storage/GDDXXPWT/Qi and Myers - 2009 - Masked Types for Sound Object Initialization.pdf}
}

@inproceedings{bloomThornRobustConcurrent2009,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '09},
  title = {Thorn: {{Robust}}, {{Concurrent}}, {{Extensible Scripting}} on the {{JVM}}},
  isbn = {978-1-60558-766-0},
  shorttitle = {Thorn},
  doi = {10.1145/1640089.1640098},
  abstract = {Scripting languages enjoy great popularity due to their support for rapid and exploratory development. They typically have lightweight syntax, weak data privacy, dynamic typing, powerful aggregate data types, and allow execution of the completed parts of incomplete programs. The price of these features comes later in the software life cycle. Scripts are hard to evolve and compose, and often slow. An additional weakness of most scripting languages is lack of support for concurrency - though concurrency is required for scalability and interacting with remote services. This paper reports on the design and implementation of Thorn, a novel programming language targeting the JVM. Our principal contributions are a careful selection of features that support the evolution of scripts into industrial grade programs - e.g., an expressive module system, an optional type annotation facility for declarations, and support for concurrency based on message passing between lightweight, isolated processes. On the implementation side, Thorn has been designed to accommodate the evolution of the language itself through a compiler plugin mechanism and target the Java virtual machine.},
  booktitle = {Proceedings of the 24th {{ACM SIGPLAN Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  publisher = {{ACM}},
  author = {Bloom, Bard and Field, John and Nystrom, Nathaniel and \"Ostlund, Johan and Richards, Gregor and Strni{\v s}a, Rok and Vitek, Jan and Wrigstad, Tobias},
  year = {2009},
  keywords = {actors,pattern matching,scripting},
  pages = {117--136},
  file = {/Users/luigi/work/zotero/storage/G36L8MEC/Bloom et al. - 2009 - Thorn Robust, Concurrent, Extensible Scripting on.pdf}
}

@article{phadtareScientificWritingRandomized2009,
  title = {Scientific Writing: A Randomized Controlled Trial Comparing Standard and on-Line Instruction},
  volume = {9},
  issn = {1472-6920},
  shorttitle = {Scientific Writing},
  doi = {10.1186/1472-6920-9-27},
  abstract = {Writing plays a central role in the communication of scientific ideas and is therefore a key aspect in researcher education, ultimately determining the success and long-term sustainability of their careers. Despite the growing popularity of e-learning, we are not aware of any existing study comparing on-line vs. traditional classroom-based methods for teaching scientific writing.},
  journal = {BMC Medical Education},
  author = {Phadtare, Amruta and Bahmani, Anu and Shah, Anand and Pietrobon, Ricardo},
  month = may,
  year = {2009},
  pages = {27},
  file = {/Users/luigi/work/zotero/storage/QXBXNY93/Phadtare et al. - 2009 - Scientific writing a randomized controlled trial .pdf}
}

@inproceedings{auerbachLimeJavacompatibleSynthesizable2010,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '10},
  title = {Lime: {{A Java}}-Compatible and {{Synthesizable Language}} for {{Heterogeneous Architectures}}},
  isbn = {978-1-4503-0203-6},
  shorttitle = {Lime},
  doi = {10.1145/1869459.1869469},
  abstract = {The halt in clock frequency scaling has forced architects and language designers to look elsewhere for continued improvements in performance. We believe that extracting maximum performance will require compilation to highly heterogeneous architectures that include reconfigurable hardware. We present a new language, Lime, which is designed to be executable across a broad range of architectures, from FPGAs to conventional CPUs. We present the language as a whole, focusing on its novel features for limiting side-effects and integration of the streaming paradigm into an object- oriented language. We conclude with some initial results demonstrating applications running either on a CPU or co- executing on a CPU and an FPGA.},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  publisher = {{ACM}},
  author = {Auerbach, Joshua and Bacon, David F. and Cheng, Perry and Rabbah, Rodric},
  year = {2010},
  keywords = {fpga,functional programming,high level synthesis,object oriented,reconfigurable architecture,streaming,value type},
  pages = {89--108},
  file = {/Users/luigi/work/zotero/storage/QNDPVMS6/Auerbach et al. - 2010 - Lime A Java-compatible and Synthesizable Language.pdf}
}

@inproceedings{posnettTHEXMiningMetapatterns2010,
  title = {{{THEX}}: {{Mining}} Metapatterns from Java},
  shorttitle = {{{THEX}}},
  doi = {10.1109/MSR.2010.5463349},
  abstract = {Design patterns are codified solutions to common object-oriented design (OOD) problems in software development. One of the proclaimed benefits of the use of design patterns is that they decouple functionality and enable different parts of a system to change frequently without undue disruption throughout the system. These OOD patterns have received a wealth of attention in the research community since their introduction; however, identifying them in source code is a difficult problem. In contrast, metapatterns have similar effects on software design by enabling portions of the system to be extended or modified easily, but are purely structural in nature, and thus easier to detect. Our long-term goal is to evaluate the effects of different OOD patterns on coordination in software teams as well as outcomes such as developer productivity and software quality. we present THEX, a metapattern detector that scales to large codebases and works on any Java bytecode. We evaluate THEX by examining its performance on codebases with known design patterns (and therefore metapatterns) and find that it performs quite well, with recall of over 90\%.},
  booktitle = {2010 7th {{IEEE Working Conference}} on {{Mining Software Repositories}} ({{MSR}} 2010)},
  author = {Posnett, D. and Bird, C. and Devanbu, P.},
  month = may,
  year = {2010},
  keywords = {Java,Object oriented modeling,Computer science,Birds,data mining,design patterns,Detectors,developer productivity,Java bytecode,metapattern detector,metapattern mining,object-oriented design,object-oriented programming,Productivity,Programming,software design,Software design,software development,software development management,Software performance,software quality,Software quality,software team coordination,source code,team working,THEX},
  pages = {122-125},
  file = {/Users/luigi/work/zotero/storage/F4PA2HXC/Posnett et al. - 2010 - THEX Mining metapatterns from java.pdf;/Users/luigi/work/zotero/storage/NK9GS3HR/5463349.html}
}

@inproceedings{mitchellRethinkingSupercompilation2010,
  address = {New York, NY, USA},
  series = {{{ICFP}} '10},
  title = {Rethinking {{Supercompilation}}},
  isbn = {978-1-60558-794-3},
  doi = {10.1145/1863543.1863588},
  abstract = {Supercompilation is a program optimisation technique that is particularly effective at eliminating unnecessary overheads. We have designed a new supercompiler, making many novel choices, including different termination criteria and handling of let bindings. The result is a supercompiler that focuses on simplicity, compiles programs quickly and optimises programs well. We have benchmarked our supercompiler, with some programs running more than twice as fast than when compiled with GHC.},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Mitchell, Neil},
  year = {2010},
  keywords = {supercompilation,haskell,optimisation},
  pages = {309--320},
  file = {/Users/luigi/work/zotero/storage/4Z6QI4LW/Mitchell - 2010 - Rethinking Supercompilation.pdf}
}

@inproceedings{bolingbrokeSupercompilationEvaluation2010,
  address = {New York, NY, USA},
  series = {Haskell '10},
  title = {Supercompilation by {{Evaluation}}},
  isbn = {978-1-4503-0252-4},
  doi = {10.1145/1863523.1863540},
  abstract = {This paper shows how call-by-need supercompilation can be recast to be based explicitly on an evaluator, contrasting with standard presentations which are specified as algorithms that mix evaluation rules with reductions that are unique to supercompilation. Building on standard operational-semantics technology for call-by-need languages, we show how to extend the supercompilation algorithm to deal with recursive let expressions.},
  booktitle = {Proceedings of the {{Third ACM Haskell Symposium}} on {{Haskell}}},
  publisher = {{ACM}},
  author = {Bolingbroke, Maximilian and Peyton Jones, Simon},
  year = {2010},
  keywords = {supercompilation,haskell,optimisation,deforestation,specialisation},
  pages = {135--146},
  file = {/Users/luigi/work/zotero/storage/P88II97P/Bolingbroke and Peyton Jones - 2010 - Supercompilation by Evaluation.pdf}
}

@inproceedings{grechanikEmpiricalInvestigationLargescale2010,
  address = {New York, NY, USA},
  series = {{{ESEM}} '10},
  title = {An {{Empirical Investigation}} into a {{Large}}-Scale {{Java Open Source Code Repository}}},
  isbn = {978-1-4503-0039-1},
  doi = {10.1145/1852786.1852801},
  abstract = {Getting insight into different aspects of source code artifacts is increasingly important -- yet there is little empirical research using large bodies of source code, and subsequently there are not much statistically significant evidence of common patterns and facts of how programmers write source code. We pose 32 research questions, explain rationale behind them, and obtain facts from 2,080 randomly chosen Java applications from Sourceforge. Among these facts we find that most methods have one or zero arguments or they do not return any values, few methods are overridden, most inheritance hierarchies have the depth of one, close to 50\% of classes are not explicitly inherited from any classes, and the number of methods is strongly correlated with the number of fields in a class.},
  booktitle = {Proceedings of the 2010 {{ACM}}-{{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  publisher = {{ACM}},
  author = {Grechanik, Mark and McMillan, Collin and DeFerrari, Luca and Comi, Marco and Crespi, Stefano and Poshyvanyk, Denys and Fu, Chen and Xie, Qing and Ghezzi, Carlo},
  year = {2010},
  keywords = {empirical study,large-scale software,mining software repositories,open source,patterns,practice,software repository},
  pages = {11:1--11:10},
  file = {/Users/luigi/work/zotero/storage/NV6YI8WT/Grechanik et al. - 2010 - An Empirical Investigation into a Large-scale Java.pdf}
}

@inproceedings{flanaganFastTrackEfficientPrecise2009,
  address = {New York, NY, USA},
  series = {{{PLDI}} '09},
  title = {{{FastTrack}}: {{Efficient}} and {{Precise Dynamic Race Detection}}},
  isbn = {978-1-60558-392-1},
  shorttitle = {{{FastTrack}}},
  doi = {10.1145/1542476.1542490},
  abstract = {\textbackslash{}begin\{abstract\} Multithreaded programs are notoriously prone to race conditions. Prior work on dynamic race detectors includes fast but imprecise race detectors that report false alarms, as well as slow but precise race detectors that never report false alarms. The latter typically use expensive vector clock operations that require time linear in the number of program threads. This paper exploits the insight that the full generality of vector clocks is unnecessary in most cases. That is, we can replace heavyweight vector clocks with an adaptive lightweight representation that, for almost all operations of the target program, requires only constant space and supports constant-time operations. This representation change significantly improves time and space performance, with no loss in precision. Experimental results on Java benchmarks including the Eclipse development environment show that our FastTrack race detector is an order of magnitude faster than a traditional vector-clock race detector, and roughly twice as fast as the high-performance DJIT+ algorithm. FastTrack is even comparable in speed to Eraser on our Java benchmarks, while never reporting false alarms.},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Flanagan, Cormac and Freund, Stephen N.},
  year = {2009},
  keywords = {concurrency,dynamic analysis,race conditions},
  pages = {121--133},
  file = {/Users/luigi/work/zotero/storage/XN94VAUL/Flanagan and Freund - 2009 - FastTrack Efficient and Precise Dynamic Race Dete.pdf}
}

@inproceedings{richardsAnalysisDynamicBehavior2010,
  address = {New York, NY, USA},
  series = {{{PLDI}} '10},
  title = {An {{Analysis}} of the {{Dynamic Behavior}} of {{JavaScript Programs}}},
  isbn = {978-1-4503-0019-3},
  doi = {10.1145/1806596.1806598},
  abstract = {The JavaScript programming language is widely used for web programming and, increasingly, for general purpose computing. As such, improving the correctness, security and performance of JavaScript applications has been the driving force for research in type systems, static analysis and compiler techniques for this language. Many of these techniques aim to reign in some of the most dynamic features of the language, yet little seems to be known about how programmers actually utilize the language or these features. In this paper we perform an empirical study of the dynamic behavior of a corpus of widely-used JavaScript programs, and analyze how and why the dynamic features are used. We report on the degree of dynamism that is exhibited by these JavaScript programs and compare that with assumptions commonly made in the literature and accepted industry benchmark suites.},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Richards, Gregor and Lebresne, Sylvain and Burg, Brian and Vitek, Jan},
  year = {2010},
  keywords = {program analysis,dynamic metrics,dynamic behavior,execution tracing,javascript},
  pages = {1--12},
  file = {/Users/luigi/work/zotero/storage/PT4IYUIL/Richards et al. - 2010 - An Analysis of the Dynamic Behavior of JavaScript .pdf}
}

@article{korlandNoninvasiveConcurrencyJava2010,
  title = {Noninvasive Concurrency with {{Java STM}}},
  abstract = {In this paper we present a complete Java STM framework, called Deuce, intended as a platform for developing scalable concurrent applications and as a research tool for designing new STM algorithms. It was not clear if one could build an ecient Java STM without compiler support. Deuce provides several benets over existing Java STM frameworks: it avoids any changes or additions to the JVM, it does not require language extensions or intrusive APIs, and it does not impose any memory footprint or GC overhead. To support legacy libraries, Deuce dynamically instruments classes at load time and uses an original eld-based locking strategy to improve concur-rency. Deuce also provides a simple internal API allowing dierent STMs algorithms to be plugged in. We show empirical results that highlight the scalability of our framework running benchmarks with hundreds of concur-rent threads. This paper shows for the rst time that one can actually design a Java STM with reasonable performance without compiler support.},
  author = {Korland, Guy and Shavit, Nir and Felber, Pascal},
  month = jan,
  year = {2010},
  file = {/Users/luigi/work/zotero/storage/E7ND7BU7/e023f5543ad1b1128f84e463cc00d3e7775a.pdf;/Users/luigi/work/zotero/storage/F8S48DQY/267398972_Noninvasive_concurrency_with_Java_STM.html}
}

@inproceedings{flanaganRoadRunnerDynamicAnalysis2010,
  address = {New York, NY, USA},
  series = {{{PASTE}} '10},
  title = {The {{RoadRunner Dynamic Analysis Framework}} for {{Concurrent Programs}}},
  isbn = {978-1-4503-0082-7},
  doi = {10.1145/1806672.1806674},
  abstract = {RoadRunner is a dynamic analysis framework designed to facilitate rapid prototyping and experimentation with dynamic analyses for concurrent Java programs. It provides a clean API for communicating an event stream to back-end analyses, where each event describes some operation of interest performed by the target program, such as accessing memory, synchronizing on a lock, forking a new thread, and so on. This API enables the developer to focus on the essential algorithmic issues of the dynamic analysis, rather than on orthogonal infrastructure complexities. Each back-end analysis tool is expressed as a filter over the event stream, allowing easy composition of analyses into tool chains. This tool-chain architecture permits complex analyses to be described and implemented as a sequence of more simple, modular steps, and it facilitates experimentation with different tool compositions. Moreover, the ability to insert various monitoring tools into the tool chain facilitates debugging and performance tuning. Despite RoadRunner's flexibility, careful implementation and optimization choices enable RoadRunner-based analyses to offer comparable performance to traditional, monolithic analysis prototypes, while being up to an order of magnitude smaller in code size. We have used RoadRunner to develop several dozen tools and have successfully applied them to programs as large as the Eclipse programming environment.},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN}}-{{SIGSOFT Workshop}} on {{Program Analysis}} for {{Software Tools}} and {{Engineering}}},
  publisher = {{ACM}},
  author = {Flanagan, Cormac and Freund, Stephen N.},
  year = {2010},
  keywords = {concurrency,dynamic analysis},
  pages = {1--8},
  file = {/Users/luigi/work/zotero/storage/62KZXRZW/Flanagan and Freund - 2010 - The RoadRunner Dynamic Analysis Framework for Conc.pdf;/Users/luigi/work/zotero/storage/BPW3T4MK/Flanagan and Freund - 2010 - The roadrunner dynamic analysis framework for conc.pdf}
}

@article{stoneOpenCLParallelProgramming2010,
  title = {{{OpenCL}}: {{A Parallel Programming Standard}} for {{Heterogeneous Computing Systems}}},
  volume = {12},
  issn = {1521-9615},
  shorttitle = {{{OpenCL}}},
  doi = {10.1109/MCSE.2010.69},
  abstract = {The OpenCL standard offers a common API for program execution on systems composed of different types of computational devices such as multicore CPUs, GPUs, or other accelerators.},
  number = {3},
  journal = {Computing in Science Engineering},
  author = {Stone, J. E. and Gohara, D. and Shi, G.},
  month = may,
  year = {2010},
  keywords = {Runtime,Concurrent computing,Computer architecture,Kernel,Microprocessors,parallel programming,API,application program interfaces,computational devices,computer graphic equipment,Computer interfaces,coprocessors,GPU,Hardware,heterogeneous computing systems,High performance computing,multicore CPU,OpenCL standard,Parallel programming,parallel programming standard,program execution,Software standards},
  pages = {66-73},
  file = {/Users/luigi/work/zotero/storage/PNLEY8EW/Stone et al. - 2010 - OpenCL A Parallel Programming Standard for Hetero.pdf;/Users/luigi/work/zotero/storage/8M6598NZ/5457293.html}
}

@inproceedings{temperoQualitasCorpusCurated2010,
  title = {The {{Qualitas Corpus}}: {{A Curated Collection}} of {{Java Code}} for {{Empirical Studies}}},
  shorttitle = {The {{Qualitas Corpus}}},
  doi = {10.1109/APSEC.2010.46},
  abstract = {In order to increase our ability to use measurement to support software development practise we need to do more analysis of code. However, empirical studies of code are expensive and their results are difficult to compare. We describe the Qualitas Corpus, a large curated collection of open source Java systems. The corpus reduces the cost of performing large empirical studies of code and supports comparison of measurements of the same artifacts. We discuss its design, organisation, and issues associated with its development.},
  booktitle = {2010 {{Asia Pacific Software Engineering Conference}}},
  author = {Tempero, E. and Anslow, C. and Dietrich, J. and Han, T. and Li, J. and Lumpe, M. and Melton, H. and Noble, J.},
  month = nov,
  year = {2010},
  keywords = {Java,Libraries,Benchmark testing,software development,codes,curated code corpus,curated collection,Empirical studies,experimental infrastructure,Java code,open source Java systems,Pragmatics,Qualitas Corpus,Software,software engineering,Software engineering},
  pages = {336-345},
  file = {/Users/luigi/work/zotero/storage/Z3DLRYSK/Tempero et al. - 2010 - The Qualitas Corpus A Curated Collection of Java .pdf;/Users/luigi/work/zotero/storage/2T9DTH3U/5693210.html}
}

@inproceedings{counsellStrategyCodeSmell2010,
  address = {New York, NY, USA},
  series = {{{WETSoM}} '10},
  title = {Is a {{Strategy}} for {{Code Smell Assessment Long Overdue}}?},
  isbn = {978-1-60558-976-3},
  doi = {10.1145/1809223.1809228},
  abstract = {Code smells reflect code decay and, as such, developers should seek to eradicate such smells through application of 'deodorant' in the form of one or more refactorings. However, a dearth of studies exploring code smells either theoretically or empirically suggests that there are reasons why smell eradication is neither being applied in anger, nor the subject of significant research. In this paper, we present three studies as supporting evidence for this claim. The first is an analysis of a set of five, open-source Java systems, the second an empirical study of a sub-system of a proprietary, C\# web-based application and the third, a theoretical enumeration of smell-related refactorings. Key findings of the study were first, that developers seemed to avoid eradicating superficially 'simple' smells in favor of more 'obscure' ones; second, a wide range of conflicts and anomalies soon emerged when trying to identify smelly code. Finally, perceived effort to eradicate a smell may be a key factor. The study highlights the need for a clearer research strategy on the issue of code smells and all aspects of their identification and measurement.},
  booktitle = {Proceedings of the 2010 {{ICSE Workshop}} on {{Emerging Trends}} in {{Software Metrics}}},
  publisher = {{ACM}},
  author = {Counsell, S. and Hierons, R. M. and Hamza, H. and Black, S. and Durrand, M.},
  year = {2010},
  keywords = {Java,C\#,code smells,empirical studies,refactoring},
  pages = {32--38},
  file = {/Users/luigi/work/zotero/storage/M2SJU7SP/Counsell et al. - 2010 - Is a Strategy for Code Smell Assessment Long Overd.pdf}
}

@inproceedings{jonssonTamingCodeExplosion2011,
  address = {New York, NY, USA},
  series = {{{PEPM}} '11},
  title = {Taming {{Code Explosion}} in {{Supercompilation}}},
  isbn = {978-1-4503-0485-6},
  doi = {10.1145/1929501.1929507},
  abstract = {Supercompilation algorithms can perform great optimizations but sometimes suffer from the problem of code explosion. This results in huge binaries which might hurt the performance on a modern processor. We present a supercompilation algorithm that is fast enough to speculatively supercompile expressions and discard the result if it turned out bad. This allows us to supercompile large parts of the imaginary and spectral parts of nofib in a matter of seconds while keeping the binary size increase below 5\%.},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  publisher = {{ACM}},
  author = {Jonsson, Peter A. and Nordlander, Johan},
  year = {2011},
  keywords = {supercompilation,haskell,deforestation},
  pages = {33--42},
  file = {/Users/luigi/work/zotero/storage/TFWSKD9T/Jonsson and Nordlander - 2011 - Taming Code Explosion in Supercompilation.pdf}
}

@inproceedings{wintherGuardedTypePromotion2011,
  address = {New York, NY, USA},
  series = {{{FTfJP}} '11},
  title = {Guarded {{Type Promotion}}: {{Eliminating Redundant Casts}} in {{Java}}},
  isbn = {978-1-4503-0893-9},
  shorttitle = {Guarded {{Type Promotion}}},
  doi = {10.1145/2076674.2076680},
  abstract = {In Java, explicit casts are ubiquitous since they bridge the gap between compile-time and runtime type safety. Since casts potentially throw a ClassCastException, many programmers use a defensive programming style of guarded casts. In this programming style casts are protected by a preceding conditional using the instanceof operator and thus the cast type is redundantly mentioned twice. We propose a new typing rule for Java called Guarded Type Promotion aimed at eliminating the need for the explicit casts when guarded. This new typing rule is backward compatible and has been fully implemented in a Java 6 compiler. Through our extensive testing of real-life code we show that guarded casts account for approximately one fourth of all casts and that Guarded Type Promotion can eliminate the need for 95 percent of these guarded casts.},
  booktitle = {Proceedings of the 13th {{Workshop}} on {{Formal Techniques}} for {{Java}}-{{Like Programs}}},
  publisher = {{ACM}},
  author = {Winther, Johnni},
  year = {2011},
  keywords = {Java,type cast,type checking},
  pages = {6:1--6:8},
  file = {/Users/luigi/work/zotero/storage/2LVFNFIF/Winther - 2011 - Guarded Type Promotion Eliminating Redundant Cast.pdf}
}

@inproceedings{gligoricCoDeSeFastDeserialization2011,
  address = {New York, NY, USA},
  series = {{{ISSTA}} '11},
  title = {{{CoDeSe}}: {{Fast Deserialization}} via {{Code Generation}}},
  isbn = {978-1-4503-0562-4},
  shorttitle = {{{CoDeSe}}},
  doi = {10.1145/2001420.2001456},
  abstract = {Many tools for automated testing, model checking, and debugging store and restore program states multiple times. Storing/restoring a program state is commonly done with serialization/deserialization. Traditionally, the format for stored states is based on data: serialization generates the data that encodes the state, and deserialization interprets this data to restore the state. We propose a new approach, called CoDeSe, where the format for stored states is based on code: serialization generates code whose execution restores the state, and deserialization simply executes the code. We implemented CoDeSe in Java and performed a number of experiments on deserialization of states. CoDeSe provides on average more than 6X speedup over the highly optimized deserialization from the standard Java library. Our new format also allows simple parallel deserialization that can provide additional speedup on top of the sequential CoDeSe but only for larger states.},
  booktitle = {Proceedings of the 2011 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  publisher = {{ACM}},
  author = {Gligoric, Milos and Marinov, Darko and Kamin, Sam},
  year = {2011},
  keywords = {code generation,deserialization},
  pages = {298--308},
  file = {/Users/luigi/work/zotero/storage/TZAETHML/Gligoric et al. - 2011 - CoDeSe Fast Deserialization via Code Generation.pdf}
}

@inproceedings{parninJavaGenericsAdoption2011,
  address = {New York, NY, USA},
  series = {{{MSR}} '11},
  title = {Java {{Generics Adoption}}: {{How New Features Are Introduced}}, {{Championed}}, or {{Ignored}}},
  isbn = {978-1-4503-0574-7},
  shorttitle = {Java {{Generics Adoption}}},
  doi = {10.1145/1985441.1985446},
  abstract = {Support for generic programming was added to the Java language in 2004, representing perhaps the most significant change to one of the most widely used programming languages today. Researchers and language designers anticipated this addition would relieve many long-standing problems plaguing developers, but surprisingly, no one has yet measured whether generics actually provide such relief. In this paper, we report on the first empirical investigation into how Java generics have been integrated into open source software by automatically mining the history of 20 popular open source Java programs, traversing more than 500 million lines of code in the process. We evaluate five hypotheses, each based on assertions made by prior researchers, about how Java developers use generics. For example, our results suggest that generics do not significantly reduce the number of type casts and that generics are usually adopted by a single champion in a project, rather than all committers.},
  booktitle = {Proceedings of the 8th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Parnin, Chris and Bird, Christian and {Murphy-Hill}, Emerson},
  year = {2011},
  keywords = {generics,java,languages,post-mortem analysis},
  pages = {3--12},
  file = {/Users/luigi/work/zotero/storage/IPUEDD6G/Parnin et al. - 2011 - Java Generics Adoption How New Features Are Intro.pdf}
}

@inproceedings{richardsEvalThatMen2011,
  address = {Berlin, Heidelberg},
  series = {{{ECOOP}}'11},
  title = {The {{Eval That Men Do}}: {{A Large}}-Scale {{Study}} of the {{Use}} of {{Eval}} in {{Javascript Applications}}},
  isbn = {978-3-642-22654-0},
  shorttitle = {The {{Eval That Men Do}}},
  abstract = {Transforming text into executable code with a function such as Java-Script's eval endows programmers with the ability to extend applications, at any time, and in almost any way they choose. But, this expressive power comes at a price: reasoning about the dynamic behavior of programs that use this feature becomes challenging. Any ahead-of-time analysis, to remain sound, is forced to make pessimistic assumptions about the impact of dynamically created code. This pessimism affects the optimizations that can be applied to programs and significantly limits the kinds of errors that can be caught statically and the security guarantees that can be enforced. A better understanding of how eval is used could lead to increased performance and security. This paper presents a large-scale study of the use of eval in JavaScript-based web applications. We have recorded the behavior of 337 MB of strings given as arguments to 550,358 calls to the eval function exercised in over 10,000 web sites. We provide statistics on the nature and content of strings used in eval expressions, as well as their provenance and data obtained by observing their dynamic behavior.},
  booktitle = {Proceedings of the 25th {{European Conference}} on {{Object}}-Oriented {{Programming}}},
  publisher = {{Springer-Verlag}},
  author = {Richards, Gregor and Hammer, Christian and Burg, Brian and Vitek, Jan},
  year = {2011},
  pages = {52--78},
  file = {/Users/luigi/work/zotero/storage/YVZCQDBG/10.1007%2F978-3-642-22655-7_4.pdf}
}

@inproceedings{bacchelliExtractingStructuredData2011,
  title = {Extracting Structured Data from Natural Language Documents with Island Parsing},
  doi = {10.1109/ASE.2011.6100103},
  abstract = {The design and evolution of a software system leave traces in various kinds of artifacts. In software, produced by humans for humans, many artifacts are written in natural language by people involved in the project. Such entities contain structured information which constitute a valuable source of knowledge for analyzing and comprehending a system's design and evolution. However, the ambiguous and informal nature of narrative is a serious challenge in gathering such information, which is scattered throughout natural language text. We present an approach-based on island parsing-to recognize and enable the parsing of structured information that occur in natural language artifacts. We evaluate our approach by applying it to mailing lists pertaining to three software systems. We show that this approach allows us to extract structured data from emails with high precision and recall.},
  booktitle = {2011 26th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}} 2011)},
  author = {Bacchelli, A. and Cleve, A. and Lanza, M. and Mocci, A.},
  month = nov,
  year = {2011},
  keywords = {Java,Data mining,grammars,History,data mining,Electronic mail,Grammar,island parsing,natural language document,natural language processing,Natural languages,Production,structured information},
  pages = {476-479},
  file = {/Users/luigi/work/zotero/storage/UZCFXMZC/Bacchelli et al. - 2011 - Extracting structured data from natural language d.pdf;/Users/luigi/work/zotero/storage/5BKB8IXM/6100103.html}
}

@inproceedings{stuchlikStaticVsDynamic2011,
  address = {New York, NY, USA},
  series = {{{DLS}} '11},
  title = {Static vs. {{Dynamic Type Systems}}: {{An Empirical Study About}} the {{Relationship Between Type Casts}} and {{Development Time}}},
  isbn = {978-1-4503-0939-4},
  shorttitle = {Static vs. {{Dynamic Type Systems}}},
  doi = {10.1145/2047849.2047861},
  abstract = {Static type systems are essential in computer science. However, there is hardly any knowledge about the impact of type systems on the resulting piece of software. While there are authors that state that static types increase the development speed, other authors argue the other way around. A previous experiment suggests that there are multiple factors that play a role for a comparison of statically and dynamically typed language. As a follow-up, this paper presents an empirical study with 21 subjects that compares programming tasks performed in Java and Groovy - programming tasks where the number of expected type casts vary in the statically typed language. The result of the study is, that the dynamically typed group solved the complete programming tasks significantly faster for most tasks - but that for larger tasks with a higher number of type casts no significant difference could be found.},
  booktitle = {Proceedings of the 7th {{Symposium}} on {{Dynamic Languages}}},
  publisher = {{ACM}},
  author = {Stuchlik, Andreas and Hanenberg, Stefan},
  year = {2011},
  keywords = {software engineering,empirical research,programming language research,type systems},
  pages = {97--106},
  file = {/Users/luigi/work/zotero/storage/65LVDU64/Stuchlik and Hanenberg - 2011 - Static vs. Dynamic Type Systems An Empirical Stud.pdf}
}

@inproceedings{jensenRemedyingEvalThat2012,
  address = {New York, NY, USA},
  series = {{{ISSTA}} 2012},
  title = {Remedying the {{Eval That Men Do}}},
  isbn = {978-1-4503-1454-1},
  doi = {10.1145/2338965.2336758},
  abstract = {A range of static analysis tools and techniques have been developed in recent years with the aim of helping JavaScript web application programmers produce code that is more robust, safe, and efficient. However, as shown in a previous large-scale study, many web applications use the JavaScript eval function to dynamically construct code from text strings in ways that obstruct existing static analyses. As a consequence, the analyses either fail to reason about the web applications or produce unsound or useless results.   We present an approach to soundly and automatically transform many common uses of eval into other language constructs to enable sound static analysis of web applications. By eliminating calls to eval, we expand the applicability of static analysis for JavaScript web applications in general.   The transformation we propose works by incorporating a refactoring technique into a dataflow analyzer. We report on our experimental results with a small collection of programming patterns extracted from popular web sites. Although there are inevitably cases where the transformation must give up, our technique succeeds in eliminating many nontrivial occurrences of eval.},
  booktitle = {Proceedings of the 2012 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  publisher = {{ACM}},
  author = {Jensen, Simon Holm and Jonsson, Peter A. and M\o{}ller, Anders},
  year = {2012},
  pages = {34--44},
  file = {/Users/luigi/work/zotero/storage/H9PRP9GC/Jensen et al. - 2012 - Remedying the Eval That Men Do.pdf}
}

@inproceedings{mayerEmpiricalStudyInfluence2012,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '12},
  title = {An {{Empirical Study}} of the {{Influence}} of {{Static Type Systems}} on the {{Usability}} of {{Undocumented Software}}},
  isbn = {978-1-4503-1561-6},
  doi = {10.1145/2384616.2384666},
  abstract = {Abstract Although the study of static and dynamic type systems plays a major role in research, relatively little is known about the impact of type systems on software development. Perhaps one of the more common arguments for static type systems in languages such as Java or C++ is that they require developers to annotate their code with type names, which is thus claimed to improve the documentation of software. In contrast, one common argument against static type systems is that they decrease flexibility, which may make them harder to use. While these arguments are found in the literature, rigorous empirical evidence is lacking. We report on a controlled experiment where 27 subjects performed programming tasks on an undocumented API with a static type system (requiring type annotations) as well as a dynamic type system (which does not). Our results show that for some tasks, programmers had faster completion times using a static type system, while for others, the opposite held. We conduct an exploratory study to try and theorize why.},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  publisher = {{ACM}},
  author = {Mayer, Clemens and Hanenberg, Stefan and Robbes, Romain and Tanter, \'Eric and Stefik, Andreas},
  year = {2012},
  keywords = {empirical research,type systems,programming languages},
  pages = {683--702},
  file = {/Users/luigi/work/zotero/storage/SPSTV566/Mayer et al. - 2012 - An Empirical Study of the Influence of Static Type.pdf}
}

@inproceedings{dyerDeclarativeVisitorsEase2013,
  address = {New York, NY, USA},
  series = {{{GPCE}} '13},
  title = {Declarative {{Visitors}} to {{Ease Fine}}-Grained {{Source Code Mining}} with {{Full History}} on {{Billions}} of {{AST Nodes}}},
  isbn = {978-1-4503-2373-4},
  doi = {10.1145/2517208.2517226},
  abstract = {Software repositories contain a vast wealth of information about software development. Mining these repositories has proven useful for detecting patterns in software development, testing hypotheses for new software engineering approaches, etc. Specifically, mining source code has yielded significant insights into software development artifacts and processes. Unfortunately, mining source code at a large-scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse-grained, or sacrifice studying the history of the code due to both human and computational scalability issues. In this paper we address the substantial challenges of mining source code: a) at a very large scale; b) at a fine-grained level of detail; and c) with full history information. To address these challenges, we present domain-specific language features for source code mining. Our language features are inspired by object-oriented visitors and provide a default depth-first traversal strategy along with two expressions for defining custom traversals. We provide an implementation of these features in the Boa infrastructure for software repository mining and describe a code generation strategy into Java code. To show the usability of our domain-specific language features, we reproduced over 40 source code mining tasks from two large-scale previous studies in just 2 person-weeks. The resulting code for these tasks show between 2.0x--4.8x reduction in code size. Finally we perform a small controlled experiment to gain insights into how easily mining tasks written using our language features can be understood, with no prior training. We show a substantial number of tasks (77\%) were understood by study participants, in about 3 minutes per task.},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Generative Programming}}: {{Concepts}} \& {{Experiences}}},
  publisher = {{ACM}},
  author = {Dyer, Robert and Rajan, Hridesh and Nguyen, Tien N.},
  year = {2013},
  keywords = {boa,source code mining,visitor pattern},
  pages = {23--32},
  file = {/Users/luigi/work/zotero/storage/2SY4FFZ5/Dyer et al. - 2013 - Declarative Visitors to Ease Fine-grained Source C.pdf;/Users/luigi/work/zotero/storage/9YPBK7KS/Dyer et al. - 2013 - Declarative visitors to ease fine-grained source c.pdf}
}

@inproceedings{gousiosGHTorentDatasetTool2013,
  address = {Piscataway, NJ, USA},
  series = {{{MSR}} '13},
  title = {The {{GHTorent Dataset}} and {{Tool Suite}}},
  isbn = {978-1-4673-2936-1},
  abstract = {During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it.},
  booktitle = {Proceedings of the 10th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{IEEE Press}},
  author = {Gousios, Georgios},
  year = {2013},
  pages = {233--236},
  file = {/Users/luigi/work/zotero/storage/H84M3ICU/Gousios - 2013 - The GHTorent Dataset and Tool Suite.pdf}
}

@inproceedings{mohamedinByteSTMVirtualMachineLevel2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {{{ByteSTM}}: {{Virtual Machine}}-{{Level Java Software Transactional Memory}}},
  isbn = {978-3-642-38492-9 978-3-642-38493-6},
  shorttitle = {{{ByteSTM}}},
  doi = {10.1007/978-3-642-38493-6_12},
  abstract = {We present ByteSTM, a virtual machine-level Java STM implementation that is built by extending the Jikes RVM. We modify Jikes RVM's optimizing compiler to transparently support implicit transactions. Being implemented at the VM-level, it accesses memory directly, avoids Java garbage collection overhead by manually managing memory for transactional metadata, and provides pluggable support for implementing different STM algorithms to the VM. Our experimental studies reveal throughput improvement over other non-VM STMs by 6\textendash{}70\% on micro-benchmarks and by 7\textendash{}60\% on macro-benchmarks.},
  language = {en},
  booktitle = {Coordination {{Models}} and {{Languages}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Mohamedin, Mohamed and Ravindran, Binoy and Palmieri, Roberto},
  month = jun,
  year = {2013},
  pages = {166-180},
  file = {/Users/luigi/work/zotero/storage/BP7RVPQA/Mohamedin et al. - 2013 - ByteSTM Virtual Machine-Level Java Software Trans.pdf;/Users/luigi/work/zotero/storage/VRHBRWK9/10.html}
}

@inproceedings{raemaekersMavenRepositoryDataset2013,
  title = {The {{Maven}} Repository Dataset of Metrics, Changes, and Dependencies},
  doi = {10.1109/MSR.2013.6624031},
  abstract = {We present the Maven Dependency Dataset (MDD), containing metrics, changes and dependencies of 148,253 jar files. Metrics and changes have been calculated at the level of individual methods, classes and packages of multiple library versions. A complete call graph is also presented which includes call, inheritance, containment and historical relationships between all units of the entire repository. In this paper, we describe our dataset and the methodology used to obtain it. We present different conceptual views of MDD and we also describe limitations and data quality issues that researchers using this data should be aware of.},
  booktitle = {2013 10th {{Working Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Raemaekers, S. and van Deursen, A. and Visser, J.},
  month = may,
  year = {2013},
  keywords = {Java,Data mining,Libraries,software libraries,data mining,Software,complete call graph,data quality issues,Dataset,Indexes,jar file changes,jar file dependencies,jar file metrics,library version packages,Maven repository,Maven repository dataset,MDD,Measurement,software metrics,software packages,Supercomputers},
  pages = {221-224},
  file = {/Users/luigi/work/zotero/storage/6IQXLTHP/Raemaekers et al. - 2013 - The Maven repository dataset of metrics, changes, .pdf;/Users/luigi/work/zotero/storage/EFD7IMIC/Raemaekers et al. - 2013 - The Maven repository dataset of metrics, changes, .pdf;/Users/luigi/work/zotero/storage/G84GZZZ7/6624031.html}
}

@inproceedings{marekDiSLExtensibleLanguage2012,
  address = {New York, NY, USA},
  series = {{{DSAL}} '12},
  title = {{{DiSL}}: {{An Extensible Language}} for {{Efficient}} and {{Comprehensive Dynamic Program Analysis}}},
  isbn = {978-1-4503-1128-1},
  shorttitle = {{{DiSL}}},
  doi = {10.1145/2162037.2162046},
  abstract = {Dynamic program analysis tools support numerous software engineering tasks, including profiling, debugging, and reverse engineering. Prevailing techniques for building dynamic analysis tools are based on low-level abstractions that make tool development tedious, error-prone, and expensive. To simplify the development of dynamic analysis tools, some researchers promoted the use of aspect-oriented programming (AOP). However, as mainstream AOP languages have not been designed to meet the requirements of dynamic analysis, the success of using AOP in this context remains limited. For example, in AspectJ, join points that are important for dynamic program analysis (e.g., the execution of bytecodes or basic blocks of code) are missing, access to reflective dynamic join\textasciitilde{}point information is expensive, data passing between woven advice in local variables is not supported, and the mixing of low-level bytecode instrumentation and high-level AOP code is not foreseen. In this talk, we present DiSL [1], a new domain-specific aspect language for bytecode instrumentation. DiSL uses Java annotation syntax such that standard Java compilers can be used for compiling DiSL code. The language features an open join point model, novel constructs inspired by weave-time evaluation of conditional join\textasciitilde{}points and by staged execution, and access to custom static and dynamic context information. Moreover, the DiSL weaver guarantees complete bytecode coverage. We have implemented several dynamic analysis tools in DiSL, including profilers for the inter- and intra-procedural control flow, debuggers, dynamic metrics collectors integrated in the Eclipse IDE to augment the static source views with dynamic information, and tools for workload characterization. These tools are concise and perform equally well as implementations using low-level techniques. DiSL has also been conceived as an intermediate language for future domain-specific analysis languages, as well as for AOP languages.},
  booktitle = {Proceedings of the {{Seventh Workshop}} on {{Domain}}-{{Specific Aspect Languages}}},
  publisher = {{ACM}},
  author = {Marek, Luk\'a{\v s} and Zheng, Yudi and Ansaloni, Danilo and Binder, Walter and Qi, Zhengwei and Tuma, Petr},
  year = {2012},
  keywords = {bytecode instrumentation,aspect-oriented programming,JVM,dynamic program analysis},
  pages = {27--28},
  file = {/Users/luigi/work/zotero/storage/SNUW3RAU/Marek et al. - 2012 - DiSL An Extensible Language for Efficient and Com.pdf}
}

@article{pukallJavAdaptorFlexibleRuntimeUpdates2013,
  title = {{{JavAdaptor}}-{{Flexible Runtime Updates}} of {{Java Applications}}},
  volume = {43},
  issn = {0038-0644},
  doi = {10.1002/spe.2107},
  abstract = {Software is changed frequently during its life cycle. New requirements come, and bugs must be fixed. To update an application, it usually must be stopped, patched, and restarted. This causes time periods of unavailability, which is always a problem for highly available applications. Even for the development of complex applications, restarts to test new program parts can be time consuming and annoying. Thus, we aim at dynamic software updates to update programs at runtime. There is a large body of research on dynamic software updates, but so far, existing approaches have shortcomings either in terms of flexibility or performance. In addition, some of them depend on specific runtime environments and dictate the program's architecture. We present JavAdaptor, the first runtime update approach based on Java that a offers flexible dynamic software updates, b is platform independent, c introduces only minimal performance overhead, and d does not dictate the program architecture. JavAdaptor combines schema changing class replacements by class renaming and caller updates with Java HotSwap using containers and proxies. It runs on top of all major standard Java virtual machines. We evaluate our approach's applicability and performance in non-trivial case studies and compare it with existing dynamic software update approaches. Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  number = {2},
  journal = {Softw. Pract. Exper.},
  author = {Pukall, Mario and K\"astner, Christian and Cazzola, Walter and G\"otz, Sebastian and Grebhahn, Alexander and Schr\"oter, Reimar and Saake, Gunter},
  month = feb,
  year = {2013},
  keywords = {dynamic software updates,program evolution,state migration: tool support},
  pages = {153--185},
  file = {/Users/luigi/work/zotero/storage/73H4BFZ6/Pukall_et_al-2013-Software__Practice_and_Experience.pdf}
}

@article{callauHowWhyDevelopers2013,
  title = {How (and Why) Developers Use the Dynamic Features of Programming Languages: The Case of Smalltalk},
  volume = {18},
  issn = {1382-3256, 1573-7616},
  shorttitle = {How (and Why) Developers Use the Dynamic Features of Programming Languages},
  doi = {10.1007/s10664-012-9203-2},
  abstract = {The dynamic and reflective features of programming languages are powerful constructs that programmers often mention as extremely useful. However, the ability to modify a program at runtime can be both a boon\textemdash{}in terms of flexibility\textemdash, and a curse\textemdash{}in terms of tool support. For instance, usage of these features hampers the design of type systems, the accuracy of static analysis techniques, or the introduction of optimizations by compilers. In this paper, we perform an empirical study of a large Smalltalk codebase\textemdash{}often regarded as the poster-child in terms of availability of these features\textemdash, in order to assess how much these features are actually used in practice, whether some are used more than others, and in which kinds of projects. In addition, we performed a qualitative analysis of a representative sample of usages of dynamic features in order to uncover (1) the principal reasons that drive people to use dynamic features, and (2) whether and how these dynamic feature usages can be removed or converted to safer usages. These results are useful to make informed decisions about which features to consider when designing language extensions or tool support.},
  language = {en},
  number = {6},
  journal = {Empirical Software Engineering},
  author = {Calla\'u, Oscar and Robbes, Romain and Tanter, \'Eric and R\"othlisberger, David},
  month = dec,
  year = {2013},
  pages = {1156-1194},
  file = {/Users/luigi/work/zotero/storage/ZZ2WU8Y3/Callaú et al. - 2013 - How (and why) developers use the dynamic features .pdf;/Users/luigi/work/zotero/storage/HPIRJ2GK/10.html}
}

@inproceedings{dyerBoaLanguageInfrastructure2013,
  title = {Boa: {{A}} Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories},
  shorttitle = {Boa},
  doi = {10.1109/ICSE.2013.6606588},
  abstract = {In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers.},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Dyer, R. and Nguyen, H. A. and Rajan, H. and Nguyen, T. N.},
  month = may,
  year = {2013},
  keywords = {Java,Protocols,Runtime,Data mining,Libraries,Software,software packages,Alexandria new library,Boa,Boa infrastructure,domain specific language,ease of use,GitHub,Google code,Internet,lower barrier to entry,mining,mining software repository,MSR related hypotheses,repository,reproducible,scalable,software,software centric world,SourceForge,systematic extraction,ultra-large-scale software repositories analysis},
  pages = {422-431},
  file = {/Users/luigi/work/zotero/storage/27Z4QYMP/Dyer et al. - 2013 - Boa A language and infrastructure for analyzing u.pdf;/Users/luigi/work/zotero/storage/698PK2BX/Dyer et al. - 2013 - Boa A language and infrastructure for analyzing u.pdf;/Users/luigi/work/zotero/storage/DASN4FQY/6606588.html}
}

@article{parninAdoptionUseJava2013,
  title = {Adoption and Use of {{Java}} Generics},
  volume = {18},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-012-9236-6},
  abstract = {Support for generic programming was added to the Java language in 2004, representing perhaps the most significant change to one of the most widely used programming languages today. Researchers and language designers anticipated this addition would relieve many long-standing problems plaguing developers, but surprisingly, no one has yet measured how generics have been adopted and used in practice. In this paper, we report on the first empirical investigation into how Java generics have been integrated into open source software by automatically mining the history of 40 popular open source Java programs, traversing more than 650 million lines of code in the process. We evaluate five hypotheses and research questions about how Java developers use generics. For example, our results suggest that generics sometimes reduce the number of type casts and that generics are usually adopted by a single champion in a project, rather than all committers. We also offer insights into why some features may be adopted sooner and others features may be held back.},
  language = {en},
  number = {6},
  journal = {Empirical Software Engineering},
  author = {Parnin, Chris and Bird, Christian and {Murphy-Hill}, Emerson},
  month = dec,
  year = {2013},
  pages = {1047-1089},
  file = {/Users/luigi/work/zotero/storage/3QIDK2PH/Parnin et al. - 2013 - Adoption and use of Java generics.pdf;/Users/luigi/work/zotero/storage/J7SWD45J/10.html}
}

@article{hallKardashianIndexMeasure2014,
  title = {The {{Kardashian}} Index: A Measure of Discrepant Social Media Profile for Scientists},
  volume = {15},
  issn = {1474-760X},
  shorttitle = {The {{Kardashian}} Index},
  doi = {10.1186/s13059-014-0424-0},
  abstract = {In the era of social media there are now many different ways that a scientist can build their public profile; the publication of high-quality scientific papers being just one. While social media is a valuable tool for outreach and the sharing of ideas, there is a danger that this form of communication is gaining too high a value and that we are losing sight of key metrics of scientific value, such as citation indices. To help quantify this, I propose the `Kardashian Index', a measure of discrepancy between a scientist's social media profile and publication record based on the direct comparison of numbers of citations and Twitter followers.},
  journal = {Genome Biology},
  author = {Hall, Neil},
  month = jul,
  year = {2014},
  pages = {424},
  file = {/Users/luigi/work/zotero/storage/SWBETZRL/Hall - 2014 - The Kardashian index a measure of discrepant soci.pdf;/Users/luigi/work/zotero/storage/S8BEALCR/s13059-014-0424-0.html}
}

@inproceedings{stefikWhatFoundationEvidence2014,
  address = {New York, NY, USA},
  series = {{{ICPC}} 2014},
  title = {What Is the {{Foundation}} of {{Evidence}} of {{Human Factors Decisions}} in {{Language Design}}? {{An Empirical Study}} on {{Programming Language Workshops}}},
  isbn = {978-1-4503-2879-1},
  shorttitle = {What Is the {{Foundation}} of {{Evidence}} of {{Human Factors Decisions}} in {{Language Design}}?},
  doi = {10.1145/2597008.2597154},
  abstract = {In recent years, the programming language design community has engaged in rigorous debate on the role of empirical evidence in the design of general purpose programming languages. Some scholars contend that the language community has failed to embrace a form of evidence that is non-controversial in other disciplines (e.g., medicine, biology, psychology, sociology, physics, chemistry), while others argue that a science of language design is unrealistic. While the discussion will likely persist for some time, we begin here a systematic evaluation of the use of empirical evidence with human users, documenting, paper-by-paper, the evidence provided for human factors decisions, beginning with 359 papers from the workshops PPIG, Plateau, and ESP. This preliminary work provides the following contributions: an analysis of the 1) overall quantity and quality of empirical evidence used in the workshops, and of the 2) overall significant challenges to reliably coding academic papers. We hope that, once complete, this long-term research project will serve as a practical catalog designers can use when evaluating the impact of a language feature on human users.},
  booktitle = {Proceedings of the {{22Nd International Conference}} on {{Program Comprehension}}},
  publisher = {{ACM}},
  author = {Stefik, Andreas and Hanenberg, Stefan and McKenney, Mark and Andrews, Anneliese and Yellanki, Srinivas Kalyan and Siebert, Susanna},
  year = {2014},
  keywords = {Empirical Evidence,Meta-analysis,The Programming Language Wars},
  pages = {223--231},
  file = {/Users/luigi/work/zotero/storage/4KUAXMTV/Stefik et al. - 2014 - What is the Foundation of Evidence of Human Factor.pdf}
}

@inproceedings{marekShadowVMRobustComprehensive2013,
  address = {New York, NY, USA},
  series = {{{GPCE}} '13},
  title = {{{ShadowVM}}: {{Robust}} and {{Comprehensive Dynamic Program Analysis}} for the {{Java Platform}}},
  isbn = {978-1-4503-2373-4},
  shorttitle = {{{ShadowVM}}},
  doi = {10.1145/2517208.2517219},
  abstract = {Dynamic analysis tools are often implemented using instrumentation, particularly on managed runtimes including the Java Virtual Machine (JVM). Performing instrumentation robustly is especially complex on such runtimes: existing frameworks offer limited coverage and poor isolation, while previous work has shown that apparently innocuous instrumentation can cause deadlocks or crashes in the observed application. This paper describes ShadowVM, a system for instrumentation-based dynamic analyses on the JVM which combines a number of techniques to greatly improve both isolation and coverage. These centre on the offload of analysis to a separate process; we believe our design is the first system to enable genuinely full bytecode coverage on the JVM. We describe a working implementation, and use a case study to demonstrate its improved coverage and to evaluate its runtime overhead.},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Generative Programming}}: {{Concepts}} \& {{Experiences}}},
  publisher = {{ACM}},
  author = {Marek, Luk\'a{\v s} and Kell, Stephen and Zheng, Yudi and Bulej, Lubom\'ir and Binder, Walter and T\r{u}ma, Petr and Ansaloni, Danilo and Sarimbekov, Aibek and Sewe, Andreas},
  year = {2013},
  keywords = {dynamic analysis,instrumentation,jvm},
  pages = {105--114},
  file = {/Users/luigi/work/zotero/storage/CWFCG7TE/Marek et al. - 2013 - ShadowVM Robust and Comprehensive Dynamic Program.pdf}
}

@inproceedings{petersenEmpiricalComparisonStatic2014,
  address = {New York, NY, USA},
  series = {{{ICPC}} 2014},
  title = {An {{Empirical Comparison}} of {{Static}} and {{Dynamic Type Systems}} on {{API Usage}} in the {{Presence}} of an {{IDE}}: {{Java}} vs. {{Groovy}} with {{Eclipse}}},
  isbn = {978-1-4503-2879-1},
  shorttitle = {An {{Empirical Comparison}} of {{Static}} and {{Dynamic Type Systems}} on {{API Usage}} in the {{Presence}} of an {{IDE}}},
  doi = {10.1145/2597008.2597152},
  abstract = {Several studies have concluded that static type systems offer an advantage over dynamic type systems for programming tasks involving the discovery of a new API. However, these studies did not take into account modern IDE features; the advanced navigation and code completion techniques available in modern IDEs could drastically alter their conclusions. This study describes an experiment that compares the usage of an unknown API using Java and Groovy using the IDE Eclipse. It turns out that the previous finding that static type systems improve the usability of an unknown API still holds, even in the presence of a modern IDE.},
  booktitle = {Proceedings of the {{22Nd International Conference}} on {{Program Comprehension}}},
  publisher = {{ACM}},
  author = {Petersen, Pujan and Hanenberg, Stefan and Robbes, Romain},
  year = {2014},
  keywords = {empirical research,type systems,programming languages},
  pages = {212--222},
  file = {/Users/luigi/work/zotero/storage/E2NHYDB3/Petersen et al. - 2014 - An Empirical Comparison of Static and Dynamic Type.pdf}
}

@inproceedings{dyerMiningBillionsAST2014,
  address = {New York, NY, USA},
  series = {{{ICSE}} 2014},
  title = {Mining {{Billions}} of {{AST Nodes}} to {{Study Actual}} and {{Potential Usage}} of {{Java Language Features}}},
  isbn = {978-1-4503-2756-5},
  doi = {10.1145/2568225.2568295},
  abstract = {Programming languages evolve over time, adding additional language features to simplify common tasks and make the language easier to use. For example, the Java Language Specification has four editions and is currently drafting a fifth. While the addition of language features is driven by an assumed need by the community (often with direct requests for such features), there is little empirical evidence demonstrating how these new features are adopted by developers once released. In this paper, we analyze over 31k open-source Java projects representing over 9 million Java files, which when parsed contain over 18 billion AST nodes. We analyze this corpus to find uses of new Java language features over time. Our study gives interesting insights, such as: there are millions of places features could potentially be used but weren't; developers convert existing code to use new features; and we found thousands of instances of potential resource handling bugs.},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Dyer, Robert and Rajan, Hridesh and Nguyen, Hoan Anh and Nguyen, Tien N.},
  year = {2014},
  keywords = {Java,empirical study,language feature use,software mining},
  pages = {779--790},
  file = {/Users/luigi/work/zotero/storage/BB2H2QVF/Dyer et al. - 2014 - Mining billions of AST nodes to study actual and p.pdf;/Users/luigi/work/zotero/storage/BTC3S8VE/Dyer et al. - 2014 - Mining Billions of AST Nodes to Study Actual and P.pdf}
}

@inproceedings{gorlaCheckingAppBehavior2014,
  address = {New York, NY, USA},
  series = {{{ICSE}} 2014},
  title = {Checking {{App Behavior Against App Descriptions}}},
  isbn = {978-1-4503-2756-5},
  doi = {10.1145/2568225.2568276},
  abstract = {How do we know a program does what it claims to do? After clustering Android apps by their description topics, we identify outliers in each cluster with respect to their API usage. A "weather" app that sends messages thus becomes an anomaly; likewise, a "messaging" app would typically not be expected to access the current location. Applied on a set of 22,500+ Android applications, our CHABADA prototype identified several anomalies; additionally, it flagged 56\% of novel malware as such, without requiring any known malware patterns.},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Gorla, Alessandra and Tavecchia, Ilaria and Gross, Florian and Zeller, Andreas},
  year = {2014},
  keywords = {Android,clustering,description analysis,malware detection},
  pages = {1025--1035},
  file = {/Users/luigi/work/zotero/storage/N63DCL4D/Gorla et al. - 2014 - Checking App Behavior Against App Descriptions.pdf}
}

@inproceedings{gousiosLeanGHTorrentGitHub2014,
  address = {New York, NY, USA},
  series = {{{MSR}} 2014},
  title = {Lean {{GHTorrent}}: {{GitHub Data}} on {{Demand}}},
  isbn = {978-1-4503-2863-0},
  shorttitle = {Lean {{GHTorrent}}},
  doi = {10.1145/2597073.2597126},
  abstract = {In recent years, GitHub has become the largest code host in the world, with more than 5M developers collaborating across 10M repositories. Numerous popular open source projects (such as Ruby on Rails, Homebrew, Bootstrap, Django or jQuery) have chosen GitHub as their host and have migrated their code base to it. GitHub offers a tremendous research potential. For instance, it is a flagship for current open source development, a place for developers to showcase their expertise to peers or potential recruiters, and the platform where social coding features or pull requests emerged. However, GitHub data is, to date, largely underexplored. To facilitate studies of GitHub, we have created GHTorrent, a scalable, queriable, offline mirror of the data offered through the GitHub REST API. In this paper we present a novel feature of GHTorrent designed to offer customisable data dumps on demand. The new GHTorrent data-on-demand service offers users the possibility to request via a web form up-to-date GHTorrent data dumps for any collection of GitHub repositories. We hope that by offering customisable GHTorrent data dumps we will not only lower the "barrier for entry" even further for researchers interested in mining GitHub data (thus encourage researchers to intensify their mining efforts), but also enhance the replicability of GitHub studies (since a snapshot of the data on which the results were obtained can now easily accompany each study).},
  booktitle = {Proceedings of the 11th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Gousios, Georgios and Vasilescu, Bogdan and Serebrenik, Alexander and Zaidman, Andy},
  year = {2014},
  keywords = {GitHub,data on demand,dataset},
  pages = {384--387},
  file = {/Users/luigi/work/zotero/storage/A42M4X22/Gousios et al. - 2014 - Lean GHTorrent GitHub data on demand.pdf;/Users/luigi/work/zotero/storage/HP67IK6T/Gousios et al. - 2014 - Lean GHTorrent GitHub Data on Demand.pdf}
}

@inproceedings{ponzanelliStORMeDStackOverflow2015,
  title = {{{StORMeD}}: {{Stack Overflow Ready Made Data}}},
  shorttitle = {{{StORMeD}}},
  doi = {10.1109/MSR.2015.67},
  abstract = {Stack Overflow is the de facto Question and Answer (Q\&A) website for developers, and it has been used in many approaches by software engineering researchers to mine useful data. However, the contents of a Stack Overflow discussion are inherently heterogeneous, mixing natural language, source code, stack traces and configuration files in XML or JSON format. We constructed a full island grammar capable of modeling the set of 700,000 Stack Overflow discussions talking about Java, building a heterogeneous abstract syntax tree (H-AST) of each post (question, answer or comment) in a discussion. The resulting dataset models every Stack Overflow discussion, providing a full H-AST for each type of structured fragment (i.e., JSON, XML, Java, Stack traces), and complementing this information with a set of basic meta-information like term frequency to enable natural language analyses. Our dataset allows the end-user to perform combined analyses of the Stack Overflow by visiting the H-AST of a discussion.},
  booktitle = {2015 {{IEEE}}/{{ACM}} 12th {{Working Conference}} on {{Mining Software Repositories}}},
  author = {Ponzanelli, L. and Mocci, A. and Lanza, M.},
  month = may,
  year = {2015},
  keywords = {Java,Data mining,source code,Software,software engineering,Grammar,island parsing,Natural languages,configuration files,Data models,h-ast,heterogeneous abstract syntax tree,JSON format,natural language,question and answer Website,question answering (information retrieval),software engineering researchers,stack overflow ready made data,stack traces,StORMeD,term frequency,unstructured data,Web sites,XML},
  pages = {474-477},
  file = {/Users/luigi/work/zotero/storage/GSDBBEKF/Ponzanelli et al. - 2015 - StORMeD Stack Overflow Ready Made Data.pdf;/Users/luigi/work/zotero/storage/ZAFQEIGF/7180121.html}
}

@inproceedings{sawantDatasetAPIUsage2015,
  title = {A {{Dataset}} for {{API Usage}}},
  doi = {10.1109/MSR.2015.75},
  abstract = {An Application Programming Interface (API) provides a specific set of functionalities to a developer. The main aim of an API is to encourage the reuse of already existing functionality. There has been some work done into API popularity trends, API evolution and API usage. For all the aforementioned research avenues there has been a need to mine the usage of an API in order to perform any kind of analysis. Each one of the approaches that has been employed in the past involved a certain degree of inaccuracy as there was no type check that takes place. We introduce an approach that takes type information into account while mining API method invocations and annotation usages. This approach accurately makes a connection between a method invocation and the class of the API to which the method belongs to. We try collecting as many usages of an API as possible, this is achieved by targeting projects hosted on GitHub. Additionally, we look at the history of every project to collect the usage of an API from earliest version onwards. By making such a large and rich dataset public, we hope to stimulate some more research in the field of APIs with the aid of accurate API usage samples.},
  booktitle = {2015 {{IEEE}}/{{ACM}} 12th {{Working Conference}} on {{Mining Software Repositories}}},
  author = {Sawant, A. A. and Bacchelli, A.},
  month = may,
  year = {2015},
  keywords = {Java,Data mining,Libraries,History,application program interfaces,Software,GitHub,dataset,API evolution,API method annotation usage mining,API method invocationusage mining,API popularity trends,API usage,application programming interface,Databases,functionality reuse,Market research,public dataset,software reusability},
  pages = {506-509},
  file = {/Users/luigi/work/zotero/storage/8FXAQWVX/Sawant and Bacchelli - 2015 - A Dataset for API Usage.pdf;/Users/luigi/work/zotero/storage/VSFRDBNM/7180129.html}
}

@article{livshitsDefenseSoundinessManifesto2015,
  title = {In {{Defense}} of {{Soundiness}}: {{A Manifesto}}},
  volume = {58},
  issn = {0001-0782},
  shorttitle = {In {{Defense}} of {{Soundiness}}},
  doi = {10.1145/2644805},
  abstract = {Soundy is the new sound.},
  number = {2},
  journal = {Commun. ACM},
  author = {Livshits, Benjamin and Sridharan, Manu and Smaragdakis, Yannis and Lhot\'ak, Ond{\v r}ej and Amaral, J. Nelson and Chang, Bor-Yuh Evan and Guyer, Samuel Z. and Khedker, Uday P. and M\o{}ller, Anders and Vardoulakis, Dimitrios},
  month = jan,
  year = {2015},
  pages = {44--46},
  file = {/Users/luigi/work/zotero/storage/K2DFZ9IK/Livshits et al. - 2015 - In Defense of Soundiness A Manifesto.pdf}
}

@inproceedings{stefanescuSemanticsbasedProgramVerifiers2016,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} 2016},
  title = {Semantics-Based {{Program Verifiers}} for {{All Languages}}},
  isbn = {978-1-4503-4444-9},
  doi = {10.1145/2983990.2984027},
  abstract = {We present a language-independent verification framework that can be instantiated with an operational semantics to automatically generate a program verifier. The framework treats both the operational semantics and the program correctness specifications as reachability rules between matching logic patterns, and uses the sound and relatively complete reachability logic proof system to prove the specifications using the semantics. We instantiate the framework with the semantics of one academic language, KernelC, as well as with three recent semantics of real-world languages, C, Java, and JavaScript, developed independently of our verification infrastructure. We evaluate our approach empirically and show that the generated program verifiers can check automatically the full functional correctness of challenging heap-manipulating programs implementing operations on list and tree data structures, like AVL trees. This is the first approach that can turn the operational semantics of real-world languages into correct-by-construction automatic verifiers.},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Stef{\u a}nescu, Andrei and Park, Daejun and Yuwen, Shijiao and Li, Yilong and Ro{\c s}u, Grigore},
  year = {2016},
  keywords = {K framework,matching logic,reachability logic},
  pages = {74--91},
  file = {/Users/luigi/work/zotero/storage/EHPB7J75/Stefănescu et al. - 2016 - Semantics-based Program Verifiers for All Language.pdf}
}

@inproceedings{nakshatriAnalysisExceptionHandling2016,
  address = {New York, NY, USA},
  series = {{{MSR}} '16},
  title = {Analysis of {{Exception Handling Patterns}} in {{Java Projects}}: {{An Empirical Study}}},
  isbn = {978-1-4503-4186-8},
  shorttitle = {Analysis of {{Exception Handling Patterns}} in {{Java Projects}}},
  doi = {10.1145/2901739.2903499},
  abstract = {Exception handling is a powerful tool provided by many programming languages to help developers deal with unforeseen conditions. Java is one of the few programming languages to enforce an additional compilation check on certain subclasses of the Exception class through checked exceptions. As part of this study, empirical data was extracted from software projects developed in Java. The intent is to explore how developers respond to checked exceptions and identify common patterns used by them to deal with exceptions, checked or otherwise. Bloch's book - "Effective Java" [1] was used as reference for best practices in exception handling - these recommendations were compared against results from the empirical data. Results of this study indicate that most programmers ignore checked exceptions and leave them unnoticed. Additionally, it is observed that classes higher in the exception class hierarchy are more frequently used as compared to specific exception subclasses.},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Nakshatri, Suman and Hegde, Maithri and Thandra, Sahithi},
  year = {2016},
  keywords = {Boa,best practices,Github,Java exception handling},
  pages = {500--503},
  file = {/Users/luigi/work/zotero/storage/5IMZFABG/Nakshatri et al. - 2016 - Analysis of Exception Handling Patterns in Java Pr.pdf}
}

@inproceedings{asaduzzamanHowDevelopersUse2016,
  address = {New York, NY, USA},
  series = {{{MSR}} '16},
  title = {How {{Developers Use Exception Handling}} in {{Java}}?},
  isbn = {978-1-4503-4186-8},
  doi = {10.1145/2901739.2903500},
  abstract = {Exception handling is a technique that addresses exceptional conditions in applications, allowing the normal flow of execution to continue in the event of an exception and/or to report on such events. Although exception handling techniques, features and bad coding practices have been discussed both in developer communities and in the literature, there is a marked lack of empirical evidence on how developers use exception handling in practice. In this paper we use the Boa language and infrastructure to analyze 274k open source Java projects in GitHub to discover how developers use exception handling. We not only consider various exception handling features but also explore bad coding practices and their relation to the experience of developers. Our results provide some interesting insights. For example, we found that bad exception handling coding practices are common in open source Java projects and regardless of experience all developers use bad exception handling coding practices.},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Asaduzzaman, Muhammad and Ahasanuzzaman, Muhammad and Roy, Chanchal K. and Schneider, Kevin A.},
  year = {2016},
  keywords = {Java,source code mining,exception,language feature},
  pages = {516--519},
  file = {/Users/luigi/work/zotero/storage/JD8WHSL5/Asaduzzaman et al. - 2016 - How Developers Use Exception Handling in Java.pdf}
}

@inproceedings{keryExaminingProgrammerPractices2016,
  address = {New York, NY, USA},
  series = {{{MSR}} '16},
  title = {Examining {{Programmer Practices}} for {{Locally Handling Exceptions}}},
  isbn = {978-1-4503-4186-8},
  doi = {10.1145/2901739.2903497},
  abstract = {Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed. A major complaint is that programmers often write minimal and low quality handlers. We used the Boa tool to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions. We found that programmers handle exceptions locally in catch blocks much of the time, rather than propagating by throwing an Exception. Programmers make heavy use of actions like Log, Print, Return, or Throw in catch blocks, and also frequently copy code between handlers. We found bad practices like empty catch blocks or catching Exception are indeed widespread. We discuss evidence that programmers may misjudge risk when catching Exception, and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception. Some of these issues might be addressed by future tools which autocomplete more complete handlers.},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Kery, Mary Beth and Le Goues, Claire and Myers, Brad A.},
  year = {2016},
  keywords = {Boa,GitHub,error handlers,Java exceptions},
  pages = {484--487},
  file = {/Users/luigi/work/zotero/storage/2H8HKBJF/Kery et al. - 2016 - Examining Programmer Practices for Locally Handlin.pdf}
}

@inproceedings{senaUnderstandingExceptionHandling2016,
  address = {New York, NY, USA},
  series = {{{MSR}} '16},
  title = {Understanding the {{Exception Handling Strategies}} of {{Java Libraries}}: {{An Empirical Study}}},
  isbn = {978-1-4503-4186-8},
  shorttitle = {Understanding the {{Exception Handling Strategies}} of {{Java Libraries}}},
  doi = {10.1145/2901739.2901757},
  abstract = {This paper presents an empirical study whose goal was to investigate the exception handling strategies adopted by Java libraries and their potential impact on the client applications. In this study, exception flow analysis was used in combination with manual inspections in order: (i) to characterize the exception handling strategies of existing Java libraries from the perspective of their users; and (ii) to identify exception handling anti-patterns. We extended an existing static analysis tool to reason about exception flows and handler actions of 656 Java libraries selected from 145 categories in the Maven Central Repository. The study findings suggest a current trend of a high number of undocumented API runtime exceptions (i.e., @throws in Javadoc) and Unintended Handler problem. Moreover, we could also identify a considerable number of occurrences of exception handling anti-patterns (e.g. Catch and Ignore). Finally, we have also analyzed 647 bug issues of the 7 most popular libraries and identified that 20.71\% of the reports are defects related to the problems of the exception strategies and anti-patterns identified in our study. The results of this study point to the need of tools to better understand and document the exception handling behavior of libraries.},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Sena, Dem\'ostenes and Coelho, Roberta and Kulesza, Uir\'a and Bonif\'acio, Rodrigo},
  year = {2016},
  keywords = {software libraries,empirical study,exception flows analysis,exception handling,exception handling anti-patterns,static analysis tool},
  pages = {212--222},
  file = {/Users/luigi/work/zotero/storage/HSRBLDIX/Sena et al. - 2016 - Understanding the Exception Handling Strategies of.pdf}
}

@article{lopesDeJaVuMapCode2017,
  title = {{{D\'eJ\`aVu}}: {{A Map}} of {{Code Duplicates}} on {{GitHub}}},
  volume = {1},
  issn = {2475-1421},
  shorttitle = {{{D\'eJ\`aVu}}},
  doi = {10.1145/3133908},
  abstract = {Previous studies have shown that there is a non-trivial amount of duplication in source code. This paper analyzes a corpus of 4.5 million non-fork projects hosted on GitHub representing over 428 million files written in Java, C++, Python, and JavaScript. We found that this corpus has a mere 85 million unique files. In other words, 70\% of the code on GitHub consists of clones of previously created files. There is considerable variation between language ecosystems. JavaScript has the highest rate of file duplication, only 6\% of the files are distinct. Java, on the other hand, has the least duplication, 60\% of files are distinct. Lastly, a project-level analysis shows that between 9\% and 31\% of the projects contain at least 80\% of files that can be found elsewhere. These rates of duplication have implications for systems built on open source software as well as for researchers interested in analyzing large code bases. As a concrete artifact of this study, we have created D\'ej\`aVu, a publicly available map of code duplicates in GitHub repositories.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Lopes, Cristina V. and Maj, Petr and Martins, Pedro and Saini, Vaibhav and Yang, Di and Zitny, Jakub and Sajnani, Hitesh and Vitek, Jan},
  month = oct,
  year = {2017},
  keywords = {Clone Detection,Source Code Analysis},
  pages = {84:1--84:28},
  file = {/Users/luigi/work/zotero/storage/P84LN2ED/Lopes et al. - 2017 - DéJàVu A Map of Code Duplicates on GitHub.pdf}
}

@inproceedings{harlinImpactUsingStaticType2017,
  title = {Impact of {{Using}} a {{Static}}-{{Type System}} in {{Computer Programming}}},
  doi = {10.1109/HASE.2017.17},
  abstract = {Static-type systems are a major topic in programming language research and the software industry because they should reduce the development time and increase the code quality. Additionally, they are predicted to decrease the number of defects in a code due to early error detection. However, only a few empirical experiments exist on the potential benefits of static-type systems in programming activities. This paper describes an experiment that tests whether static-type systems help developers create solutions for certain programming tasks. The results indicate that although the existence of a static-type system has no positive impact when subjects code a program from scratch, it does allow more errors in program debugging to be fixed.},
  booktitle = {2017 {{IEEE}} 18th {{International Symposium}} on {{High Assurance Systems Engineering}} ({{HASE}})},
  author = {Harlin, I. R. and Washizaki, H. and Fukazawa, Y.},
  month = jan,
  year = {2017},
  keywords = {Debugging,Computer languages,Programming,empirical study,Software,Encryption,Measurement uncertainty,program debugging,programming language,static-type systems},
  pages = {116-119},
  file = {/Users/luigi/work/zotero/storage/2VQI6AB3/Harlin et al. - 2017 - Impact of Using a Static-Type System in Computer P.pdf;/Users/luigi/work/zotero/storage/M5GFH7LE/7911881.html}
}

@inproceedings{dilorenzoIncrementalForestDSL2016,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} 2016},
  title = {Incremental {{Forest}}: {{A DSL}} for {{Efficiently Managing Filestores}}},
  isbn = {978-1-4503-4444-9},
  shorttitle = {Incremental {{Forest}}},
  doi = {10.1145/2983990.2984034},
  abstract = {File systems are often used to store persistent application data, but manipulating file systems using standard APIs can be difficult for programmers. Forest is a domain-specific language that bridges the gap between the on-disk and in-memory representations of file system data. Given a high-level specification of the structure, contents, and properties of a collection of directories, files, and symbolic links, the Forest compiler generates tools for loading, storing, and validating that data. Unfortunately, the initial implementation of Forest offered few mechanisms for controlling cost\^ae.g., the run-time system could load gigabytes of data, even if only a few bytes were needed. This paper introduces Incremental Forest (iForest), an extension to Forest with an explicit delay construct that programmers can use to precisely control costs. We describe the design of iForest using a series of running examples, present a formal semantics in a core calculus, and define a simple cost model that accurately characterizes the resources needed to use a given specification. We propose skins, which allow programmers to modify the delay structure of a specification in a compositional way, and develop a static type system for ensuring compatibility between specifications and skins. We prove the soundness and completeness of the type system and a variety of algebraic properties of skins. We describe an OCaml implementation and evaluate its performance on applications developed in collaboration with watershed hydrologists.},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {DiLorenzo, Jonathan and Zhang, Richard and Menzies, Erin and Fisher, Kathleen and Foster, Nate},
  year = {2016},
  keywords = {ad hoc data,Data description languages,domain-specific languages,file systems,filestores,laziness},
  pages = {252--271},
  file = {/Users/luigi/work/zotero/storage/U2TGPEQE/DiLorenzo et al. - 2016 - Incremental Forest A DSL for Efficiently Managing.pdf}
}

@article{arnoldSurveyAdaptiveOptimization2005,
  title = {A {{Survey}} of {{Adaptive Optimization}} in {{Virtual Machines}}},
  volume = {93},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2004.840305},
  abstract = {Virtual machines face significant performance challenges beyond those confronted by traditional static optimizers. First, portable program representations and dynamic language features, such as dynamic class loading, force the deferral of most optimizations until runtime, inducing runtime optimization overhead. Second, modular program representations preclude many forms of whole-program interprocedural optimization. Third, virtual machines incur additional costs for runtime services such as security guarantees and automatic memory management. To address these challenges, vendors have invested considerable resources into adaptive optimization systems in production virtual machines. Today, mainstream virtual machine implementations include substantial infrastructure for online monitoring and profiling, runtime compilation, and feedback-directed optimization. As a result, adaptive optimization has begun to mature as a widespread production-level technology. This paper surveys the evolution and current state of adaptive optimization technology in virtual machines.},
  number = {2},
  journal = {Proceedings of the IEEE},
  author = {Arnold, M. and Fink, S. J. and Grove, D. and Hind, M. and Sweeney, P. F.},
  month = feb,
  year = {2005},
  keywords = {Runtime,virtual machines,optimisation,Adaptive optimization,adaptive optimization systems,Adaptive systems,automatic memory management,Condition monitoring,Costs,dynamic optimization,feedback directed optimization,feedback-directed optimization (FDO),Memory management,modular program representations,online monitoring,online profiling,optimising compilers,Optimized production technology,production level technology,Production systems,runtime compilation,Security,software performance evaluation,static optimizers,Virtual machine monitors,Virtual machining},
  pages = {449-466},
  file = {/Users/luigi/work/zotero/storage/DNPCDW47/Arnold et al. - 2005 - A Survey of Adaptive Optimization in Virtual Machi.pdf;/Users/luigi/work/zotero/storage/K9ZEXRL4/1386662.html}
}

@inproceedings{anconaSemanticSubtypingImperative2016,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} 2016},
  title = {Semantic {{Subtyping}} for {{Imperative Object}}-Oriented {{Languages}}},
  isbn = {978-1-4503-4444-9},
  doi = {10.1145/2983990.2983992},
  abstract = {Semantic subtyping is an approach for defining sound and complete procedures to decide subtyping for expressive types, including union and intersection types; although it has been exploited especially in functional languages for XML based programming, recently it has been partially investigated in the context of object-oriented languages, and a sound and complete subtyping algorithm has been proposed for record types, but restricted to immutable fields, with union and recursive types interpreted coinductively to support cyclic objects. In this work we address the problem of studying semantic subtyping for imperative object-oriented languages, where fields can be mutable; in particular, we add read/write field annotations to record types, and, besides union, we consider intersection types as well, while maintaining coinductive interpretation of recursive types. In this way, we get a richer notion of type with a flexible subtyping relation, able to express a variety of type invariants useful for enforcing static guarantees for mutable objects. The addition of these features radically changes the defi- nition of subtyping, and, hence, the corresponding decision procedure, and surprisingly invalidates some subtyping laws that hold in the functional setting. We propose an intuitive model where mutable record val- ues contain type information to specify the values that can be correctly stored in fields. Such a model, and the correspond- ing subtyping rules, require particular care to avoid circularity between coinductive judgments and their negations which, by duality, have to be interpreted inductively. A sound and complete subtyping algorithm is provided, together with a prototype implementation.},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Ancona, Davide and Corradi, Andrea},
  year = {2016},
  keywords = {Read/Write Field Annotations,Semantic Subtyp- ing,Structural Types for Objects},
  pages = {568--587},
  file = {/Users/luigi/work/zotero/storage/SS9HNA3A/Ancona and Corradi - 2016 - Semantic Subtyping for Imperative Object-oriented .pdf}
}

@inproceedings{landmanChallengesStaticAnalysis2017,
  title = {Challenges for {{Static Analysis}} of {{Java Reflection}} - {{Literature Review}} and {{Empirical Study}}},
  doi = {10.1109/ICSE.2017.53},
  abstract = {The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78\% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Landman, D. and Serebrenik, A. and Vinju, J. J.},
  month = may,
  year = {2017},
  keywords = {Java,computational linguistics,program diagnostics,source code,application program interfaces,Software,Grammar,static analysis tool,abstract syntax trees,Bibliographies,code idioms,collected descriptive statistics,collections semantics,dynamic proxies,Empirical Study,Java projects,Java Reflection API,Java systems,literature review,nonexceptional exceptions,programmatic filtering meta objects,public domain software,real-world Java code analysis,Reflection,reflection code analysis,reflective Java code,Semantics,software behavior,software tools,source code (software),Static Analysis,Systematic Literature Review,Systematics,Tools,trees (mathematics)},
  pages = {507-518},
  file = {/Users/luigi/work/zotero/storage/64TAFNJG/Landman et al. - 2017 - Challenges for Static Analysis of Java Reflection .pdf;/Users/luigi/work/zotero/storage/XSVPXWLC/7985689.html}
}

@article{rayLargescaleStudyProgramming2017,
  title = {A {{Large}}-Scale {{Study}} of {{Programming Languages}} and {{Code Quality}} in {{GitHub}}},
  volume = {60},
  issn = {0001-0782},
  doi = {10.1145/3126905},
  abstract = {What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (728 projects, 63 million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static versus dynamic typing and allowing versus disallowing type confusion on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that disallowing type confusion is modestly better than allowing it, and among functional languages, static typing is also somewhat better than dynamic typing. We also find that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, for example, the preference of certain personality types for functional, static languages that disallow type confusion.},
  number = {10},
  journal = {Commun. ACM},
  author = {Ray, Baishakhi and Posnett, Daryl and Devanbu, Premkumar and Filkov, Vladimir},
  month = sep,
  year = {2017},
  pages = {91--100},
  file = {/Users/luigi/work/zotero/storage/WME4ZMHT/Ray et al. - 2017 - A Large-scale Study of Programming Languages and C.pdf}
}

@inproceedings{nystromScalaFrameworkSupercompilation2017,
  address = {New York, NY, USA},
  series = {{{SCALA}} 2017},
  title = {A {{Scala Framework}} for {{Supercompilation}}},
  isbn = {978-1-4503-5529-2},
  doi = {10.1145/3136000.3136011},
  abstract = {Supercompilation is a program transformation technique that attempts to evaluate programs as much as possible at compile time. Supercompilation has been used for theorem proving, function inversion, and most notably optimization, especially of functional programs. However, the technique has numerous practical problems that prevent it from being applied in mainstream compilers. In this paper, we describe a framework that can be used for experimenting with supercompilation techniques. Our framework allows supercompilers to be constructed directly from an interpreter. The user specifies the interpreter using rewrite rules and the framework handles termination checking, generalization, and residualization. We demonstrate the approach by implementing a supercompiler for JavaScript.},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN International Symposium}} on {{Scala}}},
  publisher = {{ACM}},
  author = {Nystrom, Nathaniel},
  year = {2017},
  keywords = {supercompilation,abstract machines,language frameworks,partial evaluation},
  pages = {18--28},
  file = {/Users/luigi/work/zotero/storage/J4X53MU6/Nystrom - 2017 - A Scala Framework for Supercompilation.pdf}
}

@article{wuHowTypeErrors2017,
  title = {How {{Type Errors Were Fixed}} and {{What Students Did}}?},
  volume = {1},
  issn = {2475-1421},
  doi = {10.1145/3133929},
  abstract = {Providing better supports for debugging type errors has been an active research area in the last three decades. Numerous approaches from different perspectives have been developed. Most approaches work well under certain conditions only, for example, when type errors are caused by single leaves and when type annotations are correct. However, the research community is still unaware of which conditions hold in practice and what the real debugging situations look like. We address this problem with a study of 3 program data sets, which were written in different years, using different compilers, and were of diverse sizes. They include more than 55,000 programs, among which more than 2,700 are ill typed. We investigated all the ill-typed programs, and our results indicate that current error debugging support is far from sufficient in practice since only about 35\% of all type errors were caused by single leaves. In addition, type annotations cannot always be trusted in error debuggers since about 30\% of the time type errors were caused by wrong type annotations. Our study also provides many insights about the debugging behaviors of students in functional programming, which could be exploited for developing more effective error debuggers.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Wu, Baijun and Chen, Sheng},
  month = oct,
  year = {2017},
  keywords = {empirical study,type inference,Type-error debugging},
  pages = {105:1--105:27},
  file = {/Users/luigi/work/zotero/storage/RDJHMS2I/Wu and Chen - 2017 - How Type Errors Were Fixed and What Students Did.pdf}
}

@article{dietrichContractsWildStudy2017,
  title = {Contracts in the {{Wild}}: {{A Study}} of {{Java Programs}} ({{Artifact}})},
  volume = {3},
  issn = {2509-8195},
  shorttitle = {Contracts in the {{Wild}}},
  doi = {10.4230/DARTS.3.2.6},
  number = {1},
  journal = {Dagstuhl Artifacts Series},
  author = {Dietrich, Jens and Pearce, David J. and Jezek, Kamil and Brada, Premek},
  year = {2017},
  keywords = {verification,java,assertions,design-by-contract,input validation,postconditions,preconditions,runtime checking},
  pages = {6:1--6:4},
  file = {/Users/luigi/work/zotero/storage/GAC558CJ/Dietrich et al. - 2017 - Contracts in the Wild A Study of Java Programs (A.pdf;/Users/luigi/work/zotero/storage/R668MBWW/7287.html}
}

@inproceedings{brandauerSpencerInteractiveHeap2017,
  title = {Spencer: {{Interactive Heap Analysis}} for the {{Masses}}},
  shorttitle = {Spencer},
  doi = {10.1109/MSR.2017.35},
  abstract = {Programming language-design and run-time-implementation require detailed knowledge about the programs that users want to implement. Acquiring this knowledge is hard, and there is little tool support to effectively estimate whether a proposed tradeoff actually makes sense in the context of real world applications. Ideally, knowledge about behaviour of "typical" programs is 1) easily obtainable, 2) easily reproducible, and 3) easily sharable. We present Spencer, an open source web service and APIframework for dynamic analysis of a continuously growing set of traces of standard program corpora. Users do not obtain traces on their own, but can instead send queries to the web service that will be executed on a set of program traces. Queries are built in terms of a set of query combinators that present a high level interface for working with trace data. Since the framework is high level, and there is a hosted collection of recorded traces, queries are easy to implement. Since the data sets are shared by the research community, results are reproducible. Since the actual queries run on one (or many) servers that provide analysis as a service, obtaining results is possible on commodity hardware. Data in Spencer is meant to be obtained once, and analysed often, making the overhead of data collection mostly irrelevant. This allows Spencer to collect more data than traditional tracing tools can afford within their performance budget. Results in Spencer are cached, making complicated analyses that build on cached primitive queries speedy.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Brandauer, S. and Wrigstad, T.},
  month = may,
  year = {2017},
  keywords = {Optimization,Computer languages,Performance analysis,program diagnostics,dynamic analysis,public domain software,Tools,APIframework,data analysis,Data visualization,heap analysis,interactive heap analysis,masses,open source Web service,program knowledge,program traces,programming language-design,query combinators,query processing,Resource management,run-time-implementation,Spencer,tracing,Web services},
  pages = {113-123},
  file = {/Users/luigi/work/zotero/storage/9MELSS2W/Brandauer and Wrigstad - 2017 - Spencer Interactive Heap Analysis for the Masses.pdf;/Users/luigi/work/zotero/storage/FZDGWGCB/7962361.html}
}

@inproceedings{costaEmpiricalStudyUsage2017,
  address = {New York, NY, USA},
  series = {{{ICPE}} '17},
  title = {Empirical {{Study}} of {{Usage}} and {{Performance}} of {{Java Collections}}},
  isbn = {978-1-4503-4404-3},
  doi = {10.1145/3030207.3030221},
  abstract = {Collection data structures have a major impact on the performance of applications, especially in languages such as Java, C\#, or C++. This requires a developer to select an appropriate collection from a large set of possibilities, including different abstractions (e.g. list, map, set, queue), and multiple implementations. In Java, the default implementation of collections is provided by the standard Java Collection Framework (JCF). However, there exist a large variety of less known third-party collection libraries which can provide substantial performance benefits with minimal code changes. In this paper, we first study the popularity and usage patterns of collection implementations by mining a code corpus comprised of 10,986 Java projects. We use the results to evaluate and compare the performance of the six most popular alternative collection libraries in a large variety of scenarios. We found that for almost every scenario and JCF collection type there is an alternative implementation that greatly decreases memory consumption while offering comparable or even better execution time. Memory savings range from 60\% to 88\% thanks to reduced overhead and some operations execute 1.5x to 50x faster. We present our results as a comprehensive guideline to help developers in identifying the scenarios in which an alternative implementation can provide a substantial performance improvement. Finally, we discuss how some coding patterns result in substantial performance differences of collections.},
  booktitle = {Proceedings of the 8th {{ACM}}/{{SPEC}} on {{International Conference}} on {{Performance Engineering}}},
  publisher = {{ACM}},
  author = {Costa, Diego and Andrzejak, Artur and Seboek, Janos and Lo, David},
  year = {2017},
  keywords = {empirical study,java,collections,execution time,memory,performance},
  pages = {389--400},
  file = {/Users/luigi/work/zotero/storage/SIKHR6LM/Costa et al. - 2017 - Empirical Study of Usage and Performance of Java C.pdf;/Users/luigi/work/zotero/storage/U8BPE5UB/Costa et al. - 2017 - Empirical Study of Usage and Performance of Java C.pdf}
}

@inproceedings{dietrichContractsWildStudy2017a,
  address = {Dagstuhl, Germany},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  title = {Contracts in the {{Wild}}: {{A Study}} of {{Java Programs}}},
  volume = {74},
  isbn = {978-3-95977-035-4},
  shorttitle = {Contracts in the {{Wild}}},
  doi = {10.4230/LIPIcs.ECOOP.2017.9},
  booktitle = {31st {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2017)},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  author = {Dietrich, Jens and Pearce, David J. and Jezek, Kamil and Brada, Premek},
  editor = {M\"uller, Peter},
  year = {2017},
  keywords = {verification,java,assertions,design-by-contract,input validation,postconditions,preconditions,runtime checking},
  pages = {9:1--9:29},
  file = {/Users/luigi/work/zotero/storage/MA2KGGVP/Dietrich et al. - 2017 - Contracts in the Wild A Study of Java Programs.pdf;/Users/luigi/work/zotero/storage/Y2BVGBRZ/7259.html}
}

@inproceedings{tiwariCandoiaPlatformBuilding2017,
  title = {Candoia: {{A Platform}} for {{Building}} and {{Sharing Mining Software Repositories Tools}} as {{Apps}}},
  shorttitle = {Candoia},
  doi = {10.1109/MSR.2017.56},
  abstract = {We propose Candoia, a novel platform and ecosystemfor building and sharing Mining Software Repositories(MSR) tools. Using Candoia, MSR tools are built as apps, and Candoia ecosystem, acting as an appstore, allows effective sharing. Candoia platform provides, data extraction tools for curating custom datasets for user projects, and data abstractions for enabling uniform access to MSR artifacts from disparate sources, which makes apps portable and adoptable across diverse software project settings of MSR researchers and practitioners. The structured design of a Candoia app and the languages selected for building various components of a Candoia app promotes easy customization. To evaluate Candoia we have built over two dozen MSR apps for analyzing bugs, software evolution, project management aspects, and source code and programming practices showing the applicability of the platform for buildinga variety of MSR apps. For testing portability of apps acrossdiverse project settings, we tested the apps using ten popularproject repositories, such as Apache Tomcat, JUnit, Node.js, etc, and found that apps required no changes to be portable. We performed a user study to test customizability and we found that five of eight Candoia users found it very easy to customize an existing app. Candoia is available for download.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Tiwari, N. M. and Upadhyaya, G. and Nguyen, H. A. and Rajan, H.},
  month = may,
  year = {2017},
  keywords = {Java,Data mining,Buildings,source code,Software,program debugging,software tools,Tools,app customizability,apps sharing,appstore,Candioa ecosystem,Candoia,Candoia app,Candoia exchange,Computer bugs,data abstractions,data extraction tools,Ecosystems,mining software repositories tools,MSR,MSR apps,MSR data,MSR tools,programming practices,project management,software bugs,software evolution,software project settings},
  pages = {53-63},
  file = {/Users/luigi/work/zotero/storage/A2BL7GWK/Tiwari et al. - 2017 - Candoia A Platform for Building and Sharing Minin.pdf;/Users/luigi/work/zotero/storage/WY37L35S/7962355.html}
}

@article{wuLearningUserFriendly2017,
  title = {Learning {{User Friendly Type}}-Error {{Messages}}},
  volume = {1},
  issn = {2475-1421},
  doi = {10.1145/3133930},
  abstract = {Type inference is convenient by allowing programmers to elide type annotations, but this comes at the cost of often generating very confusing and opaque type error messages that are of little help to fix type errors. Though there have been many successful attempts at making type error messages better in the past thirty years, many classes of errors are still difficult to fix. In particular, current approaches still generate imprecise and uninformative error messages for type errors arising from errors in grouping constructs like parentheses and brackets. Worse, a recent study shows that these errors usually take more than 10 steps to fix and occur quite frequently (around 45\% to 60\% of all type errors) in programs written by students learning functional programming. We call this class of errors, nonstructural errors. We solve this problem by developing Learnskell, a type error debugger that uses machine learning to help diagnose and deliver high quality error messages, for programs that contain nonstructural errors. While previous approaches usually report type errors on typing constraints or on the type level, Learnskell generates suggestions on the expression level. We have performed an evaluation on more than 1,500 type errors, and the result shows that Learnskell is quite precise. It can correctly capture 86\% of all nonstructural errors and locate the error cause with a precision of 63\%/87\% with the first 1/3 messages, respectively. This is several times more than the precision of state-of-the-art compilers and debuggers. We have also studied the performance of Learnskell and found out that it scales to large programs.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Wu, Baijun and Campora III, John Peter and Chen, Sheng},
  month = oct,
  year = {2017},
  keywords = {concrete messages,machine learning,structure- changing errors,Type error debugging},
  pages = {106:1--106:29},
  file = {/Users/luigi/work/zotero/storage/8IEJVW23/Wu et al. - 2017 - Learning User Friendly Type-error Messages.pdf}
}

@inproceedings{avgustinovTrackingStaticAnalysis2015,
  title = {Tracking {{Static Analysis Violations}} over {{Time}} to {{Capture Developer Characteristics}}},
  volume = {1},
  doi = {10.1109/ICSE.2015.62},
  abstract = {Many interesting questions about the software quality of a code base can only be answered adequately if fine-grained information about the evolution of quality metrics over time and the contributions of individual developers is known. We present an approach for tracking static analysis violations (which are often indicative of defects) over the revision history of a program, and for precisely attributing the introduction and elimination of these violations to individual developers. As one application, we demonstrate how this information can be used to compute ``fingerprints'' of developers that reflect which kinds of violations they tend to introduce or to fix. We have performed an experimental study on several large open-source projects written in different languages, providing evidence that these fingerprints are well-defined and capture characteristic information about the coding habits of individual developers.},
  booktitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  author = {Avgustinov, P. and Baars, A. I. and Henriksen, A. S. and Lavender, G. and Menzel, G. and d Moor, O. and Sch\"afer, M. and Tibble, J.},
  month = may,
  year = {2015},
  keywords = {Java,Libraries,History,program diagnostics,software quality,Software quality,public domain software,coding habits,fine-grained information,Open source software,open-source projects,Position measurement,program revision history,quality metrics,static analysis violations tracking},
  pages = {437-447},
  file = {/Users/luigi/work/zotero/storage/7WFT9HDY/Avgustinov et al. - 2015 - Tracking Static Analysis Violations over Time to C.pdf;/Users/luigi/work/zotero/storage/38QVG2IW/7194595.html}
}

@article{mazinanianUnderstandingUseLambda2017,
  title = {Understanding the {{Use}} of {{Lambda Expressions}} in {{Java}}},
  volume = {1},
  issn = {2475-1421},
  doi = {10.1145/3133909},
  abstract = {Java 8 retrofitted lambda expressions, a core feature of functional programming, into a mainstream object-oriented language with an imperative paradigm. However, we do not know how Java developers have adapted to the functional style of thinking, and more importantly, what are the reasons motivating Java developers to adopt functional programming. Without such knowledge, researchers miss opportunities to improve the state of the art, tool builders use unrealistic assumptions, language designers fail to improve upon their designs, and developers are unable to explore efficient and effective use of lambdas.   We present the first large-scale, quantitative and qualitative empirical study to shed light on how imperative programmers use lambda expressions as a gateway into functional thinking. Particularly, we statically scrutinize the source code of 241 open-source projects with 19,770 contributors, to study the characteristics of 100,540 lambda expressions. Moreover, we investigate the historical trends and adoption rates of lambdas in the studied projects. To get a complementary perspective, we seek the underlying reasons on why developers introduce lambda expressions, by surveying 97 developers who are introducing lambdas in their projects, using the firehouse interview method.   Among others, our findings revealed an increasing trend in the adoption of lambdas in Java: in 2016, the ratio of lambdas introduced per added line of code increased by 54\% compared to 2015. Lambdas were used for various reasons, including but not limited to (i) making existing code more succinct and readable, (ii) avoiding code duplication, and (iii) simulating lazy evaluation of functions. Interestingly, we found out that developers are using Java's built-in functional interfaces inefficiently, i.e., they prefer to use general functional interfaces over the specialized ones, overlooking the performance overheads that might be imposed. Furthermore, developers are not adopting techniques from functional programming, e.g., currying. Finally, we present the implications of our findings for researchers, tool builders, language designers, and developers.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Mazinanian, Davood and Ketkar, Ameya and Tsantalis, Nikolaos and Dig, Danny},
  month = oct,
  year = {2017},
  keywords = {Empirical Studies,Functional Programming,Java 8,Lambda Expressions,Multi-paradigm Programming,The Firehouse Interview Method},
  pages = {85:1--85:31},
  file = {/Users/luigi/work/zotero/storage/Q8CU569X/Mazinanian et al. - 2017 - Understanding the Use of Lambda Expressions in Jav.pdf}
}

@inproceedings{mastrangeloUseYourOwn2015,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} 2015},
  title = {Use at {{Your Own Risk}}: {{The Java Unsafe API}} in the {{Wild}}},
  isbn = {978-1-4503-3689-5},
  shorttitle = {Use at {{Your Own Risk}}},
  doi = {10.1145/2814270.2814313},
  abstract = {Java is a safe language. Its runtime environment provides strong safety guarantees that any Java application can rely on. Or so we think. We show that the runtime actually does not provide these guarantees---for a large fraction of today's Java code. Unbeknownst to many application developers, the Java runtime includes a "backdoor" that allows expert library and framework developers to circumvent Java's safety guarantees. This backdoor is there by design, and is well known to experts, as it enables them to write high-performance "systems-level" code in Java. For much the same reasons that safe languages are preferred over unsafe languages, these powerful---but unsafe---capabilities in Java should be restricted. They should be made safe by changing the language, the runtime system, or the libraries. At the very least, their use should be restricted. This paper is a step in that direction. We analyzed 74 GB of compiled Java code, spread over 86,479 Java archives, to determine how Java's unsafe capabilities are used in real-world libraries and applications. We found that 25\% of Java bytecode archives depend on unsafe third-party Java code, and thus Java's safety guarantees cannot be trusted. We identify 14 different usage patterns of Java's unsafe capabilities, and we provide supporting evidence for why real-world code needs these capabilities. Our long-term goal is to provide a foundation for the design of new language features to regain safety in Java.},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Mastrangelo, Luis and Ponzanelli, Luca and Mocci, Andrea and Lanza, Michele and Hauswirth, Matthias and Nystrom, Nathaniel},
  year = {2015},
  keywords = {Java,patterns,mining,Maven Central,Stack Overflow,unsafe},
  pages = {695--710},
  file = {/Users/luigi/work/zotero/storage/SKY5CYFY/Mastrangelo et al. - 2015 - Use at Your Own Risk The Java Unsafe API in the W.pdf}
}

@article{shenEmpiricalStudyFortran1990,
  title = {An Empirical Study of {{Fortran}} Programs for Parallelizing Compilers},
  volume = {1},
  issn = {1045-9219},
  doi = {10.1109/71.80162},
  abstract = {Some results are reported from an empirical study of program characteristics, that are important in parallelizing compiler writers, especially in the area of data dependence analysis and program transformations. The state of the art in data dependence analysis and some parallel execution techniques are examined. The major findings are included. Many subscripts contain symbolic terms with unknown values. A few methods of determining their values at compile time are evaluated. Array references with coupled subscripts appear quite frequently; these subscripts must be handled simultaneously in a dependence test, rather than being handled separately as in current test algorithms. Nonzero coefficients of loop indexes in most subscripts are found to be simple: they are either 1 or -1. This allows an exact real-valued test to be as accurate as an exact integer-valued test for one-dimensional or two-dimensional arrays. Dependencies with uncertain distance are found to be rather common, and one of the main reasons is the frequent appearance of symbolic terms with unknown values},
  number = {3},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  author = {Shen, Z. and Li, Z. and Yew, P. C.},
  month = jul,
  year = {1990},
  keywords = {FORTRAN,program compilers,program transformations,array references,Councils,Data analysis,data dependence analysis,Fortran programs,Helium,integer-valued test,NASA,parallelizing compilers,program characteristics,Program processors,Scheduling,Space technology,Statistics,Testing,US Department of Energy},
  pages = {356-364},
  file = {/Users/luigi/work/zotero/storage/EFFFV5FN/Shen et al. - 1990 - An empirical study of Fortran programs for paralle.pdf;/Users/luigi/work/zotero/storage/EAI25Q9Z/80162.html}
}

@article{chevanceStaticProfileDynamic1978,
  title = {Static {{Profile}} and {{Dynamic Behavior}} of {{COBOL Programs}}},
  volume = {13},
  issn = {0362-1340},
  doi = {10.1145/953411.953414},
  abstract = {A measurement system for gathering a static profile and dynamic characteristics of COBOL programs at the source language level is described. Static and dynamic results are presented and discussed.},
  number = {4},
  journal = {SIGPLAN Not.},
  author = {Chevance, R. J. and Heidet, T.},
  month = apr,
  year = {1978},
  pages = {44--57},
  file = {/Users/luigi/work/zotero/storage/YZUTQ85D/Chevance and Heidet - 1978 - Static Profile and Dynamic Behavior of COBOL Progr.pdf}
}

@article{salvadoriStaticProfileCOBOL1975,
  title = {Static {{Profile}} of {{COBOL Programs}}},
  volume = {10},
  issn = {0362-1340},
  doi = {10.1145/956028.956031},
  number = {8},
  journal = {SIGPLAN Not.},
  author = {Salvadori, A and Gordon, J. and Capstick, C.},
  month = aug,
  year = {1975},
  pages = {20--33},
  file = {/Users/luigi/work/zotero/storage/QLJAPQNC/Salvadori et al. - 1975 - Static Profile of COBOL Programs.pdf}
}

@inproceedings{saalPropertiesAPLPrograms1975,
  address = {New York, NY, USA},
  series = {{{APL}} '75},
  title = {Some {{Properties}} of {{APL Programs}}},
  doi = {10.1145/800117.803819},
  abstract = {Some results of a study of the static usage of features of the APL language is presented. We compare several characterizations of APL programs with previously measured FORTRAN data, and discuss the significant differences observed. The verity of popular rumors and intuitions about APL programs is also examined. APL users appear to take advantage of the unique matrix features inherent in APL, but in general, use extremely heavily only a small fraction of the available features. The distribution of use agrees well with the so-called ``80-20 rule''.},
  booktitle = {Proceedings of {{Seventh International Conference}} on {{APL}}},
  publisher = {{ACM}},
  author = {Saal, Harry J. and Weiss, Zvi},
  year = {1975},
  pages = {292--297},
  file = {/Users/luigi/work/zotero/storage/L2NLKES5/Saal and Weiss - 1975 - Some Properties of APL Programs.pdf}
}

@article{cookContextualAnalysisPascal1982,
  title = {A Contextual Analysis of {{Pascal}} Programs},
  volume = {12},
  issn = {1097-024X},
  doi = {10.1002/spe.4380120209},
  abstract = {More than 120,000 lines of Pascal programs, written by graduate students and faculty members, have been statically analysed to provide a better understanding of how the language is `really' used. The analysis was done within twelve distinct contexts to discover differences in usage patterns among the various contexts. For example, it was found that 47 per cent of the operands in arguments lists were constants. The results are displayed as tables of frequency counts which show how often each construct is used within a context. Also, we have compared our findings to the results from studies of other languages, such as FORTRAN, SAL and XPL.},
  language = {en},
  number = {2},
  journal = {Software: Practice and Experience},
  author = {Cook, Robert P. and Lee, Insup},
  month = feb,
  year = {1982},
  keywords = {Contextual analysis,Pascal,Static analysis},
  pages = {195-203},
  file = {/Users/luigi/work/zotero/storage/T2FVEF3Y/abstract.html}
}

@article{saalEmpiricalStudyAPL1977,
  title = {An Empirical Study of {{APL}} Programs},
  volume = {2},
  issn = {0096-0551},
  doi = {10.1016/0096-0551(77)90007-8},
  abstract = {The statistical results of a study of the static usage of features of the APL language is presented. The distributions of the appearance of the APL primitive functions and the functions derived from the APL operators are presented. APL users appear to use extremely heavily only a small fraction of the available features. The paper compares several characterizations of APL programs with previously measured FORTRAN data, and discusses the significant differences observed.},
  number = {3},
  journal = {Computer Languages},
  author = {Saal, Harry J. and Weiss, Zvi},
  month = jan,
  year = {1977},
  keywords = {80-20 rule,APL; a programming language,Programming style,Static measurements,Usage statistics,Very high level languages},
  pages = {47-59},
  file = {/Users/luigi/work/zotero/storage/EFI525BJ/0096055177900078.html}
}

@article{bohmAutomaticSynthesisTyped1985,
  series = {Third {{Conference}} on {{Foundations}} of {{Software Technology}} and {{Theoretical Computer Science}}},
  title = {Automatic Synthesis of Typed {{$\Lambda$}}-Programs on Term Algebras},
  volume = {39},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(85)90135-5},
  abstract = {The notion of iteratively defined functions from and to heterogeneous term algebras is introduced as the solution of a finite set of equations of a special shape. Such a notion has remarkable consequences: (1) Choosing the second-order typed lamdda-calculus ({$\Lambda$} for short) as a programming language enables one to represent algebra elements and iterative functions by automatic uniform synthesis paradigms, using neither conditional nor recursive constructs. (2) A completeness theorem for {$\Lambda$}-terms with type of degree at most two and a companion corollary for {$\Lambda$}-programs have been proved. (3) A new congruence relation for the last-mentioned {$\Lambda$}-terms which is stronger than {$\Lambda$}-convertibility is introduced and proved to have the meaning of a {$\Lambda$}-program equivalence. Moreover, an extension of the paradigms to the synthesis of functions of higher complexity is considered and exemplified. All the concepts are explained and motivated by examples over integers, list- and tree-structures.},
  number = {Supplement C},
  journal = {Theoretical Computer Science},
  author = {B\"ohm, Corrado and Berarducci, Alessandro},
  month = jan,
  year = {1985},
  pages = {135-154},
  file = {/Users/luigi/work/zotero/storage/HKM388G4/Böhm and Berarducci - 1985 - Automatic synthesis of typed Λ-programs on term al.pdf;/Users/luigi/work/zotero/storage/5DEDNES5/0304397585901355.html}
}

@article{cookEmpiricalAnalysisLilith1989,
  title = {An Empirical Analysis of the {{Lilith}} Instruction Set},
  volume = {38},
  issn = {0018-9340},
  doi = {10.1109/12.8740},
  abstract = {A static analysis of the instructions used to implement all of the system software on the Lilith computer is described. The results are compared to those of a similar analysis performed on the Mesa instruction set architecture. The data provide a good illustration of how code generation strategies and language usage can affect opcode statistics, even for machines with similar architectures},
  number = {1},
  journal = {IEEE Transactions on Computers},
  author = {Cook, R. P.},
  month = jan,
  year = {1989},
  keywords = {Operating systems,Performance analysis,Computer architecture,Hardware,Statistics,code generation strategies,Computer aided instruction,High level languages,instruction sets,language usage,Lilith instruction set,Lilith software environment,Mesa instruction set architecture,Modula,opcode statistics,Packaging machines,Software packages,static analysis,system software,System software,systems software},
  pages = {156-158},
  file = {/Users/luigi/work/zotero/storage/6R6EB6W8/Cook - 1989 - An empirical analysis of the Lilith instruction se.pdf;/Users/luigi/work/zotero/storage/SFFJIB9S/8740.html}
}

@inproceedings{odonoghueBigramAnalysisJava2002,
  address = {Maynooth, County Kildare, Ireland, Ireland},
  series = {{{PPPJ}} '02/{{IRE}} '02},
  title = {Bigram {{Analysis}} of {{Java Bytecode Sequences}}},
  isbn = {978-0-901519-87-0},
  booktitle = {Proceedings of the {{Inaugural Conference}} on the {{Principles}} and {{Practice}} of {{Programming}}, 2002 and {{Proceedings}} of the {{Second Workshop}} on {{Intermediate Representation Engineering}} for {{Virtual Machines}}, 2002},
  publisher = {{National University of Ireland}},
  author = {O'Donoghue, Diarmuid and Leddy, Aine and Power, James and Waldron, John},
  year = {2002},
  pages = {187--192},
  file = {/Users/luigi/work/zotero/storage/MNCE7CVB/O'Donoghue et al. - 2002 - Bigram Analysis of Java Bytecode Sequences.pdf}
}

@article{kaijanahoEvidencebasedProgrammingLanguage2015,
  title = {Evidence-Based Programming Language Design : A Philosophical and Methodological Exploration},
  issn = {1456-5390},
  shorttitle = {Evidence-Based Programming Language Design},
  abstract = {Background: Programming language design is not usually informed by empirical 
studies. In other fields similar problems have inspired an evidence-based paradigm 
of practice. Such a paradigm is practically inevitable in language design, as well. 
Aims: The content of evidence-based programming design (EB-PLD) is explored, 
as is the concept of evidence in general. Additionally, the extent of evidence 
potentially useful for EB-PLD is mapped, and the appropriateness of Cohen's 
kappa for evaluating coder agreement in a secondary study is evaluated. Method: 
Philosophical analysis and explication are used to clarify the unclear. A systematic mapping study was conducted to map out the existing body of evidence. 
Results: Evidence is a report of observations that affects the strength of an argument. There is some but not much evidence. EB-PLD is a five-step process for 
resolving uncertainty about design problems. Cohen's kappa is inappropriate for 
coder agreement evaluation in systematic secondary studies. Conclusions: Coder 
agreement evaluation should use Scott's pi, Fleiss' kappa, or Krippendorff's alpha. EB-PLD is worthy of further research, although its usefulness was out of 
scope here.},
  language = {eng},
  journal = {Jyv\"askyl\"a studies in computing 222.},
  author = {Kaijanaho, Antti-Juhani},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/767TLGZR/Kaijanaho - 2015 - Evidence-based programming language design  a phi.pdf;/Users/luigi/work/zotero/storage/Q6QQ6TJ2/47698.html}
}

@inproceedings{liAccessingInaccessibleAndroid2016,
  title = {Accessing {{Inaccessible Android APIs}}: {{An Empirical Study}}},
  shorttitle = {Accessing {{Inaccessible Android APIs}}},
  doi = {10.1109/ICSME.2016.35},
  abstract = {As Android becomes a de-facto choice of development platform for mobile apps, developers extensively leverage its accompanying Software Development Kit to quickly build their apps. This SDK comes with a set of APIs which developers may find limited in comparison to what system apps can do or what framework developers are preparing to harness capabilities of new generation devices. Thus, developers may attempt to explore in advance the normally "inaccessible" APIs for building unique API-based functionality in their app. The Android programming model is unique in its kind. Inaccessible APIs, which however are used by developers, constitute yet another specificity of Android development, and is worth investigating to understand what they are, how they evolve over time, and who uses them. To that end, in this work, we empirically investigate 17 important releases of the Android framework source code base, and we find that inaccessible APIs are commonly implemented in the Android framework, which are further neither forward nor backward compatible. Moreover, a small set of inaccessible APIs can eventually become publicly accessible, while most of them are removed during the evolution, resulting in risks for such apps that have leveraged inaccessible APIs. Finally, we show that inaccessible APIs are indeed accessed by third-party apps, and the official Google Play store has tolerated the proliferation of apps leveraging inaccessible API methods.},
  booktitle = {2016 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Li, L. and Bissyand\'e, T. F. and Traon, Y. L. and Klein, J.},
  month = oct,
  year = {2016},
  keywords = {Runtime,Libraries,application program interfaces,Software,software engineering,source code (software),Ecosystems,unsafe,Android (operating system),Android programming model,Androids,API-based functionality,application program interface,authorisation,Google,Humanoid robots,inaccessible Android API,mobile app risk,mobile computing,risk management,SDK,smart phones,software development kit,source code base},
  pages = {411-422},
  file = {/Users/luigi/work/zotero/storage/QFQSUX6L/Li et al. - 2016 - Accessing Inaccessible Android APIs An Empirical .pdf;/Users/luigi/work/zotero/storage/4C4Q9XKX/7816486.html}
}

@inproceedings{horaWhenShouldInternal2016,
  address = {New York, NY, USA},
  series = {{{FSE}} 2016},
  title = {When {{Should Internal Interfaces Be Promoted}} to {{Public}}?},
  isbn = {978-1-4503-4218-6},
  doi = {10.1145/2950290.2950306},
  abstract = {Commonly, software systems have public (and stable) interfaces, and internal (and possibly unstable) interfaces. Despite being discouraged, client developers often use internal interfaces, which may cause their systems to fail when they evolve. To overcome this problem, API producers may promote internal interfaces to public. In practice, however, API producers have no assistance to identify public interface candidates. In this paper, we study the transition from internal to public interfaces. We aim to help API producers to deliver a better product and API clients to benefit sooner from public interfaces. Our empirical investigation on five widely adopted Java systems present the following observations. First, we identified 195 promotions from 2,722 internal interfaces. Second, we found that promoted internal interfaces have more clients. Third, we predicted internal interface promotion with precision between 50\%-80\%, recall 26\%-82\%, and AUC 74\%-85\%. Finally, by applying our predictor on the last version of the analyzed systems, we automatically detected 382 public interface candidates.},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Hora, Andr\'e and Valente, Marco Tulio and Robbes, Romain and Anquetil, Nicolas},
  year = {2016},
  keywords = {unsafe,API Usage,Internal Interface Analysis,Software Evolution},
  pages = {278--289},
  file = {/Users/luigi/work/zotero/storage/5N3TT5YR/Hora et al. - 2016 - When Should Internal Interfaces Be Promoted to Pub.pdf}
}

@inproceedings{saiedCooperativeApproachCombining2016,
  title = {A Cooperative Approach for Combining Client-Based and Library-Based {{API}} Usage Pattern Mining},
  doi = {10.1109/ICPC.2016.7503717},
  abstract = {Software developers need to cope with the complexity of Application Programming Interfaces (APIs) of external libraries or frameworks. Typical APIs provide thousands of methods to their client programs, and these methods are not used independently of each other. Much existing work has provided different techniques to mine API usage patterns based on client programs in order to help developers understanding and using existing libraries. Other techniques propose to overcome the strong constraint of clients' dependency and infer API usage patterns only using the library source code. In this paper, we propose a cooperative usage pattern mining technique (COUPminer) that combines client-based and library-based usage pattern mining. We evaluated our technique through four APIs and the obtained results show that the cooperative approach allows taking advantage at the same time from the precision of client-based technique and from the generalizability of library-based techniques.},
  booktitle = {2016 {{IEEE}} 24th {{International Conference}} on {{Program Comprehension}} ({{ICPC}})},
  author = {Saied, M. A. and Sahraoui, H.},
  month = may,
  year = {2016},
  keywords = {Java,Libraries,software libraries,data mining,application program interfaces,Semantics,source code (software),unsafe,API Usage,API Documentation,application programming interfaces,client programs,client-based API usage pattern mining,client-based technique,Context,cooperative usage pattern mining technique,COUPminer,Digital signatures,Documentation,library source code,library-based API usage pattern mining,library-based techniques,Software Clustering,software developers,Usage Pattern},
  pages = {1-10},
  file = {/Users/luigi/work/zotero/storage/JDFHH96C/Saied and Sahraoui - 2016 - A cooperative approach for combining client-based .pdf;/Users/luigi/work/zotero/storage/N2Y6FBVG/7503717.html}
}

@inproceedings{brunoNG2CPretenuringGarbage2017,
  address = {New York, NY, USA},
  series = {{{ISMM}} 2017},
  title = {{{NG2C}}: {{Pretenuring Garbage Collection}} with {{Dynamic Generations}} for {{HotSpot Big Data Applications}}},
  isbn = {978-1-4503-5044-0},
  shorttitle = {{{NG2C}}},
  doi = {10.1145/3092255.3092272},
  abstract = {Big Data applications suffer from unpredictable and unacceptably high pause times due to Garbage Collection (GC). This is the case in latency-sensitive applications such as on-line credit-card fraud detection, graph-based computing for analysis on social networks, etc. Such pauses compromise latency requirements of the whole application stack and result from applications' aggressive buffering/caching of data, exposing an ill-suited GC design, which assumes that most objects will die young and does not consider that applications hold large amounts of middle-lived data in memory.   To avoid such pauses, we propose NG2C, a new GC algorithm that combines pretenuring with user-defined dynamic generations. By being able to allocate objects into different generations, NG2C is able to group objects with similar lifetime profiles in the same generation. By allocating objects with similar lifetime profiles close to each other, i.e. in the same generation, we avoid object promotion (copying between generations) and heap fragmentation (which leads to heap compactions) both responsible for most of the duration of HotSpot GC pause times.   NG2C is implemented for the OpenJDK 8 HotSpot Java Virtual Machine, as an extension of the Garbage First GC. We evaluate NG2C using Cassandra, Lucene, and GraphChi with three different GCs: Garbage First (G1), Concurrent Mark Sweep (CMS), and NG2C. Results show that NG2C decreases the worst observable GC pause time by up to 94.8\% for Cassandra, 85.0\% for Lucene and 96.45\% for GraphChi, when compared to current collectors (G1 and CMS). In addition, NG2c has no negative impact on application throughput or memory usage.},
  booktitle = {Proceedings of the 2017 {{ACM SIGPLAN International Symposium}} on {{Memory Management}}},
  publisher = {{ACM}},
  author = {Bruno, Rodrigo and Oliveira, Lu\'is Picciochi and Ferreira, Paulo},
  year = {2017},
  keywords = {unsafe,Big Data,Garbage Collection,Latency},
  pages = {2--13},
  file = {/Users/luigi/work/zotero/storage/PWDU74BA/Bruno et al. - 2017 - NG2C Pretenuring Garbage Collection with Dynamic .pdf}
}

@inproceedings{holzingerInDepthStudyMore2016,
  address = {New York, NY, USA},
  series = {{{CCS}} '16},
  title = {An {{In}}-{{Depth Study}} of {{More Than Ten Years}} of {{Java Exploitation}}},
  isbn = {978-1-4503-4139-4},
  doi = {10.1145/2976749.2978361},
  abstract = {When created, the Java platform was among the first runtimes designed with security in mind. Yet, numerous Java versions were shown to contain far-reaching vulnerabilities, permitting denial-of-service attacks or even worse allowing intruders to bypass the runtime's sandbox mechanisms, opening the host system up to many kinds of further attacks. This paper presents a systematic in-depth study of 87 publicly available Java exploits found in the wild. By collecting, minimizing and categorizing those exploits, we identify their commonalities and root causes, with the goal of determining the weak spots in the Java security architecture and possible countermeasures. Our findings reveal that the exploits heavily rely on a set of nine weaknesses, including unauthorized use of restricted classes and confused deputies in combination with caller-sensitive methods. We further show that all attack vectors implemented by the exploits belong to one of three categories: single-step attacks, restricted-class attacks, and information hiding attacks. The analysis allows us to propose ideas for improving the security architecture to spawn further research in this area.},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  publisher = {{ACM}},
  author = {Holzinger, Philipp and Triller, Stefan and Bartel, Alexandre and Bodden, Eric},
  year = {2016},
  keywords = {unsafe,access control,exploits,java security,security analysis},
  pages = {779--790},
  file = {/Users/luigi/work/zotero/storage/VDLLXUBH/Holzinger et al. - 2016 - An In-Depth Study of More Than Ten Years of Java E.pdf}
}

@inproceedings{reboucasEmpiricalStudyUsage2016,
  title = {An {{Empirical Study}} on the {{Usage}} of the {{Swift Programming Language}}},
  volume = {1},
  doi = {10.1109/SANER.2016.66},
  abstract = {Recently, Apple released Swift, a modern programming language built to be the successor of Objective-C. In less than a year and a half after its first release, Swift became one of the most popular programming languages in the world, considering different popularity measures. A significant part of this success is due to Apple's strict control over its ecosystem, and the clear message that it will replace Objective-C in a near future. According to Apple, "Swift is a powerful and intuitive programming language[...]. Writing Swift code is interactive and fun, the syntax is concise yet expressive." However, little is known about how Swift developers perceive these benefits. In this paper, we conducted two studies aimed at uncovering the questions and strains that arise from this early adoption. First, we perform a thorough analysis on 59,156 questions asked about Swift on StackOverflow. Second, we interviewed 12 Swift developers to cross-validate the initial results. Our study reveals that developers do seem to find the language easy to understand and adopt, although 17.5\% of the questions are about basic elements of the language. Still, there are many questions about problems in the toolset (compiler, Xcode, libraries). Some of our interviewees reinforced these problems.},
  booktitle = {2016 {{IEEE}} 23rd {{International Conference}} on {{Software Analysis}}, {{Evolution}}, and {{Reengineering}} ({{SANER}})},
  author = {Rebou{\c c}as, M. and Pinto, G. and Ebert, F. and Torres, W. and Serebrenik, A. and Castor, F.},
  month = mar,
  year = {2016},
  keywords = {computational linguistics,Computer languages,Libraries,Programming,Software,programming languages,unsafe,Testing,compiler,Interviews,libraries,StackOverflow,Standards,Swift developers,Swift programming languages,Xcode},
  pages = {634-638},
  file = {/Users/luigi/work/zotero/storage/8WTKH9YQ/Rebouças et al. - 2016 - An Empirical Study on the Usage of the Swift Progr.pdf;/Users/luigi/work/zotero/storage/JY8EQ5RC/7476687.html}
}

@inproceedings{zhangAcceptingBlameSafe2016,
  address = {New York, NY, USA},
  series = {{{PLDI}} '16},
  title = {Accepting {{Blame}} for {{Safe Tunneled Exceptions}}},
  isbn = {978-1-4503-4261-2},
  doi = {10.1145/2908080.2908086},
  abstract = {Unhandled exceptions crash programs, so a compile-time check that exceptions are handled should in principle make software more reliable. But designers of some recent languages have argued that the benefits of statically checked exceptions are not worth the costs. We introduce a new statically checked exception mechanism that addresses the problems with existing checked-exception mechanisms. In particular, it interacts well with higher-order functions and other design patterns. The key insight is that whether an exception should be treated as a "checked" exception is not a property of its type but rather of the context in which the exception propagates. Statically checked exceptions can "tunnel" through code that is oblivious to their presence, but the type system nevertheless checks that these exceptions are handled. Further, exceptions can be tunneled without being accidentally caught, by expanding the space of exception identifiers to identify the exception-handling context. The resulting mechanism is expressive and syntactically light, and can be implemented efficiently. We demonstrate the expressiveness of the mechanism using significant codebases and evaluate its performance. We have implemented this new exception mechanism as part of the new Genus programming language, but the mechanism could equally well be applied to other programming languages.},
  booktitle = {Proceedings of the 37th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Zhang, Yizhou and Salvaneschi, Guido and Beightol, Quinn and Liskov, Barbara and Myers, Andrew C.},
  year = {2016},
  keywords = {exception handling,unsafe,Exception tunneling,Genus},
  pages = {281--295},
  file = {/Users/luigi/work/zotero/storage/JQTY67XV/Zhang et al. - 2016 - Accepting Blame for Safe Tunneled Exceptions.pdf}
}

@inproceedings{jiangUnsupervisedApproachDiscovering2017,
  address = {Piscataway, NJ, USA},
  series = {{{ICSE}} '17},
  title = {An {{Unsupervised Approach}} for {{Discovering Relevant Tutorial Fragments}} for {{APIs}}},
  isbn = {978-1-5386-3868-2},
  doi = {10.1109/ICSE.2017.12},
  abstract = {Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely {$<$}u{$>$}F{$<$}/u{$>$}ragment {$<$}u{$>$}R{$<$}/u{$>$}ecommender for {$<$}u{$>$}A{$<$}/u{$>$}PIs with {$<$}u{$>$}P{$<$}/u{$>$}ageRank and {$<$}u{$>$}T{$<$}/u{$>$}opic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-the-art approach by 8.77\% and 12.32\% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Software Engineering}}},
  publisher = {{IEEE Press}},
  author = {Jiang, He and Zhang, Jingxuan and Ren, Zhilei and Zhang, Tao},
  year = {2017},
  keywords = {application programming interface,unsafe,pagerank algorithm,topic model,unsupervised approaches},
  pages = {38--48},
  file = {/Users/luigi/work/zotero/storage/DK8GYWNG/Jiang et al. - 2017 - An Unsupervised Approach for Discovering Relevant .pdf}
}

@article{staicuUnderstandingAutomaticallyPreventing2017,
  title = {Understanding and {{Automatically Preventing Injection Attacks}} on {{Node}}.Js},
  abstract = {The NODE.JS ecosystem has lead to the creation of many modern applications, such as server-side web applications and desktop applications. Unlike client-side JavaScript code, NODE.JS applications can interact freely with the operating system without the benefits of a security sandbox. The complex interplay between NODE.JS modules leads to subtle injection vulnerabilities being introduced across module \ldots{}},
  journal = {Microsoft Research},
  author = {Staicu, Cristian-Alexandru and Pradel, Michael and Livshits, Ben},
  month = jan,
  year = {2017},
  keywords = {unsafe},
  file = {/Users/luigi/work/zotero/storage/FKSXN8IE/Staicu et al. - 2017 - Understanding and Automatically Preventing Injecti.pdf;/Users/luigi/work/zotero/storage/GLMVIEMT/Staicu et al. - 2017 - Understanding and Automatically Preventing Injecti.pdf;/Users/luigi/work/zotero/storage/2MG3B8CL/understanding-automatically-preventing-injection-attacks-node-js.html;/Users/luigi/work/zotero/storage/JFUQJN4L/understanding-automatically-preventing-injection-attacks-node-js.html}
}

@article{diaconescuLogicalFoundationsCafeOBJ2002,
  series = {Rewriting {{Logic}} and Its {{Applications}}},
  title = {Logical Foundations of {{CafeOBJ}}},
  volume = {285},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(01)00361-9},
  abstract = {This paper surveys the logical and mathematical foundations of CafeOBJ, which is a successor of the famous algebraic specification language OBJ but adds to it several new primitive paradigms such as behavioural concurrent specification and rewriting logic. We first give a concise overview of CafeOBJ. Then we focus on the actual logical foundations of the language at two different levels: basic specification and structured specification, including also the definition of the CafeOBJ institution. We survey some novel or more classical theoretical concepts supporting the logical foundations of CafeOBJ, pointing out the main results but without giving proofs and without discussing all mathematical details. Novel theoretical concepts include the coherent hidden algebra formalism and its combination with rewriting logic, and Grothendieck (or fibred) institutions. However, for proofs and for some of the mathematical details not discussed here we give pointers to relevant publications. The logical foundations of CafeOBJ are structured by the concept of institution. Moreover, the design of CafeOBJ emerged from its logical foundations, and institution concepts played a crucial r\^ole in structuring the language design.},
  number = {2},
  journal = {Theoretical Computer Science},
  author = {Diaconescu, R{\u a}zvan and Futatsugi, Kokichi},
  month = aug,
  year = {2002},
  keywords = {Algebraic specification,Behavioural specification,Institutions},
  pages = {289-318},
  file = {/Users/luigi/work/zotero/storage/57XXM964/Diaconescu and Futatsugi - 2002 - Logical foundations of CafeOBJ.pdf;/Users/luigi/work/zotero/storage/DZSU2EG9/S0304397501003619.html}
}

@article{fruhwirthTheoryPracticeConstraint1998,
  title = {Theory and Practice of Constraint Handling Rules},
  volume = {37},
  issn = {0743-1066},
  doi = {10.1016/S0743-1066(98)10005-5},
  abstract = {Constraint Handling Rules (CHR) are our proposal to allow more flexibility and application-oriented customization of constraint systems. CHR are a declarative language extension especially designed for writing user-defined constraints. CHR are essentially a committed-choice language consisting of multi-headed guarded rules that rewrite constraints into simpler ones until they are solved. In this broad survey we aim at covering all aspects of CHR as they currently present themselves. Going from theory to practice, we will define syntax and semantics for CHR, introduce an important decidable property, confluence, of CHR programs and define a tight integration of CHR with constraint logic programming languages. This survey then describes implementations of the language before we review several constraint solvers \textendash{} both traditional and nonstandard ones \textendash{} written in the CHR language. Finally we introduce two innovative applications that benefited from using CHR.},
  number = {1},
  journal = {The Journal of Logic Programming},
  author = {Fr\"uhwirth, Thom},
  month = oct,
  year = {1998},
  pages = {95-138},
  file = {/Users/luigi/work/zotero/storage/UCH4KIFM/Frühwirth - 1998 - Theory and practice of constraint handling rules.pdf;/Users/luigi/work/zotero/storage/U72YJZ6F/S0743106698100055.html}
}

@book{gluckRoadmapMetacomputationSupercompilation1996,
  title = {A {{Roadmap}} to {{Metacomputation}} by {{Supercompilation}}},
  abstract = {This paper gives a gentle introduction to Turchin's supercompilation and its applications in metacomputation with an emphasis on recent developments. First, a complete supercompiler, including positive driving and generalization, is defined for a functional language and illustrated with examples. Then a taxonomy of related transformers is given and compared to the supercompiler. Finally, we put supercompilation into the larger perspective of metacomputation and consider three metacomputation tasks: specialization, composition, and inversion.},
  author = {Gl\"uck, Robert and S\o{}rensen, Morten Heine},
  year = {1996},
  file = {/Users/luigi/work/zotero/storage/G53PFBYS/Glück and Sørensen - 1996 - A Roadmap to Metacomputation by Supercompilation.pdf;/Users/luigi/work/zotero/storage/EPQP6QQF/summary.html}
}

@inproceedings{couttsStreamFusionLists2007,
  address = {New York, NY, USA},
  series = {{{ICFP}} '07},
  title = {Stream {{Fusion}}: {{From Lists}} to {{Streams}} to {{Nothing}} at {{All}}},
  isbn = {978-1-59593-815-2},
  shorttitle = {Stream {{Fusion}}},
  doi = {10.1145/1291151.1291199},
  abstract = {This paper presents an automatic deforestation system, stream fusion, based on equational transformations, that fuses a wider range of functions than existing short-cut fusion systems. In particular, stream fusion is able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing feature of the framework is its simplicity: by transforming list functions to expose their structure, intermediate values are eliminated by general purpose compiler optimisations. We have reimplemented the Haskell standard List library on top of our framework, providing stream fusion for Haskell lists. By allowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion in typical Haskell programs. We present benchmarks documenting time and space improvements.},
  booktitle = {Proceedings of the 12th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Coutts, Duncan and Leshchinskiy, Roman and Stewart, Don},
  year = {2007},
  keywords = {program transformation,functional programming,deforestation,program fusion,program optimisation},
  pages = {315--326},
  file = {/Users/luigi/work/zotero/storage/NRDHG6ZW/Coutts et al. - 2007 - Stream Fusion From Lists to Streams to Nothing at.pdf}
}

@inproceedings{altenkirchWhyDependentTypes2005,
  title = {Why {{Dependent Types Matter}}},
  abstract = {We exhibit the rationale behind the design of Epigram, a dependently typed programming language and interactive program development system, using refinements of a well known program\textemdash{}merge sort\textemdash{}as a running example. We discuss its relationship with other proposals to introduce aspects of dependent types into functional programming languages and sketch some topics for further work in this area. 1.},
  booktitle = {In Preparation, {{http://www.e-pig.org/downloads/ydtm.pdf}}},
  author = {Altenkirch, Thorsten and Mcbride, Conor and Mckinna, James},
  year = {2005},
  file = {/Users/luigi/work/zotero/storage/F7Z66P8B/Altenkirch et al. - 2005 - Why dependent types matter.pdf;/Users/luigi/work/zotero/storage/Y4KDBFCK/summary.html}
}

@inproceedings{yangSurveyCoverageBased2006,
  address = {New York, NY, USA},
  series = {{{AST}} '06},
  title = {A {{Survey}} of {{Coverage Based Testing Tools}}},
  isbn = {978-1-59593-408-6},
  doi = {10.1145/1138929.1138949},
  abstract = {Test coverage is sometimes used as a way to measure how thoroughly software is tested. Coverage is used by software developers and sometimes by vendors to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools focusing on, but not restricted to coverage measurement. We also survey additional features, including program prioritization for testing, assistance in debugging, automatic generation of test cases, and customization of test reports. Such features make tools more useful and practical, especially for large-scale, real-life commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage1 (a tool suite that includes code coverage testing, debugging, performance profiling, and reporting). Our study shows that each tool has its unique features tailored to its application domains. Therefore this study can be used to pick the right coverage testing tools depending on various requirements.},
  booktitle = {Proceedings of the 2006 {{International Workshop}} on {{Automation}} of {{Software Test}}},
  publisher = {{ACM}},
  author = {Yang, Qian and Li, J. Jenny and Weiss, David},
  year = {2006},
  keywords = {automate test case generation,code coverage,coverage-based testing tool,dominator analysis,eXVantage,prioritization},
  pages = {99--103},
  file = {/Users/luigi/work/zotero/storage/NQXDWMNJ/Yang et al. - 2006 - A Survey of Coverage Based Testing Tools.pdf}
}

@inproceedings{blackburnDaCapoBenchmarksJava2006,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '06},
  title = {The {{DaCapo Benchmarks}}: {{Java Benchmarking Development}} and {{Analysis}}},
  isbn = {978-1-59593-348-5},
  shorttitle = {The {{DaCapo Benchmarks}}},
  doi = {10.1145/1167473.1167488},
  abstract = {Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.},
  booktitle = {Proceedings of the 21st {{Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Blackburn, Stephen M. and Garner, Robin and Hoffmann, Chris and Khang, Asjad M. and McKinley, Kathryn S. and Bentzur, Rotem and Diwan, Amer and Feinberg, Daniel and Frampton, Daniel and Guyer, Samuel Z. and Hirzel, Martin and Hosking, Antony and Jump, Maria and Lee, Han and Moss, J. Eliot B. and Phansalkar, Aashish and Stefanovi\'c, Darko and VanDrunen, Thomas and {von Dincklage}, Daniel and Wiedermann, Ben},
  year = {2006},
  keywords = {Java,benchmark,DaCapo,methodology,SPEC},
  pages = {169--190},
  file = {/Users/luigi/work/zotero/storage/GCUIDM8X/Blackburn et al. - 2006 - The DaCapo Benchmarks Java Benchmarking Developme.pdf}
}

@article{bradyHintsProofsRecursion1977,
  title = {Hints on Proofs by Recursion Induction},
  volume = {20},
  issn = {0010-4620},
  doi = {10.1093/comjnl/20.4.353},
  abstract = {In 1963 John McCarthy proposed a formalism based on conditional expression and recursion for use in the emergent theory of computation. Included in his proposals was a proof technique, known as recursive induction, which could be used to establish the equivalence of recursively defined functions. This paper shows that the discovery of an equations to serve in a proof by recursive induction does not have to rely on luck or inspiration, but can be developed rationally hand in hand with the development of the proof.},
  number = {4},
  journal = {The Computer Journal},
  author = {Brady, J. M.},
  month = jan,
  year = {1977},
  pages = {353-355},
  file = {/Users/luigi/work/zotero/storage/CZ7CGRRC/Brady - 1977 - Hints on proofs by recursion induction.pdf;/Users/luigi/work/zotero/storage/7KNADDWH/Hints-on-proofs-by-recursion-induction.html}
}

@inproceedings{jhalaStructuralInvariants2006,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Structural {{Invariants}}},
  isbn = {978-3-540-37756-6 978-3-540-37758-0},
  doi = {10.1007/11823230_6},
  abstract = {We present structural invariants (SI), a new technique for incrementally overapproximating the verification condition of a program in static single assignment form by making a linear pass over the dominator tree of the program. The 1-level SI at a program location is the conjunction of all dominating program statements viewed as constraints. For any k, we define a k-level SI by recursively strengthening the dominating join points of the 1-level SI with the (k \textendash{} 1)-level SI of the predecessors of the join point, thereby providing a tunable selector to add path-sensitivity incrementally. By ignoring program paths, the size of the SI and correspondingly the time to discharge the validity query remains small, allowing the technique to scale to large programs. We show experimentally that even with k {$\leq$}2, for a set of open-source programs totaling 570K lines and properties for which specialized analyses have been previously devised, our method provides an automatic and scalable algorithm with a low false positive rate.},
  language = {en},
  booktitle = {Static {{Analysis}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Jhala, Ranjit and Majumdar, Rupak and Xu, Ru-Gang},
  month = aug,
  year = {2006},
  pages = {71-87},
  file = {/Users/luigi/work/zotero/storage/HZ9JIP9B/Jhala et al. - 2006 - Structural Invariants.pdf;/Users/luigi/work/zotero/storage/IQKXKB4E/11823230_6.html}
}

@article{lamportStateProblemDescribing1978,
  title = {State the {{Problem Before Describing}} the {{Solution}}},
  volume = {3},
  issn = {0163-5948},
  doi = {10.1145/1010734.1010737},
  number = {1},
  journal = {SIGSOFT Softw. Eng. Notes},
  author = {Lamport, Leslie},
  month = jan,
  year = {1978},
  pages = {26--26},
  file = {/Users/luigi/work/zotero/storage/5D2JKFWN/Lamport - 1978 - State the Problem Before Describing the Solution.pdf}
}

@inproceedings{weiserProgramSlicing1981,
  address = {Piscataway, NJ, USA},
  series = {{{ICSE}} '81},
  title = {Program {{Slicing}}},
  isbn = {978-0-89791-146-7},
  abstract = {Program slicing is a method used by experienced computer programmers for abstracting from programs. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a ``slice'', is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior. Finding a slice is in general unsolvable. A dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. Experimental evidence is presented that these slices are used by programmers during debugging. Experience with two automatic slicing tools is summarized. New measures of program complexity are suggested based on the organization of a program's slices.},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Software Engineering}}},
  publisher = {{IEEE Press}},
  author = {Weiser, Mark},
  year = {1981},
  keywords = {Debugging,Data flow analysis,Human factors,Program maintenance,Program metrics,Software tools},
  pages = {439--449},
  file = {/Users/luigi/work/zotero/storage/YY44SUIU/Weiser - 1981 - Program Slicing.pdf}
}

@article{tipSurveyProgramSlicing1995,
  title = {A {{Survey}} of {{Program Slicing Techniques}}},
  volume = {3},
  abstract = {A program slice consists of the parts of a program that (potentially) affect the  values computed at some point of interest, referred to as a slicing criterion. The task  of computing program slices is called program slicing. The original definition of a  program slice was presented by Weiser in 1979. Since then, various slightly different  notions of program slices have been proposed, as well as a number of methods to  compute them. An important distinction is that between a static and a dynamic slice.  The former notion is computed without making assumptions regarding a program's  input, whereas the latter relies on some specific test case.  Procedures, arbitrary control flow, composite datatypes and pointers, and interprocess  communication each require a specific solution. We classify static and dynamic  slicing methods for each of these features, and compare their accuracy and efficiency.  Moreover, the possibilities for combining solutions for different features are investigated....},
  journal = {Journal of Programming Languages},
  author = {Tip, F.},
  year = {1995},
  pages = {121--189},
  file = {/Users/luigi/work/zotero/storage/D9S2386I/Tip - 1995 - A Survey of Program Slicing Techniques.pdf;/Users/luigi/work/zotero/storage/E8HSKQXM/summary.html}
}

@inproceedings{huDynamicAnalysisDesign2008,
  title = {Dynamic {{Analysis}} and {{Design Pattern Detection}} in {{Java Programs}}.},
  abstract = {Identifying design patterns within an existing software system can support understandability and reuse of the system's core functionality. In this context, incorporating behavioral features into the design pattern recovery would enhance the scalability of the process. The main advantage of the new approach in this paper over the existing approaches is incorporating dynamic analysis and feature localization in source code. This allows us to perform a goal-driven design pattern detection and focus ourselves on patterns that implement specific software functionality, as opposed to conducting a general pattern detection which is susceptible to high complexity problem. Using a new pattern description language and a matching process we identify the instances of these patterns within the obtained classes and interactions. We use a two-phase matching process: i) an approximate matching of class attributes generates a list of candidate patterns; and ii) a structural matching of classes identifies exact matched patterns. One target application domain can be software product line which emphasizes on reusing core software artifacts to construct a reference architecture for several similar products. Finally, we present the result of a case study.},
  booktitle = {20th {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}} 2008},
  author = {Hu, Lei and Sartipi, Kamran},
  month = jan,
  year = {2008},
  pages = {842-846},
  file = {/Users/luigi/work/zotero/storage/J7BRZ7SC/Hu and Sartipi - Dynamic Analysis and Design Pattern Detection in J.pdf;/Users/luigi/work/zotero/storage/3UVHHRUH/221391328_Dynamic_Analysis_and_Design_Pattern_Detection_in_Java_Programs.html}
}

@inproceedings{arcelliDesignPatternDetection2008,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {Design {{Pattern Detection}} in {{Java Systems}}: {{A Dynamic Analysis Based Approach}}},
  isbn = {978-3-642-14818-7 978-3-642-14819-4},
  shorttitle = {Design {{Pattern Detection}} in {{Java Systems}}},
  doi = {10.1007/978-3-642-14819-4_12},
  abstract = {In the context of reverse engineering, the recognition of design patterns provides additional information related to the rationale behind the design. This paper presents our approach to the recognition of the behavioral design patterns based on dynamic analysis of Java software systems. The idea behind our solution is to identify a set of rules capturing information necessary to identify a design pattern instance. Rules are characterized by weights indicating their importance in the detection of a specific design pattern. The core behavior of each design pattern may be described through a subset of these rules forming a macrorule. Macrorules define the main traits of a pattern. JADEPT (JAva DEsign Pattern deTector) is our software for design pattern identification based on this idea. It captures static and dynamic aspects through a dynamic analysis of the software by exploiting the JPDA (Java Platform Debugger Architecture). The extracted information is stored in a database. Queries to the database implement the rules defined to recognize the design patterns. The tool has been validated with positive results on different implementations of design patterns and on systems such as JADEPT itself.},
  language = {en},
  booktitle = {Evaluation of {{Novel Approaches}} to {{Software Engineering}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Arcelli, Francesca and Perin, Fabrizio and Raibulet, Claudia and Ravani, Stefano},
  month = may,
  year = {2008},
  pages = {163-179},
  file = {/Users/luigi/work/zotero/storage/KS9GYNA3/978-3-642-14819-4_12.html}
}

@inproceedings{jangEmpiricalStudyPrivacyviolating2010,
  address = {New York, NY, USA},
  series = {{{CCS}} '10},
  title = {An {{Empirical Study}} of {{Privacy}}-Violating {{Information Flows}} in {{JavaScript Web Applications}}},
  isbn = {978-1-4503-0245-6},
  doi = {10.1145/1866307.1866339},
  abstract = {The dynamic nature of JavaScript web applications has given rise to the possibility of privacy violating information flows. We present an empirical study of the prevalence of such flows on a large number of popular websites. We have (1) designed an expressive, fine-grained information flow policy language that allows us to specify and detect different kinds of privacy-violating flows in JavaScript code,(2) implemented a new rewriting-based JavaScript information flow engine within the Chrome browser, and (3) used the enhanced browser to conduct a large-scale empirical study over the Alexa global top 50,000 websites of four privacy-violating flows: cookie stealing, location hijacking, history sniffing, and behavior tracking. Our survey shows that several popular sites, including Alexa global top-100 sites, use privacy-violating flows to exfiltrate information about users' browsing behavior. Our findings show that steps must be taken to mitigate the privacy threat from covert flows in browsers.},
  booktitle = {Proceedings of the 17th {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  publisher = {{ACM}},
  author = {Jang, Dongseok and Jhala, Ranjit and Lerner, Sorin and Shacham, Hovav},
  year = {2010},
  keywords = {dynamic analysis,JavaScript,history sniffing,information flow,privacy,rewriting,web application,web security},
  pages = {270--283},
  file = {/Users/luigi/work/zotero/storage/G2QBZDRK/Jang et al. - 2010 - An Empirical Study of Privacy-violating Informatio.pdf}
}

@inproceedings{latozaDevelopersAskReachability2010,
  address = {New York, NY, USA},
  series = {{{ICSE}} '10},
  title = {Developers {{Ask Reachability Questions}}},
  isbn = {978-1-60558-719-6},
  doi = {10.1145/1806799.1806829},
  abstract = {A reachability question is a search across feasible paths through a program for target statements matching search criteria. In three separate studies, we found that reachability questions are common and often time consuming to answer. In the first study, we observed 13 developers in the lab and found that half of the bugs developers inserted were associated with reachability questions. In the second study, 460 professional software developers reported asking questions that may be answered using reachability questions more than 9 times a day, and 82\% rated one or more as at least somewhat hard to answer. In the third study, we observed 17 developers in the field and found that 9 of the 10 longest activities were associated with reachability questions. These findings suggest that answering reachability questions is an important source of difficulty understanding large, complex codebases.},
  booktitle = {Proceedings of the {{32Nd ACM}}/{{IEEE International Conference}} on {{Software Engineering}} - {{Volume}} 1},
  publisher = {{ACM}},
  author = {LaToza, Thomas D. and Myers, Brad A.},
  year = {2010},
  keywords = {software maintenance,empirical study,code navigation,developer questions,program comprehension},
  pages = {185--194},
  file = {/Users/luigi/work/zotero/storage/N3RPZDCU/LaToza and Myers - 2010 - Developers Ask Reachability Questions.pdf}
}

@inproceedings{dietrichBrokenPromisesEmpirical2014,
  title = {Broken Promises: {{An}} Empirical Study into Evolution Problems in {{Java}} Programs Caused by Library Upgrades},
  shorttitle = {Broken Promises},
  doi = {10.1109/CSMR-WCRE.2014.6747226},
  abstract = {It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs. We have studied the extent of the problem on the qualitas corpus, a data set consisting of Java open-source programs widely used in empirical studies. In this paper, we describe the study and report its key findings. We found that the above mentioned issues do occur in practice, albeit not on a wide scale.},
  booktitle = {2014 {{Software Evolution Week}} - {{IEEE Conference}} on {{Software Maintenance}}, {{Reengineering}}, and {{Reverse Engineering}} ({{CSMR}}-{{WCRE}})},
  author = {Dietrich, J. and Jezek, K. and Brada, P.},
  month = feb,
  year = {2014},
  keywords = {Java,Runtime,Libraries,program compilers,virtual machines,object-oriented programming,application program interface,compiler,API changes,Contracts,Couplings,Educational institutions,Java open-source programs,Joining processes,library upgrades,library versions,partially upgrading systems,qualitas corpus,reuse approach,system runtime failures,virtual machine},
  pages = {64-73},
  file = {/Users/luigi/work/zotero/storage/YFBBZSD3/Dietrich et al. - 2014 - Broken promises An empirical study into evolution.pdf;/Users/luigi/work/zotero/storage/5CQSS7AJ/6747226.html}
}

@inproceedings{sorensenAlgorithmGeneralizationPositive1995,
  title = {An {{Algorithm}} of {{Generalization}} in {{Positive Supercompilation}}},
  abstract = {This paper presents a termination technique for positive supercompilation, based on notions from term algebra. The technique is not particularily biased towards positive supercompilation, but also works for deforestation and partial evaluation. It appears to be well suited for partial deduction too. The technique guarantees termination, yet it is not overly conservative. Our technique can be viewed as an instance of Martens ' and Gallagher's recent framework for global termination of partial deduction, but it is more general in some important respects, e.g. it uses well-quasi orderings rather than well-founded orderings. Its merits are illustrated on several examples.},
  booktitle = {Proceedings of {{ILPS}}'95, the {{International Logic Programming Symposium}}},
  publisher = {{MIT Press}},
  author = {S\o{}rensen, Morten H. and Gl\"uck, Robert},
  year = {1995},
  pages = {465--479},
  file = {/Users/luigi/work/zotero/storage/FZRVPQDD/Sørensen and Glück - 1995 - An Algorithm of Generalization in Positive Superco.pdf;/Users/luigi/work/zotero/storage/AB294Z57/summary.html}
}

@book{sorensenTurchinSupercompilerRevisited1996,
  title = {Turchin's {{Supercompiler Revisited}} - {{An}} Operational Theory of Positive Information Propagation},
  abstract = {Turchin`s supercompiler is a program transformer that includes both partial evaluation and deforestation. Although known in the West since 1979, the essence of its techniques, its more precise relations to other transformers, and the properties of the programs that it produces are only now becoming apparent in the Western functional programming community. This thesis gives a new formulation of the supercompiler in familiar terms; we study the essence of it, how it achieves its effects, and its relations to related transformers; and we develop results dealing with the problems of preserving semantics, assessing the efficiency of transformed programs, and ensuring termination.},
  author = {S\o{}rensen, Morten Heine},
  year = {1996},
  file = {/Users/luigi/work/zotero/storage/KMWFFDQC/Sørensen - 1996 - Turchin's Supercompiler Revisited - An operational.pdf;/Users/luigi/work/zotero/storage/Q79BHCQY/summary.html}
}

@inproceedings{saraswatConcurrentConstraintProgramming1990,
  address = {New York, NY, USA},
  series = {{{POPL}} '90},
  title = {Concurrent {{Constraint Programming}}},
  isbn = {978-0-89791-343-0},
  doi = {10.1145/96709.96733},
  abstract = {This paper presents a new and very rich class of (concurrent) programming languages, based on the notion of computing with partial information, and the concomitant notions of consistency and entailment.1 In this framework, computation emerges from the interaction of concurrently executing agents that communicate by placing, checking and instantiating constraints on shared variables. Such a view of computation is interesting in the context of programming languages because of the ability to represent and manipulate partial information about the domain of discourse, in the context of concurrency because of the use of constraints for communication and control, and in the context of AI because of the availability of simple yet powerful mechanisms for controlling inference, and the promise that very rich representational/programming languages, sharing the same set of abstract properties, may be possible.
To reflect this view of computation, [Sar89] develops the cc family of languages. We present here one member of the family, cc({$\downarrow$}, {$\rightarrow$}) (pronounced ``cc with Ask and Choose'') which provides the basic operations of blocking Ask and atomic Tell and an algebra of behaviors closed under prefixing, indeterministic choice, interleaving, and hiding, and provides a mutual recursion operator. cc({$\downarrow$}, {$\rightarrow$}) is (intentionally!) very similar to Milner's CCS, but for the radically different underlying concept of communication, which, in fact, provides a general\textemdash{}and elegant\textemdash{}alternative approach to ``value-passing'' in CCS. At the same time it adequately captures the framework of committed choice concurrent logic programming languages. We present the rules of behavior for cc agents, motivate a notion of ``visible action'' for them, and develop the notion of c-trees and reactive congruence analogous to Milner's synchronization trees and observational congruence. We also present an equational characterization of reactive congruence for Finitary cc({$\downarrow$}, {$\rightarrow$}).},
  booktitle = {Proceedings of the 17th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Saraswat, Vijay A. and Rinard, Martin},
  year = {1990},
  pages = {232--245},
  file = {/Users/luigi/work/zotero/storage/QZKCNCZQ/Saraswat and Rinard - 1990 - Concurrent Constraint Programming.pdf}
}

@book{sorensenLecturesCurryHowardIsomorphism1998,
  title = {Lectures on the {{Curry}}-{{Howard Isomorphism}}},
  abstract = {The Curry-Howard isomorphism states an amazing correspondence between systems of formal logic as encountered in proof theory and computational calculi as found in type theory. For instance, minimal propositional logic corresponds to simply typed-calculus, first-order logic corresponds to dependent types, second-order logic corresponds to polymorphic types, etc. The isomorphism has many aspects, even at the syntactic level: formulas correspond to types, proofs correspond to terms, provability corresponds to inhabitation, proof normalization corresponds to term reduction, etc. But there is much more to the isomorphism than this. For instance, it is an old idea---due to Brouwer, Kolmogorov, and Heyting, and later formalized by Kleene's realizability interpretation---that a constructive proof of an implication is a procedure that transforms proofs of the antecedent into proofs of the succedent; the Curry-Howard isomorphism gives syntactic representations of such procedures. These notes give an introduction to parts of proof theory and related aspects of type theory relevant for the Curry-Howard isomorphism.},
  author = {S\o{}rensen, Morten Heine B. and Urzyczyn, Pawel},
  year = {1998},
  file = {/Users/luigi/work/zotero/storage/TQY43C4X/Sørensen and Urzyczyn - 1998 - Lectures on the Curry-Howard Isomorphism.pdf;/Users/luigi/work/zotero/storage/S3L5JDT3/summary.html}
}

@inproceedings{sorensenUnifyingPartialEvaluation1994,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Towards Unifying Partial Evaluation, Deforestation, Supercompilation, and {{GPC}}},
  isbn = {978-3-540-57880-2 978-3-540-48376-2},
  doi = {10.1007/3-540-57880-3_32},
  abstract = {We study four transformation methodologies which are automatic instances of Burstall and Darlington's fold/unfold framework: partial evaluation, deforestation, supercompilation, and generalized partial computation (GPC). One can classify these and other fold/unfold based transformers by how much information they maintain during transformation.We introduce the positive supercompiler, a version of deforestation including more information propagation, to study such a classification in detail. Via the study of positive supercompilation we are able to show that partial evaluation and deforestation have simple information propagation, positive supercompilation has more information propagation, and supercompilation and GPC have even more information propagation. The amount of information propagation is significant: positive supercompilation, GPC, and supercompilation can specialize a general pattern matcher to a fixed pattern so as to obtain efficient output similar to that of the Knuth-Morris-Pratt algorithm. In the case of partial evaluation and deforestation, the general matcher must be rewritten to achieve this.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}} \textemdash{} {{ESOP}} '94},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {S\o{}rensen, Morten Heine and Gl\"uck, Robert and Jones, Neil D.},
  month = apr,
  year = {1994},
  pages = {485-500},
  file = {/Users/luigi/work/zotero/storage/7PPPEJ67/Sørensen et al. - 1994 - Towards unifying partial evaluation, deforestation.pdf;/Users/luigi/work/zotero/storage/2T5DUQUW/3-540-57880-3_32.html}
}

@inproceedings{horridgeIgnitingOWLTouch2007,
  title = {Igniting the {{OWL}} 1.1 {{Touch Paper}}: {{The OWL API}}},
  shorttitle = {Igniting the {{OWL}} 1.1 {{Touch Paper}}},
  abstract = {Abstract. This paper describes the design and implementation of an OWL 1.1 API, herein referred to as the OWL API. The API is designed to facilitate the manipulation of OWL 1.1 ontologies at a high level of abstraction for use by editors, reasoners and other tools. The API is based on the OWL 1.1 specification and influenced by the experience of designing and using the WonderWeb API and OWL-based applications. An overview of the basis for the design of the API is discussed along with major API functionality. The API is available from Source Forge:},
  booktitle = {In {{Proc}}. {{OWL}}-{{ED}} 2007, Volume 258 of {{CEUR}}},
  author = {Horridge, Matthew and Bechhofer, Sean},
  year = {2007},
  file = {/Users/luigi/work/zotero/storage/7DAAFDMR/Horridge and Bechhofer - 2007 - Igniting the OWL 1.1 Touch Paper The OWL API.pdf;/Users/luigi/work/zotero/storage/6NLQHGZK/summary.html}
}

@inproceedings{mitchellRunTimeCohesionMetrics2004,
  address = {Las Vegas, Nevada, USA},
  title = {Run-{{Time Cohesion Metrics}}: {{An Empirical Investigation}}},
  shorttitle = {Run-{{Time Cohesion Metrics}}},
  abstract = {Cohesion is one of the fundamental measures of the
'goodness' of a software design. The most accepted and
widely studied object-oriented cohesion metric is Chidamber
and Kemerer's Lack of Cohesion in Methods measure.
However due to the nature of object-oriented programs,
static design metrics fail to quantify all the underlying
dimensions of cohesion, as program behaviour is a
function of it operational environment as well as the complexity
of the source code. For these reasons two run-time
object-oriented cohesion metrics are described in this paper,
and applied to Java programs from the SPECjvm98
benchmark suite. A statistical analysis is conducted to assess
the fundamental properties of the measures and investigate
whether they are redundant with respect to the static
cohesion metric. Results to date indicate that run-time cohesion
metrics can provide an interesting and informative
qualitative analysis of a program and complement existing
static cohesion metrics.},
  language = {en},
  booktitle = {International {{Conference}} on {{Software Engineering Research}} and {{Practice}}},
  author = {Mitchell, Aine and Power, James F.},
  year = {2004},
  pages = {532-537},
  file = {/Users/luigi/work/zotero/storage/RNTGZH5E/Mitchell and Power - 2004 - Run-Time Cohesion Metrics An Empirical Investigat.pdf;/Users/luigi/work/zotero/storage/D7IQXXRG/6436.html}
}

@inproceedings{mcbrideEpigramPracticalProgramming2004,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Epigram: {{Practical Programming}} with {{Dependent Types}}},
  isbn = {978-3-540-28540-3 978-3-540-31872-9},
  shorttitle = {Epigram},
  doi = {10.1007/11546382_3},
  abstract = {Find the type error in the following Haskell expression:if null xs then tail xs else xsYou can't, of course: this program is obviously nonsense unless you're a typechecker. The trouble is that only certain computations make sense if the null xs test is True, whilst others make sense if it is False. However, as far as the type system is concerned, the type of the then branch is the type of the else branch is the type of the entire conditional. Statically, the test is irrelevant. Which is odd, because if the test really were irrelevant, we wouldn't do it. Of course, tail [] doesn't go wrong\textemdash{}well-typed programs don't go wrong\textemdash{}so we'd better pick a different word for the way they do go.Abstraction and application, tupling and projection: these provide the `software engineering' superstructure for programs, and our familiar type systems ensure that these operations are used compatibly. However, sooner or later, most programs inspect data and make a choice\textemdash{}at that point our familiar type systems fall silent. They simply can't talk about specific data. All this time, we thought our programming was strongly typed, when it was just our software engineering. In order to do better, we need a static language capable of expressing the significance of particular values in legitimizing some computations rather than others. We should not give up on programming.},
  language = {en},
  booktitle = {Advanced {{Functional Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {McBride, Conor},
  month = aug,
  year = {2004},
  pages = {130-170},
  file = {/Users/luigi/work/zotero/storage/KYZWA445/McBride - 2004 - Epigram Practical Programming with Dependent Type.pdf;/Users/luigi/work/zotero/storage/IC4XWP98/11546382_3.html}
}

@book{skalskiSyntaxextendingTypereflectingMacros2005,
  title = {Syntax-Extending and Type-Reflecting Macros in an Object-Oriented Language},
  author = {Skalski, Kamil},
  year = {2005},
  file = {/Users/luigi/work/zotero/storage/PRU7VGXM/Skalski - 2005 - Syntax-extending and type-reflecting macros in an .pdf;/Users/luigi/work/zotero/storage/FVXIVRPW/summary.html}
}

@inproceedings{gluckOccamRazorMetacomputation1993,
  title = {Occam's {{Razor}} in {{Metacomputation}}: The {{Notion}} of a {{Perfect Process Tree}}},
  shorttitle = {Occam's {{Razor}} in {{Metacomputation}}},
  abstract = {Abstract. We introduce the notion of a perfect process tree as a model for the full propagation of information in metacomputation. Starting with constant propagation we construct step-by-step the driving mechanism used in super-compila tion which ensures the perfect propagation of information. The concept of a simple supercompiler based on perfect driving coupled with a simple folding strategy is explained. As an example we demonstrate that specializing a naive pattern matcher with respect to a fixed pattern obtains the efficiency of a matcher generated by the Knuth, Morris \& Pratt algorithm. 1},
  booktitle = {In {{Proc}}. of the 3rd {{Int}}'l {{Workshop}} on {{Static Analysis}} ({{WSA}}'93). {{Springer LNCS}} 724},
  author = {Gl\"uck, Robert and Klimov, Andrei V.},
  year = {1993},
  pages = {112--123},
  file = {/Users/luigi/work/zotero/storage/VVUHB3HW/Glück and Klimov - 1993 - Occam’s Razor in Metacomputation the Notion of a .pdf;/Users/luigi/work/zotero/storage/CR5GYJJW/summary.html}
}

@article{sasirekhaProgramSlicingTechniques2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1108.1352},
  primaryClass = {cs},
  title = {Program Slicing Techniques and Its Applications},
  abstract = {Program understanding is an important aspect in Software Maintenance and Reengineering. Understanding the program is related to execution behaviour and relationship of variable involved in the program. The task of finding all statements in a program that directly or indirectly influence the value for an occurrence of a variable gives the set of statements that can affect the value of a variable at some point in a program is called a program slice. Program slicing is a technique for extracting parts of computer programs by tracing the programs' control and data flow related to some data item. This technique is applicable in various areas such as debugging, program comprehension and understanding, program integration, cohesion measurement, re-engineering, maintenance, testing where it is useful to be able to focus on relevant parts of large programs. This paper focuses on the various slicing techniques (not limited to) like static slicing, quasi static slicing, dynamic slicing and conditional slicing. This paper also includes various methods in performing the slicing like forward slicing, backward slicing, syntactic slicing and semantic slicing. The slicing of a program is carried out using Java which is a object oriented programming language.},
  journal = {arXiv:1108.1352 [cs]},
  author = {Sasirekha, N. and Robert, A. Edwin and Hemalatha, Dr M.},
  month = aug,
  year = {2011},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/luigi/work/zotero/storage/KVUSJNFH/Sasirekha et al. - 2011 - Program slicing techniques and its applications.pdf;/Users/luigi/work/zotero/storage/78ETG8PT/1108.html}
}

@article{galeottiInferringLoopInvariants2015,
  title = {Inferring {{Loop Invariants}} by {{Mutation}}, {{Dynamic Analysis}}, and {{Static Checking}}},
  volume = {41},
  issn = {0098-5589},
  doi = {10.1109/TSE.2015.2431688},
  abstract = {Verifiers that can prove programs correct against their full functional specification require, for programs with loops, additional annotations in the form of loop invariants-properties that hold for every iteration of a loop. We show that significant loop invariant candidates can be generated by systematically mutating postconditions; then, dynamic checking (based on automatically generated tests) weeds out invalid candidates, and static checking selects provably valid ones. We present a framework that automatically applies these techniques to support a program prover, paving the way for fully automatic verification without manually written loop invariants: Applied to 28 methods (including 39 different loops) from various java.util classes (occasionally modified to avoid using Java features not fully supported by the static checker), our DYNAMATE prototype automatically discharged 97 percent of all proof obligations, resulting in automatic complete correctness proofs of 25 out of the 28 methods-outperforming several state-of-the-art tools for fully automatic verification.},
  number = {10},
  journal = {IEEE Transactions on Software Engineering},
  author = {Galeotti, J. P. and Furia, C. A. and May, E. and Fraser, G. and Zeller, A.},
  month = oct,
  year = {2015},
  keywords = {Java,Instruments,program verification,Detectors,dynamic analysis,Arrays,automatic complete correctness proofs,automatic verification,DYNAMATE prototype,formal specification,functional properties,functional specification,Generators,Heuristic algorithms,inference,Java.util classes,loop invariant inference,Loop invariants,mutation,program control structures,program prover,program testing,Prototypes,static checking,system monitoring,test automatic generation},
  pages = {1019-1037},
  file = {/Users/luigi/work/zotero/storage/NSK7PMIL/Galeotti et al. - 2015 - Inferring Loop Invariants by Mutation, Dynamic Ana.pdf;/Users/luigi/work/zotero/storage/ZRUIZB4J/7105412.html}
}

@article{mccarthyRecursiveFunctionsSymbolic1960,
  title = {Recursive {{Functions}} of {{Symbolic Expressions}} and {{Their Computation}} by {{Machine}}, {{Part I}}},
  volume = {3},
  issn = {0001-0782},
  doi = {10.1145/367177.367199},
  number = {4},
  journal = {Commun. ACM},
  author = {McCarthy, John},
  month = apr,
  year = {1960},
  pages = {184--195},
  file = {/Users/luigi/work/zotero/storage/CN8RPXLH/McCarthy - 1960 - Recursive Functions of Symbolic Expressions and Th.pdf}
}

@article{conwayDesignSeparableTransitiondiagram1963,
  title = {Design of a {{Separable Transition}}-Diagram {{Compiler}}},
  volume = {6},
  issn = {0001-0782},
  doi = {10.1145/366663.366704},
  number = {7},
  journal = {Commun. ACM},
  author = {Conway, Melvin E.},
  month = jul,
  year = {1963},
  pages = {396--408},
  file = {/Users/luigi/work/zotero/storage/3J46KTPE/Conway - 1963 - Design of a Separable Transition-diagram Compiler.pdf}
}

@article{landinNext700Programming1966,
  title = {The {{Next}} 700 {{Programming Languages}}},
  volume = {9},
  issn = {0001-0782},
  doi = {10.1145/365230.365257},
  abstract = {A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework. This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships. Within this framework the design of a specific language splits into two independent parts. One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, list of them, functional relations among them) that can be referred to in the language.
The system is biased towards ``expressions'' rather than ``statements.'' It includes a nonprocedural (purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand.},
  number = {3},
  journal = {Commun. ACM},
  author = {Landin, P. J.},
  month = mar,
  year = {1966},
  pages = {157--166},
  file = {/Users/luigi/work/zotero/storage/RF4R2M5C/Landin - 1966 - The Next 700 Programming Languages.pdf;/Users/luigi/work/zotero/storage/Z6MDDID6/Landin - 1966 - The next 700 programming languages.pdf}
}

@article{backusCanProgrammingBe1978,
  title = {Can {{Programming Be Liberated}} from the {{Von Neumann Style}}?: {{A Functional Style}} and {{Its Algebra}} of {{Programs}}},
  volume = {21},
  issn = {0001-0782},
  shorttitle = {Can {{Programming Be Liberated}} from the {{Von Neumann Style}}?},
  doi = {10.1145/359576.359579},
  abstract = {Conventional programming languages are growing ever more enormous, but not stronger. Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor\textemdash{}the von Neumann computer, their close coupling of semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs.
An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable. Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages.
Associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose ``unknowns'' are programs in much the same way one transforms equations in high school algebra. These transformations are given by algebraic laws and are carried out in the same language in which programs are written. Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws. General theorems of the algebra give the detailed behavior and termination conditions for large classes of programs.
 A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules. Unlike von Neumann languages, these systems have semantics loosely coupled to states\textemdash{}only one state transition occurs per major computation.},
  number = {8},
  journal = {Commun. ACM},
  author = {Backus, John},
  month = aug,
  year = {1978},
  keywords = {program transformation,functional programming,programming languages,algebra of programs,applicative computing systems,applicative state transition systems,combining forms,functional forms,metacomposition,models of computing systems,program correctness,program termination,von Neumann computers,von Neumann languages},
  pages = {613--641},
  file = {/Users/luigi/work/zotero/storage/LVBPQNCD/Backus - 1978 - Can Programming Be Liberated from the Von Neumann .pdf}
}

@article{mcgillVariationsBoxPlots1978,
  title = {Variations of {{Box Plots}}},
  volume = {32},
  issn = {0003-1305},
  doi = {10.2307/2683468},
  abstract = {Box plots display batches of data. Five values from a set of data are conventionally used; the extremes, the upper and lower hinges (quartiles), and the median. Such plots are becoming a widely used tool in exploratory data analysis and in preparing visual summaries for statisticians and nonstatisticians alike. Three variants of the basic display, devised by the authors, are described. The first visually incorporates a measure of group size; the second incorporates an indication of rough significance of differences between medians; the third combines the features of the first two. These techniques are displayed by examples.},
  number = {1},
  journal = {The American Statistician},
  author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
  year = {1978},
  pages = {12-16},
  file = {/Users/luigi/work/zotero/storage/WXMWTEK2/McGill et al. - 1978 - Variations of Box Plots.pdf}
}

@article{milnerTheoryTypePolymorphism1978,
  title = {A Theory of Type Polymorphism in Programming},
  volume = {17},
  abstract = {The aim of this work is largely a practical one. A widely employed style of programming, particularly in structure-processing languages which impose no discipline of types, entails defining procedures which work well on objects of a wide variety. We present a formal type discipline for such polymorphic procedures in the context of a simple pro-gramming language, and a compile time type-checking algorithm w which enforces the discipline. A Semantic Soundness Theorem (based on a formal semantics for the language) states that well-type programs cannot ``go wrong '' and a Syntactic Soundness Theorem states that if fl accepts a program then it is well typed. We also discuss extending these results to richer languages; a type-checking algorithm based on w is in fact already implemented and working, for the metalanguage ML in the Edinburgh LCF system, 1.},
  journal = {Journal of Computer and System Sciences},
  author = {Milner, Robin},
  year = {1978},
  pages = {348--375},
  file = {/Users/luigi/work/zotero/storage/NPTHLXFI/Milner - 1978 - A theory of type polymorphism in programming.pdf;/Users/luigi/work/zotero/storage/5ZMW9IJ4/summary.html}
}

@article{bentleyProgrammingPearlsLittle1986,
  title = {Programming {{Pearls}}: {{Little Languages}}},
  volume = {29},
  issn = {0001-0782},
  shorttitle = {Programming {{Pearls}}},
  doi = {10.1145/6424.315691},
  number = {8},
  journal = {Commun. ACM},
  author = {Bentley, Jon},
  month = aug,
  year = {1986},
  pages = {711--721},
  file = {/Users/luigi/work/zotero/storage/T6W5UH3X/Bentley - 1986 - Programming Pearls Little Languages.pdf;/Users/luigi/work/zotero/storage/VQR62GW5/Bentley - 1986 - Programming pearls.pdf}
}

@inproceedings{wadlerLinearTypesCan1990,
  title = {Linear {{Types Can Change}} the {{World}}!},
  abstract = {The linear logic of J.-Y. Girard suggests a new type system for functional  languages, one which supports operations that "change the world". Values belonging  to a linear type must be used exactly once: like the world, they cannot be  duplicated or destroyed. Such values require no reference counting or garbage collection, and safely admit destructive array update. Linear types extend Schmidt's  notion of single threading; provide an alternative to Hudak and Bloss' update  analysis; and offer a practical complement to Lafont and Holmstr\"om's elegant linear languages.},
  booktitle = {Programming {{Concepts}} and {{Methods}}},
  publisher = {{North}},
  author = {Wadler, Philip},
  year = {1990},
  file = {/Users/luigi/work/zotero/storage/S2RWRZGF/Wadler - 1990 - Linear Types Can Change the World!.pdf;/Users/luigi/work/zotero/storage/9CJGCCYS/summary.html}
}

@inproceedings{liangDynamicClassLoading1998,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '98},
  title = {Dynamic {{Class Loading}} in the {{Java Virtual Machine}}},
  isbn = {978-1-58113-005-8},
  doi = {10.1145/286936.286945},
  abstract = {Class loaders are a powerful mechanism for dynamically loading software components on the Java platform. They are unusual in supporting all of the following features: laziness, type-safe linkage, user-defined extensibility, and multiple communicating namespaces.We present the notion of class loaders and demonstrate some of their interesting uses. In addition, we discuss how to maintain type safety in the presence of user-defined dynamic class loading.},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Liang, Sheng and Bracha, Gilad},
  year = {1998},
  pages = {36--44},
  file = {/Users/luigi/work/zotero/storage/3BSCK3VV/Liang and Bracha - 1998 - Dynamic Class Loading in the Java Virtual Machine.pdf}
}

@inproceedings{fosterTheoryTypeQualifiers1999,
  address = {New York, NY, USA},
  series = {{{PLDI}} '99},
  title = {A {{Theory}} of {{Type Qualifiers}}},
  isbn = {978-1-58113-094-2},
  doi = {10.1145/301618.301665},
  abstract = {We describe a framework for adding type qualifiers to a language. Type qualifiers encode a simple but highly useful form of subtyping. Our framework extends standard type rules to model the flow of qualifiers through a program, where each qualifier or set of qualifiers comes with additional rules that capture its semantics. Our framework allows types to be polymorphic in the type qualifiers. We present a const-inference system for C as an example application of the framework. We show that for a set of real C programs, many more consts can be used than are actually present in the original code.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1999 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Foster, Jeffrey S. and F\"ahndrich, Manuel and Aiken, Alexander},
  year = {1999},
  pages = {192--203},
  file = {/Users/luigi/work/zotero/storage/DJE4GFLJ/Foster et al. - 1999 - A Theory of Type Qualifiers.pdf}
}

@article{alpernJalapenoVirtualMachine2000,
  title = {The {{Jalape\~no}} Virtual Machine},
  volume = {39},
  issn = {0018-8670},
  doi = {10.1147/sj.391.0211},
  abstract = {Jalape\~no is a virtual machine for Java\texttrademark{} servers written in the Java language. To be able to address the requirements of servers (performance and scalability in particular), Jalape\~no was designed ``from scratch`` to be as self-sufficient as possible. Jalape\~no's unique object model and memory layout allows a hardware null-pointer check as well as fast access to array elements, fields, and methods. Run-time services conventionally provided in native code are implemented primarily in Java. Java threads are multiplexed by virtual processors (implemented as operating system threads). A family of concurrent object allocators and parallel type-accurate garbage collectors is supported. Jalape\~no's interoperable compilers enable quasi-preemptive thread switching and precise location of object references. Jalape\~no's dynamic optimizing compiler is designed to obtain high quality code for methods that are observed to be frequently executed or computationally intensive.},
  number = {1},
  journal = {IBM Systems Journal},
  author = {Alpern, B. and Attanasio, C. R. and Barton, J. J. and Burke, M. G. and Cheng, P. and Choi, J. D. and Cocchi, A. and Fink, S. J. and Grove, D. and Hind, M. and Hummel, S. F. and Lieber, D. and Litvinov, V. and Mergen, M. F. and Ngo, T. and Russell, J. R. and Sarkar, V. and Serrano, M. J. and Shepherd, J. C. and Smith, S. E. and Sreedhar, V. C. and Srinivasan, H. and Whaley, J.},
  year = {2000},
  pages = {211-238},
  file = {/Users/luigi/work/zotero/storage/LSVPYBSW/Alpern et al. - 2000 - The Jalape #x00F1\;o virtual machine.pdf;/Users/luigi/work/zotero/storage/R3HBL8CY/5387060.html}
}

@inproceedings{flattSubmodulesRacketYou2013,
  address = {New York, NY, USA},
  series = {{{GPCE}} '13},
  title = {Submodules in {{Racket}}: {{You Want It}} When, {{Again}}?},
  isbn = {978-1-4503-2373-4},
  shorttitle = {Submodules in {{Racket}}},
  doi = {10.1145/2517208.2517211},
  abstract = {In an extensible programming language, programmers write code that must run at different times - in particular, at compile time versus run time. The module system of the Racket programming language enables a programmer to reason about programs in the face of such extensibility, because the distinction between run-time and compile-time phases is built into the language model. Submodules extend Racket's module system to make the phase-separation facet of the language extensible. That is, submodules give programmers the capability to define new phases, such as "test time" or "documentation time," with the same reasoning and code-management benefits as the built-in distinction between run time and compile time.},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Generative Programming}}: {{Concepts}} \& {{Experiences}}},
  publisher = {{ACM}},
  author = {Flatt, Matthew},
  year = {2013},
  keywords = {language tower,macros,modules},
  pages = {13--22},
  file = {/Users/luigi/work/zotero/storage/9ZIC9MCT/Flatt - 2013 - Submodules in Racket You Want It when, Again.pdf;/Users/luigi/work/zotero/storage/E4K6X6UQ/Flatt - 2013 - Submodules in racket you want it when, again.pdf}
}

@inproceedings{diceTransactionalLockingII2006,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Transactional {{Locking II}}},
  isbn = {978-3-540-44624-8 978-3-540-44627-9},
  doi = {10.1007/11864219_14},
  abstract = {The transactional memory programming paradigm is gaining momentum as the approach of choice for replacing locks in concurrent programming. This paper introduces the transactional locking II (TL2) algorithm, a software transactional memory (STM) algorithm based on a combination of commit-time locking and a novel global version-clock based validation technique. TL2 improves on state-of-the-art STMs in the following ways: (1) unlike all other STMs it fits seamlessly with any system's memory life-cycle, including those using malloc/free (2) unlike all other lock-based STMs it efficiently avoids periods of unsafe execution, that is, using its novel version-clock validation, user code is guaranteed to operate only on consistent memory states, and (3) in a sequence of high performance benchmarks, while providing these new properties, it delivered overall performance comparable to (and in many cases better than) that of all former STM algorithms, both lock-based and non-blocking. Perhaps more importantly, on various benchmarks, TL2 delivers performance that is competitive with the best hand-crafted fine-grained concurrent structures. Specifically, it is ten-fold faster than a single lock. We believe these characteristics make TL2 a viable candidate for deployment of transactional memory today, long before hardware transactional support is available.},
  language = {en},
  booktitle = {Distributed {{Computing}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Dice, Dave and Shalev, Ori and Shavit, Nir},
  month = sep,
  year = {2006},
  pages = {194-208},
  file = {/Users/luigi/work/zotero/storage/WJCICUI2/Dice et al. - 2006 - Transactional Locking II.pdf;/Users/luigi/work/zotero/storage/7L6T2IF8/11864219_14.html}
}

@inproceedings{fluetLinearRegionsAre2006,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Linear {{Regions Are All You Need}}},
  isbn = {978-3-540-33095-0 978-3-540-33096-7},
  doi = {10.1007/11693024_2},
  abstract = {The type-and-effects system of the Tofte-Talpin region calculus makes it possible to safely reclaim objects without a garbage collector. However, it requires that regions have last-in-first-out (LIFO) lifetimes following the block structure of the language. We introduce {$\lambda$}rgnUL, a core calculus that is powerful enough to encode Tofte-Talpin-like languages, and that eliminates the LIFO restriction. The target language has an extremely simple, substructural type system. To prove the power of the language, we sketch how Tofte-Talpin-style regions, as well as the first-class dynamic regions and unique pointers of the Cyclone programming language can be encoded in {$\lambda$}rgnUL.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Fluet, Matthew and Morrisett, Greg and Ahmed, Amal},
  month = mar,
  year = {2006},
  pages = {7-21},
  file = {/Users/luigi/work/zotero/storage/TSRQCCB8/Fluet et al. - 2006 - Linear Regions Are All You Need.pdf;/Users/luigi/work/zotero/storage/DTF65I98/11693024_2.html}
}

@inproceedings{govindarajuMemoryModelScientific2006,
  address = {New York, NY, USA},
  series = {{{SC}} '06},
  title = {A {{Memory Model}} for {{Scientific Algorithms}} on {{Graphics Processors}}},
  isbn = {978-0-7695-2700-0},
  doi = {10.1145/1188455.1188549},
  abstract = {We present a memory model to analyze and improve the performance of scientific algorithms on graphics processing units (GPUs). Our memory model is based on texturing hardware, which uses a 2D block-based array representation to perform the underlying computations. We incorporate many characteristics of GPU architectures including smaller cache sizes, 2D block representations, and use the 3C's model to analyze the cache misses. Moreover. we present techniques to improve the performance of nested loops on GPUs. In order to demonstrate the effectiveness of our model, we highlight its performance on three memory-intensive scientific applications - sorting, fast Fourier transform and dense matrix-multiplication. In practice, our cache-efficient algorithms for these applications are able to achieve memory throughput of 30-50 GB/s on a NVIDIA 7900 GTX GPU. We also compare our results with prior GPU-based and CPU-based implementations on high-end processors. In practice, we are able to achieve 2-5 x performance improvement.},
  booktitle = {Proceedings of the 2006 {{ACM}}/{{IEEE Conference}} on {{Supercomputing}}},
  publisher = {{ACM}},
  author = {Govindaraju, Naga K. and Larsen, Scott and Gray, Jim and Manocha, Dinesh},
  year = {2006},
  keywords = {graphics processors,memory model,scientific algorithms},
  file = {/Users/luigi/work/zotero/storage/FE48SUFG/Govindaraju et al. - 2006 - A Memory Model for Scientific Algorithms on Graphi.pdf}
}

@article{grossmanQuantifiedTypesImperative2006,
  title = {Quantified {{Types}} in an {{Imperative Language}}},
  volume = {28},
  issn = {0164-0925},
  doi = {10.1145/1133651.1133653},
  abstract = {We describe universal types, existential types, and type constructors in Cyclone, a strongly typed C-like language. We show how the language naturally supports first-class polymorphism and polymorphic recursion while requiring an acceptable amount of explicit type information. More importantly, we consider the soundness of type variables in the presence of C-style mutation and the address-of operator. For polymorphic references, we describe a solution more natural for the C level than the ML-style ``value restriction.'' For existential types, we discover and subsequently avoid a subtle unsoundness issue resulting from the address-of operator. We develop a formal abstract machine and type-safety proof that capture the essence of type variables at the C level.},
  number = {3},
  journal = {ACM Trans. Program. Lang. Syst.},
  author = {Grossman, Dan},
  month = may,
  year = {2006},
  keywords = {Cyclone,existential types,polymorphism,type variables},
  pages = {429--475},
  file = {/Users/luigi/work/zotero/storage/JA5CG7SD/Grossman - 2006 - Quantified Types in an Imperative Language.pdf}
}

@inproceedings{schererScalableSynchronousQueues2006,
  address = {New York, NY, USA},
  series = {{{PPoPP}} '06},
  title = {Scalable {{Synchronous Queues}}},
  isbn = {978-1-59593-189-4},
  doi = {10.1145/1122971.1122994},
  abstract = {We present two new nonblocking and contention-free implementations of synchronous queues ,concurrent transfer channels in which producers wait for consumers just as consumers wait for producers. Our implementations extend our previous work in dual queues and dual stacks to effect very high-performance handoff. We present performance results on 16-processor SPARC and 4-processor Opteron machines. We compare our algorithms to commonly used alternatives from the literature and from the Java SE 5.0 class java. util. concurrent. SynchronousQueue both directly in synthetic microbenchmarks and indirectly as the core of Java's Thread-PoolExecutor mechanism (which in turn is the core of many Java server programs).Our new algorithms consistently outperform the Java SE 5.0 SynchronousQueue by factors of three in unfair mode and 14 in fair mode; this translates to factors of two and ten for the ThreadPoolExecutor. Our synchronous queues have been adopted for inclusion in Java 6.},
  booktitle = {Proceedings of the {{Eleventh ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  publisher = {{ACM}},
  author = {Scherer, III, William N. and Lea, Doug and Scott, Michael L.},
  year = {2006},
  keywords = {contention freedom,dual data structures,dual queue,dual stack,lock freedom,nonblocking synchronization,synchronous queue},
  pages = {147--156},
  file = {/Users/luigi/work/zotero/storage/E84DB5D6/Scherer et al. - 2006 - Scalable Synchronous Queues.pdf}
}

@inproceedings{maebeJavanaSystemBuilding2006,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '06},
  title = {Javana: {{A System}} for {{Building Customized Java Program Analysis Tools}}},
  isbn = {978-1-59593-348-5},
  shorttitle = {Javana},
  doi = {10.1145/1167473.1167487},
  abstract = {Understanding the behavior of applications running on high-level language virtual machines, as is the case in Java, is non-trivial because of the tight entanglement at the lowest execution level between the application and the virtual machine. This paper proposes Javana, a system for building Java program analysis tools. Javana provides an easy-to-use instrumentation infrastructure that allows for building customized profiling tools very quickly.Javana runs a dynamic binary instrumentation tool underneath the virtual machine. The virtual machine communicates with the instrumentation layer through an event handling mechanism for building a vertical map that links low-level native instruction pointers and memory addresses to high-level language concepts such as objects, methods, threads, lines of code, etc. The dynamic binary instrumentation tool then intercepts all memory accesses and instructions executed and provides the Javana end user with high-level language information for all memory accesses and natively executed instructions.We demonstrate the power of Javana through a number of applications: memory address tracing, vertical cache simulation and object lifetime computation. For each of these applications, the instrumentation specification requires only a small number of lines of code. Developing similarly powerful profiling tools within a virtual machine (as done in current practice) is both time-consuming and error-prone; in addition, the accuracy of the obtained profiling results might be questionable as we show in this paper.},
  booktitle = {Proceedings of the 21st {{Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Maebe, Jonas and Buytaert, Dries and Eeckhout, Lieven and De Bosschere, Koen},
  year = {2006},
  keywords = {Java,aspect-oriented instrumentation,customized program analysis tool},
  pages = {153--168},
  file = {/Users/luigi/work/zotero/storage/WY9HPGG8/Maebe et al. - 2006 - Javana A System for Building Customized Java Prog.pdf}
}

@article{xiDependentMLApproach2007,
  title = {Dependent {{ML An}} Approach to Practical Programming with Dependent Types},
  volume = {17},
  issn = {1469-7653, 0956-7968},
  doi = {10.1017/S0956796806006216},
  abstract = {AbstractWe present an approach to enriching the type system of ML with a restricted form of dependent types, where type index terms are required to be drawn from a given type index language  that is completely separate from run-time programs, leading to the DML() language schema. This enrichment allows for specification and inference of significantly more precise type information, facilitating program error detection and compiler optimization. The primary contribution of the paper lies in our language design, which can effectively support the use of dependent types in practical programming. In particular, this design makes it both natural and straightforward to accommodate dependent types in the presence of effects such as references and exceptions.},
  number = {2},
  journal = {Journal of Functional Programming},
  author = {Xi, Hongwei},
  month = mar,
  year = {2007},
  pages = {215-286},
  file = {/Users/luigi/work/zotero/storage/FMM8TH8F/Xi - 2007 - Dependent ML An approach to practical programming .pdf;/Users/luigi/work/zotero/storage/3XTMU5CH/4A1FC643ACD49EF31DAF5EB955D2CCC7.html}
}

@inproceedings{beyerRelationalProgrammingCrocoPat2006,
  address = {New York, NY, USA},
  series = {{{ICSE}} '06},
  title = {Relational {{Programming}} with {{CrocoPat}}},
  isbn = {978-1-59593-375-1},
  doi = {10.1145/1134285.1134420},
  abstract = {Many structural analyses of software systems are naturally formalized as relational queries, for example, the detection of design patterns, patterns of problematic design, code clones, dead code, and differences between the as-built and the as-designed architecture. This paper describes CrocoPat, an application-independent tool for relational programming. Through its efficiency and its expressive language, CrocoPat enables practically important analyses of real-world software systems that are not possible with other graph analysis tools, in particular analyses that involve transitive closures and the detection of patterns in graphs. The language is easy to use, because it is based on the well-known first-order predicate logic. The tool is easy to integrate into other software systems, because it is a small command-line tool that uses a simple text format for input and output of relations.},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Beyer, Dirk},
  year = {2006},
  keywords = {pattern matching,BDD,graph models,predicate logic,relational algebra,software analysis,transitive closure},
  pages = {807--810},
  file = {/Users/luigi/work/zotero/storage/7D7EDKKI/Beyer - 2006 - Relational Programming with CrocoPat.pdf}
}

@inproceedings{albertHeapSpaceAnalysis2007,
  address = {New York, NY, USA},
  series = {{{ISMM}} '07},
  title = {Heap {{Space Analysis}} for {{Java Bytecode}}},
  isbn = {978-1-59593-893-0},
  doi = {10.1145/1296907.1296922},
  abstract = {This article presents a heap space analysis for (sequential) Java bytecode. The analysis generates heap space cost relations which define at compile-time the heap consumption of a program as a function of its data size. These relations can be used to obtain upper bounds on the heap space located during the execution of the different methods. In addition, we describe how to refine the cost relations, by relying on escape analysis, in order to take into account the heap space that can be safely deallocated by the garbage collector upon exit from a corresponding method. These refined cost relations are then used to infer upper bounds on the active heap space upon methods return. Example applications for the analysis consider inference of constant heap usage and heap usage proportional to the data size (including polynomial and exponential heap consumption). Our prototype implementation is reported and demonstrated by means of a series of examples which illustrate how the analysis naturally encompasses standard data-structures like lists, trees and arrays with several dimensions written in object-oriented programming style.},
  booktitle = {Proceedings of the 6th {{International Symposium}} on {{Memory Management}}},
  publisher = {{ACM}},
  author = {Albert, Elvira and Genaim, Samir and {Gomez-Zamalloa}, Miguel},
  year = {2007},
  keywords = {Java bytecode,heap consumption,heap space analysis,low-level languages},
  pages = {105--116},
  file = {/Users/luigi/work/zotero/storage/MZG6ZL4H/Albert et al. - 2007 - Heap Space Analysis for Java Bytecode.pdf}
}

@inproceedings{binderAdvancedJavaBytecode2007,
  address = {New York, NY, USA},
  series = {{{PPPJ}} '07},
  title = {Advanced {{Java Bytecode Instrumentation}}},
  isbn = {978-1-59593-672-1},
  doi = {10.1145/1294325.1294344},
  abstract = {Bytecode instrumentation is a valuable technique for transparently enhancing virtual execution environments for purposes such as monitoring or profiling. Current approaches to bytecode instrumentation either exclude some methods from instrumentation, severely restrict the ways certain methods may be instrumented, or require the use of native code. In this paper we compare different approaches to bytecode instrumentation in Java and come up with a novel instrumentation framework that goes beyond the aforementioned limitations. We evaluate our approach with an instrumentation for profiling which generates calling context trees of various platform-independent dynamic metrics.},
  booktitle = {Proceedings of the 5th {{International Symposium}} on {{Principles}} and {{Practice}} of {{Programming}} in {{Java}}},
  publisher = {{ACM}},
  author = {Binder, Walter and Hulaas, Jarle and Moret, Philippe},
  year = {2007},
  keywords = {Java,dynamic bytecode instrumentation,dynamic metrics,JVM,profiling,program transformations},
  pages = {135--144},
  file = {/Users/luigi/work/zotero/storage/7FGNB9FA/Binder et al. - 2007 - Advanced Java Bytecode Instrumentation.pdf}
}

@book{blackburnMoxieJVMExperience2008,
  title = {The {{Moxie JVM Experience}}},
  abstract = {By January 1998, only two years after the launch of the first Java virtual machine, almost all JVMs in use today had been architected. In the nine years since, technology has advanced enormously, with respect to the underlying hardware, language implementation, and in the application domain. Although JVM technology has moved forward in leaps and bounds, basic design decisions made in the 90's has anchored JVM implementation. The Moxie project set out to explore the question: `How would we design a JVM from scratch knowing what we know today?' Amid the mass of design questions we faced, the tension between performance and flexibility was pervasive, persistent and problematic. In this experience paper we describe the Moxie project and its lessons, a process which began with consulting experts from industry and academia, and ended with a fully working prototype.},
  author = {Blackburn, Stephen M. and Salishev, Sergey I. and Danilov, Mikhail and Mokhovikov, Oleg A. and Nashatyrev, Anton A. and Novodvorsky, Peter A. and Bogdanov, Vadim I. and Li, Xiao Feng and Ushakov, Dennis},
  year = {2008},
  file = {/Users/luigi/work/zotero/storage/ALURTFGR/Blackburn et al. - The Moxie JVM Experience.pdf;/Users/luigi/work/zotero/storage/AT4YQ53N/summary.html}
}

@inproceedings{mitchellCausesBloatLimits2007,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '07},
  title = {The {{Causes}} of {{Bloat}}, the {{Limits}} of {{Health}}},
  isbn = {978-1-59593-786-5},
  doi = {10.1145/1297027.1297046},
  abstract = {Applications often have large runtime memory requirements. In some cases, large memory footprint helps accomplish an important functional, performance, or engineering requirement. A large cache,for example, may ameliorate a pernicious performance problem. In general, however, finding a good balance between memory consumption and other requirements is quite challenging. To do so, the development team must distinguish effective from excessive use of memory. We introduce health signatures to enable these distinctions. Using data from dozens of applications and benchmarks, we show that they provide concise and application-neutral summaries of footprint. We show how to use them to form value judgments about whether a design or implementation choice is good or bad. We show how being independent ofany application eases comparison across disparate implementations. We demonstrate the asymptotic nature of memory health: certain designsare limited in the health they can achieve, no matter how much the data size scales up. Finally, we show how to use health signatures to automatically generate formulas that predict this asymptotic behavior, and show how they enable powerful limit studies on memory health.},
  booktitle = {Proceedings of the {{22Nd Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming Systems}} and {{Applications}}},
  publisher = {{ACM}},
  author = {Mitchell, Nick and Sevitsky, Gary},
  year = {2007},
  keywords = {bloat,limit studies,memory footprint,metrics},
  pages = {245--260},
  file = {/Users/luigi/work/zotero/storage/ZK76FBXJ/Mitchell and Sevitsky - 2007 - The Causes of Bloat, the Limits of Health.pdf}
}

@article{musuvathiCHESSSystematicTesting2007,
  title = {{{CHESS}}: {{A}} Systematic Testing Tool for Concurrent Software},
  shorttitle = {{{CHESS}}},
  abstract = {Concurrency is used pervasively in the development of large systems programs. However, concurrent programming is difficult because of the possibility of unexpected interference among concurrently executing tasks. Such interference often results in ``Heisenbugs'' that appear rarely and are extremely difficult to reproduce and debug. Stress testing, in which the system is run under heavy load \ldots{}},
  journal = {Microsoft Research},
  author = {Musuvathi, Madan and Qadeer, Shaz and Ball, Tom},
  month = nov,
  year = {2007},
  file = {/Users/luigi/work/zotero/storage/I8QIJRAL/Musuvathi et al. - 2007 - CHESS A systematic testing tool for concurrent so.pdf;/Users/luigi/work/zotero/storage/K8SXA6EB/chess-a-systematic-testing-tool-for-concurrent-software.html}
}

@inproceedings{nethercoteValgrindFrameworkHeavyweight2007,
  address = {New York, NY, USA},
  series = {{{PLDI}} '07},
  title = {Valgrind: {{A Framework}} for {{Heavyweight Dynamic Binary Instrumentation}}},
  isbn = {978-1-59593-633-2},
  shorttitle = {Valgrind},
  doi = {10.1145/1250734.1250746},
  abstract = {Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited. In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values-a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.},
  booktitle = {Proceedings of the 28th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Nethercote, Nicholas and Seward, Julian},
  year = {2007},
  keywords = {dynamic binary analysis,dynamic binary instrumentation,Memcheck,shadow values,Valgrind},
  pages = {89--100},
  file = {/Users/luigi/work/zotero/storage/5QUIUZLW/Nethercote and Seward - 2007 - Valgrind A Framework for Heavyweight Dynamic Bina.pdf}
}

@inproceedings{phansalkarAnalysisRedundancyApplication2007,
  address = {New York, NY, USA},
  series = {{{ISCA}} '07},
  title = {Analysis of {{Redundancy}} and {{Application Balance}} in the {{SPEC CPU2006 Benchmark Suite}}},
  isbn = {978-1-59593-706-3},
  doi = {10.1145/1250662.1250713},
  abstract = {The recently released SPEC CPU2006 benchmark suite is expected to be used by computer designers and computer architecture researchers for pre-silicon early design analysis. Partial use of benchmark suites by researchers, due to simulation time constraints, compiler difficulties, or library or system call issues is likely to happen; but a random subset can lead to misleading results. This paper analyzes the SPEC CPU2006 benchmarks using performance counter based experimentation from several state of the art systems, and uses statistical techniques such as principal component analysis and clustering to draw inferences on the similarity of the benchmarks and the redundancy in the suite and arrive at meaningful subsets. The SPEC CPU2006 benchmark suite contains several programs from areas such as artificial intelligence and includes none from the electronic design automation (EDA) application area. Hence there is a concern on the application balance in the suite. An analysis from the perspective of fundamental program characteristics shows that the included programs offer characteristics broader than the EDA programs' space. A subset of 6 integer programs and 8 floating point programs can yield most of the information from the entire suite.},
  booktitle = {Proceedings of the 34th {{Annual International Symposium}} on {{Computer Architecture}}},
  publisher = {{ACM}},
  author = {Phansalkar, Aashish and Joshi, Ajay and John, Lizy K.},
  year = {2007},
  keywords = {clustering,benchmark,SPEC,microprocessor performance counters},
  pages = {412--423},
  file = {/Users/luigi/work/zotero/storage/YLG5BRB8/Phansalkar et al. - 2007 - Analysis of Redundancy and Application Balance in .pdf}
}

@inproceedings{xiDependentlyTypedAssembly2001,
  address = {New York, NY, USA},
  series = {{{ICFP}} '01},
  title = {A {{Dependently Typed Assembly Language}}},
  isbn = {978-1-58113-415-5},
  doi = {10.1145/507635.507657},
  abstract = {We present a dependently typed assembly language (DTAL) in which the type system supports the use of a restricted form of dependent types, reaping some benefits of dependent types at the assembly level. DTAL improves upon TAL , enabling certain important compiler optimizations such as run-time array bound check elimination and tag check elimination. Also, DTAL formally addresses the issue of representing sum types at assembly level, making it suitable for handling not only datatypes in ML but also dependent datatypes in Dependent ML (DML).},
  booktitle = {Proceedings of the {{Sixth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Xi, Hongwei and Harper, Robert},
  year = {2001},
  pages = {169--180},
  file = {/Users/luigi/work/zotero/storage/DTSRVI37/Xi and Harper - 2001 - A Dependently Typed Assembly Language.pdf}
}

@inproceedings{jonesFlexibleApproachInterprocedural1982,
  address = {New York, NY, USA},
  series = {{{POPL}} '82},
  title = {A {{Flexible Approach}} to {{Interprocedural Data Flow Analysis}} and {{Programs}} with {{Recursive Data Structures}}},
  isbn = {978-0-89791-065-1},
  doi = {10.1145/582153.582161},
  abstract = {A new approach to data flow analysis of procedural programs and programs with recursive data structures is described. The method depends on simulation of the interpreter for the subject programming language using a retrieval function to approximate a program's data structures.},
  booktitle = {Proceedings of the 9th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Jones, Neil D. and Muchnick, Steven S.},
  year = {1982},
  pages = {66--74},
  file = {/Users/luigi/work/zotero/storage/9LIQQXI8/Jones and Muchnick - 1982 - A Flexible Approach to Interprocedural Data Flow A.pdf}
}

@book{martin-lofIntuitionisticTypeTheory1984,
  title = {Intuitionistic Type Theory},
  language = {en},
  publisher = {{Bibliopolis}},
  author = {{Martin-L\"of}, Per},
  year = {1984},
  keywords = {Intuitionistic mathematics,Logic; Symbolic and mathematical,Mathematics / Logic,Proof theory,Set theory},
  file = {/Users/luigi/work/zotero/storage/YJC8MIL8/71779e2cd1423f6e4fd3f93f2ec668462f1f.pdf}
}

@article{ferranteProgramDependenceGraph1987,
  title = {The {{Program Dependence Graph}} and {{Its Use}} in {{Optimization}}},
  volume = {9},
  issn = {0164-0925},
  doi = {10.1145/24039.24041},
  abstract = {In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
  number = {3},
  journal = {ACM Trans. Program. Lang. Syst.},
  author = {Ferrante, Jeanne and Ottenstein, Karl J. and Warren, Joe D.},
  month = jul,
  year = {1987},
  pages = {319--349},
  file = {/Users/luigi/work/zotero/storage/KKWRVLHT/Ferrante et al. - 1987 - The Program Dependence Graph and Its Use in Optimi.pdf}
}

@inproceedings{janzenNavigatingQueryingCode2003,
  address = {New York, NY, USA},
  series = {{{AOSD}} '03},
  title = {Navigating and {{Querying Code Without Getting Lost}}},
  isbn = {978-1-58113-660-9},
  doi = {10.1145/643603.643622},
  abstract = {A development task related to a crosscutting concern is challenging because a developer can easily get lost when exploring scattered elements of code and the complex tangle of relationships between them. In this paper we present a source browsing tool that improves the developer's ability to work with crosscutting concerns by providing better support for exploring code. Our tool helps the developer to remain oriented while exploring and navigating across a code base. The cognitive burden placed on a developer is reduced by avoiding disorienting view switches and by providing an explicit representation of the exploration process in terms of exploration paths. While our tool is generally useful, good navigation support is particularly important when exploring crosscutting concerns.},
  booktitle = {Proceedings of the {{2Nd International Conference}} on {{Aspect}}-Oriented {{Software Development}}},
  publisher = {{ACM}},
  author = {Janzen, Doug and De Volder, Kris},
  year = {2003},
  keywords = {proposal},
  pages = {178--187},
  file = {/Users/luigi/work/zotero/storage/QH8Y37R8/Janzen and De Volder - 2003 - Navigating and Querying Code Without Getting Lost.pdf;/Users/luigi/work/zotero/storage/UZUPHZDJ/Janzen and De Volder - 2003 - Navigating and querying code without getting lost.pdf}
}

@inproceedings{kerrCharacterizationAnalysisPTX2009,
  title = {A Characterization and Analysis of {{PTX}} Kernels},
  doi = {10.1109/IISWC.2009.5306801},
  abstract = {General purpose application development for GPUs (GPGPU) has recently gained momentum as a cost-effective approach for accelerating data- and compute-intensive applications. It has been driven by the introduction of C-based programming environments such as NVIDIA's CUDA, OpenCL, and Intel's Ct. While significant effort has been focused on developing and evaluating applications and software tools, comparatively little has been devoted to the analysis and characterization of applications to assist future work in compiler optimizations, application re-structuring, and micro-architecture design. This paper proposes a set of metrics for GPU workloads and uses these metrics to analyze the behavior of GPU programs. We report on an analysis of over 50 kernels and applications including the full NVIDIA CUDA SDK and UIUC's Parboil Benchmark Suite covering control flow, data flow, parallelism, and memory behavior. The analysis was performed using a full function emulator we developed that implements the NVIDIA virtual machine referred to as PTX (parallel thread execution architecture) - a machine model and low level virtual ISA that is representative of ISAs for data parallel execution. The emulator can execute compiled kernels from the CUDA compiler, currently supports the full PTX 1.4 specification, and has been validated against the full CUDA SDK. The results quantify the importance of optimizations such as those for branch reconvergence, the prevalance of sharing between threads, and highlights opportunities for additional parallelism.},
  booktitle = {2009 {{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}})},
  author = {Kerr, A. and Diamos, G. and Yalamanchili, S.},
  month = oct,
  year = {2009},
  keywords = {Application software,Computer applications,Yarn,virtual machines,Kernel,Parallel processing,GPU,optimising compilers,Program processors,Software tools,virtual machine,proposal,Acceleration,application re-structuring,branch reconvergence,C language,C-based programming environment,compiler optimization,compute-intensive application,control flow,cost-effective approach,data flow,data flow analysis,data parallel execution,data-intensive application,full function emulator,general purpose application development,Instruction sets,low level virtual ISA,machine model,memory behavior,microarchitecture design,multi-threading,NVIDIA CUDA SDK,OpenCL,operating system kernels,parallel thread execution architecture,parallelism,program behavior analysis,programming environments,Programming environments,PTX kernel,software tool,UIUC Parboil Benchmark Suite},
  pages = {3-12},
  file = {/Users/luigi/work/zotero/storage/LYFT8KBG/Kerr et al. - 2009 - A characterization and analysis of PTX kernels.pdf;/Users/luigi/work/zotero/storage/TLR2QB5N/5306801.html}
}

@inproceedings{milevaMiningTrendsLibrary2009,
  address = {New York, NY, USA},
  series = {{{IWPSE}}-{{Evol}} '09},
  title = {Mining {{Trends}} of {{Library Usage}}},
  isbn = {978-1-60558-678-6},
  doi = {10.1145/1595808.1595821},
  abstract = {A library is available in multiple versions. Which one should I use? Has it been widely adopted already? Was it a good decision to switch to the newest version? We have mined hundreds of open-source projects for their library dependencies, and determined global trends in library usage. This wisdom of the crowds can be helpful for developers when deciding when to use which version of a library - by helping them avoid pitfalls experienced by other developers, and by showing important emerging trends in library usage.},
  booktitle = {Proceedings of the {{Joint International}} and {{Annual ERCIM Workshops}} on {{Principles}} of {{Software Evolution}} ({{IWPSE}}) and {{Software Evolution}} ({{Evol}}) {{Workshops}}},
  publisher = {{ACM}},
  author = {Mileva, Yana Momchilova and Dallmeier, Valentin and Burger, Martin and Zeller, Andreas},
  year = {2009},
  keywords = {global usage trends,library versions usage,mining software archives},
  pages = {57--62},
  file = {/Users/luigi/work/zotero/storage/H65YSF37/Mileva et al. - 2009 - Mining trends of library usage.pdf;/Users/luigi/work/zotero/storage/Z3C7M8PS/Mileva et al. - 2009 - Mining Trends of Library Usage.pdf}
}

@inproceedings{nightingaleHeliosHeterogeneousMultiprocessing2009,
  address = {New York, NY, USA},
  series = {{{SOSP}} '09},
  title = {Helios: {{Heterogeneous Multiprocessing}} with {{Satellite Kernels}}},
  isbn = {978-1-60558-752-3},
  shorttitle = {Helios},
  doi = {10.1145/1629575.1629597},
  abstract = {Helios is an operating system designed to simplify the task of writing, deploying, and tuning applications for heterogeneous platforms. Helios introduces satellite kernels, which export a single, uniform set of OS abstractions across CPUs of disparate architectures and performance characteristics. Access to I/O services such as file systems are made transparent via remote message passing, which extends a standard microkernel message-passing abstraction to a satellite kernel infrastructure. Helios retargets applications to available ISAs by compiling from an intermediate language. To simplify deploying and tuning application performance, Helios exposes an affinity metric to developers. Affinity provides a hint to the operating system about whether a process would benefit from executing on the same platform as a service it depends upon. We developed satellite kernels for an XScale programmable I/O card and for cache-coherent NUMA architectures. We offloaded several applications and operating system components, often by changing only a single line of metadata. We show up to a 28\% performance improvement by offloading tasks to the XScale I/O card. On a mail-server benchmark, we show a 39\% improvement in performance by automatically splitting the application among multiple NUMA domains.},
  booktitle = {Proceedings of the {{ACM SIGOPS 22Nd Symposium}} on {{Operating Systems Principles}}},
  publisher = {{ACM}},
  author = {Nightingale, Edmund B. and Hodson, Orion and McIlroy, Ross and Hawblitzel, Chris and Hunt, Galen},
  year = {2009},
  keywords = {operating systems,heterogeneous computing},
  pages = {221--234},
  file = {/Users/luigi/work/zotero/storage/7XJ4J8ZZ/Nightingale et al. - 2009 - Helios Heterogeneous Multiprocessing with Satelli.pdf}
}

@inproceedings{aminJavaScalaType2016,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} 2016},
  title = {Java and {{Scala}}'s {{Type Systems Are Unsound}}: {{The Existential Crisis}} of {{Null Pointers}}},
  isbn = {978-1-4503-4444-9},
  shorttitle = {Java and {{Scala}}'s {{Type Systems Are Unsound}}},
  doi = {10.1145/2983990.2984004},
  abstract = {We present short programs that demonstrate the unsoundness of Java and Scala's current type systems. In particular, these programs provide parametrically polymorphic functions that can turn any type into any type without (down)casting. Fortunately, parametric polymorphism was not integrated into the Java Virtual Machine (JVM), so these examples do not demonstrate any unsoundness of the JVM. Nonetheless, we discuss broader implications of these findings on the field of programming languages.},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Amin, Nada and Tate, Ross},
  year = {2016},
  keywords = {Java,Existential,Null,Scala,Unsoundness},
  pages = {838--848},
  file = {/Users/luigi/work/zotero/storage/TQT6KYYP/Amin and Tate - 2016 - Java and Scala's Type Systems Are Unsound The Exi.pdf}
}

@inproceedings{lavazzaEmpiricalStudyEffect2016,
  address = {New York, NY, USA},
  series = {{{SAC}} '16},
  title = {An {{Empirical Study}} on the {{Effect}} of {{Programming Languages}} on {{Productivity}}},
  isbn = {978-1-4503-3739-7},
  doi = {10.1145/2851613.2851780},
  abstract = {Background -- Software development productivity is of great practical interest and has been widely investigated in the past. However, due to the rapid evolution of software development techniques and methods and the constant improvement in the use of existing ones, continuously updated evidence on productivity is constantly needed. Objectives -- The research documented in this paper has the main goal to identify how different programming languages may affect productivity. Method -- We analysed the ISBSG dataset, probably the largest public repository of data on software projects, with focus on the primary programming language used to develop each software project. We followed a rigorous statistical analysis approach. Moreover, we compared our analysis with the productivity data provided by Capers Jones in 1996 and 2013 and with an investigation on open-source software by Delorey et al. Results -- The implementation programming language of software projects seems to affect productivity. The comparison between the productivity level of each of the analysed programming languages shows important differences with the results by Capers Jones and Delorey et al. Conclusions --This paper provides some more evidence about how each programming language has its own productivity level and highlights some interesting divergences with the results reported by Capers Jones and Delorey et al.},
  booktitle = {Proceedings of the 31st {{Annual ACM Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  author = {Lavazza, Luigi and Morasca, Sandro and Tosi, Davide},
  year = {2016},
  keywords = {empirical study,programming languages,software productivity},
  pages = {1434--1439},
  file = {/Users/luigi/work/zotero/storage/A7APUC2P/Lavazza et al. - 2016 - An Empirical Study on the Effect of Programming La.pdf}
}

@inproceedings{hanenbergWhyWeKnow2014,
  address = {New York, NY, USA},
  series = {{{DLS}} '14},
  title = {Why {{Do We Know So Little About Programming Languages}}, and {{What Would Have Happened}} If {{We Had Known More}}?},
  isbn = {978-1-4503-3211-8},
  doi = {10.1145/2661088.2661102},
  abstract = {Programming language research in the last decades was mainly driven by mathematical methods (such as formal semantics, correctness proofs, type soundness proofs, etc.) or run-time arguments based on benchmark tests. This happened despite the frequent discussion over programming language usability. We have now been through decade after decade of one language after another domainating the field, forcing companies to switch languages and migrate libraries. Now that Javascript seems to be the next language to dominate, people start to ask old questions anew. The first goal of this talk is to discuss why the application of empirical methods is (still) relatively rare in PL research, and to discuss what could be done in empirical methods to make them a substantial part of PL research. The second goal is to speculate about the possible effects that concrete empirical knowledge could have had on the programming language community. For example, what would have happened to programming languages if current knowledge would have been available 30 years ago? What if knowledge about programming languages from the year 2050 would be available today?},
  booktitle = {Proceedings of the 10th {{ACM Symposium}} on {{Dynamic Languages}}},
  publisher = {{ACM}},
  author = {Hanenberg, Stefan},
  year = {2014},
  keywords = {empirical studies,programming languages,controlled experiments},
  pages = {1--1},
  file = {/Users/luigi/work/zotero/storage/U3IAZ9NG/Hanenberg - 2014 - Why Do We Know So Little About Programming Languag.pdf}
}

@inproceedings{hanenbergExperimentStaticDynamic2010,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '10},
  title = {An {{Experiment About Static}} and {{Dynamic Type Systems}}: {{Doubts About}} the {{Positive Impact}} of {{Static Type Systems}} on {{Development Time}}},
  isbn = {978-1-4503-0203-6},
  shorttitle = {An {{Experiment About Static}} and {{Dynamic Type Systems}}},
  doi = {10.1145/1869459.1869462},
  abstract = {Although static type systems are an essential part in teach-ing and research in software engineering and computer science, there is hardly any knowledge about what the impact of static type systems on the development time or the resulting quality for a piece of software is. On the one hand there are authors that state that static type systems decrease an application's complexity and hence its development time (which means that the quality must be improved since developers have more time left in their projects). On the other hand there are authors that argue that static type systems increase development time (and hence decrease the code quality) since they restrict developers to express themselves in a desired way. This paper presents an empirical study with 49 subjects that studies the impact of a static type system for the development of a parser over 27 hours working time. In the experiments the existence of the static type system has neither a positive nor a negative impact on an application's development time (under the conditions of the experiment).},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  publisher = {{ACM}},
  author = {Hanenberg, Stefan},
  year = {2010},
  keywords = {empirical study,type systems,programming languages,dynamically typed languages},
  pages = {22--35},
  file = {/Users/luigi/work/zotero/storage/5X3ESV2L/Hanenberg - 2010 - An Experiment About Static and Dynamic Type System.pdf}
}

@inproceedings{latozaAnsweringControlFlow2008,
  address = {New York, NY, USA},
  series = {{{OOPSLA Companion}} '08},
  title = {Answering {{Control Flow Questions About Code}}},
  isbn = {978-1-60558-220-7},
  doi = {10.1145/1449814.1449909},
  abstract = {Empirical observations of developers editing code revealed that difficulties following control flow relationships led to poor changes, wasted time, and bugs. I am designing a static analysis to compute interprocedural path-sensitive control flow to help developers more quickly and accurately visually answer these common questions about code.},
  booktitle = {Companion to the 23rd {{ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming Systems Languages}} and {{Applications}}},
  publisher = {{ACM}},
  author = {LaToza, Thomas D.},
  year = {2008},
  keywords = {empirical study,program comprehension,dataflow analysis},
  pages = {921--922},
  file = {/Users/luigi/work/zotero/storage/3HFACSWQ/LaToza - 2008 - Answering Control Flow Questions About Code.pdf}
}

@inproceedings{voineaBenefitsSessionTypes2016,
  address = {New York, NY, USA},
  series = {{{PLATEAU}} 2016},
  title = {Benefits of {{Session Types}} for {{Software Development}}},
  isbn = {978-1-4503-4638-2},
  doi = {10.1145/3001878.3001883},
  abstract = {Session types are a formalism used to specify and check the correctness of communication based systems. Within their scope, they can guarantee the absence of communication errors such as deadlock, sending an unexpected message or failing to handle an incoming message. Introduced over two decades ago, they have developed into a significant theme in programming languages. In this paper we examine the beliefs that drive research into this area and make it popular. We look at the claims and motivation behind session types throughout the literature. We identify the hypotheses upon which session types have been designed and implemented, and attempt to clarify and formulate them in a more suitable manner for testing.},
  booktitle = {Proceedings of the 7th {{International Workshop}} on {{Evaluation}} and {{Usability}} of {{Programming Languages}} and {{Tools}}},
  publisher = {{ACM}},
  author = {Voinea, A. Laura and Gay, Simon J.},
  year = {2016},
  keywords = {empirical studies,hypotheses,session types},
  pages = {26--29},
  file = {/Users/luigi/work/zotero/storage/H5AYDP4G/Voinea and Gay - 2016 - Benefits of Session Types for Software Development.pdf}
}

@inproceedings{salvaneschiEmpiricalStudyProgram2014,
  address = {New York, NY, USA},
  series = {{{FSE}} 2014},
  title = {An {{Empirical Study}} on {{Program Comprehension}} with {{Reactive Programming}}},
  isbn = {978-1-4503-3056-5},
  doi = {10.1145/2635868.2635895},
  abstract = {Starting from the first investigations with strictly functional languages, reactive programming has been proposed as THE programming paradigm for reactive applications. The advantages of designs based on this style over designs based on the Observer design pattern have been studied for a long time. Over the years, researchers have enriched reactive languages with more powerful abstractions, embedded these abstractions into mainstream languages \textendash{} including object-oriented languages \textendash{} and applied reactive programming to several domains, like GUIs, animations, Web applications, robotics, and sensor networks. However, an important assumption behind this line of research \textendash{} that, beside other advantages, reactive programming makes a wide class of otherwise cumbersome applications more comprehensible \textendash{} has never been evaluated. In this paper, we present the design and the results of the first empirical study that evaluates the effect of reactive programming on comprehensibility compared to the traditional object-oriented style with the Observer design pattern. Results confirm the conjecture that comprehensibility is enhanced by reactive programming. In the experiment, the reactive programming group significantly outperforms the other group.},
  booktitle = {Proceedings of the {{22Nd ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Salvaneschi, Guido and Amann, Sven and Proksch, Sebastian and Mezini, Mira},
  year = {2014},
  keywords = {Empirical Study,Controlled Experiment,Reactive Programming},
  pages = {564--575},
  file = {/Users/luigi/work/zotero/storage/TWQ3GCBY/Salvaneschi et al. - 2014 - An Empirical Study on Program Comprehension with R.pdf}
}

@article{binkleyEmpiricalStudyOptimization2007,
  title = {Empirical {{Study}} of {{Optimization Techniques}} for {{Massive Slicing}}},
  volume = {30},
  issn = {0164-0925},
  doi = {10.1145/1290520.1290523},
  abstract = {This article presents results from a study of techniques that improve the performance of graph-based interprocedural slicing of the System Dependence Graph (SDG). This is useful in ``massive slicing'' where slices are required for many or all of the possible set of slicing criteria. Several different techniques are considered, including forming strongly connected components, topological sorting, and removing transitive edges. Data collected from a test bed of just over 1,000,000 lines of code are presented. This data illustrates the impact on computation time of the techniques. Together, the best combination produces a 71\% reduction in run-time (and a 64\% reduction in memory usage). The complete set of techniques also illustrates the point at which faster computation is not viable due to prohibitive preprocessing costs.},
  number = {1},
  journal = {ACM Trans. Program. Lang. Syst.},
  author = {Binkley, David and Harman, Mark and Krinke, Jens},
  month = nov,
  year = {2007},
  keywords = {empirical study,internal representation,performance enhancement,Slicing},
  file = {/Users/luigi/work/zotero/storage/Y4TSYWTK/Binkley et al. - 2007 - Empirical Study of Optimization Techniques for Mas.pdf}
}

@inproceedings{gudeJavaScriptUsedParts2012,
  address = {New York, NY, USA},
  series = {{{SPLASH}} '12},
  title = {{{JavaScript}}: {{The Used Parts}}},
  isbn = {978-1-4503-1563-0},
  shorttitle = {{{JavaScript}}},
  doi = {10.1145/2384716.2384762},
  abstract = {We describe an empirical study to understand how JavaScript language features are used by the programmers. Our test corpus is larger than any previous work (more than 1 million scripts) and it attempts to target JS usage from various points of view. We describe the usage results of JS language features.},
  booktitle = {Proceedings of the 3rd {{Annual Conference}} on {{Systems}}, {{Programming}}, and {{Applications}}: {{Software}} for {{Humanity}}},
  publisher = {{ACM}},
  author = {Gude, Sharath Chowdary},
  year = {2012},
  keywords = {empirical study,javascript,variable scope},
  pages = {109--110},
  file = {/Users/luigi/work/zotero/storage/FQRPWRZF/Gude - 2012 - JavaScript The Used Parts.pdf}
}

@article{chenEmpiricalStudyProgramming2005,
  title = {An Empirical Study of Programming Language Trends},
  volume = {22},
  issn = {0740-7459},
  doi = {10.1109/MS.2005.55},
  abstract = {Predicting software engineering trends is a strategically important asset for both developers and managers, but it's also difficult, due to the wide range of factors involved and the complexity of their interactions. This paper reveals some interesting trends and a method for studying other important software engineering trends. This article trades breadth for depth by focusing on a small, compact set of trends involving 17 high-level programming languages. We quantified many of their relevant factors, and then collected data on their evolution over 10 years. By applying statistical methods to this data, we aim to gain insight into what does and does not make a language successful.},
  number = {3},
  journal = {IEEE Software},
  author = {Chen, Yaofei and Dios, R. and Mili, A. and Wu, Lan and Wang, Kefei},
  month = may,
  year = {2005},
  keywords = {Java,high level languages,Programming profession,Computer languages,History,software engineering,Software engineering,programming languages,empirical software engineering,Engineering management,ISO standards,programming language trends,Software development management,software engineering trends,statistical analysis,statistical modeling},
  pages = {72-79},
  file = {/Users/luigi/work/zotero/storage/5CXE2YRH/Chen et al. - 2005 - An empirical study of programming language trends.pdf;/Users/luigi/work/zotero/storage/F7S9V39U/1438333.html}
}

@inproceedings{nanzDesignEmpiricalStudy2011,
  title = {Design of an {{Empirical Study}} for {{Comparing}} the {{Usability}} of {{Concurrent Programming Languages}}},
  doi = {10.1109/ESEM.2011.41},
  abstract = {The recent turn towards multicore processing architectures has made concurrency an important part of mainstream software development. As a result, an increasing number of developers have to learn to write concurrent programs, a task that is known to be hard even for the expert. Language designers are therefore working on languages that promise to make concurrent programming "easier". However, the claim that a new language is more usable than another cannot be supported by purely theoretical considerations, but calls for empirical studies. In this paper, we present the design of a study to compare concurrent programming languages with respect to comprehending and debugging existing programs and writing correct new programs. A critical challenge for such a study is avoiding the bias that might be introduced during the training phase and when interpreting participants' solutions. We address these issues by the use of self-study material and an evaluation scheme that exposes any subjective decisions of the corrector, or eliminates them altogether. We apply our design to a comparison of two object-oriented languages for concurrency, multithreaded Java and SCOOP (Simple Concurrent Object-Oriented Programming), in an academic setting. We obtain results in favor of SCOOP even though the study participants had previous training in writing multithreaded Java programs.},
  booktitle = {2011 {{International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  author = {Nanz, S. and Torshizi, F. and Pedroni, M. and Meyer, B.},
  month = sep,
  year = {2011},
  keywords = {Java,Concurrent computing,Programming,empirical study,concurrency,programming languages,Instruction sets,multi-threading,concurrent programming languages,language designers,mainstream software development,Materials,multicore processing architectures,multiprocessing systems,multithreaded Java,object oriented languages,object-oriented languages,simple concurrent object oriented programming,Synchronization,Training,usability},
  pages = {325-334},
  file = {/Users/luigi/work/zotero/storage/UECRDBYQ/Nanz et al. - 2011 - Design of an Empirical Study for Comparing the Usa.pdf;/Users/luigi/work/zotero/storage/NJKH2G53/6092581.html}
}

@inproceedings{klintRASCALDomainSpecific2009,
  title = {{{RASCAL}}: {{A Domain Specific Language}} for {{Source Code Analysis}} and {{Manipulation}}},
  shorttitle = {{{RASCAL}}},
  doi = {10.1109/SCAM.2009.28},
  abstract = {Many automated software engineering tools require tight integration of techniques for source code analysis and manipulation. State-of-the-art tools exist for both, but the domains have remained notoriously separate because different computational paradigms fit each domain best. This impedance mismatch hampers the development of new solutions because the desired functionality and scalability can only be achieved by repeated and ad hoc integration of different techniques. RASCAL is a domain-specific language that takes away most of this boilerplate by integrating source code analysis and manipulation at the conceptual, syntactic, semantic and technical level. We give an overview of the language and assess its merits by implementing a complex refactoring.},
  booktitle = {2009 {{Ninth IEEE International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}}},
  author = {Klint, P. and v d Storm, T. and Vinju, J.},
  month = sep,
  year = {2009},
  keywords = {Java,Libraries,program diagnostics,software maintenance,Informatics,Software engineering,domain specific language,object-oriented languages,ad hoc integration,automated software engineering tool,complex software refactoring,conceptual-syntactic-semantic-technical level,Domain specific languages,Impedance,impedance mismatch,Logic programming,meta-programming,Pattern matching,RASCAL,Scalability,source code analysis,source code manipulation,Storms,transformation},
  pages = {168-177},
  file = {/Users/luigi/work/zotero/storage/KM9NZDJ2/Klint et al. - 2009 - RASCAL A Domain Specific Language for Source Code.pdf;/Users/luigi/work/zotero/storage/X8X2RWMI/5279910.html}
}

@inproceedings{filhoExceptionsAspectsDevil2006,
  address = {New York, NY, USA},
  series = {{{SIGSOFT}} '06/{{FSE}}-14},
  title = {Exceptions and {{Aspects}}: {{The Devil}} Is in the {{Details}}},
  isbn = {978-1-59593-468-0},
  shorttitle = {Exceptions and {{Aspects}}},
  doi = {10.1145/1181775.1181794},
  abstract = {It is usually assumed that the implementation of exception handling can be better modularized by the use of aspect-oriented programming (AOP). However, the trade-offs involved in using AOP with this goal are not well-understood. This paper presents an in-depth study of the adequacy of the AspectJ language for modularizing exception handling code. The study consisted in refactoring existing applications so that the code responsible for implementing heterogeneous error handling strategies was moved to separate aspects. We have performed quantitative assessments of four systems - three object-oriented and one aspect-oriented - based on four quality attributes, namely separation of concerns, coupling, cohesion, and conciseness. Our investigation also included a multi-perspective analysis of the refactored systems, including (i) the reusability of the aspectized error handling code, (ii) the beneficial and harmful aspectization scenarios, and (iii) the scalability of AOP to aspectize exception handling in the presence of other crosscutting concerns.},
  booktitle = {Proceedings of the 14th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Filho, Fernando Castor and Cacho, Nelio and Figueiredo, Eduardo and Maranh\~ao, Raquel and Garcia, Alessandro and Rubira, Cec\'ilia Mary F.},
  year = {2006},
  keywords = {empirical studies,exception handling,aspectJ},
  pages = {152--162},
  file = {/Users/luigi/work/zotero/storage/DTCA8M3X/Filho et al. - 2006 - Exceptions and Aspects The Devil is in the Detail.pdf}
}

@inproceedings{osmanExceptionEvolutionLonglived2017,
  address = {Piscataway, NJ, USA},
  series = {{{MSR}} '17},
  title = {Exception {{Evolution}} in {{Long}}-Lived {{Java Systems}}},
  isbn = {978-1-5386-1544-7},
  doi = {10.1109/MSR.2017.21},
  abstract = {Exception handling allows developers to deal with abnormal situations that disrupt the execution flow of a program. There are mainly three types of exceptions: standard exceptions provided by the programming language itself, custom exceptions defined by the project developers, and third-party exceptions defined in external libraries. We conjecture that there are multiple factors that affect the use of these exception types. We perform an empirical study on long-lived Java projects to investigate these factors. In particular, we analyze how developers rely on the different types of exceptions in throw statements and exception handlers. We confirm that the domain, the type, and the development phase of a project affect the exception handling patterns. We observe that applications have significantly more error handling code than libraries and they increasingly rely on custom exceptions. Also, projects that belong to different domains have different preferences of exception types. For instance, content management systems rely more on custom exceptions than standard exceptions whereas the opposite is true in parsing frameworks.},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Mining Software Repositories}}},
  publisher = {{IEEE Press}},
  author = {Osman, Haidar and Chi{\c s}, Andrei and Corrodi, Claudio and Ghafari, Mohammad and Nierstrasz, Oscar},
  year = {2017},
  keywords = {empirical study,exception handling,software evolution},
  pages = {302--311},
  file = {/Users/luigi/work/zotero/storage/LJPNZGWT/Osman et al. - 2017 - Exception Evolution in Long-lived Java Systems.pdf}
}

@inproceedings{dulaighMeasurementExceptionhandlingCode2012,
  address = {Piscataway, NJ, USA},
  series = {{{WEH}} '12},
  title = {Measurement of {{Exception}}-Handling {{Code}}: {{An Exploratory Study}}},
  isbn = {978-1-4673-1766-5},
  shorttitle = {Measurement of {{Exception}}-Handling {{Code}}},
  abstract = {This paper presents some preliminary results from an empirical study of 12 Java applications from the Qualitas corpus. We measure the quantity and distribution of exception-handling constructs, and study their change as the systems evolve through several versions.},
  booktitle = {Proceedings of the 5th {{International Workshop}} on {{Exception Handling}}},
  publisher = {{IEEE Press}},
  author = {D\'ulaigh, Keith \'O and Power, James F. and Clarke, Peter J.},
  year = {2012},
  keywords = {software metrics,empirical software engineering,exception handling code},
  pages = {55--61},
  file = {/Users/luigi/work/zotero/storage/UIAWLLGE/Dúlaigh et al. - 2012 - Measurement of Exception-handling Code An Explora.pdf}
}

@inproceedings{cachoTradingRobustnessMaintainability2014,
  address = {New York, NY, USA},
  series = {{{ICSE}} 2014},
  title = {Trading {{Robustness}} for {{Maintainability}}: {{An Empirical Study}} of {{Evolving C}}\# {{Programs}}},
  isbn = {978-1-4503-2756-5},
  shorttitle = {Trading {{Robustness}} for {{Maintainability}}},
  doi = {10.1145/2568225.2568308},
  abstract = {Mainstream programming languages provide built-in exception handling mechanisms to support robust and maintainable implementation of exception handling in software systems. Most of these modern languages, such as C\#, Ruby, Python and many others, are often claimed to have more appropriated exception handling mechanisms. They reduce programming constraints on exception handling to favor agile changes in the source code. These languages provide what we call maintenance-driven exception handling mechanisms. It is expected that the adoption of these mechanisms improve software maintainability without hindering software robustness. However, there is still little empirical knowledge about the impact that adopting these mechanisms have on software robustness. This paper addressed this gap by conducting an empirical study aimed at understanding the relationship between changes in C\# programs and their robustness. In particular, we evaluated how changes in the normal and exceptional code were related to exception handling faults. We applied a change impact analysis and a control flow analysis in 119 versions of 16 C\# programs. The results showed that: (i) most of the problems hindering software robustness in those programs are caused by changes in the normal code, (ii) many potential faults were introduced even when improving exception handling in C\# code, and (iii) faults are often facilitated by the maintenance-driven flexibility of the exception handling mechanism. Moreover, we present a series of change scenarios that decrease the program robustness.},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Cacho, N\'elio and C\'esar, Thiago and Filipe, Thomas and Soares, Eliezio and Cassio, Arthur and Souza, Rafael and Garcia, Israel and Barbosa, Eiji Adachi and Garcia, Alessandro},
  year = {2014},
  keywords = {Exception handling,maintainability,robustness},
  pages = {584--595},
  file = {/Users/luigi/work/zotero/storage/VKBIQB4P/Cacho et al. - 2014 - Trading Robustness for Maintainability An Empiric.pdf}
}

@inproceedings{senaIntegratedAnalysisException2016,
  address = {New York, NY, USA},
  series = {{{SAC}} '16},
  title = {Integrated {{Analysis}} of {{Exception Flows}} and {{Handler Actions}} in {{Java Libraries}}: {{An Empirical Study}}},
  isbn = {978-1-4503-3739-7},
  shorttitle = {Integrated {{Analysis}} of {{Exception Flows}} and {{Handler Actions}} in {{Java Libraries}}},
  doi = {10.1145/2851613.2851793},
  abstract = {This paper presents an empirical study of exception handling strategies in Java libraries. The study conducts an integrated analysis of exception flows and handler actions from Java libraries with the aim to understand the impact of adopted exception handling strategies from the perspective of libraries' users. We extended an existing static analysis tool to identify exception flows in software libraries and collected data from the eight most downloaded Java libraries in Maven repository. After that, manual analysis was performed to categorize the tailored handler actions for each exception handler. Our results show that a high number of uncaught runtime exceptions and subsumption handlers were applied in some libraries. We also investigated the community anti-patterns implemented by handler actions in exception flows. Our results reveal the need to have automated support to allow the integrated analysis of exception flows and their handler actions explicitly.},
  booktitle = {Proceedings of the 31st {{Annual ACM Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  author = {Sena, Dem\'ostenes and Coelho, Roberta and Kulesza, Uir\'a},
  year = {2016},
  keywords = {software libraries,empirical study,exception handling,static analysis},
  pages = {1520--1526},
  file = {/Users/luigi/work/zotero/storage/2MULLE48/Sena et al. - 2016 - Integrated Analysis of Exception Flows and Handler.pdf}
}

@inproceedings{shahWhyDevelopersNeglect2008,
  address = {New York, NY, USA},
  series = {{{WEH}} '08},
  title = {Why {{Do Developers Neglect Exception Handling}}?},
  isbn = {978-1-60558-229-0},
  doi = {10.1145/1454268.1454277},
  abstract = {In this paper, we explore the problems associated with exception handling from a new dimension: the human. We designed a study that evaluates (1) different perspectives of software developers to understand how they perceive exception handling and what methods they adopt to deal with exception handling constructs, and (2) the usefulness of a visualization tool that we developed in previous work for exception handling. We describe the design of our study, present details about the study's participants, describe the interviews we conducted with the participants, present the results of the study, and discuss what we learned from the study. Based on our analysis, we suggest several future directions, including the proposal of a new role for the software-development process---exception engineer, who works closely with software developers throughout all phases of the software-development life cycle and who concentrates on the integration of exception handling into the core functionality of programs.},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Exception Handling}}},
  publisher = {{ACM}},
  author = {Shah, Hina and G\"org, Carsten and Harrold, Mary Jean},
  year = {2008},
  keywords = {exception handling,human aspects,interactive visualization,user studies},
  pages = {62--68},
  file = {/Users/luigi/work/zotero/storage/RDGG6SK9/Shah et al. - 2008 - Why Do Developers Neglect Exception Handling.pdf}
}

@inproceedings{marinescuAreClassesThat2011,
  address = {New York, NY, USA},
  series = {{{IWPSE}}-{{EVOL}} '11},
  title = {Are the {{Classes That Use Exceptions Defect Prone}}?},
  isbn = {978-1-4503-0848-9},
  doi = {10.1145/2024445.2024456},
  abstract = {Exception handling is a mechanism that highlights exceptional functionality of software systems. Currently many empirical studies point out that sometimes developers neglect exceptional functionality, minimizing its importance. In this paper we investigate if the design entities (classes) that use exceptions are more defect prone than the other classes. The results, based on analyzing three releases of Eclipse, show that indeed the classes that use exceptions are more defect prone than the other classes. Based on our results, developers are advertised to pay more attention to the way they handle exceptions.},
  booktitle = {Proceedings of the 12th {{International Workshop}} on {{Principles}} of {{Software Evolution}} and the 7th {{Annual ERCIM Workshop}} on {{Software Evolution}}},
  publisher = {{ACM}},
  author = {Marinescu, Cristina},
  year = {2011},
  keywords = {defects,exceptions},
  pages = {56--60},
  file = {/Users/luigi/work/zotero/storage/JLSVXYVD/Marinescu - 2011 - Are the Classes That Use Exceptions Defect Prone.pdf}
}

@inproceedings{marinescuDetectingMissingThrown2010,
  address = {New York, NY, USA},
  series = {{{ESEM}} '10},
  title = {Detecting {{Missing Thrown Exceptions}} in {{Enterprise Systems}}: {{An Empirical Study}}},
  isbn = {978-1-4503-0039-1},
  shorttitle = {Detecting {{Missing Thrown Exceptions}} in {{Enterprise Systems}}},
  doi = {10.1145/1852786.1852861},
  abstract = {Commonly enterprise systems are implemented using the object-oriented and relational paradigms, among which the communication is performed using various library methods for manipulating the persistent data. Most of the times the involved library methods throw different exceptions. An improper handling mechanism for these exceptions in the source code may bring different problems at runtime and hamper its maintenance. In this work we introduce an approach that automatically detects the methods from the source code which reveal an improper mechanism for handling exceptions involving database operations. The detected methods should be refactored in order to increase the reliability of the application, as well as its maintenance.},
  booktitle = {Proceedings of the 2010 {{ACM}}-{{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  publisher = {{ACM}},
  author = {Marinescu, Cristina},
  year = {2010},
  keywords = {static analysis,software quality assessment},
  pages = {59:1--59:1},
  file = {/Users/luigi/work/zotero/storage/2M4KESMC/Marinescu - 2010 - Detecting Missing Thrown Exceptions in Enterprise .pdf}
}

@inproceedings{oliveiraExploratoryStudyException2016,
  address = {New York, NY, USA},
  series = {{{SBES}} '16},
  title = {An {{Exploratory Study}} of {{Exception Handling Behavior}} in {{Evolving Android}} and {{Java Applications}}},
  isbn = {978-1-4503-4201-8},
  doi = {10.1145/2973839.2973843},
  abstract = {Previous work has shown that robustness-related issues like functional errors and app crashes rank among the most common causes for complaints about mobile phone apps. Since most Android applications are written in Java, exception handling is the main mechanism they employ to report and handle errors, similarly to standard Java applications. Thus, the proper use of this mechanism is closely linked to app robustness. Nonetheless, to the best of our knowledge, no previous study analyzed the relationship between source code changes and uncaught exceptions, a common cause of bugs in Android apps, nor whether exception handling code in these apps evolves in the same way as in standard Java applications. This paper presents an empirical study aimed at understanding the relationship between changes in Android programs and their robustness and comparing the evolution of the exception handling code in Android and standard Java applications.},
  booktitle = {Proceedings of the 30th {{Brazilian Symposium}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Oliveira, Juliana and Cacho, Nelio and Borges, Deise and Silva, Thaisa and Castor, Fernando},
  year = {2016},
  keywords = {Java,Robustness,Android,Exception Handling},
  pages = {23--32},
  file = {/Users/luigi/work/zotero/storage/WB7EUK9A/Oliveira et al. - 2016 - An Exploratory Study of Exception Handling Behavio.pdf}
}

@inproceedings{senaInvestigationEvolutionaryNature2015,
  address = {New York, NY, USA},
  series = {{{SAC}} '15},
  title = {An {{Investigation}} on the {{Evolutionary Nature}} of {{Exception Handling Violations}} in {{Software Product Lines}}},
  isbn = {978-1-4503-3196-8},
  doi = {10.1145/2695664.2695933},
  abstract = {The Exception Handling (EH) is a widely used mechanism for building robust systems and it is embedded in most of the mainstream programming languages. In the context of Software Product Lines (SPL), we can find exception handling code associated to common and variable features of an SPL. However, studies have shown that the exception handling code can also become a source of bugs which may affect the system the other way around, making it even less robust. This paper describes an empirical study whose goal was to investigate the evolutionary nature of exception handling violations. Can the exception handling behavior be preserved along SPL evolution scenarios? In order to carry out this study we extended a tool, called PLEA, which statically discovers the exception handling flows on SPLs (those implemented in Java and using annotation techniques), and it was executed over five to seven versions of two distinct SPLs. Our goal was to identify how the different kinds of evolution scenarios affected the exception handling policy initially defined. The results showed that the evolution of the SPLs exception handling code did not occur in a planned way in most of the scenarios. Consequently, it led to violations on the exception handling behavior. This paper points out the need of a set of EH behavior-preserving evolution scenarios for the SPLs exception handling code.},
  booktitle = {Proceedings of the 30th {{Annual ACM Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  author = {Sena, Dem\'ostenes and Coelho, Roberta and Kulesza, Uir\'a},
  year = {2015},
  keywords = {exception handling,software evolution,software product line},
  pages = {1616--1623},
  file = {/Users/luigi/work/zotero/storage/WGBUG7GD/Sena et al. - 2015 - An Investigation on the Evolutionary Nature of Exc.pdf}
}

@inproceedings{avgustinovQLObjectorientedQueries2016,
  address = {Dagstuhl, Germany},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  title = {{{QL}}: {{Object}}-Oriented {{Queries}} on {{Relational Data}}},
  volume = {56},
  isbn = {978-3-95977-014-9},
  shorttitle = {{{QL}}},
  doi = {10.4230/LIPIcs.ECOOP.2016.2},
  booktitle = {30th {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2016)},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  author = {Avgustinov, Pavel and de Moor, Oege and Jones, Michael Peyton and Sch\"afer, Max},
  editor = {Krishnamurthi, Shriram and Lerner, Benjamin S.},
  year = {2016},
  keywords = {Datalog,Object orientation,prescriptive typing,query languages},
  pages = {2:1--2:25},
  file = {/Users/luigi/work/zotero/storage/X9UUZBNI/Avgustinov et al. - 2016 - QL Object-oriented Queries on Relational Data.pdf;/Users/luigi/work/zotero/storage/SYHU6EMX/6096.html}
}

@techreport{antonioliAnalysisJavaClass1998,
  title = {Analysis of the {{Java Class File Format}}},
  abstract = {The wide acceptance of Java as a network programing language has made the Java class file to one of the most popular, portable intermediate program representations. Such a representation must be as small as possible and still ideally support interpretation and code generation. This report presents the result of an analysis that examined a set of 4016 different class files for size and bytecode usage.},
  institution = {{University of Zurich}},
  author = {Antonioli, Denis N. and Pilz, Markus},
  year = {1998},
  file = {/Users/luigi/work/zotero/storage/D7ZGERIW/ifi-98.04.pdf}
}

@inproceedings{uesbeckEmpiricalStudyImpact2016,
  address = {New York, NY, USA},
  series = {{{ICSE}} '16},
  title = {An {{Empirical Study}} on the {{Impact}} of {{C}}++ {{Lambdas}} and {{Programmer Experience}}},
  isbn = {978-1-4503-3900-1},
  doi = {10.1145/2884781.2884849},
  abstract = {Lambdas have seen increasing use in mainstream programming languages, notably in Java 8 and C++ 11. While the technical aspects of lambdas are known, we conducted the first randomized controlled trial on the human factors impact of C++ 11 lambdas compared to iterators. Because there has been recent debate on having students or professionals in experiments, we recruited undergraduates across the academic pipeline and professional programmers to evaluate these findings in a broader context. Results afford some doubt that lambdas benefit developers and show evidence that students are negatively impacted in regard to how quickly they can write correct programs to a test specification and whether they can complete a task. Analysis from log data shows that participants spent more time with compiler errors, and have more errors, when using lambdas as compared to iterators, suggesting difficulty with the syntax chosen for C++. Finally, experienced users were more likely to complete tasks, with or without lambdas, and could do so more quickly, with experience as a factor explaining 45.7\% of the variance in our sample in regard to completion time.},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Uesbeck, Phillip Merlin and Stefik, Andreas and Hanenberg, Stefan and Pedersen, Jan and Daleiden, Patrick},
  year = {2016},
  keywords = {C++11,human factors,lambda expressions},
  pages = {760--771},
  file = {/Users/luigi/work/zotero/storage/3WC9NQA8/Uesbeck et al. - 2016 - An Empirical Study on the Impact of C++ Lambdas an.pdf}
}

@inproceedings{hanenbergFaithHopeLove2010,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '10},
  title = {Faith, {{Hope}}, and {{Love}}: {{An Essay}} on {{Software Science}}'s {{Neglect}} of {{Human Factors}}},
  isbn = {978-1-4503-0203-6},
  shorttitle = {Faith, {{Hope}}, and {{Love}}},
  doi = {10.1145/1869459.1869536},
  abstract = {Research in the area of programming languages has different facets -- from formal reasoning about new programming language constructs (such as type soundness proofs for new type systems) over inventions of new abstractions, up to performance measurements of virtual machines. A closer look into the underlying research methods reveals a distressing characteristic of programming language research: developers, which are the main audience for new language constructs, are hardly considered in the research process. As a consequence, it is simply not possible to state whether a new construct that requires some kind of interaction with the developer has any positive impact on the construction of software. This paper argues for appropriate research methods in programming language research that rely on studies of developers -- and argues that the introduction of corresponding empirical methods not only requires a new understanding of research but also a different view on how to teach software science to students.},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  publisher = {{ACM}},
  author = {Hanenberg, Stefan},
  year = {2010},
  keywords = {software engineering,empirical research,programming language research,research methods},
  pages = {933--946},
  file = {/Users/luigi/work/zotero/storage/YQGMK2YV/Hanenberg - 2010 - Faith, Hope, and Love An Essay on Software Scienc.pdf}
}

@inproceedings{feigenspanMeasuringProgrammingExperience2012,
  title = {Measuring Programming Experience},
  doi = {10.1109/ICPC.2012.6240511},
  abstract = {Programming experience is an important confounding parameter in controlled experiments regarding program comprehension. In literature, ways to measure or control programming experience vary. Often, researchers neglect it or do not specify how they controlled it. We set out to find a well-defined understanding of programming experience and a way to measure it. From published comprehension experiments, we extracted questions that assess programming experience. In a controlled experiment, we compare the answers of 128 students to these questions with their performance in solving program-comprehension tasks. We found that self estimation seems to be a reliable way to measure programming experience. Furthermore, we applied exploratory factor analysis to extract a model of programming experience. With our analysis, we initiate a path toward measuring programming experience with a valid and reliable tool, so that we can control its influence on program comprehension.},
  booktitle = {2012 20th {{IEEE International Conference}} on {{Program Comprehension}} ({{ICPC}})},
  author = {Feigenspan, J. and K\"astner, C. and Liebig, J. and Apel, S. and Hanenberg, S.},
  month = jun,
  year = {2012},
  keywords = {Programming profession,reverse engineering,software engineering,Software engineering,Educational institutions,statistical analysis,Estimation,exploratory factor analysis,program-comprehension tasks,programming experience measurement,self estimation,Time factors},
  pages = {73-82},
  file = {/Users/luigi/work/zotero/storage/KK94SATY/Feigenspan et al. - 2012 - Measuring programming experience.pdf;/Users/luigi/work/zotero/storage/WHM8S8NF/6240511.html}
}

@inproceedings{kleinschmagerStaticTypeSystems2012,
  title = {Do Static Type Systems Improve the Maintainability of Software Systems? {{An}} Empirical Study},
  shorttitle = {Do Static Type Systems Improve the Maintainability of Software Systems?},
  doi = {10.1109/ICPC.2012.6240483},
  abstract = {Static type systems play an essential role in contemporary programming languages. Despite their importance, whether static type systems influence human software development capabilities remains an open question. One frequently mentioned argument for static type systems is that they improve the maintainability of software systems - an often used claim for which there is no empirical evidence. This paper describes an experiment which tests whether static type systems improve the maintainability of software systems. The results show rigorous empirical evidence that static type are indeed beneficial to these activities, except for fixing semantic errors.},
  booktitle = {2012 20th {{IEEE International Conference}} on {{Program Comprehension}} ({{ICPC}})},
  author = {Kleinschmager, S. and Robbes, R. and Stefik, A. and Hanenberg, S. and Tanter, E.},
  month = jun,
  year = {2012},
  keywords = {Java,software maintenance,Programming,Software,programming languages,Semantics,Educational institutions,contemporary programming languages,software system maintainability,static type systems,Time measurement},
  pages = {153-162},
  file = {/Users/luigi/work/zotero/storage/B5NN5NQP/Kleinschmager et al. - 2012 - Do static type systems improve the maintainability.pdf;/Users/luigi/work/zotero/storage/NZEHSRGN/6240483.html}
}

@inproceedings{fisherNext700Data2006,
  address = {New York, NY, USA},
  series = {{{POPL}} '06},
  title = {The {{Next}} 700 {{Data Description Languages}}},
  isbn = {978-1-59593-027-9},
  doi = {10.1145/1111037.1111039},
  abstract = {In the spirit of Landin, we present a calculus of dependent types to serve as the semantic foundation for a family of languages called data description languages. Such languages, which include pads, datascript, and packettypes, are designed to facilitate programming with ad hoc data, ie, data not in well-behaved relational or xml formats. In the calculus, each type describes the physical layout and semantic properties of a data source. In the semantics, we interpret types simultaneously as the in-memory representation of the data described and as parsers for the data source. The parsing functions are robust, automatically detecting and recording errors in the data stream without halting parsing. We show the parsers are type-correct, returning data whose type matches the simple-type interpretation of the specification. We also prove the parsers are "error-correct," accurately reporting the number of physical and semantic errors that occur in the returned data. We use the calculus to describe the features of various data description languages, and we discuss how we have used the calculus to improve PADS.},
  booktitle = {Conference {{Record}} of the 33rd {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Fisher, Kathleen and Mandelbaum, Yitzhak and Walker, David},
  year = {2006},
  keywords = {domain-specific languages,data description language,dependent types},
  pages = {2--15},
  file = {/Users/luigi/work/zotero/storage/3PQ96CVI/Fisher et al. - 2006 - The Next 700 Data Description Languages.pdf}
}

@inproceedings{hillsEmpiricalStudyPHP2013,
  address = {New York, NY, USA},
  series = {{{ISSTA}} 2013},
  title = {An {{Empirical Study}} of {{PHP Feature Usage}}: {{A Static Analysis Perspective}}},
  isbn = {978-1-4503-2159-4},
  shorttitle = {An {{Empirical Study}} of {{PHP Feature Usage}}},
  doi = {10.1145/2483760.2483786},
  abstract = {PHP is one of the most popular languages for server-side application development. The language is highly dynamic, providing programmers with a large amount of flexibility. However, these dynamic features also have a cost, making it difficult to apply traditional static analysis techniques used in standard code analysis and transformation tools. As part of our work on creating analysis tools for PHP, we have conducted a study over a significant corpus of open-source PHP systems, looking at the sizes of actual PHP programs, which features of PHP are actually used, how often dynamic features appear, and how distributed these features are across the files that make up a PHP website. We have also looked at whether uses of these dynamic features are truly dynamic or are, in some cases, statically understandable, allowing us to identify specific patterns of use which can then be taken into account to build more precise tools. We believe this work will be of interest to creators of analysis tools for PHP, and that the methodology we present can be leveraged for other dynamic languages with similar features.},
  booktitle = {Proceedings of the 2013 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  publisher = {{ACM}},
  author = {Hills, Mark and Klint, Paul and Vinju, Jurgen},
  year = {2013},
  keywords = {Static analysis,Dynamic language features,PHP,Static metrics,Static program behavior},
  pages = {325--335},
  file = {/Users/luigi/work/zotero/storage/LD48V54M/Hills et al. - 2013 - An Empirical Study of PHP Feature Usage A Static .pdf}
}

@inproceedings{dahseExperienceReportEmpirical2015,
  address = {New York, NY, USA},
  series = {{{ISSTA}} 2015},
  title = {Experience {{Report}}: {{An Empirical Study}} of {{PHP Security Mechanism Usage}}},
  isbn = {978-1-4503-3620-8},
  shorttitle = {Experience {{Report}}},
  doi = {10.1145/2771783.2771787},
  abstract = {The World Wide Web mainly consists of web applications written in weakly typed scripting languages, with PHP being the most popular language in practice. Empirical evidence based on the analysis of vulnerabilities suggests that security is often added as an ad-hoc solution, rather than planning a web application with security in mind during the design phase. Although some best-practice guidelines emerged, no comprehensive security standards are available for developers. Thus, developers often apply their own favorite security mechanisms for data sanitization or validation to prohibit malicious input to a web application. In the context of our development of a new static code analysis tool for vulnerability detection, we studied commonly used input sanitization or validation mechanisms in 25 popular PHP applications. Our analysis of 2.5 million lines of code and over 26 thousand secured data flows provides a comprehensive overview of how developers utilize security mechanisms in practice regarding different markup contexts. In this paper, we discuss these security mechanisms in detail and reveal common pitfalls. For example, we found certain markup contexts and security mechanisms more frequently vulnerable than others. Our empirical study helps researchers, web developers, and tool developers to focus on error-prone markup contexts and security mechanisms in order to detect and mitigate vulnerabilities.},
  booktitle = {Proceedings of the 2015 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  publisher = {{ACM}},
  author = {Dahse, Johannes and Holz, Thorsten},
  year = {2015},
  keywords = {input validation,Static analysis,PHP,input sanitization},
  pages = {60--70},
  file = {/Users/luigi/work/zotero/storage/GE7UPAPY/Dahse and Holz - 2015 - Experience Report An Empirical Study of PHP Secur.pdf}
}

@inproceedings{doyleEmpiricalStudyEvolution2011,
  title = {An {{Empirical Study}} of the {{Evolution}} of {{PHP Web Application Security}}},
  doi = {10.1109/Metrisec.2011.18},
  abstract = {Web applications are increasingly subject to mass attacks, with vulnerabilities found easily in both open source and commercial applications as evinced by the fact that approximately half of reported vulnerabilities are found in web applications. In this paper, we perform an empirical investigation of the evolution of vulnerabilities in fourteen of the most widely used open source PHP web applications, finding that vulnerabilities densities declined from 28.12 to 19.96 vulnerabilities per thousand lines of code from 2006 to 2010. We also investigate whether complexity metrics or a security resources indicator (SRI) metric can be used to identify vulnerable web application showing that average cyclomatic complexity is an effective predictor of vulnerability for several applications, especially for those with low SRI scores.},
  booktitle = {2011 {{Third International Workshop}} on {{Security Measurements}} and {{Metrics}}},
  author = {Doyle, M. and Walden, J.},
  month = sep,
  year = {2011},
  keywords = {Software,Internet,Security,static analysis,Aggregates,average cyclomatic complexity,code complexity,complexity metrics,Complexity theory,Encyclopedias,mass attacks,PHP Web application security,security metrics,security of data,security resources indicator metric,Software measurement,software security,vulnerabilities densities},
  pages = {11-20},
  file = {/Users/luigi/work/zotero/storage/LM7HENYF/Doyle and Walden - 2011 - An Empirical Study of the Evolution of PHP Web App.pdf;/Users/luigi/work/zotero/storage/F3LKFMJN/6165758.html}
}

@inproceedings{changTypeSystemsMacros2017,
  address = {New York, NY, USA},
  series = {{{POPL}} 2017},
  title = {Type {{Systems As Macros}}},
  isbn = {978-1-4503-4660-3},
  doi = {10.1145/3009837.3009886},
  abstract = {We present Turnstile, a metalanguage for creating typed embedded languages. To implement the type system, programmers write type checking rules resembling traditional judgment syntax. To implement the semantics, they incorporate elaborations into these rules. Turnstile critically depends on the idea of linguistic reuse. It exploits a macro system in a novel way to simultaneously type check and rewrite a surface program into a target language. Reusing a macro system also yields modular implementations whose rules may be mixed and matched to create other languages. Combined with typical compiler and runtime reuse, Turnstile produces performant typed embedded languages with little effort.},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Chang, Stephen and Knauth, Alex and Greenman, Ben},
  year = {2017},
  keywords = {type systems,macros,typed embedded DSLs},
  pages = {694--705},
  file = {/Users/luigi/work/zotero/storage/DCBB25YS/Chang et al. - 2017 - Type Systems As Macros.pdf}
}

@inproceedings{omarSafelyComposableTypeSpecific2014,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Safely {{Composable Type}}-{{Specific Languages}}},
  isbn = {978-3-662-44201-2 978-3-662-44202-9},
  doi = {10.1007/978-3-662-44202-9_5},
  abstract = {Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.},
  language = {en},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Omar, Cyrus and Kurilova, Darya and Nistor, Ligia and Chung, Benjamin and Potanin, Alex and Aldrich, Jonathan},
  month = jul,
  year = {2014},
  pages = {105-130},
  file = {/Users/luigi/work/zotero/storage/T6RGPYTC/Omar et al. - 2014 - Safely Composable Type-Specific Languages.pdf}
}

@article{aminEssenceDependentObject2016,
  title = {The {{Essence}} of {{Dependent Object Types}}},
  doi = {10.1007/978-3-319-30936-1_14},
  journal = {A List of Successes That Can Change the World: Essays Dedicated to Philip Wadler on the Occasion of His 60th Birthday},
  author = {Amin, Nada and Gr\"utter, Karl Samuel and Odersky, Martin and Rompf, Tiark and Stucki, Sandro},
  year = {2016},
  pages = {249-272},
  file = {/Users/luigi/work/zotero/storage/QLQTLW5M/Amin et al. - 2016 - The Essence of Dependent Object Types.pdf;/Users/luigi/work/zotero/storage/USEJRHLV/215280.html}
}

@article{ramsonActiveExpressionsBasic2017,
  title = {Active {{Expressions}}: {{Basic Building Blocks}} for {{Reactive Programming}}},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {Active {{Expressions}}},
  doi = {10.22152/programming-journal.org/2017/1/12},
  abstract = {Modern software development without reactive programming is hard to imagine. Reactive programming favors a wide class of contemporary software systems that respond to user input, network messages, and other events. While reactive programming is an active field of research, the implementation of reactive concepts remains challenging. In particular, change detection represents a hard but inevitable necessity when implementing reactive concepts. Typically, change detection mechanisms are not intended for reuse but are tightly coupled to the particular change resolution mechanism. As a result, developers often have to re-implement similar abstractions. A reusable primitive for change detection is still missing. To find a suitable primitive, we identify commonalities in existing reactive concepts. We discover a class of reactive concepts, state-based reactive concepts. All state-based reactive concepts share a common change detection mechanism: they detect changes in the evaluation result of an expression. On the basis of the identified common change detection mechanism, we propose active expressions as a reusable primitive. By abstracting the tedious implementation details of change detection, active expressions can ease the implementation of reactive programming concepts. We evaluate the design of active expressions by re-implementing a number of existing state-based reactive concepts using them. The resulting implementations highlight the expressiveness of active expressions. Active expressions enable the separation of essential from non-essential parts when reasoning about reactive programming concepts. By using active expressions as a primitive for change detection, developers of reactive language constructs and runtime support can now focus on the design of how application programmers should be able to react to change. Ultimately, we would like active expressions to encourage experiments with novel reactive programming concepts and with that to yield a wider variety of them to explore.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Ramson, Stefan and Hirschfeld, Robert},
  month = apr,
  year = {2017},
  pages = {12:1-12:49},
  file = {/Users/luigi/work/zotero/storage/PXXTCSYY/Ramson and Hirschfeld - 2017 - Active Expressions Basic Building Blocks for Reac.pdf;/Users/luigi/work/zotero/storage/NJ5VWP29/12.html}
}

@article{thompsonPragmaticsCloneDetection2017,
  title = {The Pragmatics of Clone Detection and Elimination},
  volume = {1},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2017/1/8},
  abstract = {The occurrence of similar code, or `code clones', can make program code difficult to read, modify and maintain. This paper describes industrial case studies of clone detection and elimination using a refactoring and clone detection tool. We discuss how the studies have informed the design of the tool; more importantly, we use the studies to illustrate the complex set of decisions that have to be taken when performing clone elimination in practice. The case studies were performed in collaboration with engineers from Ericsson AB, and used the refactoring tool Wrangler for Erlang. However, the conclusions we draw are largely language-independent, and set out the pragmatics of clone detection and elimination in real-world projects as well as design principles for clone detection decision-support tools.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Thompson, Simon and Li, Huiqing and Schumacher, Andreas},
  month = apr,
  year = {2017},
  pages = {8:1-8:34},
  file = {/Users/luigi/work/zotero/storage/IEX34JE4/Thompson et al. - 2017 - The pragmatics of clone detection and elimination.pdf;/Users/luigi/work/zotero/storage/K64KETG4/8.html}
}

@article{mattisEditTransactionsDynamically2017,
  title = {Edit {{Transactions}}: {{Dynamically Scoped Change Sets}} for {{Controlled Updates}} in {{Live Programming}}},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {Edit {{Transactions}}},
  doi = {10.22152/programming-journal.org/2017/1/13},
  abstract = {Live programming environments enable programmers to edit a running program and obtain immediate feedback on each individual change. The liveness quality is valued by programmers to help work in small steps and continuously add or correct small functionality while maintaining the impression of a direct connection between each edit and its manifestation at run-time. Such immediacy may conflict with the desire to perform a combined set of intermediate steps, such as a refactoring, without immediately taking effect after each individual edit. This becomes important when an incomplete sequence of small-scale changes can easily break the running program. State-of-the-art solutions focus on retroactive recovery mechanisms, such as debugging or version control. In contrast, we propose a proactive approach: Multiple individual changes to the program are collected in an Edit Transaction, which can be made effective if deemed complete. Upon activation, the combined steps become visible together. Edit Transactions are capable of dynamic scoping, allowing a set of changes to be tested in isolation before being extended to the running application. This enables a live programming workflow with full control over change granularity, immediate feedback on tests, delayed effect on the running application, and coarse-grained undos. We present an implementation of Edit Transactions along with Edit-Transaction-aware tools in Squeak/Smalltalk. We asses this implementation by conducting a case study with and without the new tool support, comparing programming activities, errors, and detours for implementing new functionality in a running simulation. We conclude that workflows using Edit Transactions have the potential to increase confidence in a change, reduce potential for run-time errors, and eventually make live programming more predictable and engaging.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Mattis, Toni and Rein, Patrick and Hirschfeld, Robert},
  month = apr,
  year = {2017},
  pages = {13:1-13:32},
  file = {/Users/luigi/work/zotero/storage/GYEZWF28/Mattis et al. - 2017 - Edit Transactions Dynamically Scoped Change Sets .pdf;/Users/luigi/work/zotero/storage/TII9S82Q/13.html}
}

@article{kaferWhatBestWay2017,
  title = {What {{Is}} the {{Best Way For Developers}} to {{Learn New Software Tools}}? {{An Empirical Comparison Between}} a {{Text}} and a {{Video Tutorial}}},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {What {{Is}} the {{Best Way For Developers}} to {{Learn New Software Tools}}?},
  doi = {10.22152/programming-journal.org/2017/1/17},
  abstract = {The better developers can learn software tools, the faster they can start using them and the more efficiently they can later work with them. Tutorials are supposed to help here. While in the early days of computing, mostly text tutorials were available, nowadays software developers can choose among a huge number of tutorials for almost any popular software tool. However, only little research was conducted to understand how text tutorials differ from other tutorials, which tutorial types are preferred and, especially, which tutorial types yield the best learning experience in terms of efficiency and effectiveness, especially for programmers. To evaluate these questions, we converted an existing video tutorial for a novel software tool into a content-equivalent text tutorial. We then conducted an experiment in three groups where 42 undergraduate students from a software engineering course were commissioned to operate the software tool after using a tutorial: the first group was provided only with the video tutorial, the second group only with the text tutorial and the third group with both. In this context, the differences in terms of efficiency were almost negligible: We could observe that participants using only the text tutorial completed the tutorial faster than the participants with the video tutorial. However, the participants using only the video tutorial applied the learned content faster, achieving roughly the same bottom line performance. We also found that if both tutorial types are offered, participants prefer video tutorials for learning new content but text tutorials for looking up ``missed'' information. We mainly gathered our data through questionnaires and screen recordings and analyzed it with suitable statistical hypotheses tests. The data is available at [12]. Since producing tutorials requires effort, knowing with which type of tutorial learnability can be increased to which extent has an immense practical relevance. We conclude that in contexts similar to ours, while it would be ideal if software tool makers would offer both tutorial types, it seems more efficient to produce only text tutorials instead of a passive video tutorial \textendash{} provided you manage to motivate your learners to use them.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {K\"afer, Verena and Kulesz, Daniel and Wagner, Stefan},
  month = apr,
  year = {2017},
  pages = {17:1-17:41},
  file = {/Users/luigi/work/zotero/storage/SUCDIWK5/Käfer et al. - 2017 - What Is the Best Way For Developers to Learn New S.pdf;/Users/luigi/work/zotero/storage/QAKYLGTR/17.html}
}

@article{murphyAnalysisIntroductoryProgramming2017,
  title = {An {{Analysis}} of {{Introductory Programming Courses}} at {{UK Universities}}},
  volume = {1},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2017/1/18},
  abstract = {Context: In the context of exploring the art, science and engineering of programming, the question of which programming languages should be taught first has been fiercely debated since computer science teaching started in universities. Failure to grasp programming readily almost certainly implies failure to progress in computer science. Inquiry: What first programming languages are being taught? There have been regular national-scale surveys in Australia and New Zealand, with the only US survey reporting on a small subset of universities. This the first such national survey of universities in the UK. Approach: We report the results of the first survey of introductory programming courses (N=80) taught at UK universities as part of their first year computer science (or related) degree programmes, conducted in the first half of 2016. We report on student numbers, programming paradigm, programming languages and environment/tools used, as well as the underpinning rationale for these choices. Knowledge: The results in this first UK survey indicate a dominance of Java at a time when universities are still generally teaching students who are new to programming (and computer science), despite the fact that Python is perceived, by the same respondents, to be both easier to teach as well as to learn. Grounding: We compare the results of this survey with a related survey conducted since 2010 (as well as earlier surveys from 2001 and 2003) in Australia and New Zealand. Importance: This survey provides a starting point for valuable pedagogic baseline data for the analysis of the art, science and engineering of programming, in the context of substantial computer science curriculum reform in UK schools, as well as increasing scrutiny of teaching excellence and graduate employability for UK universities.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Murphy, Ellen and Crick, Tom and Davenport, James H.},
  month = apr,
  year = {2017},
  pages = {18:1-18:23},
  file = {/Users/luigi/work/zotero/storage/3CL7JWTV/Murphy et al. - 2017 - An Analysis of Introductory Programming Courses at.pdf;/Users/luigi/work/zotero/storage/VGE3QZLL/18.html}
}

@article{lorenzApplicationEmbeddingLanguage2017,
  title = {Application {{Embedding}}: {{A Language Approach}} to {{Declarative Web Programming}}},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {Application {{Embedding}}},
  doi = {10.22152/programming-journal.org/2017/1/2},
  abstract = {Since the early days of the Web, web application developers have aspired to develop much of their applications declaratively. However, one aspect of the application, namely its business-logic is constantly left imperative. In this work we present Application Embedding, a novel approach to application development which allows all aspects of an application, including its business-logic, to be programmed declaratively. We develop this approach in a two-step process. First, we draw a mapping between web applications and Domain-Specific Languages (DSLs). Second, we note that out of the two methods for implementing DSLs, namely as either internal or external, most traditional web applications correspond to external DSLs, while the the technique that corresponds to DSL embedding (implementing internal DSLs) is left mostly unexplored. By projecting the well-known technique of DSL embedding onto web applications, we derive a novel technique\textemdash{}Application Embedding. Application embedding offers a separation of code assets that encourages reuse of imperative code, while keeping all application-specific assets, including those specifying its business- logic, declarative. As validation, we implemented a simple, though nontrivial web application using the proposed separation of assets. This implementation includes an application-agnostic imperative host application named FishTank, intended to be applicable for a wide variety of web applications, and a declarative definition of the different aspects of the specific application, intended to be loaded on that host. Our method of separation of code assets facilitates a better separation of work, in comparison to traditional methods. By this separation, host application developers can focus mostly on the extra-functional aspects of a web application, namely on improving performance, scalability, and availability, while developers of an embedded application can focus on the functional aspects of their application, without worrying about extra- functional concerns. The reusability of the host application makes the effort put into a better implementation cost-effective, since it can benefit all applications built on top of it.},
  language = {en},
  number = {1},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Lorenz, David H. and Rosenan, Boaz},
  month = jan,
  year = {2017},
  pages = {2:1-2:38},
  file = {/Users/luigi/work/zotero/storage/WHWMJ5TW/Lorenz and Rosenan - 2017 - Application Embedding A Language Approach to Decl.pdf;/Users/luigi/work/zotero/storage/ZAWX9QLR/2.html}
}

@article{pickeringProfunctorOpticsModular2017,
  title = {Profunctor {{Optics}}: {{Modular Data Accessors}}},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {Profunctor {{Optics}}},
  doi = {10.22152/programming-journal.org/2017/1/7},
  abstract = {CONTEXT: Data accessors allow one to read and write components of a data structure, such as the fields of a record, the variants of a union, or the elements of a container. These data accessors are collectively known as optics; they are fundamental to programs that manipulate complex data. INQUIRY: Individual data accessors for simple data structures are easy to write, for example as pairs of ``getter'' and ``setter'' methods. However, it is not obvious how to combine data accessors, in such a way that data accessors for a compound data structure are composed out of smaller data accessors for the parts of that structure. Generally, one has to write a sequence of statements or declarations that navigate step by step through the data structure, accessing one level at a time - which is to say, data accessors are traditionally not first-class citizens, combinable in their own right. APPROACH: We present a framework for modular data access, in which individual data accessors for simple data structures may be freely combined to obtain more complex data accessors for compound data structures. Data accessors become first-class citizens. The framework is based around the notion of profunctors, a flexible generalization of functions. KNOWLEDGE: The language features required are higher-order functions (``lambdas'' or ``closures''), parametrized types (``generics'' or ``abstract types''), and some mechanism for separating interfaces from implementations (``abstract classes'' or ``modules''). We use Haskell as a vehicle in which to present our constructions, but languages such as Java, C\#, or Scala that provide the necessary features should work just as well. GROUNDING: We provide implementations of all our constructions, in the form of a literate program: the manuscript file for the paper is also the source code for the program, and the extracted code is available separately for evaluation. We also prove the essential properties demonstrating that our profunctor-based representations are precisely equivalent to the more familiar concrete representations. IMPORTANCE: Our results should pave the way to simpler ways of writing programs that access the components of compound data structures.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Pickering, Matthew and Gibbons, Jeremy and Wu, Nicolas},
  month = apr,
  year = {2017},
  pages = {7:1-7:51},
  file = {/Users/luigi/work/zotero/storage/N4STAZ4W/Pickering et al. - 2017 - Profunctor Optics Modular Data Accessors.pdf;/Users/luigi/work/zotero/storage/87XF5QIL/7.html}
}

@article{petricekMiscomputationSoftwareLearning2017,
  title = {Miscomputation in Software: {{Learning}} to Live with Errors},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {Miscomputation in Software},
  doi = {10.22152/programming-journal.org/2017/1/14},
  abstract = {Computer programs do not always work as expected. In fact, ominous warnings about the desperate state of the software industry continue to be released with almost ritualistic regularity. In this paper, we look at the 60 years history of programming and at the different practical methods that software community developed to live with programming errors. We do so by observing a class of students discussing different approaches to programming errors. While learning about the different methods for dealing with errors, we uncover basic assumptions that proponents of different paradigms follow. We learn about the mathematical attempt to eliminate errors through formal methods, scientific method based on testing, a way of building reliable systems through engineering methods, as well as an artistic approach to live coding that accepts errors as a creative inspiration. This way, we can explore the differences and similarities among the different paradigms. By inviting proponents of different methods into a single discussion, we hope to open potential for new thinking about errors. When should we use which of the approaches? And what can software development learn from mathematics, science, engineering and art? When programming or studying programming, we are often enclosed in small communities and we take our basic assumptions for granted. Through the discussion in this paper, we attempt to map the large and rich space of programming ideas and provide reference points for exploring, perhaps foreign, ideas that can challenge some of our assumptions.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Petricek, Tomas},
  month = apr,
  year = {2017},
  pages = {14:1-14:24},
  file = {/Users/luigi/work/zotero/storage/V43DKU87/Petricek - 2017 - Miscomputation in software Learning to live with .pdf;/Users/luigi/work/zotero/storage/8464QWWL/14.html}
}

@article{cazzolaOpenProgrammingLanguage2017,
  title = {Open {{Programming Language Interpreters}}},
  volume = {1},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2017/1/5},
  abstract = {Context: This paper presents the concept of open programming language interpreters and the implementation of a framework-level metaobject protocol (MOP) to support them. Inquiry: We address the problem of dynamic interpreter adaptation to tailor the interpreter's behavior on the task to be solved and to introduce new features to fulfill unforeseen requirements. Many languages provide a MOP that to some degree supports reflection. However, MOPs are typically language-specific, their reflective functionality is often restricted, and the adaptation and application logic are often mixed which hardens the understanding and maintenance of the source code. Our system overcomes these limitations. Approach: We designed and implemented a system to support open programming language interpreters. The prototype implementation is integrated in the Neverlang framework. The system exposes the structure, behavior and the runtime state of any Neverlang-based interpreter with the ability to modify it. Knowledge: Our system provides a complete control over interpreter's structure, behavior and its runtime state. The approach is applicable to every Neverlang-based interpreter. Adaptation code can potentially be reused across different language implementations. Grounding: Having a prototype implementation we focused on feasibility evaluation. The paper shows that our approach well addresses problems commonly found in the research literature. We have a demonstrative video and examples that illustrate our approach on dynamic software adaptation, aspect-oriented programming, debugging and context-aware interpreters Importance: To our knowledge, our paper presents the first reflective approach targeting a general framework for language development. Our system provides full reflective support for free to any Neverlang-based interpreter. We are not aware of any prior application of open implementations to programming language interpreters in the sense defined in this paper. Rather than substituting other approaches, we believe our system can be used as a complementary technique in situations where other approaches present serious limitations.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Cazzola, Walter and Shaqiri, Albert},
  month = apr,
  year = {2017},
  pages = {5:1-5:34},
  file = {/Users/luigi/work/zotero/storage/AV6WM6M6/Cazzola and Shaqiri - 2017 - Open Programming Language Interpreters.pdf;/Users/luigi/work/zotero/storage/MKMJP9IJ/5.html}
}

@article{hartelInterconnectedLinguisticArchitecture2017,
  title = {Interconnected {{Linguistic Architecture}}},
  volume = {1},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2017/1/3},
  abstract = {The context of the reported research is the documentation of software technologies such as object/relational mappers, web-application frameworks, or code generators. We assume that documentation should model a macroscopic view on usage scenarios of technologies in terms of involved artifacts, leveraged software languages, data flows, conformance relationships, and others. In previous work, we referred to such documentation also as `linguistic architecture'. The corresponding models may also be referred to as `megamodels' while adopting this term from the technological space of modeling/model-driven engineering. This work is an inquiry into making such documentation less abstract and more effective by means of connecting (mega)models, systems, and developer experience in several ways. To this end, we adopt an approach that is primarily based on prototyping (i.e., implementa- tion of a megamodeling infrastructure with all conceivable connections) and experimentation with showcases (i.e., documentation of concrete software technologies). The knowledge gained by this research is a notion of interconnected linguistic architecture on the grounds of connecting primary model elements, inferred model elements, static and runtime system artifacts, traceability links, system contexts, knowledge resources, plugged interpretations of model elements, and IDE views. A corresponding suite of aspects of interconnected linguistic architecture is systematically described. As to the grounding of this research, we describe a literature survey which tracks scattered occurrences and thus demonstrates the relevance of the identified aspects of interconnected linguistic architecture. Further, we describe the MegaL/Xtext+IDE infrastructure which realizes interconnected linguistic architecture. The importance of this work lies in providing more formal (ontologically rich, navigable, verifiable) documentation of software technologies helping developers to better understand how to use technologies in new systems (prescriptive mode) or how technologies are used in existing systems (descriptive mode).},
  language = {en},
  number = {1},
  journal = {The Art, Science, and Engineering of Programming},
  author = {H\"artel, Johannes and H\"artel, Lukas and L\"ammel, Ralf and Varanovich, Andrei and Heinz, Marcel},
  month = jan,
  year = {2017},
  pages = {3:1-3:27},
  file = {/Users/luigi/work/zotero/storage/BUQUPMLR/Härtel et al. - 2017 - Interconnected Linguistic Architecture.pdf;/Users/luigi/work/zotero/storage/ENQ63Y52/3.html}
}

@article{lammelRelationshipMaintenanceSoftware2017,
  title = {Relationship {{Maintenance}} in {{Software Language Repositories}}},
  volume = {1},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2017/1/4},
  abstract = {The context of this research is testing and building software systems and, specifically, software language repositories (SLRs), i.e., repositories with components for language processing (interpreters, translators, analyzers, transformers, pretty printers, etc.). SLRs are typically set up for developing and using metaprogramming systems, language workbenches, language definition frameworks, executable semantic frameworks, and modeling frameworks. This work is an inquiry into testing and building SLRs in a manner that the repository is seen as a collection of language-typed artifacts being related by the applications of language-typed functions or relations which serve language processing. The notion of language is used in a broad sense to include text-, tree-, graph-based languages as well as representations based on interchange formats and also proprietary formats for serialization. The overall approach underlying this research is one of language design driven by a complex case study, i.e., a specific SLR with a significant number of processed languages and language processors as well as a noteworthy heterogeneity in terms of representation types and implementation languages. The knowledge gained by our research is best understood as a declarative language design for regression testing and build management; we introduce a corresponding language Ueber with an executable semantics which maintains relationships between language-typed artifacts in an SLR. The grounding of the reported research is based on the comprehensive, formal, executable (logic programming-based) definition of the Ueber language and its systematic application to the management of the SLR YAS which consists of hundreds of language definition and processing components (such as interpreters and transformations) for more than thirty languages (not counting different representation types) with Prolog, Haskell, Java, and Python being used as implementation languages. The importance of this work follows from the significant costs implied by regression testing and build management and also from the complexity of SLRs which calls for means to help with understanding.},
  language = {en},
  number = {1},
  journal = {The Art, Science, and Engineering of Programming},
  author = {L\"ammel, Ralf},
  month = jan,
  year = {2017},
  pages = {4:1-4:27},
  file = {/Users/luigi/work/zotero/storage/5FRKCSSL/Lämmel - 2017 - Relationship Maintenance in Software Language Repo.pdf;/Users/luigi/work/zotero/storage/9ZCN5EKJ/4.html}
}

@article{aryaTransitionWatchpointsTeaching2017,
  title = {Transition {{Watchpoints}}: {{Teaching Old Debuggers New Tricks}}},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {Transition {{Watchpoints}}},
  doi = {10.22152/programming-journal.org/2017/1/16},
  abstract = {Reversible debuggers and process replay have been developed at least since 1970. This vision enables one to execute backwards in time under a debugger. Two important problems in practice are that, first, current reversible debuggers are slow when reversing over long time periods, and, second, after building one reversible debugger, it is difficult to transfer that achievement to a new programming environment. The user observes a bug when arriving at an error. Searching backwards for the correspond- ing fault may require many reverse steps. Ultimately, the user prefers to write an expression that will transition to false upon arriving at the fault. The solution is an expression-transition watchpoint facility based on top of snapshots and record/replay. Expression-transition watch- points are implemented as binary search through the timeline of a program execution, while using the snapshots as landmarks within that timeline. This allows for debugging of subtle bugs that appear only after minutes or more of program execution. When a bug occurs within seconds of program startup, repeated debugging sessions suffice. Reversible debugging is preferred for bugs seen only after minutes. This architecture allows for an efficient and easy-to-write snapshot-based reversibe debugger on top of a conventional debugger. The validity of this approach was tested by developing four personalities (for GDB, MATLAB, Perl, and Python), with each personality typically requiring just 100 lines of code.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Arya, Kapil and Denniston, Tyler and Rabkin, Ariel and Cooperman, Gene},
  month = apr,
  year = {2017},
  pages = {16:1-16:28},
  file = {/Users/luigi/work/zotero/storage/2RWGKDWF/Arya et al. - 2017 - Transition Watchpoints Teaching Old Debuggers New.pdf;/Users/luigi/work/zotero/storage/887MFV47/16.html}
}

@article{iosif-lazarEffectiveAnalysisPrograms2017,
  title = {Effective {{Analysis}} of {{C Programs}} by {{Rewriting Variability}}},
  volume = {1},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2017/1/1},
  abstract = {Context. Variability-intensive programs (program families) appear in many application areas and for many reasons today. Different family members, called variants, are derived by switching statically configurable options (features) on and off, while reuse of the common code is maximized. Inquiry. Verification of program families is challenging since the number of variants is exponential in the number of features. Existing single-program analysis and verification tools cannot be applied directly to program families, and designing and implementing the corresponding variability-aware versions is tedious and laborious. Approach. In this work, we propose a range of variability-related transformations for translating program families into single programs by replacing compile-time variability with run-time variability (non-determinism). The obtained transformed programs can be subsequently analyzed using the conventional off- the-shelf single-program analysis tools such as type checkers, symbolic executors, model checkers, and static analyzers. Knowledge. Our variability-related transformations are outcome-preserving, which means that the relation between the outcomes in the transformed single program and the union of outcomes of all variants derived from the original program family is equality. Grounding. We show our transformation rules and their correctness with respect to a minimal core imperative language IMP. Then, we discuss our experience of implementing and using the transformations for efficient and effective analysis and verification of real-world C program families. Importance. We report some interesting variability-related bugs that we discovered using various state-of-the-art single-program C verification tools, such as Frama-C, Clang, LLBMC.},
  language = {en},
  number = {1},
  journal = {The Art, Science, and Engineering of Programming},
  author = {{Iosif-Lazar}, Alexandru Florin and Melo, Jean and Dimovski, Aleksandar S. and Brabrand, Claus and Wasowski, Andrzej},
  month = jan,
  year = {2017},
  pages = {1:1-1:25},
  file = {/Users/luigi/work/zotero/storage/5RG3G9PC/Iosif-Lazar et al. - 2017 - Effective Analysis of C Programs by Rewriting Vari.pdf;/Users/luigi/work/zotero/storage/MQLYJP7F/1.html}
}

@article{serangTRIOTFasterTensor2017,
  title = {{{TRIOT}}: {{Faster}} Tensor Manipulation in {{C}}++11},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {{{TRIOT}}},
  doi = {10.22152/programming-journal.org/2017/1/6},
  abstract = {Context: Multidimensional arrays are used by many different algorithms. As such, indexing and broadcasting complex operations over multidimensional arrays are ubiquitous tasks and can be performance limiting. Inquiry: Simultaneously indexing two or more multidimensional arrays with different shapes (e.g., copying data from one tensor to another larger, zero padded tensor in anticipation of a convolution) is difficult to do efficiently: Hard-coded nested for loops in C, Fortran, and Go cannot be applied when the dimension of a tensor is unknown at compile time. Likewise, boost::multi\_array cannot be used unless the dimensions of the array are known at compile time, and the style of implementation restricts the user from using the index tuple inside a vectorized operation (as would be required to compute an expected value of a multidimensional distribution). On the other hand, iteration methods that do not require the dimensionality or shape to be known at compile time (e.g., incrementing and applying carry operations to index tuples or remapping integer indices in the flat array), can be substantially slower than hard-coded nested for loops. Approach: Using a few tasks that broadcast operations over multidimensional arrays, several existing methods are compared: hard-coded nested for loops in C and Go, vectorized operations in Fortran, boost::multi\_array, numpy, tuple incrementing, and integer reindexing. Knowledge: A new approach to this problem, ``template-recursive iteration over tensors'' (TRIOT), is proposed. This new method, which is made possible by features of C++11, can be used when the dimensions of the tensors are unknown at compile time. Furthermore, the proposed method can access the index tuple inside the vectorized operations, permitting much more flexible operations. Grounding: Runtimes of all methods are compared, and demonstrate that the proposed TRIOT method is competitive with both hard-coded nested for loops in C and Go, as well as vectorized operations in Fortran, despite not knowing the dimensions at compile time. TRIOT outperforms boost::multi\_array, numpy, tuple incrementing, and integer reindexing. Importance: Manipulation of multidimensional arrays is a common task in software, especially in high-performance numerical methods. This paper proposes a novel way to leverage template recursion to iterate over and apply operations to multidimensional arrays, and then demonstrates the superior performance and flexibility of operations that can be achieved using this new approach.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Serang, Oliver and Heyl, Florian},
  month = apr,
  year = {2017},
  pages = {6:1-6:24},
  file = {/Users/luigi/work/zotero/storage/RQCBEQHA/Serang and Heyl - 2017 - TRIOT Faster tensor manipulation in C++11.pdf;/Users/luigi/work/zotero/storage/449ZN2CU/6.html}
}

@article{ichikawaUserDefinedOperatorsIncluding2017,
  title = {User-{{Defined Operators Including Name Binding}} for {{New Language Constructs}}},
  volume = {1},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2017/1/15},
  abstract = {User-defined syntax extensions are useful to implement an embedded domain specific language (EDSL) with good code-readability. They allow EDSL authors to define domain-natural notation, which is often different from the host language syntax. Nowadays, there are several research works of powerful user-defined syntax extensions. One promising approach uses user-defined operators. A user-defined operator is a function with user-defined syntax. It can be regarded as a syntax extension implemented without macros. An advantage of user-defined operators is that an operator can be statically typed. The compiler can find type errors in the definition of an operator before the operator is used. In addition, the compiler can resolve syntactic ambiguities by using static types. However, user-defined operators are difficult to implement language constructs involving static name binding. Name binding is association between names and values (or memory locations). Our inquiry is whether we can design a system for user-defined operators involving a new custom name binding. This paper proposes a module system for user-defined operators named a dsl class. A dsl class is similar to a normal class in Java but it contains operators instead of methods. We use operators for implementing custom name binding. For example, we use a nullary operator for emulating a variable name. An instance of a dsl class, called a dsl object, reifies an environment that expresses name binding. Programmers can control a scope of instance operators by specifying where the dsl object is active. We extend the host type system so that it can express the activation of a dsl object. In our system, a bound name is propagated through a type parameter to a dsl object. This enables us to implement user-defined language constructs involving static name binding. A contribution of this paper is that we reveal we can integrate a system for managing names and their scopes with a module and type system of an object-oriented language like Java. This allows us to implement a proposed system by adopting eager disambiguation based on expected types so that the compilation time will be acceptable. Eager disambiguation, which prunes out semantically invalid abstract parsing trees (ASTs) while a parser is running, is needed because the parser may generate a huge number of potentially valid ASTs for the same source code. We have implemented ProteaJ2, which is a programming language based on Java and it supports our proposal. We describe a parsing method that adopts eager disambiguation for fast parsing and discuss its time complexity. To show the practicality of our proposal, we have conducted two micro benchmarks to see the performance of our compiler. We also show several use cases of dsl classes for demonstrating dsl classes can express various language constructs. Our ultimate goal is to let programmers add any kind of new language construct to a host language. To do this, programmers should be able to define new syntax, name binding, and type system within the host language. This paper shows programmers can define the former two: their own syntax and name binding.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Ichikawa, Kazuhiro and Chiba, Shigeru},
  month = apr,
  year = {2017},
  pages = {15:1-15:25},
  file = {/Users/luigi/work/zotero/storage/GQAUAFN3/Ichikawa and Chiba - 2017 - User-Defined Operators Including Name Binding for .pdf;/Users/luigi/work/zotero/storage/Q46UTBRL/15.html}
}

@article{erdwegModuleSystemDisciplineModelDriven2017,
  title = {A {{Module}}-{{System Discipline}} for {{Model}}-{{Driven Software Development}}},
  volume = {1},
  issn = {2473-7321},
  doi = {10.22152/programming-journal.org/2017/1/9},
  abstract = {Model-driven development is a pragmatic approach to software development that embraces domain-specific languages (DSLs), where models correspond to DSL programs. A distinguishing feature of model-driven development is that clients of a model can select from an open set of alternative semantics of the model by applying different model transformation. However, in existing model-driven frameworks, dependencies between models, model transformations, and generated code artifacts are either implicit or globally declared in build scripts, which impedes modular reasoning, separate compilation, and programmability in general. We propose the design of a new module system that incorporates models and model transformations as modules. A programmer can apply transformations in import statements, thus declaring a dependency on generated code artifacts. Our design enables modular reasoning and separate compilation by preventing hidden dependencies, and it supports mixing modeling artifacts with conventional code artifacts as well as higher-order transformations. We have formalized our design and the aforementioned properties and have validated it by an implementation and case studies that show that our module system successfully integrates model-driven development into conventional programming languages.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Erdweg, Sebastian and Ostermann, Klaus},
  month = apr,
  year = {2017},
  pages = {9:1-9:28},
  file = {/Users/luigi/work/zotero/storage/98IHWUK8/Erdweg and Ostermann - 2017 - A Module-System Discipline for Model-Driven Softwa.pdf;/Users/luigi/work/zotero/storage/J6F3HT8R/9.html}
}

@article{hadasLanguageOrientedModularity2017,
  title = {Language {{Oriented Modularity}}: {{From Theory}} to {{Practice}}},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {Language {{Oriented Modularity}}},
  doi = {10.22152/programming-journal.org/2017/1/10},
  abstract = {Language-oriented modularity (LOM) is a methodology that complements language-oriented programming (LOP) in providing on-demand language abstraction solutions during software development. It involves the implementation and immediate utilization of domain-specific languages (DSLs) that are also aspect-oriented (DSALs). However, while DSL development is affordable thanks to modern language workbenches, DSAL development lacks similar tool support. Consequently, LOM is often impractical and underutilized. The challenge we address is making the complexity of DSAL implementation comparable to that of DSLs and the effectiveness of programming with DSALs comparable to that of general-purpose aspect languages (GPALs). Today, despite being essentially both domain-specific and aspect-oriented, DSALs seem to be second-class. Aspect development tools (e.g., AJDT) do not work on DSAL code. DSL development tools like language workbenches (e.g., Spoofax) neither deal with the backend weaving nor handle the composition of DSALs. DSAL composition frameworks (e.g., Awesome) do not provide frontend development tools. DSAL code transformation approaches (e.g., XAspects) do not preserve the semantics of DSAL programs in the presence of other aspect languages. We extend AspectJ with a small set of annotations and interfaces that allows DSAL designers to define a semantic-preserving transformation to AspectJ and interface with AspectJ tools. Our transformation approach enables the use of standard language workbench to implement DSALs and use of standard aspect development tools to program with those DSALs. As a result, DSALs regain first-class status with respect to both DSLs and aspect languages. This, on the one hand, lowers the cost of developing DSALs to the level of DSLs and, on the other hand, raises the effectiveness of using a DSAL to the level of a GPAL. Consequently, LOM becomes cost-effective compared to the LOP baseline. We modified the ajc compiler to support our approach. Using two different language workbenches (Spoofax and Xtext) we then implemented several DSALs. AspectJ was supported out-of-the-box. We implemented Cool to demonstrate that the non-trivial composition of AspectJ and Cool can be accommodated using our approach. We applied LOM to crosscutting concerns in two open source projects (oVirt and muCommander), implementing in the process application-specific DSALs, thus providing a sense of the decrease in the cost of developing composable DSALs and the increase in the effectiveness of programming with them. Crosscutting concerns remain a problem in modern real-world projects (e.g., as observed in oVirt). DSALs are often the right tool for addressing these concerns. Our work makes LOM practical, thus facilitating use of DSAL solutions in the software development process.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Hadas, Arik and Lorenz, David H.},
  month = apr,
  year = {2017},
  pages = {10:1-10:37},
  file = {/Users/luigi/work/zotero/storage/IV7AWFJ5/Hadas and Lorenz - 2017 - Language Oriented Modularity From Theory to Pract.pdf;/Users/luigi/work/zotero/storage/NS2WVXV9/10.html}
}

@article{vavrovaDoesPythonSmell2017,
  title = {Does {{Python Smell Like Java}}? {{Tool Support}} for {{Design Defect Discovery}} in {{Python}}},
  volume = {1},
  issn = {2473-7321},
  shorttitle = {Does {{Python Smell Like Java}}?},
  doi = {10.22152/programming-journal.org/2017/1/11},
  abstract = {The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we have programmed and let it loose on a huge test set obtained from open source code from thousands of GitHub projects. When it comes to knowledge, we have found that more than twice as many methods in Python can be considered too long (statistically extremely longer than their neighbours within the same project) than in Java, but long parameter lists are seven times less likely to be found in Python code than in Java code. We have also found that Functional Decomposition, the way it was defined for Java, is not found in the Python code at all, and Spaghetti Code and God Classes are extremely rare there as well. The grounding and the confidence in these results comes from the fact that we have performed our experiments on 32'058'823 lines of Python code, which is by far the largest test set for a freely available Python parser. We have also designed the experiment in such a way that it aligned with prior research on design defect detection in Java in order to ease the comparison if we treat our own actions as a replication. Thus, the importance of the work is both in the unique open Python grammar of highest quality, tested on millions of lines of code, and in the design defect detection tool which works on something else than Java.},
  language = {en},
  number = {2},
  journal = {The Art, Science, and Engineering of Programming},
  author = {Vavrov\'a, Nicole and Zaytsev, Vadim},
  month = apr,
  year = {2017},
  pages = {11:1-11:29},
  file = {/Users/luigi/work/zotero/storage/2G2J6NEI/Vavrová and Zaytsev - 2017 - Does Python Smell Like Java Tool Support for Desi.pdf;/Users/luigi/work/zotero/storage/QBEKLYLP/11.html}
}

@inproceedings{camilliEventBasedRuntimeVerification2017,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Event-{{Based Runtime Verification}} of {{Temporal Properties Using Time Basic Petri Nets}}},
  isbn = {978-3-319-57287-1 978-3-319-57288-8},
  doi = {10.1007/978-3-319-57288-8_8},
  abstract = {We introduce a formal framework to provide an efficient event-based monitoring technique, and we describe its current implementation as the MahaRAJA software tool. The framework enables the quantitative runtime verification of temporal properties extracted from occurring events on Java programs. The monitor continuously evaluates the conformance of the concrete implementation with respect to its formal specification given in terms of Time Basic Petri nets, a particular timed extension of Petri nets. The system under test is instrumented by using simple Java annotations on methods to link the implementation to its formal model. This allows a separation between implementation and specification that can be used for other purposes such as formal verification, simulation, and model-based testing. The tool has been successfully used to monitor at runtime and test a number of benchmarking case-studies. Experiments show that our approach introduces bounded overhead and effectively reduces the involvement of the monitor at run time by using negligible auxiliary memory. A comparison with a number of state-of-the-art runtime verification tools is also presented.},
  language = {en},
  booktitle = {{{NASA Formal Methods}}},
  publisher = {{Springer, Cham}},
  author = {Camilli, Matteo and Gargantini, Angelo and Scandurra, Patrizia and Bellettini, Carlo},
  month = may,
  year = {2017},
  pages = {115-130},
  file = {/Users/luigi/work/zotero/storage/3QSFCHII/Camilli et al. - 2017 - Event-Based Runtime Verification of Temporal Prope.pdf;/Users/luigi/work/zotero/storage/E3KM7Q6Q/978-3-319-57288-8_8.html}
}

@inproceedings{srinivasanPartialEvaluationMachine2015,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} 2015},
  title = {Partial {{Evaluation}} of {{Machine Code}}},
  isbn = {978-1-4503-3689-5},
  doi = {10.1145/2814270.2814321},
  abstract = {This paper presents an algorithm for off-line partial evaluation of machine code. The algorithm follows the classical two-phase approach of binding-time analysis (BTA) followed by specialization. However, machine-code partial evaluation presents a number of new challenges, and it was necessary to devise new techniques for use in each phase. - Our BTA algorithm makes use of an instruction-rewriting method that ``decouples'' multiple updates performed by a single instruction. This method counters the cascading imprecision that would otherwise occur with a more naive approach to BTA. - Our specializer specializes an explicit representation of the semantics of an instruction, and emits residual code via machine-code synthesis. Moreover, to create code that allows the stack and heap to be at different positions at run-time than at specialization-time, the specializer represents specialization-time addresses using symbolic constants, and uses a symbolic state for specialization. Our experiments show that our algorithm can be used to specialize binaries with respect to commonly used inputs to produce faster binaries, as well as to extract an executable component from a bloated binary.},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Srinivasan, Venkatesh and Reps, Thomas},
  year = {2015},
  keywords = {BTA,IA-32 instruction set,machine code,machine-code synthesis,Partial evaluation,specialization},
  pages = {860--879},
  file = {/Users/luigi/work/zotero/storage/GH3SL7PM/Srinivasan and Reps - 2015 - Partial Evaluation of Machine Code.pdf}
}

@inproceedings{rompfSurgicalPrecisionJIT2014,
  address = {New York, NY, USA},
  series = {{{PLDI}} '14},
  title = {Surgical {{Precision JIT Compilers}}},
  isbn = {978-1-4503-2784-8},
  doi = {10.1145/2594291.2594316},
  abstract = {Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable. In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables "smart" libraries to supply domain-specific compiler optimizations or safety checks. We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction. In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Rompf, Tiark and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Chafi, Hassan and Olukotun, Kunle},
  year = {2014},
  keywords = {JIT compilation,program generation,staging},
  pages = {41--52},
  file = {/Users/luigi/work/zotero/storage/3APPKP6M/Rompf et al. - 2014 - Surgical Precision JIT Compilers.pdf}
}

@inproceedings{chariBuildingEfficientHighly2016,
  address = {New York, NY, USA},
  series = {{{DLS}} 2016},
  title = {Building {{Efficient}} and {{Highly Run}}-Time {{Adaptable Virtual Machines}}},
  isbn = {978-1-4503-4445-6},
  doi = {10.1145/2989225.2989234},
  abstract = {Programming language virtual machines (VMs) realize language semantics, enforce security properties, and execute applications efficiently. Fully Reflective Execution Environments (EEs) are VMs that additionally expose their whole structure and behavior to applications. This enables develop- ers to observe and adapt VMs at run time. However, there is a belief that reflective EEs are not viable for practical usages because such flexibility would incur a high performance overhead. To refute this belief, we built a reflective EE on top of a highly optimizing dynamic compiler. We introduced a new optimization model that, based on the conjecture that variability of low-level (EE-level) reflective behavior is low in many scenarios, mitigates the most significant sources of the performance overheads related to the reflective capabilities in the EE. Our experiments indicate that reflective EEs can reach peak performance in the order of standard VMs. Concretely, that a) if reflective mechanisms are not used the execution overhead is negligible compared to standard VMs, b) VM operations can be redefined at language-level without incurring in significant overheads, c) for several software adaptation tasks, applying the reflection at the VM level is not only lightweight in terms of engineering effort, but also competitive in terms of performance in comparison to other ad-hoc solutions.},
  booktitle = {Proceedings of the 12th {{Symposium}} on {{Dynamic Languages}}},
  publisher = {{ACM}},
  author = {Chari, Guido and Garbervetsky, Diego and Marr, Stefan},
  year = {2016},
  keywords = {Reflection,Metaobject Protocols,Performance,Virtual Machines},
  pages = {60--71},
  file = {/Users/luigi/work/zotero/storage/SMNL4MJX/Chari et al. - 2016 - Building Efficient and Highly Run-time Adaptable V.pdf}
}

@article{allenProgramDataFlow1976,
  title = {A {{Program Data Flow Analysis Procedure}}},
  volume = {19},
  issn = {0001-0782},
  doi = {10.1145/360018.360025},
  abstract = {The global data relationships in a program can be exposed and codified by the static analysis methods described in this paper. A procedure is given which determines all the definitions which can possibly ``reach'' each node of the control flow graph of the program and all the definitions that are ``live'' on each edge of the graph. The procedure uses an ``interval'' ordered edge listing data structure and handles reducible and irreducible graphs indistinguishably.},
  number = {3},
  journal = {Commun. ACM},
  author = {Allen, F. E. and Cocke, J.},
  month = mar,
  year = {1976},
  keywords = {program optimization,data flow analysis,algorithms,compilers,flow graphs},
  pages = {137--},
  file = {/Users/luigi/work/zotero/storage/DU4MZC5H/Allen and Cocke - 1976 - A Program Data Flow Analysis Procedure.pdf}
}

@article{baconCompilerTransformationsHighperformance1994,
  title = {Compiler {{Transformations}} for {{High}}-Performance {{Computing}}},
  volume = {26},
  issn = {0360-0300},
  doi = {10.1145/197405.197406},
  abstract = {In the last three decades a large number of compiler transformations for optimizing programs have been implemented. Most optimizations for uniprocessors reduce the number of instructions executed by the program using transformations based on the analysis of scalar quantities and data-flow techniques. In contrast, optimizations for high-performance superscalar, vector, and parallel processors maximize parallelism and memory locality with transformations that rely on tracking the properties of arrays using loop dependence analysis.This survey is a comprehensive overview of the important high-level program restructuring techniques for imperative languages, such as C and Fortran. Transformations for both sequential and various types of parallel architectures are covered in depth. We describe the purpose of each transformation, explain how to determine if it is legal, and give an example of its application.Programmers wishing to enhance the performance of their code can use this survey to improve their understanding of the optimizations that compilers can perform, or as a reference for techniques to be applied manually. Students can obtain an overview of optimizing compiler technology. Compiler writers can use this survey as a reference for most of the important optimizations developed to date, and as bibliographic reference for the details of each optimization. Readers are expected to be familiar with modern computer architecture and basic program compilation techniques.},
  number = {4},
  journal = {ACM Comput. Surv.},
  author = {Bacon, David F. and Graham, Susan L. and Sharp, Oliver J.},
  month = dec,
  year = {1994},
  keywords = {parallelism,compilation,dependence analysis,locality,multiprocessors,optimization,superscalar processors,vectorization},
  pages = {345--420},
  file = {/Users/luigi/work/zotero/storage/2JLED8QP/Bacon et al. - 1994 - Compiler Transformations for High-performance Comp.pdf;/Users/luigi/work/zotero/storage/BSG4B7IS/Bacon et al. - 1994 - Compiler Transformations for High-performance Comp.pdf}
}

@inproceedings{baxterUnderstandingShapeJava2006,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '06},
  title = {Understanding the {{Shape}} of {{Java Software}}},
  isbn = {978-1-59593-348-5},
  doi = {10.1145/1167473.1167507},
  abstract = {Large amounts of Java software have been written since the language's escape into unsuspecting software ecology more than ten years ago. Surprisingly little is known about the structure of Java programs in the wild: about the way methods are grouped into classes and then into packages, the way packages relate to each other, or the way inheritance and composition are used to put these programs together. We present the results of the first in-depth study of the structure of Java programs. We have collected a number of Java programs and measured their key structural attributes. We have found evidence that some relationships follow power-laws, while others do not. We have also observed variations that seem related to some characteristic of the application itself. This study provides important information for researchers who can investigate how and why the structural relationships we find may have originated, what they portend, and how they can be managed.},
  booktitle = {Proceedings of the 21st {{Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Baxter, Gareth and Frean, Marcus and Noble, James and Rickerby, Mark and Smith, Hayden and Visser, Matt and Melton, Hayden and Tempero, Ewan},
  year = {2006},
  keywords = {Java,object-oriented design,power-law distributions},
  pages = {397--412},
  file = {/Users/luigi/work/zotero/storage/DZSNZZNH/Baxter et al. - 2006 - Understanding the Shape of Java Software.pdf}
}

@inproceedings{wurthingerOneVMRule2013,
  address = {New York, NY, USA},
  series = {Onward! 2013},
  title = {One {{VM}} to {{Rule Them All}}},
  isbn = {978-1-4503-2472-4},
  doi = {10.1145/2509578.2509581},
  abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efficient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new high-performance language implementations can be obtained by writing little more than a stylized interpreter.},
  booktitle = {Proceedings of the 2013 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} \& {{Software}}},
  publisher = {{ACM}},
  author = {W\"urthinger, Thomas and Wimmer, Christian and W\"o\ss, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
  year = {2013},
  keywords = {dynamic languages,javascript,java,virtual machine,optimization,language implementation},
  pages = {187--204},
  file = {/Users/luigi/work/zotero/storage/GQZ2MS64/Würthinger et al. - 2013 - One VM to Rule Them All.pdf}
}

@inproceedings{rompfOptimizingDataStructures2013,
  address = {New York, NY, USA},
  series = {{{POPL}} '13},
  title = {Optimizing {{Data Structures}} in {{High}}-Level {{Programs}}: {{New Directions}} for {{Extensible Compilers Based}} on {{Staging}}},
  isbn = {978-1-4503-1832-7},
  shorttitle = {Optimizing {{Data Structures}} in {{High}}-Level {{Programs}}},
  doi = {10.1145/2429069.2429128},
  abstract = {High level data structures are a cornerstone of modern programming and at the same time stand in the way of compiler optimizations. In order to reason about user- or library-defined data structures compilers need to be extensible. Common mechanisms to extend compilers fall into two categories. Frontend macros, staging or partial evaluation systems can be used to programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, some compilers allow extending the internal workings by adding new transformation passes at different points in the compile chain or adding new intermediate representation (IR) types. None of these mechanisms alone is sufficient to handle the challenges posed by high level data structures. This paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts. Instead of using staging merely as a front end, we implement internal compiler passes using staging as well. These internal passes delegate back to program execution to construct the transformed IR. Staging is known to simplify program generation, and in the same way it can simplify program transformation. Defining a transformation as a staged IR interpreter is simpler than implementing a low-level IR to IR transformer. With custom IR nodes, many optimizations that are expressed as rewritings from IR nodes to staged program fragments can be combined into a single pass, mitigating phase ordering problems. Speculative rewriting can preserve optimistic assumptions around loops. We demonstrate several powerful program optimizations using this architecture that are particularly geared towards data structures: a novel loop fusion and deforestation algorithm, array of struct to struct of array conversion, object flattening and code generation for heterogeneous parallel devices. We validate our approach using several non trivial case studies that exhibit order of magnitude speedups in experiments.},
  booktitle = {Proceedings of the 40th {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Rompf, Tiark and Sujeeth, Arvind K. and Amin, Nada and Brown, Kevin J. and Jovanovic, Vojin and Lee, HyoukJoong and Jonnalagedda, Manohar and Olukotun, Kunle and Odersky, Martin},
  year = {2013},
  keywords = {code generation,staging,data structures,extensible compilers},
  pages = {497--510},
  file = {/Users/luigi/work/zotero/storage/HVI9PWBB/Rompf et al. - 2013 - Optimizing Data Structures in High-level Programs.pdf}
}

@inproceedings{marrZerooverheadMetaprogrammingReflection2015,
  address = {New York, NY, USA},
  series = {{{PLDI}} '15},
  title = {Zero-Overhead {{Metaprogramming}}: {{Reflection}} and {{Metaobject Protocols Fast}} and {{Without Compromises}}},
  isbn = {978-1-4503-3468-6},
  shorttitle = {Zero-Overhead {{Metaprogramming}}},
  doi = {10.1145/2737924.2737963},
  abstract = {Runtime metaprogramming enables many useful applications and is often a convenient solution to solve problems in a generic way, which makes it widely used in frameworks, middleware, and domain-specific languages. However, powerful metaobject protocols are rarely supported and even common concepts such as reflective method invocation or dynamic proxies are not optimized. Solutions proposed in literature either restrict the metaprogramming capabilities or require application or library developers to apply performance improving techniques. For overhead-free runtime metaprogramming, we demonstrate that dispatch chains, a generalized form of polymorphic inline caches common to self-optimizing interpreters, are a simple optimization at the language-implementation level. Our evaluation with self-optimizing interpreters shows that unrestricted metaobject protocols can be realized for the first time without runtime overhead, and that this optimization is applicable for just-in-time compilation of interpreters based on meta-tracing as well as partial evaluation. In this context, we also demonstrate that optimizing common reflective operations can lead to significant performance improvements for existing applications.},
  booktitle = {Proceedings of the 36th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Marr, Stefan and Seaton, Chris and Ducasse, St\'ephane},
  year = {2015},
  keywords = {Reflection,Metaobject Protocols,Virtual Machines,Just-in-Time Compilation,Meta-tracing,Metaprogramming,Partial Evaluation,Proxies},
  pages = {545--554},
  file = {/Users/luigi/work/zotero/storage/M83GTHFZ/Marr et al. - 2015 - Zero-overhead Metaprogramming Reflection and Meta.pdf}
}

@inproceedings{leissaShallowEmbeddingDSLs2015,
  address = {New York, NY, USA},
  series = {{{GPCE}} 2015},
  title = {Shallow {{Embedding}} of {{DSLs}} via {{Online Partial Evaluation}}},
  isbn = {978-1-4503-3687-1},
  doi = {10.1145/2814204.2814208},
  abstract = {This paper investigates shallow embedding of DSLs by means of online partial evaluation. To this end, we present a novel online partial evaluator for continuation-passing style languages. We argue that it has, in contrast to prior work, a predictable termination policy that works well in practice. We present our approach formally using a continuation-passing variant of PCF and prove its termination properties. We evaluate our technique experimentally in the field of visual and high-performance computing and show that our evaluator produces highly specialized and efficient code for CPUs as well as GPUs that matches the performance of hand-tuned expert code.},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  publisher = {{ACM}},
  author = {Lei\ss{}a, Roland and Boesche, Klaas and Hack, Sebastian and Membarth, Richard and Slusallek, Philipp},
  year = {2015},
  keywords = {Partial Evaluation,Continuation-Passing Style,DSL Embedding},
  pages = {11--20},
  file = {/Users/luigi/work/zotero/storage/KPFVCPVW/Leißa et al. - 2015 - Shallow Embedding of DSLs via Online Partial Evalu.pdf}
}

@inproceedings{asaiCompilingReflectiveLanguage2014,
  address = {New York, NY, USA},
  series = {{{GPCE}} 2014},
  title = {Compiling a {{Reflective Language Using MetaOCaml}}},
  isbn = {978-1-4503-3161-6},
  doi = {10.1145/2658761.2658775},
  abstract = {A reflective language makes the language semantics open to user programs and allows them to access, extend, and modify it from within the same language framework. Because of its high flexibility and expressiveness, it can be an ideal platform for programming language research as well as practical applications in dynamic environments. However, efficient implementation of a reflective language is extremely difficult. Under the circumstance where the language semantics can change, a partial evaluator is required for compilation. This paper reports on the experience of using MetaOCaml as a compiler for a reflective language. With staging annotations, MetaOCaml achieves the same effect as using a partial evaluator. Unlike the standard partial evaluator, the run mechanism of MetaOCaml enables us to use the specialized (compiled) code in the current runtime environment. On the other hand, the lack of a binding-time analysis in MetaOCaml prohibits us from compiling a user program under modified compiled semantics.},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  publisher = {{ACM}},
  author = {Asai, Kenichi},
  year = {2014},
  keywords = {Reflection,partial evaluation,staging,binding-time analysis,metacircular interpreter,MetaOCaml},
  pages = {113--122},
  file = {/Users/luigi/work/zotero/storage/IQZ29BSV/Asai - 2014 - Compiling a Reflective Language Using MetaOCaml.pdf}
}

@article{dongarraLINPACKBenchmarkPresent2003,
  title = {The {{LINPACK Benchmark}}: Past, Present and Future},
  volume = {15},
  issn = {1532-0634},
  shorttitle = {The {{LINPACK Benchmark}}},
  doi = {10.1002/cpe.728},
  abstract = {This paper describes the LINPACK Benchmark and some of its variations commonly used to assess the performance of computer systems. Aside from the LINPACK Benchmark suite, the TOP500 and the HPL codes are presented. The latter is frequently used to obtained results for TOP500 submissions. Information is also given on how to interpret the results of the benchmark and how the results fit into the performance evaluation process. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {9},
  journal = {Concurrency and Computation: Practice and Experience},
  author = {Dongarra, Jack J. and Luszczek, Piotr and Petitet, Antoine},
  month = aug,
  year = {2003},
  keywords = {benchmarking,BLAS,high-performance computing,HPL,linear algebra,LINPACK,TOP500},
  pages = {803-820},
  file = {/Users/luigi/work/zotero/storage/RZYPRLTX/Dongarra et al. - 2003 - The LINPACK Benchmark past, present and future.pdf;/Users/luigi/work/zotero/storage/DK99Z238/abstract.html}
}

@article{leeStudyDynamicMemory2002,
  title = {A Study of Dynamic Memory Management in {{C}}++ Programs},
  volume = {28},
  issn = {1477-8424},
  doi = {10.1016/S0096-0551(02)00015-2},
  abstract = {Recently, the importance of dynamic memory management has been increased significantly as there is a growing number of development in object-oriented programs. Many studies show that dynamic memory management is one of the most expensive components in many software systems. It can consume up to 30\% of the program execution time. Especially, in C++ programs, it tends to have object creation and deletion prolifically. These objects tend to have short life-spans. This paper describes an integrated study of the C++'s memory allocation behavior, a memory tracing tool and memory managements based on the empirical study of C++ programs. First, this paper summarizes the hypothesis of situations that invoke the dynamic memory management explicitly and implicitly. They are: constructors, copy constructors, overloading assignment operator=, type conversions and application specific member functions. Second, a dynamic memory tracing tool, called mtrace++, is introduced to study the dynamic memory allocation behavior in C++ programs. Third, a dynamic memory allocation strategy, called O-Reuse, to reuse the allocated objects to speed up the object management. At the later part of this paper, an automatic dynamic memory management, called GC++, is discussed. GC++ collects unreferenced objects automatically with high speed of allocation/deallocation processes. The performance gains of O-Reuse and GC++ are come from the utilization of memory allocation/deallocation behavior.},
  number = {3},
  journal = {Computer Languages, Systems \& Structures},
  author = {Lee, Woo Hyong and Chang, Morris},
  month = oct,
  year = {2002},
  keywords = {Dynamic memory management,Garbage collection,Life-span},
  pages = {237-272},
  file = {/Users/luigi/work/zotero/storage/6PITR68T/Lee and Chang - 2002 - A study of dynamic memory management in C++ progra.pdf;/Users/luigi/work/zotero/storage/IU5UBKMD/S0096055102000152.html}
}

@inproceedings{bhattacharyaAssessingProgrammingLanguage2011,
  address = {New York, NY, USA},
  series = {{{ICSE}} '11},
  title = {Assessing {{Programming Language Impact}} on {{Development}} and {{Maintenance}}: {{A Study}} on {{C}} and {{C}}++},
  isbn = {978-1-4503-0445-0},
  shorttitle = {Assessing {{Programming Language Impact}} on {{Development}} and {{Maintenance}}},
  doi = {10.1145/1985793.1985817},
  abstract = {Billions of dollars are spent every year for building and maintaining software. To reduce these costs we must identify the key factors that lead to better software and more productive development. One such key factor, and the focus of our paper, is the choice of programming language. Existing studies that analyze the impact of choice of programming language suffer from several deficiencies with respect to methodology and the applications they consider. For example, they consider applications built by different teams in different languages, hence fail to control for developer competence, or they consider small-sized, infrequently-used, short-lived projects. We propose a novel methodology which controls for development process and developer competence, and quantifies how the choice of programming language impacts software quality and developer productivity. We conduct a study and statistical analysis on a set of long-lived, widely-used, open source projects - Firefox, Blender, VLC, and MySQL. The key novelties of our study are: (1) we only consider projects which have considerable portions of development in two languages, C and C++, and (2) a majority of developers in these projects contribute to both C and C++ code bases. We found that using C++ instead of C results in improved software quality and reduced maintenance effort, and that code bases are shifting from C to C++. Our methodology lays a solid foundation for future studies on comparative advantages of particular programming languages.},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Bhattacharya, Pamela and Neamtiu, Iulian},
  year = {2011},
  keywords = {developer productivity,software quality,empirical studies,software evolution,high-level languages},
  pages = {171--180},
  file = {/Users/luigi/work/zotero/storage/MP3YYN8B/Bhattacharya and Neamtiu - 2011 - Assessing Programming Language Impact on Developme.pdf}
}

@article{hughesWhyFunctionalProgramming1989,
  title = {Why {{Functional Programming Matters}}},
  volume = {32},
  issn = {0010-4620},
  doi = {10.1093/comjnl/32.2.98},
  number = {2},
  journal = {Comput. J.},
  author = {Hughes, J.},
  month = apr,
  year = {1989},
  pages = {98--107},
  file = {/Users/luigi/work/zotero/storage/IVFUXLWS/whyfp90.pdf;/Users/luigi/work/zotero/storage/U67ZJLAN/Hughes - 1989 - Why Functional Programming Matters.pdf}
}

@article{uralFormalMethodsTest1992,
  title = {Formal Methods for Test Sequence Generation},
  volume = {15},
  issn = {0140-3664},
  doi = {10.1016/0140-3664(92)90092-S},
  abstract = {Six formal methods and their enhancements for generating test sequences from FSM-based specifications are reviewed. These methods are: the Transition Tour (T) method; the Distinguishing Sequence (D) method; the Characterizing Set (W) method; the Unique Input/Output Sequence (UIO) method; the Single UIO (SUIO) method; and the Multiple UIO (MUIO) method. Improved variations of the D-, W-, UIO- and MUIO-methods are included in the discussions. These formal methods are compared in terms of the upper bounds on the length of resulting test sequences. A tool implementing these methods and their enhancements is presented.},
  number = {5},
  journal = {Computer Communications},
  author = {Ural, Hasan},
  month = jun,
  year = {1992},
  keywords = {finite state machines,formal methods,protocol testing,test sequence generation},
  pages = {311-325},
  file = {/Users/luigi/work/zotero/storage/ZMD6IS5N/ural.pdf;/Users/luigi/work/zotero/storage/PGLC6QYW/014036649290092S.html}
}

@article{tofteRegionBasedMemoryManagement1997,
  title = {Region-{{Based Memory Management}}},
  volume = {132},
  issn = {0890-5401},
  doi = {10.1006/inco.1996.2613},
  abstract = {This paper describes a memory management discipline for programs that perform dynamic memory allocation and de-allocation. At runtime, all values are put intoregions. The store consists of a stack of regions. All points of region allocation and de-allocation are inferred automatically, using a type and effect based program analysis. The scheme does not assume the presence of a garbage collector. The scheme was first presented in 1994 (M. Tofte and J.-P. Talpin,in``Proceedings of the 21st ACM SIGPLAN\textendash{}SIGACT Symposium on Principles of Programming Languages,'' pp. 188\textendash{}201); subsequently, it has been tested in The ML Kit with Regions, a region-based, garbage-collection free implementation of the Standard ML Core language, which includes recursive datatypes, higher-order functions and updatable references L. Birkedal, M. Tofte, and M. Vejlstrup, (1996),in``Proceedings of the 23 rd ACM SIGPLAN\textendash{}SIGACT Symposium on Principles of Programming Languages,'' pp. 171\textendash{}183. This paper defines a region-based dynamic semantics for a skeletal programming language extracted from Standard ML. We present the inference system which specifies where regions can be allocated and de-allocated and a detailed proof that the system is sound with respect to a standard semantics. We conclude by giving some advice on how to write programs that run well on a stack of regions, based on practical experience with the ML Kit.},
  number = {2},
  journal = {Information and Computation},
  author = {Tofte, Mads and Talpin, Jean-Pierre},
  month = feb,
  year = {1997},
  pages = {109-176},
  file = {/Users/luigi/work/zotero/storage/K5ZTMDWS/Tofte and Talpin - 1997 - Region-Based Memory Management.pdf;/Users/luigi/work/zotero/storage/9DY227KU/S0890540196926139.html}
}

@inproceedings{deanOptimizationObjectOrientedPrograms1995,
  address = {London, UK, UK},
  series = {{{ECOOP}} '95},
  title = {Optimization of {{Object}}-{{Oriented Programs Using Static Class Hierarchy Analysis}}},
  isbn = {978-3-540-60160-9},
  abstract = {Optimizing compilers for object-oriented languages apply static class analysis and other techniques to try to deduce precise information about the possible classes of the receivers of messages; if successful, dynamically-dispatched messages can be replaced with direct procedure calls and potentially further optimized through inline-expansion. By examining the complete inheritance graph of a program, which we call class hierarchy analysis, the compiler can improve the quality of static class information and thereby improve run-time performance. In this paper we present class hierarchy analysis and describe techniques for implementing this analysis effectively in both statically- and dynamically-typed languages and also in the presence of multi-methods. We also discuss how class hierarchy analysis can be supported in an interactive programming environment and, to some extent, in the presence of separate compilation. Finally, we assess the bottom-line performance improvement due to class hierarchy analysis alone and in combination with two other "competing" optimizations, profile-guided receiver class prediction and method specialization.},
  booktitle = {Proceedings of the 9th {{European Conference}} on {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer-Verlag}},
  author = {Dean, Jeffrey and Grove, David and Chambers, Craig},
  year = {1995},
  pages = {77--101},
  file = {/Users/luigi/work/zotero/storage/AAGL2DKU/dean-grove-chambers-ecoop95.pdf}
}

@inproceedings{goslingJavaIntermediateBytecodes1995,
  address = {New York, NY, USA},
  series = {{{IR}} '95},
  title = {Java {{Intermediate Bytecodes}}: {{ACM SIGPLAN Workshop}} on {{Intermediate Representations}} ({{IR}}'95)},
  isbn = {978-0-89791-754-4},
  shorttitle = {Java {{Intermediate Bytecodes}}},
  doi = {10.1145/202529.202541},
  abstract = {Java is a programming language loosely related to C++. Java originated in a project to produce a software development environment for small distributed embedded systems. Programs needed to be small, fast, ``safe'' and portable. These needs led to a design that is rather different from standard practice. In particular, the form of compiled programs is machine independent bytecodes. But we needed to manipulate programs in ways usually associated with higher level, more abstract intermediate representations. This lets us build systems that are safer, less fragile, more portable, and yet show little performance penalty while still being simple.},
  booktitle = {Papers from the 1995 {{ACM SIGPLAN Workshop}} on {{Intermediate Representations}}},
  publisher = {{ACM}},
  author = {Gosling, James},
  year = {1995},
  pages = {111--118},
  file = {/Users/luigi/work/zotero/storage/IT6IZUER/Gosling - 1995 - Java Intermediate Bytecodes ACM SIGPLAN Workshop .pdf}
}

@inproceedings{hauswirthLowoverheadMemoryLeak2004,
  address = {New York, NY, USA},
  series = {{{ASPLOS XI}}},
  title = {Low-Overhead {{Memory Leak Detection Using Adaptive Statistical Profiling}}},
  isbn = {978-1-58113-804-7},
  doi = {10.1145/1024393.1024412},
  abstract = {Sampling has been successfully used to identify performance optimization opportunities. We would like to apply similar techniques to check program correctness. Unfortunately, sampling provides poor coverage of infrequently executed code, where bugs often lurk. We describe an adaptive profiling scheme that addresses this by sampling executions of code segments at a rate inversely proportional to their execution frequency. To validate our ideas, we have implemented SWAT, a novel memory leak detection tool. SWAT traces program allocations/ frees to construct a heap model and uses our adaptive profiling infrastructure to monitor loads/stores to these objects with low overhead. SWAT reports 'stale' objects that have not been accessed for a 'long' time as leaks. This allows it to find all leaks that manifest during the current program execution. Since SWAT has low runtime overhead (\guilsinglleft{}5\%), and low space overhead (\guilsinglleft{}10\% in most cases and often less than 5\%), it can be used to track leaks in production code that take days to manifest. In addition to identifying the allocations that leak memory, SWAT exposes where the program last accessed the leaked data, which facilitates debugging and fixing the leak. SWAT has been used by several product groups at Microsoft for the past 18 months and has proved effective at detecting leaks with a low false positive rate (\guilsinglleft{}10\%).},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  publisher = {{ACM}},
  author = {Hauswirth, Matthias and Chilimbi, Trishul M.},
  year = {2004},
  keywords = {low-overhead monitoring,memory leaks,runtime analysis},
  pages = {156--164},
  file = {/Users/luigi/work/zotero/storage/QR5GHZTZ/Hauswirth and Chilimbi - 2004 - Low-overhead Memory Leak Detection Using Adaptive .pdf}
}

@inproceedings{okurHowDevelopersUse2012,
  address = {New York, NY, USA},
  series = {{{FSE}} '12},
  title = {How {{Do Developers Use Parallel Libraries}}?},
  isbn = {978-1-4503-1614-9},
  doi = {10.1145/2393596.2393660},
  abstract = {Parallel programming is hard. The industry leaders hope to convert the hard problem of using parallelism into the easier problem of using a parallel library. Yet, we know little about how programmers adopt these libraries in practice. Without such knowledge, other programmers cannot educate themselves about the state of the practice, library designers are unaware of API misusage, researchers make wrong assumptions, and tool vendors do not support common usage of library constructs. We present the first study that analyzes the usage of parallel libraries in a large scale experiment. We analyzed 655 open-source applications that adopted Microsoft's new parallel libraries -- Task Parallel Library (TPL) and Parallel Language Integrated Query (PLINQ) -- comprising 17.6M lines of code written in C\#. These applications are developed by 1609 programmers. Using this data, we answer 8 research question and we uncover some interesting facts. For example, (i) for two of the fundamental parallel constructs, in at least 10\% of the cases developers misuse them so that the code runs sequentially instead of concurrently, (ii) developers make their parallel code unnecessarily complex, (iii) applications of different size have different adoption trends. The library designers confirmed that our finding are useful and will influence the future development of the libraries.},
  booktitle = {Proceedings of the {{ACM SIGSOFT}} 20th {{International Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Okur, Semih and Dig, Danny},
  year = {2012},
  keywords = {empirical study,C\#,multi-core,parallel libraries},
  pages = {54:1--54:11},
  file = {/Users/luigi/work/zotero/storage/K56C7BDL/Okur and Dig - 2012 - How Do Developers Use Parallel Libraries.pdf}
}

@inproceedings{hillsCaseVisitorInterpreter2011,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Case}} of {{Visitor}} versus {{Interpreter Pattern}}},
  isbn = {978-3-642-21951-1 978-3-642-21952-8},
  doi = {10.1007/978-3-642-21952-8_17},
  abstract = {We compare the Visitor pattern with the Interpreter pattern, investigating a single case in point for the Java language. We have produced and compared two versions of an interpreter for a programming language. The first version makes use of the Visitor pattern. The second version was obtained by using an automated refactoring to transform uses of the Visitor pattern to uses of the Interpreter pattern. We compare these two nearly equivalent versions on their maintenance characteristics and execution efficiency. Using a tailored experimental research method we can highlight differences and the causes thereof. The contributions of this paper are that it isolates the choice between Visitor and Interpreter in a realistic software project and makes the difference experimentally observable.},
  language = {en},
  booktitle = {Objects, {{Models}}, {{Components}}, {{Patterns}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Hills, Mark and Klint, Paul and van der Storm, Tijs and Vinju, Jurgen},
  month = jun,
  year = {2011},
  pages = {228-243},
  file = {/Users/luigi/work/zotero/storage/6I3GGBDE/Hills et al. - 2011 - A Case of Visitor versus Interpreter Pattern.pdf;/Users/luigi/work/zotero/storage/QLW9VCEN/978-3-642-21952-8_17.html}
}

@book{klimovApproachSupercompilationObjectoriented2008,
  title = {An {{Approach}} to {{Supercompilation}} for {{Object}}-Oriented {{Languages}}: The {{Java Supercompiler Case Study}}},
  shorttitle = {An {{Approach}} to {{Supercompilation}} for {{Object}}-Oriented {{Languages}}},
  abstract = {An extension of Turchin's supercompilation from functional to object-oriented languages as it is implemented in the current version of a Java supercompiler (JScp) is reviewed. There are two novelties: first, the construction of the specialized code of operations on objects is separated into two stages\textemdash{}residualization of all operations on objects during supercompilation proper and elimination of redundant code in post-processing; and second, limited configuration analysis, which processes each Java control statement one by one using width-first unfolding of a process graph, is used. The construction of JScp is based on the principle of user control of the process of supercompilation rather than building a black-box automatic supercompiler. The rationale for this decision is discussed.},
  author = {Klimov, Andrei V.},
  year = {2008},
  file = {/Users/luigi/work/zotero/storage/65CLZZTX/Klimov - 2008 - An Approach to Supercompilation for Object-oriente.pdf;/Users/luigi/work/zotero/storage/EFRTY2QF/summary.html}
}

@inproceedings{cremetCoreCalculusScala2006,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Core Calculus}} for {{Scala Type Checking}}},
  isbn = {978-3-540-37791-7 978-3-540-37793-1},
  doi = {10.1007/11821069_1},
  abstract = {We present a minimal core calculus that captures interesting constructs of the Scala programming language: nested classes, abstract types, mixin composition, and path dependent types. We show that the problems of type assignment and subtyping in this calculus are decidable.},
  language = {en},
  booktitle = {Mathematical {{Foundations}} of {{Computer Science}} 2006},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Cremet, Vincent and Garillot, Fran{\c c}ois and Lenglet, Sergue\"i and Odersky, Martin},
  month = aug,
  year = {2006},
  pages = {1-23},
  file = {/Users/luigi/work/zotero/storage/VL3QIB7S/Cremet et al. - 2006 - A Core Calculus for Scala Type Checking.pdf;/Users/luigi/work/zotero/storage/8F2VDBVF/11821069_1.html}
}

@inproceedings{zhangWholeExecutionTraces2004,
  title = {Whole {{Execution Traces}}},
  doi = {10.1109/MICRO.2004.37},
  abstract = {Different types of program profiles (control flow, value, address, and dependence) have been collected and extensively studied by researchers to identify program characteristics that can then be exploited to develop more effective compilers and architectures. Due to the large amounts of profile data produced by realistic program runs, most work has focused on separately collecting and compressing different types of profiles. In this paper we present a unified representation of profiles called Whole Execution Trace (WET) which includes the complete information contained in each of the above types of traces. Thus WETs provide a basis for a next generation software tool that will enable mining of program profiles to identify program characteristics that require understanding of relationships among various types of profiles. The key features of our WET representation are: WET is constructed by labeling a static program representation with profile information such that relavent and related profile information can be directly accessed by analysis algorithms as they traverse the representation; a highly effective two tier strategy is used to significantly compress the WET; and compression techniques are designed such that they do not adversely affect the ability to rapidly traverse WET for extracting subsets of information corresponding to individual profile types as well as a combination of profile types (e.g., in form of dynamic slices of WETs). Our experimentation shows that on an average execution traces resulting from execution of 647 Million statements can be stored in 331 Megabytes of storage after compression. The compression factors range from 16 to 83. Moreover the rates at which different types of profiles can be individually or simultaneously extracted are high.},
  booktitle = {37th {{International Symposium}} on {{Microarchitecture}}, 2004. {{MICRO}}-37 2004},
  author = {Zhang, Xiangyu and Gupta, R.},
  month = dec,
  year = {2004},
  pages = {105-116},
  file = {/Users/luigi/work/zotero/storage/PM97DDLH/Zhang and Gupta - 2004 - Whole Execution Traces.pdf;/Users/luigi/work/zotero/storage/XPHZY82M/1550986.html}
}

@inproceedings{feautrierAutomaticParallelizationPolytope1996,
  title = {Automatic {{Parallelization}} in the {{Polytope Model}}},
  abstract = {. The aim of this paper is to explain the importance of polytope and polyhedra in automatic parallelization. We show that the semantics of parallel programs is best described geometrically, as properties of sets of integral points in n-dimensional spaces, where n is related to the maximum nesting depth of DO loops. The needed properties translate nicely to properties of polyhedra, for which many algorithms have been designed for the needs of optimization and operation research. We show how these ideas apply to scheduling, placement and parallel code generation. R'esum'e Le but de cet article est d'expliquer le role jou'e par les poly`edres et les polytopes en parall'elisation automatique. On montre que la s'emantique d'un programme parall`ele se d'ecrit naturellement sous forme g'eom'etrique, les propri'et'es du programme 'etant formalis'ees comme des propri'et'es d'ensemble de points dans un espace `a n dimensions. n est li'e `a la profondeur maximale d'imbrication des boucles DO. Les...},
  booktitle = {Laboratoire {{PRiSM}}, {{Universit\'e}} Des {{Versailles St}}-{{Quentin}} En {{Yvelines}}, 45, Avenue Des {{\'Etats}}-{{Unis}}, {{F}}-78035 {{Versailles Cedex}}},
  publisher = {{Springer-Verlag}},
  author = {Feautrier, Paul},
  year = {1996},
  pages = {79--103},
  file = {/Users/luigi/work/zotero/storage/CGLFTXBS/Feautrier - 1996 - Automatic Parallelization in the Polytope Model.pdf;/Users/luigi/work/zotero/storage/XZWPE2SL/summary.html}
}

@inproceedings{ammonsExploitingHardwarePerformance1997,
  address = {New York, NY, USA},
  series = {{{PLDI}} '97},
  title = {Exploiting {{Hardware Performance Counters}} with {{Flow}} and {{Context Sensitive Profiling}}},
  isbn = {978-0-89791-907-4},
  doi = {10.1145/258915.258924},
  abstract = {A program profile attributes run-time costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis--flow and context sensitivity--to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that efficiently captures calling contexts for procedure-level measurements.Our measurements show that the SPEC95 benchmarks execute a small number (3--28) of hot paths that account for 9--98\% of their L1 data cache misses. Moreover, these hot paths are concentrated in a few routines, which have complex dynamic behavior.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1997 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Ammons, Glenn and Ball, Thomas and Larus, James R.},
  year = {1997},
  pages = {85--96},
  file = {/Users/luigi/work/zotero/storage/D7QGCW74/Ammons et al. - 1997 - Exploiting Hardware Performance Counters with Flow.pdf}
}

@inproceedings{crewASTLOGLanguageExamining1997,
  address = {Berkeley, CA, USA},
  series = {{{DSL}}'97},
  title = {{{ASTLOG}}: {{A Language}} for {{Examining Abstract Syntax Trees}}},
  shorttitle = {{{ASTLOG}}},
  abstract = {We desired a facility for locating/analyzing syntactic artifacts in abstract syntax trees of C/C++ programs, similar to the facility grep or awk provides for locating artifacts at the lexical level. Prolog, with its implicit pattern-matching and backtracking capabilities, is a natural choice for such an application. We have developed a Prolog variant that avoids the overhead of translating the source syntactic structures into the form of a Prolog database; this is crucial to obtaining acceptable performance on large programs. An interpreter for this language has been implemented and used to find various kinds of syntactic bugs and other questionable constructs in real programs like Microsoft SQL server (450Klines) and Microsoft Word (2Mlines) in time comparable to the runtime of the actual compiler. The model in which terms are matched against an implicit current object, rather than simply proven against a database of facts, leads to a distinct "inside-out functional" programming style that is quite unlike typical Prolog, but one that is, in fact, well-suited to the examination of trees. Also, various second-order Prolog set-predicates may be implemented via manipulation of the current object, thus retaining an important feature without entailing that the database be dynamically extensible as the usual implementation does.},
  booktitle = {Proceedings of the {{Conference}} on {{Domain}}-{{Specific Languages}} on {{Conference}} on {{Domain}}-{{Specific Languages}} ({{DSL}}), 1997},
  publisher = {{USENIX Association}},
  author = {Crew, Roger F.},
  year = {1997},
  pages = {18--18},
  file = {/Users/luigi/work/zotero/storage/CALGNDHI/download.pdf}
}

@article{savageEraserDynamicData1997,
  title = {Eraser: {{A Dynamic Data Race Detector}} for {{Multithreaded Programs}}},
  volume = {15},
  issn = {0734-2071},
  shorttitle = {Eraser},
  doi = {10.1145/265924.265927},
  abstract = {Multithreaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This article describes a new tool, called Eraser, for dynamically detecting data races in lock-based multithreaded programs. Eraser uses binary rewriting techniques to monitor every shared-monory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multithreaded Web search engine, that demonstrate the effectiveness of this approach.},
  number = {4},
  journal = {ACM Trans. Comput. Syst.},
  author = {Savage, Stefan and Burrows, Michael and Nelson, Greg and Sobalvarro, Patrick and Anderson, Thomas},
  month = nov,
  year = {1997},
  keywords = {binary code modification,multithreaded programming,race detection},
  pages = {391--411},
  file = {/Users/luigi/work/zotero/storage/CMMTXIF6/Savage et al. - 1997 - Eraser A Dynamic Data Race Detector for Multithre.pdf}
}

@inproceedings{shavitSoftwareTransactionalMemory1995,
  address = {New York, NY, USA},
  series = {{{PODC}} '95},
  title = {Software {{Transactional Memory}}},
  isbn = {978-0-89791-710-0},
  doi = {10.1145/224964.224987},
  booktitle = {Proceedings of the {{Fourteenth Annual ACM Symposium}} on {{Principles}} of {{Distributed Computing}}},
  publisher = {{ACM}},
  author = {Shavit, Nir and Touitou, Dan},
  year = {1995},
  pages = {204--213},
  file = {/Users/luigi/work/zotero/storage/NHMYJL73/Shavit and Touitou - 1995 - Software Transactional Memory.pdf}
}

@techreport{ducasseCocaDebuggerBased1998,
  type = {Report},
  title = {Coca: {{A Debugger}} for {{C Based}} on {{Fine Grained Control Flow}} and {{Data Events}}},
  shorttitle = {Coca},
  abstract = {We present Coca, an automated debugger for C, where the breakpoint mechanism is based on events related to language constructs. Events have semantics whereas source lines used by most debuggers do not have any. A trace is a sequence of events. It can be seen as an ordered relation in a database. Users can specify precisely which events they want to see by specifying values for event attributes. At each event, visible variables can be queried. The trace query language is Prolog with a handful of primitives. The trace query mechanism searches through the execution traces using both control flow and data whereas debuggers usually search according to either control flow or data. As opposed to fully \guillemotleft{}relational\guillemotright{} debuggers which use plain database querying mechanisms, Coca trace querying mechanism does not require any storage. The analysis is done on the fly, synchronously with the traced execution. Coca is therefore more powerful than \guillemotleft{}source line\guillemotright{} debuggers and more efficient than relational debuggers.},
  language = {en},
  institution = {{INRIA}},
  author = {Ducass\'e, Mireille},
  year = {1998},
  file = {/Users/luigi/work/zotero/storage/GSCD3HPN/Ducassé - 1998 - Coca A Debugger for C Based on Fine Grained Contr.pdf;/Users/luigi/work/zotero/storage/VM9ZNKUB/en.html}
}

@inproceedings{augustssonCayenneLanguageDependent1998,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Cayenne \textemdash{} {{A Language}} with {{Dependent Types}}},
  isbn = {978-3-540-66241-9 978-3-540-48506-3},
  doi = {10.1007/10704973_6},
  abstract = {Cayenne is a Haskell-like language. The main difference between Haskell and Cayenne is that Cayenne has dependent types, i.e., the result type of a function may depend on the argument value, and types of record components (which can be types or values) may depend on other components. Cayenne also combines the syntactic categories for value expressions and type expressions; thus reducing the number of language concepts.Having dependent types and combined type and value expressions makes the language very powerful. It is powerful enough that a special module concept is unnecessary; ordinary records suffice. It is also powerful enough to encode predicate logic at the type level, allowing types to be used as specifications of programs. However, this power comes at a cost: type checking of Cayenne is undecidable. While this may appear to be a steep price to pay, it seems to work well in practice.},
  language = {en},
  booktitle = {Advanced {{Functional Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Augustsson, Lennart},
  month = sep,
  year = {1998},
  pages = {240-267},
  file = {/Users/luigi/work/zotero/storage/RYDPIAUF/Augustsson - 1998 - Cayenne — A Language with Dependent Types.pdf;/Users/luigi/work/zotero/storage/R7Q2B7VA/10704973_6.html}
}

@inproceedings{pughFixingJavaMemory1999,
  address = {New York, NY, USA},
  series = {{{JAVA}} '99},
  title = {Fixing the {{Java Memory Model}}},
  isbn = {978-1-58113-161-1},
  doi = {10.1145/304065.304106},
  booktitle = {Proceedings of the {{ACM}} 1999 {{Conference}} on {{Java Grande}}},
  publisher = {{ACM}},
  author = {Pugh, William},
  year = {1999},
  pages = {89--98},
  file = {/Users/luigi/work/zotero/storage/7MW7IE9U/Pugh - 1999 - Fixing the Java Memory Model.pdf;/Users/luigi/work/zotero/storage/I8DGMMY9/Pugh - 1999 - Fixing the Java memory model.pdf}
}

@inproceedings{xiDependentTypesPractical1999,
  address = {New York, NY, USA},
  series = {{{POPL}} '99},
  title = {Dependent {{Types}} in {{Practical Programming}}},
  isbn = {978-1-58113-095-9},
  doi = {10.1145/292540.292560},
  abstract = {We present an approach to enriching the type system of ML with a restricted form of dependent types, where type index objects are drawn from a constraint domain C, leading to the DML(C) language schema. This allows specification and inference of significantly more precise type information, facilitating program error detection and compiler optimization. A major complication resulting from introducing dependent types is that pure type inference for the enriched system is no longer possible, but we show that type-checking a sufficiently annotated program in DML(C) can be reduced to constraint satisfaction in the constraint domain C. We exhibit the unobtrusiveness of our approach through practical examples and prove that DML(C) is conservative over ML. The main contribution of the paper lies in our language design, including the formulation of type-checking rules which makes the approach practical. To our knowledge, no previous type system for a general purpose programming language such as ML has combined dependent types with features including datatype declarations, higher-order functions, general recursions, let-polymorphism, mutable references, and exceptions. In addition, we have finished a prototype implementation of DML(C) for an integer constraint domain C, where constraints are linear inequalities (Xi and Pfenning 1998).},
  booktitle = {Proceedings of the 26th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Xi, Hongwei and Pfenning, Frank},
  year = {1999},
  pages = {214--227},
  file = {/Users/luigi/work/zotero/storage/3MKD2I2T/Xi and Pfenning - 1999 - Dependent types in practical programming.pdf;/Users/luigi/work/zotero/storage/5VXUPQVI/Xi and Pfenning - 1999 - Dependent Types in Practical Programming.pdf}
}

@inproceedings{chibaLoadTimeStructuralReflection2000,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Load-{{Time Structural Reflection}} in {{Java}}},
  isbn = {978-3-540-67660-7 978-3-540-45102-0},
  doi = {10.1007/3-540-45102-1_16},
  abstract = {The standard reflection API of Java provides the ability to introspect a program but not to alter program behavior. This paper presents an extension to the reflection API for addressing this limitation. Unlike other extensions enabling behavioral reflection, our extension called Javassist enables structural reflection in Java. For using a standard Java virtual machine (JVM) and avoiding a performance problem, Javassist allows structural reflection only before a class is loaded into the JVM. However, Javassist still covers various applications including a language extension emulating behavioral reflection. This paper also presents the design principles of Javassist, which distinguish Javassist from related work.},
  language = {en},
  booktitle = {{{ECOOP}} 2000 \textemdash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Chiba, Shigeru},
  month = jun,
  year = {2000},
  pages = {313-336},
  file = {/Users/luigi/work/zotero/storage/M8LRDSEZ/Chiba - 2000 - Load-Time Structural Reflection in Java.pdf;/Users/luigi/work/zotero/storage/9DVSWD3X/3-540-45102-1_16.html}
}

@inproceedings{guitartfernandezJavaInstrumentationSuite2000,
  title = {Java Instrumentation Suite: Accurate Analysis of {{Java}} Threaded Applications},
  copyright = {Open Access},
  shorttitle = {Java Instrumentation Suite},
  language = {eng},
  booktitle = {Proceedings of the {{Second Annual Workshop}} on {{Java}} for {{High}}-{{Performance Computing}}},
  author = {Guitart Fern\'andez, Jordi and Torres Vi\~nals, Jordi and Ayguad\'e Parra, Eduard and Oliver, Jose and Mancho, Labarta and Jos\'e, Jes\'us},
  year = {2000},
  keywords = {Àrees temàtiques de la UPC::Informàtica::Llenguatges de programació,Java (Computer program language),Java (Llenguatge de programació)},
  pages = {15-25},
  file = {/Users/luigi/work/zotero/storage/CYB58AVX/Guitart Fernández et al. - 2000 - Java instrumentation suite accurate analysis of J.pdf;/Users/luigi/work/zotero/storage/6S25UF3T/28290.html}
}

@inproceedings{alpernEfficientImplementationJava2001,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '01},
  title = {Efficient {{Implementation}} of {{Java Interfaces}}: {{Invokeinterface Considered Harmless}}},
  isbn = {978-1-58113-335-6},
  shorttitle = {Efficient {{Implementation}} of {{Java Interfaces}}},
  doi = {10.1145/504282.504291},
  abstract = {Single superclass inheritance enables simple and efficient table-driven virtual method dispathc. However, virtual method table dispatch does not handle multiple inheritance and interfaces. This complication has led to a widespread misimpression that interface method dispatch is inherently inefficient. This paper argues that with proper implementation techniques, Java interfaces need not be a source of significant performance degradation.},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Alpern, Bowen and Cocchi, Anthony and Fink, Stephen and Grove, David},
  year = {2001},
  pages = {108--124},
  file = {/Users/luigi/work/zotero/storage/AXHNCVKU/Alpern et al. - 2001 - Efficient Implementation of Java Interfaces Invok.pdf}
}

@inproceedings{ingallsSmalltalk76ProgrammingSystem1978,
  address = {New York, NY, USA},
  series = {{{POPL}} '78},
  title = {The {{Smalltalk}}-76 {{Programming System Design}} and {{Implementation}}},
  doi = {10.1145/512760.512762},
  abstract = {This paper describes a programming system based on the metaphor of communicating objects. Experience with a running system shows that this model provides flexibility, modularity and compactness. A compiled representation for the language is presented, along with an interpreter suitable for microcoding. The object-oriented model provides naturally efficient addressing; a corresponding virtual memory is described which offers dense utilization of resident space.},
  booktitle = {Proceedings of the 5th {{ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Ingalls, Daniel H. H.},
  year = {1978},
  pages = {9--16},
  file = {/Users/luigi/work/zotero/storage/4GQ7A9IJ/Ingalls - 1978 - The Smalltalk-76 Programming System Design and Imp.pdf;/Users/luigi/work/zotero/storage/MCMC6K98/Ingalls - 1978 - The Smalltalk-76 programming system design and imp.pdf}
}

@article{igarashiFeatherweightJavaMinimal2001,
  title = {Featherweight {{Java}}: {{A Minimal Core Calculus}} for {{Java}} and {{GJ}}},
  volume = {23},
  issn = {0164-0925},
  shorttitle = {Featherweight {{Java}}},
  doi = {10.1145/503502.503505},
  abstract = {Several recent studies have introduced lightweight versions of Java: reduced languages in which complex features like threads and reflection are dropped to enable rigorous arguments about key properties such as type safety. We carry this process a step further, omitting almost all features of the full language (including interfaces and even assignment) to obtain a small calculus, Featherweight Java, for which rigorous proofs are not only possible but easy. Featherweight Java bears a similar relation to Java as the lambda-calculus does to languages such as ML and Haskell. It offers a similar computational "feel," providing classes, methods, fields, inheritance, and dynamic typecasts with a semantics closely following Java's. A proof of type safety for Featherweight Java thus illustrates many of the interesting features of a safety proof for the full language, while remaining pleasingly compact. The minimal syntax, typing rules, and operational semantics of Featherweight Java make it a handy tool for studying the consequences of extensions and variations. As an illustration of its utility in this regard, we extend Featherweight Java with generic classes in the style of GJ (Bracha, Odersky, Stoutamire, and Wadler) and give a detailed proof of type safety. The extended system formalizes for the first time some of the key features of GJ.},
  number = {3},
  journal = {ACM Trans. Program. Lang. Syst.},
  author = {Igarashi, Atsushi and Pierce, Benjamin C. and Wadler, Philip},
  month = may,
  year = {2001},
  keywords = {Java,Compilation,generic classes,language design,language semantics},
  pages = {396--450},
  file = {/Users/luigi/work/zotero/storage/LNEIATY6/Igarashi et al. - 2001 - Featherweight Java A Minimal Core Calculus for Ja.pdf}
}

@inproceedings{beebeeImplementationScopedMemory2001,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {An {{Implementation}} of {{Scoped Memory}} for {{Real}}-{{Time Java}}},
  isbn = {978-3-540-42673-8 978-3-540-45449-6},
  doi = {10.1007/3-540-45449-7_21},
  abstract = {This paper presents our experience implementing the memory management extensions in the Real-Time Specification for Java. These extensions are designed to given real-time programmers the control they need to obtain predictable memory system behavior while preserving Java's safe memory model.We describe our implementation of certain dynamic checks required by the Real-Time Java extensions. In particular, we describe how to perform these checks in a way that avoids harmful interactions between the garbage collector and the memory management system. We also found that extensive debugging support was necessary during the development of Real-Time Java programs. We therefore used a static analysis and a dynamic debugging package during the development of our benchmark applications.},
  language = {en},
  booktitle = {Embedded {{Software}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Beebee, William S. and Rinard, Martin},
  month = oct,
  year = {2001},
  pages = {289-305},
  file = {/Users/luigi/work/zotero/storage/83VL2CWC/Beebee and Rinard - 2001 - An Implementation of Scoped Memory for Real-Time J.pdf;/Users/luigi/work/zotero/storage/96K25Z6E/3-540-45449-7_21.html}
}

@inproceedings{grimmerDynamicallyComposingLanguages2015,
  address = {New York, NY, USA},
  series = {{{MODULARITY}} 2015},
  title = {Dynamically {{Composing Languages}} in a {{Modular Way}}: {{Supporting C Extensions}} for {{Dynamic Languages}}},
  isbn = {978-1-4503-3249-1},
  shorttitle = {Dynamically {{Composing Languages}} in a {{Modular Way}}},
  doi = {10.1145/2724525.2728790},
  abstract = {Many dynamic languages such as Ruby, Python and Perl offer some kind of functionality for writing parts of applications in a lower-level language such as C. These C extension modules are usually written against the API of an interpreter, which provides access to the higher-level language's internal data structures. Alternative implementations of the high-level languages often do not support such C extensions because implementing the same API as in the original implementations is complicated and limits performance. In this paper we describe a novel approach for modular composition of languages that allows dynamic languages to support C extensions through interpretation. We propose a flexible and reusable cross-language mechanism that allows composing multiple language interpreters, which run on the same VM and share the same form of intermediate representation - in this case abstract syntax trees. This mechanism allows us to efficiently exchange runtime data across different interpreters and also enables the dynamic compiler of the host VM to inline and optimize programs across multiple language boundaries. We evaluate our approach by composing a Ruby interpreter with a C interpreter. We run existing Ruby C extensions and show how our system executes combined Ruby and C modules on average over 3x faster than the conventional implementation of Ruby with native C extensions, and on average over 20x faster than an existing alternate Ruby implementation on the JVM (JRuby) calling compiled C extensions through a bridge interface. We demonstrate that cross-language inlining, which is not possible with native code, is performance-critical by showing how speedup is reduced by around 50\% when it is disabled.},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Modularity}}},
  publisher = {{ACM}},
  author = {Grimmer, Matthias and Seaton, Chris and W\"urthinger, Thomas and M\"ossenb\"ock, Hanspeter},
  year = {2015},
  keywords = {Optimization,C,Cross-language,Language Interoperability,Native Extension,Ruby,Virtual Machine},
  pages = {1--13},
  file = {/Users/luigi/work/zotero/storage/RUJXVNWF/Grimmer et al. - 2015 - Dynamically Composing Languages in a Modular Way .pdf}
}

@inproceedings{fosterFlowsensitiveTypeQualifiers2002,
  address = {New York, NY, USA},
  series = {{{PLDI}} '02},
  title = {Flow-Sensitive {{Type Qualifiers}}},
  isbn = {978-1-58113-463-6},
  doi = {10.1145/512529.512531},
  abstract = {We present a system for extending standard type systems with flow-sensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flow-sensitively---the underlying standard types are unchanged, which allows us to obtain an efficient constraint-based inference algorithm that integrates flow-insensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flow-sensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2002 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Foster, Jeffrey S. and Terauchi, Tachio and Aiken, Alex},
  year = {2002},
  keywords = {alias analysis,constraints,effect inference,flow-sensitivity,linux kernel,locking,restrict,type qualifiers,types},
  pages = {1--12},
  file = {/Users/luigi/work/zotero/storage/GV3ZW85U/Foster et al. - 2002 - Flow-sensitive Type Qualifiers.pdf}
}

@inproceedings{grossmanRegionbasedMemoryManagement2002,
  address = {New York, NY, USA},
  series = {{{PLDI}} '02},
  title = {Region-Based {{Memory Management}} in {{Cyclone}}},
  isbn = {978-1-58113-463-6},
  doi = {10.1145/512529.512563},
  abstract = {Cyclone is a type-safe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing type-safety. In this paper, we focus on the region-based memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region effects reduces this burden. As a result, we integrate C idioms in a region-based framework. In our experience, porting legacy C to Cyclone has required altering about 8\% of the code; of the changes, only 6\% (of the 8\%) were region annotations.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2002 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Grossman, Dan and Morrisett, Greg and Jim, Trevor and Hicks, Michael and Wang, Yanling and Cheney, James},
  year = {2002},
  pages = {282--293},
  file = {/Users/luigi/work/zotero/storage/DUQYG33H/Grossman et al. - 2002 - Region-based Memory Management in Cyclone.pdf}
}

@article{solowayEmpiricalStudiesProgramming1984,
  title = {Empirical {{Studies}} of {{Programming Knowledge}}},
  volume = {SE-10},
  issn = {0098-5589},
  doi = {10.1109/TSE.1984.5010283},
  abstract = {We suggest that expert programmers have and use two types of programming knowledge: 1) programming plans, which are generic program fragments that represent stereotypic action sequences in programming, and 2) rules of programming discourse, which capture the conventions in programming and govern the composition of the plans into programs. We report here on two empirical studies that attempt to evaluate the above hypothesis. Results from these studies do in fact support our claim.},
  number = {5},
  journal = {IEEE Transactions on Software Engineering},
  author = {Soloway, E. and Ehrlich, K.},
  month = sep,
  year = {1984},
  keywords = {Programming profession,Circuits,Cognitive models of programming,Functional programming,novice/expert differences,Physics,program conprehension,Psychology,software psychology,Text processing},
  pages = {595-609},
  file = {/Users/luigi/work/zotero/storage/VEG4H8ZW/Soloway and Ehrlich - 1984 - Empirical Studies of Programming Knowledge.pdf;/Users/luigi/work/zotero/storage/UVYQFJAB/5010283.html}
}

@inproceedings{buddTheoreticalEmpiricalStudies1980,
  address = {New York, NY, USA},
  series = {{{POPL}} '80},
  title = {Theoretical and {{Empirical Studies}} on {{Using Program Mutation}} to {{Test}} the {{Functional Correctness}} of {{Programs}}},
  isbn = {978-0-89791-011-8},
  doi = {10.1145/567446.567468},
  booktitle = {Proceedings of the 7th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Budd, Timothy A. and DeMillo, Richard A. and Lipton, Richard J. and Sayward, Frederick G.},
  year = {1980},
  pages = {220--233},
  file = {/Users/luigi/work/zotero/storage/KAUFK5G4/Budd et al. - 1980 - Theoretical and Empirical Studies on Using Program.pdf}
}

@article{precheltEmpiricalComparisonSeven2000,
  title = {An Empirical Comparison of Seven Programming Languages},
  volume = {33},
  issn = {0018-9162},
  doi = {10.1109/2.876288},
  abstract = {Often heated, debates regarding different programming languages' effectiveness remain inconclusive because of scarce data and a lack of direct comparisons. The author addresses that challenge, comparatively analyzing 80 implementations of the phone-code program in seven different languages (C, C++, Java, Perl, Python, Rexx and Tcl). Further, for each language, the author analyzes several separate implementations by different programmers. The comparison investigates several aspects of each language, including program length, programming effort, runtime efficiency, memory consumption, and reliability. The author uses comparisons to present insight into program language performance},
  number = {10},
  journal = {Computer},
  author = {Prechelt, L.},
  month = oct,
  year = {2000},
  keywords = {Java,Programming profession,Runtime,Computer languages,Production,Program processors,Statistics,C language,authoring languages,C++,C++ language,memory consumption,Perl,phone-code program,program language performance,program length,programming,programming language comparison,Python,Read-write memory,Rexx,runtime efficiency,software reliability,Sun,Tcl,Workstations},
  pages = {23-29},
  file = {/Users/luigi/work/zotero/storage/4MSJ8NRW/Prechelt - 2000 - An empirical comparison of seven programming langu.pdf;/Users/luigi/work/zotero/storage/Q7MZKNJJ/876288.html}
}

@inproceedings{haasBringingWebSpeed2017,
  address = {New York, NY, USA},
  series = {{{PLDI}} 2017},
  title = {Bringing the {{Web Up}} to {{Speed}} with {{WebAssembly}}},
  isbn = {978-1-4503-4988-8},
  doi = {10.1145/3062341.3062363},
  abstract = {The maturation of the Web platform has given rise to sophisticated and demanding Web applications such as interactive 3D visualization, audio and video software, and games. With that, efficiency and security of code on the Web has become more important than ever. Yet JavaScript as the only built-in language of the Web is not well-equipped to meet these requirements, especially as a compilation target.   Engineers from the four major browser vendors have risen to the challenge and collaboratively designed a portable low-level bytecode called WebAssembly. It offers compact representation, efficient validation and compilation, and safe low to no-overhead execution. Rather than committing to a specific programming model, WebAssembly is an abstraction over modern hardware, making it language-, hardware-, and platform-independent, with use cases beyond just the Web. WebAssembly has been designed with a formal semantics from the start. We describe the motivation, design and formal semantics of WebAssembly and provide some preliminary experience with implementations.},
  booktitle = {Proceedings of the 38th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Haas, Andreas and Rossberg, Andreas and Schuff, Derek L. and Titzer, Ben L. and Holman, Michael and Gohman, Dan and Wagner, Luke and Zakai, Alon and Bastien, JF},
  year = {2017},
  keywords = {virtual machines,type systems,programming languages,assembly languages,just-in-time compilers},
  pages = {185--200},
  file = {/Users/luigi/work/zotero/storage/EF3PQK7Y/Haas et al. - 2017 - Bringing the Web Up to Speed with WebAssembly.pdf}
}

@inproceedings{brachaStrongtalkTypecheckingSmalltalk1993,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '93},
  title = {Strongtalk: {{Typechecking Smalltalk}} in a {{Production Environment}}},
  isbn = {978-0-89791-587-8},
  shorttitle = {Strongtalk},
  doi = {10.1145/165854.165893},
  booktitle = {Proceedings of the {{Eighth Annual Conference}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Bracha, Gilad and Griswold, David},
  year = {1993},
  pages = {215--230},
  file = {/Users/luigi/work/zotero/storage/H8G9RBI3/Bracha and Griswold - 1993 - Strongtalk Typechecking Smalltalk in a Production.pdf}
}

@inproceedings{livshitsReflectionAnalysisJava2005,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Reflection {{Analysis}} for {{Java}}},
  isbn = {978-3-540-29735-2 978-3-540-32247-4},
  doi = {10.1007/11575467_11},
  abstract = {Reflection has always been a thorn in the side of Java static analysis tools. Without a full treatment of reflection, static analysis tools are both incomplete because some parts of the program may not be included in the application call graph, and unsound because the static analysis does not take into account reflective features of Java that allow writes to object fields and method invocations. However, accurately analyzing reflection has always been difficult, leading to most static analysis tools treating reflection in an unsound manner or just ignoring it entirely. This is unsatisfactory as many modern Java applications make significant use of reflection.In this paper we propose a static analysis algorithm that uses points-to information to approximate the targets of reflective calls as part of call graph construction. Because reflective calls may rely on input to the application, in addition to performing reflection resolution, our algorithm also discovers all places in the program where user-provided specifications are necessary to fully resolve reflective targets. As an alternative to user-provided specifications, we also propose a reflection resolution approach based on type cast information that reduces the need for user input, but typically results in a less precise call graph.We have implemented the reflection resolution algorithms described in this paper and applied them to a set of six large, widely-used benchmark applications consisting of more than 600,000 lines of code combined. Experiments show that our technique is effective for resolving most reflective calls without any user input. Certain reflective calls, however, cannot be resolved at compile time precisely. Relying on a user-provided specification to obtain a conservative call graph results in graphs that contain 1.43 to 6.58 times more methods that the original. In one case, a conservative call graph has 7,047 more methods than a call graph that does not interpret reflective calls. In contrast, ignoring reflection leads to missing substantial portions of the application call graph.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Livshits, Benjamin and Whaley, John and Lam, Monica S.},
  month = nov,
  year = {2005},
  pages = {139-160},
  file = {/Users/luigi/work/zotero/storage/NXVAYMZH/Livshits et al. - 2005 - Reflection Analysis for Java.pdf;/Users/luigi/work/zotero/storage/RJE4DL2I/11575467_11.html}
}

@inproceedings{banadosschwerterTheoryGradualEffect2014,
  address = {New York, NY, USA},
  series = {{{ICFP}} '14},
  title = {A {{Theory}} of {{Gradual Effect Systems}}},
  isbn = {978-1-4503-2873-9},
  doi = {10.1145/2628136.2628149},
  abstract = {Effect systems have the potential to help software developers, but their practical adoption has been very limited. We conjecture that this limited adoption is due in part to the difficulty of transitioning from a system where effects are implicit and unrestricted to a system with a static effect discipline, which must settle for conservative checking in order to be decidable. To address this hindrance, we develop a theory of gradual effect checking, which makes it possible to incrementally annotate and statically check effects, while still rejecting statically inconsistent programs. We extend the generic type-and-effect framework of Marino and Millstein with a notion of unknown effects, which turns out to be significantly more subtle than unknown types in traditional gradual typing. We appeal to abstract interpretation to develop and validate the concepts of gradual effect checking. We also demonstrate how an effect system formulated in Marino and Millstein's framework can be automatically extended to support gradual checking.},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Ba\~nados Schwerter, Felipe and Garcia, Ronald and Tanter, \'Eric},
  year = {2014},
  keywords = {abstract interpretation,gradual typing,type-and-effect systems},
  pages = {283--295},
  file = {/Users/luigi/work/zotero/storage/DQ423LAM/Bañados Schwerter et al. - 2014 - A Theory of Gradual Effect Systems.pdf}
}

@inproceedings{liSelfinferencingReflectionResolution2014,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Self-Inferencing {{Reflection Resolution}} for {{Java}}},
  isbn = {978-3-662-44201-2 978-3-662-44202-9},
  doi = {10.1007/978-3-662-44202-9_2},
  abstract = {Reflection has always been an obstacle both for sound and for effective under-approximate pointer analysis for Java applications. In pointer analysis tools, reflection is either ignored or handled partially, resulting in missed, important behaviors. In this paper, we present our findings on reflection usage in Java benchmarks and applications. Guided by these findings, we introduce a static reflection analysis, called Elf, by exploiting a self-inferencing property inherent in many reflective calls. Given a reflective call, the basic idea behind Elf is to automatically infer its targets (methods or fields) based on the dynamic types of the arguments of its target calls and the downcasts (if any) on their returned values, if its targets cannot be already obtained from the Class, Method or Field objects on which the reflective call is made. We evaluate Elf against Doop's state-of-the-art reflection analysis performed in the same context-sensitive Andersen's pointer analysis using all 11 DaCapo benchmarks and two applications. Elf can make a disciplined tradeoff among soundness, precision and scalability while also discovering usually more reflective targets. Elf is useful for any pointer analysis, particularly under-approximate techniques deployed for such clients as bug detection, program understanding and speculative compiler optimization.},
  language = {en},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Li, Yue and Tan, Tian and Sui, Yulei and Xue, Jingling},
  month = jul,
  year = {2014},
  pages = {27-53},
  file = {/Users/luigi/work/zotero/storage/EMWPLUKU/Li et al. - 2014 - Self-inferencing Reflection Resolution for Java.pdf;/Users/luigi/work/zotero/storage/4XPRPCYV/978-3-662-44202-9_2.html}
}

@inproceedings{liEffectiveSoundnessGuidedReflection2015,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Effective {{Soundness}}-{{Guided Reflection Analysis}}},
  isbn = {978-3-662-48287-2 978-3-662-48288-9},
  doi = {10.1007/978-3-662-48288-9_10},
  abstract = {We introduce Solar, the first reflection analysis that allows its soundness to be reasoned about when some assumptions are met and produces significantly improved under-approximations otherwise. In both settings, Solar has three novel aspects: (1) lazy heap modeling for reflective allocation sites, (2) collective inference for improving the inferences on related reflective calls, and (3) automatic identification of ``problematic'' reflective calls that may threaten its soundness, precision and scalability, thereby enabling their improvement via lightweight annotations. We evaluate Solar against two state-of-the-art solutions, Doop and Elf, with the three treated as under-approximate reflection analyses, using 11 large Java benchmarks and applications. Solar is significantly more sound while achieving nearly the same precision and running only several-fold more slowly, subject to only 7 annotations in 3 programs.},
  language = {en},
  booktitle = {Static {{Analysis}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Li, Yue and Tan, Tian and Xue, Jingling},
  month = sep,
  year = {2015},
  pages = {162-180},
  file = {/Users/luigi/work/zotero/storage/RRKJYQEP/Li et al. - 2015 - Effective Soundness-Guided Reflection Analysis.pdf;/Users/luigi/work/zotero/storage/27HKMVWG/978-3-662-48288-9_10.html}
}

@phdthesis{livshitsImprovingSoftwareSecurity2006,
  address = {Stanford, California},
  title = {Improving {{Software Security}} with {{Precise Static}} and {{Runtime Analysis}}},
  school = {Stanford University},
  author = {Livshits, Benjamin},
  year = {2006},
  file = {/Users/luigi/work/zotero/storage/SAMQZDXG/thesis.pdf}
}

@inproceedings{hirzelPointerAnalysisPresence2004,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Pointer {{Analysis}} in the {{Presence}} of {{Dynamic Class Loading}}},
  isbn = {978-3-540-22159-3 978-3-540-24851-4},
  doi = {10.1007/978-3-540-24851-4_5},
  abstract = {Many optimizations need precise pointer analyses to be effective. Unfortunately, some Java features, such as dynamic class loading, reflection, and native methods, make pointer analyses difficult to develop. Hence, prior pointer analyses for Java either ignore these features or are overly conservative. This paper presents the first non-trivial pointer analysis that deals with all Java language features. This paper identifies all problems in performing Andersen's pointer analysis for the full Java language, presents solutions to those problems, and uses a full implementation of the solutions in Jikes RVM for validation and performance evaluation. The results from this work should be transferable to other analyses and to other languages.},
  language = {en},
  booktitle = {{{ECOOP}} 2004 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Hirzel, Martin and Diwan, Amer and Hind, Michael},
  month = jun,
  year = {2004},
  pages = {96-122},
  file = {/Users/luigi/work/zotero/storage/XKESHMCS/Hirzel et al. - 2004 - Pointer Analysis in the Presence of Dynamic Class .pdf;/Users/luigi/work/zotero/storage/BXGQ42JQ/10.html}
}

@incollection{jonesReflectionTypes2016,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Reflection}} on {{Types}}},
  isbn = {978-3-319-30935-4 978-3-319-30936-1},
  abstract = {The ability to perform type tests at runtime blurs the line between statically-typed and dynamically-checked languages. Recent developments in Haskell's type system allow even programs that use reflection to themselves be statically typed, using a type-indexed runtime representation of types called \textbackslash{$\mathsf{t}\mathsf{e}\mathsf{x}\mathsf{t}\mathsf{i}\mathsf{t}\lbrace\mathsf{T}\mathsf{y}\mathsf{p}\mathsf{e}\mathsf{R}\mathsf{e}\mathsf{p}\rbrace\backslash$}textit\{TypeRep\}\textbackslash{}textsf \{\textbackslash{}textit\{TypeRep\}\}. As a result we can build dynamic types as an ordinary, statically-typed library, on top of \textbackslash{$\mathsf{t}\mathsf{e}\mathsf{x}\mathsf{t}\mathsf{i}\mathsf{t}\lbrace\mathsf{T}\mathsf{y}\mathsf{p}\mathsf{e}\mathsf{R}\mathsf{e}\mathsf{p}\rbrace\backslash$}textit\{TypeRep\}\textbackslash{}textsf \{\textbackslash{}textit\{TypeRep\}\} in an open-world context.},
  language = {en},
  booktitle = {A {{List}} of {{Successes That Can Change}} the {{World}}},
  publisher = {{Springer, Cham}},
  author = {Jones, Simon Peyton and Weirich, Stephanie and Eisenberg, Richard A. and Vytiniotis, Dimitrios},
  year = {2016},
  pages = {292-317},
  file = {/Users/luigi/work/zotero/storage/YSXWDV9B/Jones et al. - 2016 - A Reflection on Types.pdf;/Users/luigi/work/zotero/storage/M6ZD5RH6/978-3-319-30936-1_16.html},
  doi = {10.1007/978-3-319-30936-1_16}
}

@inproceedings{zhangGeneralDiagnosisStatic2014,
  address = {New York, NY, USA},
  series = {{{POPL}} '14},
  title = {Toward {{General Diagnosis}} of {{Static Errors}}},
  isbn = {978-1-4503-2544-8},
  doi = {10.1145/2535838.2535870},
  abstract = {We introduce a general way to locate programmer mistakes that are detected by static analyses such as type checking. The program analysis is expressed in a constraint language in which mistakes result in unsatisfiable constraints. Given an unsatisfiable system of constraints, both satisfiable and unsatisfiable constraints are analyzed, to identify the program expressions most likely to be the cause of unsatisfiability. The likelihood of different error explanations is evaluated under the assumption that the programmer's code is mostly correct, so the simplest explanations are chosen, following Bayesian principles. For analyses that rely on programmer-stated assumptions, the diagnosis also identifies assumptions likely to have been omitted. The new error diagnosis approach has been implemented for two very different program analyses: type inference in OCaml and information flow checking in Jif. The effectiveness of the approach is evaluated using previously collected programs containing errors. The results show that when compared to existing compilers and other tools, the general technique identifies the location of programmer errors significantly more accurately.},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Zhang, Danfeng and Myers, Andrew C.},
  year = {2014},
  keywords = {type inference,information flow,error diagnosis,static program analysis},
  pages = {569--581},
  file = {/Users/luigi/work/zotero/storage/C9XSKHFR/Zhang and Myers - 2014 - Toward General Diagnosis of Static Errors.pdf}
}

@inproceedings{derooverSOULToolSuite2011,
  address = {New York, NY, USA},
  series = {{{PPPJ}} '11},
  title = {The {{SOUL Tool Suite}} for {{Querying Programs}} in {{Symbiosis}} with {{Eclipse}}},
  isbn = {978-1-4503-0935-6},
  doi = {10.1145/2093157.2093168},
  abstract = {Program queries can answer important software engineering questions that range from "which expressions are cast to this type?" over "does my program attempt to read from a closed file?" to "does my code follow the prescribed design?". In this paper, we present a comprehensive tool suite for querying Java programs. It consists of the logic program query language Soul, the Cava library of predicates for quantifying over an Eclipse workspace and the Eclipse plugin Barista for launching queries and inspecting their results. Barista allows other Eclipse plugins to peruse program query results which is facilitated by the symbiosis of Soul with Java -- setting Soul apart from other program query languages. This symbiosis enables the Cava library to forego the predominant transcription to logic facts of the queried program. Instead, the library queries the actual AST nodes used by Eclipse itself, making it trivial for any Eclipse plugin to find the AST nodes that correspond to a query result. Moreover, such plugins do not have to worry about having queried stale program information. We illustrate the extensibility of our suite by implementing a tool for co-evolving source code and annotations using program queries.},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Principles}} and {{Practice}} of {{Programming}} in {{Java}}},
  publisher = {{ACM}},
  author = {De Roover, Coen and Noguera, Carlos and Kellens, Andy and Jonckers, Vivane},
  year = {2011},
  keywords = {program analysis,integrated development environments,logic programming,program queries,software engineering tools},
  pages = {71--80},
  file = {/Users/luigi/work/zotero/storage/FT47EZRD/De Roover et al. - 2011 - The SOUL Tool Suite for Querying Programs in Symbi.pdf;/Users/luigi/work/zotero/storage/L8RH93JM/De Roover et al. - 2011 - The SOUL tool suite for querying programs in symbi.pdf}
}

@inproceedings{wadlerTheoremsFree1989,
  address = {New York, NY, USA},
  series = {{{FPCA}} '89},
  title = {Theorems for {{Free}}!},
  isbn = {978-0-89791-328-7},
  doi = {10.1145/99370.99404},
  booktitle = {Proceedings of the {{Fourth International Conference}} on {{Functional Programming Languages}} and {{Computer Architecture}}},
  publisher = {{ACM}},
  author = {Wadler, Philip},
  year = {1989},
  pages = {347--359},
  file = {/Users/luigi/work/zotero/storage/TSZ592RZ/Wadler - 1989 - Theorems for Free!.pdf}
}

@article{wadlerCritiqueAbelsonSussman1987,
  title = {A {{Critique}} of {{Abelson}} and {{Sussman}} or {{Why Calculating}} Is {{Better Than Scheming}}},
  volume = {22},
  issn = {0362-1340},
  doi = {10.1145/24697.24706},
  number = {3},
  journal = {SIGPLAN Not.},
  author = {Wadler, P},
  month = mar,
  year = {1987},
  pages = {83--94},
  file = {/Users/luigi/work/zotero/storage/QV5AZYG3/wadler87.pdf}
}

@inproceedings{wadlerComprehendingMonads1990,
  address = {New York, NY, USA},
  series = {{{LFP}} '90},
  title = {Comprehending {{Monads}}},
  isbn = {978-0-89791-368-3},
  doi = {10.1145/91556.91592},
  abstract = {Category theorists invented monads in the 1960's to concisely express certain aspects of universal algebra. Functional programmers invented list comprehensions in the 1970's to concisely express certain programs involving lists. This paper shows how list comprehensions may be generalised to an arbitrary monad, and how the resulting programming feature can concisely express in a pure functional language some programs that manipulate state, handle exceptions, parse text, or invoke continuations. A new solution to the old problem of destructive array update is also presented. No knowledge of category theory is assumed.},
  booktitle = {Proceedings of the 1990 {{ACM Conference}} on {{LISP}} and {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Wadler, Philip},
  year = {1990},
  pages = {61--78},
  file = {/Users/luigi/work/zotero/storage/B39RNP65/Wadler - 1990 - Comprehending Monads.pdf}
}

@inproceedings{stancuComparingPointstoStatic2014,
  address = {New York, NY, USA},
  series = {{{PPPJ}} '14},
  title = {Comparing {{Points}}-to {{Static Analysis}} with {{Runtime Recorded Profiling Data}}},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647524},
  abstract = {We present an empirical study that sheds new light on static analysis results precision by comparing them with runtime collected data. Our motivation is finding additional sources of information that can guide static analysis for increased application performance. This is the first step in formulating an adaptive approach to static analysis that uses dynamic information to increase results precision of frequently executed code. The adaptive approach allows static analysis to (i) scale to real world applications (ii) identify important optimization opportunities. Our preliminary results show that runtime profiling is 10\% more accurate in optimizing frequently executed virtual calls and 73\% more accurate in optimizing frequently executed type checks.},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform}}: {{Virtual Machines}}, {{Languages}}, and {{Tools}}},
  publisher = {{ACM}},
  author = {Stancu, Codru{\c t} and Wimmer, Christian and Brunthaler, Stefan and Larsen, Per and Franz, Michael},
  year = {2014},
  keywords = {static program analysis,feedback directed optimizations,performance optimization,runtime profiling},
  pages = {157--168},
  file = {/Users/luigi/work/zotero/storage/9T6KEK9K/Stancu et al. - 2014 - Comparing Points-to Static Analysis with Runtime R.pdf}
}

@inproceedings{hackettUnderPerformingUnfoldNew2014,
  address = {New York, NY, USA},
  series = {{{IFL}} '13},
  title = {The {{Under}}-{{Performing Unfold}}: {{A New Approach}} to {{Optimising Corecursive Programs}}},
  isbn = {978-1-4503-2988-0},
  shorttitle = {The {{Under}}-{{Performing Unfold}}},
  doi = {10.1145/2620678.2620679},
  abstract = {This paper presents a new approach to optimising corecursive programs by factorisation. In particular, we focus on programs written using the corecursion operator unfold. We use and expand upon the proof techniques of guarded coinduction and unfold fusion, capturing a pattern of generalising coinductive hypotheses by means of abstraction and representation functions. The pattern we observe is simple, has not been observed before, and is widely applicable. We develop a general program factorisation theorem from this pattern, demonstrating its utility with a range of practical examples.},
  booktitle = {Proceedings of the 25th {{Symposium}} on {{Implementation}} and {{Application}} of {{Functional Languages}}},
  publisher = {{ACM}},
  author = {Hackett, Jennifer and Hutton, Graham and Jaskelioff, Mauro},
  year = {2014},
  keywords = {coinduction,factorisation,fusion,unfolds},
  pages = {1:4321--1:4332},
  file = {/Users/luigi/work/zotero/storage/HEAQWJ9Z/Hackett et al. - 2014 - The Under-Performing Unfold A New Approach to Opt.pdf}
}

@techreport{bolingbrokeCallbyneedSupercompilation2013,
  title = {Call-by-Need Supercompilation},
  number = {UCAM-CL-TR-835},
  institution = {{University of Cambridge, Computer Laboratory}},
  author = {Bolingbroke, Maximilian C.},
  year = {2013},
  file = {/Users/luigi/work/zotero/storage/IQYQHP27/Bolingbroke - 2013 - Call-by-need supercompilation.pdf;/Users/luigi/work/zotero/storage/SQ7MSG7G/UCAM-CL-TR-835.html}
}

@inproceedings{atkeyCoqJVMExecutableSpecification2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {{{CoqJVM}}: {{An Executable Specification}} of the {{Java Virtual Machine Using Dependent Types}}},
  isbn = {978-3-540-68084-0 978-3-540-68103-8},
  shorttitle = {{{CoqJVM}}},
  doi = {10.1007/978-3-540-68103-8_2},
  abstract = {We describe an executable specification of the Java Virtual Machine (JVM) within the Coq proof assistant. The principal features of the development are that it is executable, meaning that it can be tested against a real JVM to gain confidence in the correctness of the specification; and that it has been written with heavy use of dependent types, this is both to structure the model in a useful way, and to constrain the model to prevent spurious partiality. We describe the structure of the formalisation and the way in which we have used dependent types.},
  language = {en},
  booktitle = {Types for {{Proofs}} and {{Programs}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Atkey, Robert},
  month = may,
  year = {2007},
  pages = {18-32},
  file = {/Users/luigi/work/zotero/storage/WFLFUDWD/Atkey - 2007 - CoqJVM An Executable Specification of the Java Vi.pdf;/Users/luigi/work/zotero/storage/ZSEV5ZB3/978-3-540-68103-8_2.html}
}

@inproceedings{bettiniGenericTraitsJava2014,
  address = {New York, NY, USA},
  series = {{{PPPJ}} '14},
  title = {Generic {{Traits}} for the {{Java Platform}}},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647518},
  abstract = {A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. Traits were proposed as a mechanism for fine-grained code reuse to overcome many limitations of class-based inheritance. In this paper we present the extended version of Xtraitj, a trait-based programming language that features complete compatibility and interoperability with the Java platform. Xtraitj provides a full Eclipse IDE that aims to support an incremental adoption of traits in existing Java projects. This new version fully supports Java generics: traits can have type parameters just like in Java, so that they can completely interoperate with any existing Java library. Furthermore, Xtraitj now supports Java annotations, so that it can integrate with frameworks like JUnit 4.},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform}}: {{Virtual Machines}}, {{Languages}}, and {{Tools}}},
  publisher = {{ACM}},
  author = {Bettini, Lorenzo and Damiani, Ferruccio},
  year = {2014},
  keywords = {Java,implementation,eclipse,IDE,trait},
  pages = {5--16},
  file = {/Users/luigi/work/zotero/storage/C54M5RVS/Bettini and Damiani - 2014 - Generic Traits for the Java Platform.pdf}
}

@inproceedings{bockischInstancePointcutsSelecting2014,
  address = {New York, NY, USA},
  series = {{{PPPJ}} '14},
  title = {Instance {{Pointcuts}}: {{Selecting Object Sets Based}} on {{Their Usage History}}},
  isbn = {978-1-4503-2926-2},
  shorttitle = {Instance {{Pointcuts}}},
  doi = {10.1145/2647508.2647526},
  abstract = {At runtime, how objects have to be handled frequently depends on how they were used before. But with current programming-language support, selecting objects according to their previous usage patterns often results in scattered and tangled code. In this study, we propose a new kind of pointcut, called Instance Pointcuts, for maintaining sets that contain objects with a specified usage history. Instance pointcut specifications can be reused, by refining their selection criteria, e.g., by restricting the scope of an existing instance pointcut; and they can be composed, e.g., by set operations. These features make instance pointcuts easy to evolve according to new requirements. Our approach improves modularity by providing a fine-grained mechanism and a declarative syntax to create and maintain usage-specific object sets.},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform}}: {{Virtual Machines}}, {{Languages}}, and {{Tools}}},
  publisher = {{ACM}},
  author = {Bockisch, Christoph and Hatun, Kardelen and Aksit, Mehmet},
  year = {2014},
  keywords = {aspect-oriented programming,programming languages,instance pointcuts,objects},
  pages = {27--38},
  file = {/Users/luigi/work/zotero/storage/NFSCKHFJ/Bockisch et al. - 2014 - Instance Pointcuts Selecting Object Sets Based on.pdf}
}

@inproceedings{burtscherStaticLoadClassification2002,
  address = {New York, NY, USA},
  series = {{{PLDI}} '02},
  title = {Static {{Load Classification}} for {{Improving}} the {{Value Predictability}} of {{Data}}-Cache {{Misses}}},
  isbn = {978-1-58113-463-6},
  doi = {10.1145/512529.512556},
  abstract = {While caches are effective at avoiding most main-memory accesses, the few remaining memory references are still expensive. Even one cache miss per one hundred accesses can double a program's execution time. To better tolerate the data-cache miss latency, architects have proposed various speculation mechanisms, including load-value prediction. A load-value predictor guesses the result of a load so that the dependent instructions can immediately proceed without having to wait for the memory access to complete. To use the prediction resources most effectively, speculation should be restricted to loads that are likely to miss in the cache and that are likely to be predicted correctly. Prior work has considered hardware- and profile-based methods to make these decisions. Our work focuses on making these decisions at compile time. We show that a simple compiler classification is effective at separating the loads that should be speculated from the loads that should not. We present results for a number of C and Java programs and demonstrate that our results are consistent across programming languages and across program inputs.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2002 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Burtscher, Martin and Diwan, Amer and Hauswirth, Matthias},
  year = {2002},
  keywords = {load-value prediction,type-based analysis},
  pages = {222--233},
  file = {/Users/luigi/work/zotero/storage/ANPREV2W/Burtscher et al. - 2002 - Static Load Classification for Improving the Value.pdf}
}

@inproceedings{cabriImplementingAgentRoles2014,
  address = {New York, NY, USA},
  series = {{{PPPJ}} '14},
  title = {Implementing {{Agent Roles}} in {{Java}}},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2655183.2655184},
  abstract = {Roles represent a powerful means to enable software agents to act in open environments. They can be implemented in different ways, and in this talk I will show two directions exploiting Java. The former one is quite traditional and exploits composition; the latter one adds the capabilities of roles to agents' classes in form of injected bytecode. I will compare the two approaches trying to generalize the considerations.},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform}}: {{Virtual Machines}}, {{Languages}}, and {{Tools}}},
  publisher = {{ACM}},
  author = {Cabri, Giacomo},
  year = {2014},
  keywords = {Java,agents,roles},
  pages = {1--3},
  file = {/Users/luigi/work/zotero/storage/QAPJVLUX/Cabri - 2014 - Implementing Agent Roles in Java.pdf}
}

@article{chughDependentTypesJavaScript2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1112.4106},
  primaryClass = {cs},
  title = {Dependent {{Types}} for {{JavaScript}}},
  abstract = {We present Dependent JavaScript (DJS), a statically-typed dialect of the imperative, object-oriented, dynamic language. DJS supports the particularly challenging features such as run-time type-tests, higher-order functions, extensible objects, prototype inheritance, and arrays through a combination of nested refinement types, strong updates to the heap, and heap unrolling to precisely track prototype hierarchies. With our implementation of DJS, we demonstrate that the type system is expressive enough to reason about a variety of tricky idioms found in small examples drawn from several sources, including the popular book JavaScript: The Good Parts and the SunSpider benchmark suite.},
  journal = {arXiv:1112.4106 [cs]},
  author = {Chugh, Ravi and Herman, David and Jhala, Ranjit},
  month = dec,
  year = {2011},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/luigi/work/zotero/storage/QRDA9VP9/Chugh et al. - 2011 - Dependent Types for JavaScript.pdf;/Users/luigi/work/zotero/storage/T5HQIQIK/Chugh et al. - 2011 - Dependent Types for JavaScript.pdf;/Users/luigi/work/zotero/storage/WIZRRS9L/Chugh et al. - Dependent types for JavaScript.pdf;/Users/luigi/work/zotero/storage/GU5TLHF4/1112.html}
}

@inproceedings{mitschkeI3QLLanguageintegratedLive2014,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '14},
  title = {{{I3QL}}: {{Language}}-Integrated {{Live Data Views}}},
  isbn = {978-1-4503-2585-1},
  shorttitle = {{{I3QL}}},
  doi = {10.1145/2660193.2660242},
  abstract = {An incremental computation updates its result based on a change to its input, which is often an order of magnitude faster than a recomputation from scratch. In particular, incrementalization can make expensive computations feasible for settings that require short feedback cycles, such as interactive systems, IDEs, or (soft) real-time systems. This paper presents i3QL, a general-purpose programming language for specifying incremental computations. i3QL provides a declarative SQL-like syntax and is based on incremental versions of operators from relational algebra, enriched with support for general recursion. We integrated i3QL into Scala as a library, which enables programmers to use regular Scala code for non-incremental subcomputations of an i3QL query and to easily integrate incremental computations into larger software projects. To improve performance, i3QL optimizes user-defined queries by applying algebraic laws and partial evaluation. We describe the design and implementation of i3QL and its optimizations, demonstrate its applicability, and evaluate its performance.},
  booktitle = {Proceedings of the 2014 {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} \& {{Applications}}},
  publisher = {{ACM}},
  author = {Mitschke, Ralf and Erdweg, Sebastian and K\"ohler, Mirko and Mezini, Mira and Salvaneschi, Guido},
  year = {2014},
  keywords = {incremental computation,reactive programming,scala},
  pages = {417--432},
  file = {/Users/luigi/work/zotero/storage/PQLWILUE/Mitschke et al. - 2014 - I3QL Language-integrated Live Data Views.pdf}
}

@inproceedings{iuQueryllJavaDatabase2006,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Queryll: {{Java Database Queries Through Bytecode Rewriting}}},
  isbn = {978-3-540-49023-4 978-3-540-68256-1},
  shorttitle = {Queryll},
  doi = {10.1007/11925071_11},
  abstract = {When interfacing Java with other systems such as databases, programmers must often program in special interface languages like SQL. Code written in these languages often needs to be embedded in strings where they cannot be error-checked at compile-time, or the Java compiler needs to be altered to directly recognize code written in these languages. We have taken a different approach to adding database query facilities to Java. Bytecode rewriting allows us to add query facilities to Java whose correctness can be checked at compile-time but which don't require any changes to the Java language, Java compilers, Java VMs, or IDEs. Like traditional object-relational mapping tools, we provide Java libraries for accessing individual database entries as objects and navigating among them. To express a query though, a programmer simply writes code that takes a Collection representing the entire contents of a database, iterates over each entry like they would with a normal Collection, and choose the entries of interest. The query is fully valid Java code that, if executed, will read through an entire database and copy entries into Java objects where they will be inspected. Executing queries in this way is obviously inefficient, but we have a special bytecode rewriting tool that can decompile Java class files, identify queries in the bytecode, and rewrite the code to use SQL instead. The rewritten bytecode can then be run using any standard Java VM. Since queries use standard Java set manipulation syntax, Java programmers do not need to learn any new syntax. Our system is able to handle complex queries that make use of all the basic relational operations and exhibits performance comparable to that of hand-written SQL.},
  language = {en},
  booktitle = {Middleware 2006},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Iu, Ming-Yee and Zwaenepoel, Willy},
  month = nov,
  year = {2006},
  pages = {201-218},
  file = {/Users/luigi/work/zotero/storage/MQXSWHN6/Iu and Zwaenepoel - 2006 - Queryll Java Database Queries Through Bytecode Re.pdf;/Users/luigi/work/zotero/storage/3NTEUZ4X/11925071_11.html}
}

@inproceedings{buseSynthesizingAPIUsage2012,
  address = {Piscataway, NJ, USA},
  series = {{{ICSE}} '12},
  title = {Synthesizing {{API Usage Examples}}},
  isbn = {978-1-4673-1067-3},
  abstract = {Key program interfaces are sometimes documented with usage examples: concrete code snippets that characterize common use cases for a particular data type. While such documentation is known to be of great utility, it is burdensome to create and can be incomplete, out of date, or not representative of actual practice. We present an automatic technique for mining and synthesizing succinct and representative human-readable documentation of program interfaces. Our algorithm is based on a combination of path sensitive dataflow analysis, clustering, and pattern abstraction. It produces output in the form of well-typed program snippets which document initialization, method calls, assignments, looping constructs, and exception handling. In a human study involving over 150 participants, 82\% of our generated examples were found to be at least as good at human-written instances and 94\% were strictly preferred to state of the art code search.},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Software Engineering}}},
  publisher = {{IEEE Press}},
  author = {Buse, Raymond P. L. and Weimer, Westley},
  year = {2012},
  pages = {782--792},
  file = {/Users/luigi/work/zotero/storage/NU9P3XHT/Buse and Weimer - 2012 - Synthesizing API Usage Examples.pdf}
}

@misc{ContextualCodeCompletion,
  title = {Contextual {{Code Completion Using Machine Learning}} - {{Semantic Scholar}}},
  abstract = {Large projects such as kernels, drivers and libraries follow a code style, and have recurring patterns. In this project, we explore learning based code recommendation, to use the project context and give meaningful suggestions. Using word vectors to model code tokens, and neural network based learning techniques, we are able to capture interesting patterns, and predict code that that cannot be predicted by a simple grammar and syntax based approach as in conventional IDEs. We achieve a total prediction accuracy of 56.0\% on Linux kernel, a C project, and 40.6\% on Twisted, a Python networking library.},
  howpublished = {/paper/Contextual-Code-Completion-Using-Machine-Learning-Das-Shah/3d426d5d686db3dfa5cad88dbbf0bcf443828cf6},
  file = {/Users/luigi/work/zotero/storage/U68IUF7I/6d5d686db3dfa5cad88dbbf0bcf443828cf6.pdf;/Users/luigi/work/zotero/storage/SHN4R6TU/3d426d5d686db3dfa5cad88dbbf0bcf443828cf6.html}
}

@misc{AutomaticCodeCompletion,
  title = {Automatic {{Code Completion}} - {{Semantic Scholar}}},
  abstract = {Code completion software is an important tool for many developers, but it traditionally fails to model any long term program dependencies such as scoped variables, instead settling for suggestions based on static libraries. In this paper we present a deep learning approach to code completion for non-terminals (program structural components) and terminals (program text) that takes advantage of running dependencies to improve predictions. We develop an LSTM model and augment it with several approaches to Attention in order to better capture the relative value of the input, hidden state, and context. After evaluating on a large dataset of JavaScript programs, we demonstrate that our Gated LSTM model significantly improves over a Vanilla LSTM baseline, achieving an accuracy of 77\% on the non-terminal prediction problem and 46\% accuracy on the terminal prediction problem.},
  howpublished = {/paper/Automatic-Code-Completion-Ginzberg-Kostas/acaa0541c7ba79049a6e39c791b9da4b740b7f4a},
  file = {/Users/luigi/work/zotero/storage/IAF4JSIF/0541c7ba79049a6e39c791b9da4b740b7f4a.pdf;/Users/luigi/work/zotero/storage/7TZPBTCW/acaa0541c7ba79049a6e39c791b9da4b740b7f4a.html}
}

@inproceedings{vazouAbstractRefinementTypes2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Abstract {{Refinement Types}}},
  isbn = {978-3-642-37035-9 978-3-642-37036-6},
  doi = {10.1007/978-3-642-37036-6_13},
  abstract = {We present abstract refinement types which enable quantification over the refinements of data- and function-types. Our key insight is that we can avail of quantification while preserving SMT-based decidability, simply by encoding refinement parameters as uninterpreted propositions within the refinement logic. We illustrate how this mechanism yields a variety of sophisticated means for reasoning about programs, including: parametric refinements for reasoning with type classes, index-dependent refinements for reasoning about key-value maps, recursive refinements for reasoning about recursive data types, and inductive refinements for reasoning about higher-order traversal routines. We have implemented our approach in a refinement type checker for Haskell and present experiments using our tool to verify correctness invariants of various programs.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Vazou, Niki and Rondon, Patrick M. and Jhala, Ranjit},
  month = mar,
  year = {2013},
  pages = {209-228},
  file = {/Users/luigi/work/zotero/storage/UKL7BZP6/Vazou et al. - 2013 - Abstract Refinement Types.pdf;/Users/luigi/work/zotero/storage/3L5UD35L/978-3-642-37036-6_13.html}
}

@article{casoEnablednessbasedProgramAbstractions2013,
  title = {Enabledness-Based {{Program Abstractions}} for {{Behavior Validation}}},
  volume = {22},
  issn = {1049-331X},
  doi = {10.1145/2491509.2491519},
  abstract = {Code artifacts that have nontrivial requirements with respect to the ordering in which their methods or procedures ought to be called are common and appear, for instance, in the form of API implementations and objects. This work addresses the problem of validating if API implementations provide their intended behavior when descriptions of this behavior are informal, partial, or nonexistent. The proposed approach addresses this problem by generating abstract behavior models which resemble typestates. These models are statically computed and encode all admissible sequences of method calls. The level of abstraction at which such models are constructed has shown to be useful for validating code artifacts and identifying findings which led to the discovery of bugs, adjustment of the requirements expected by the engineer to the requirements implicit in the code, and the improvement of available documentation.},
  number = {3},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  author = {Caso, Guido De and Braberman, Victor and Garbervetsky, Diego and Uchitel, Sebastian},
  month = jul,
  year = {2013},
  keywords = {enabledness abstractions,Source-code validation},
  pages = {25:1--25:46},
  file = {/Users/luigi/work/zotero/storage/NLDUHA8T/Caso et al. - 2013 - Enabledness-based Program Abstractions for Behavio.pdf;/Users/luigi/work/zotero/storage/PDG3GATI/Caso et al. - 2013 - Enabledness-based program abstractions for behavio.pdf}
}

@inproceedings{wadlerHowMakeAdhoc1989,
  address = {New York, NY, USA},
  series = {{{POPL}} '89},
  title = {How to {{Make Ad}}-Hoc {{Polymorphism Less Ad Hoc}}},
  isbn = {978-0-89791-294-5},
  doi = {10.1145/75277.75283},
  abstract = {This paper presents type classes, a new approach to ad-hoc polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the ``eqtype variables'' of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in object-oriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules.},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Wadler, P. and Blott, S.},
  year = {1989},
  pages = {60--76},
  file = {/Users/luigi/work/zotero/storage/KMUC358R/Wadler and Blott - 1989 - How to Make Ad-hoc Polymorphism Less Ad Hoc.pdf}
}

@article{sussmanSCHEMEInterpreterExtended1975,
  title = {{{SCHEME}}: {{An Interpreter}} for {{Extended Lambda Calculus}}},
  shorttitle = {{{SCHEME}}},
  abstract = {Inspired by ACTORS [Greif and Hewitt] [Smith and Hewitt], we have implemented an interpreter for a LISP-like language, SCHEME, based on the lambda calculus [Church], but extended for side effects, multiprocessing, and process synchronization. The purpose of this implementation is tutorial. We wish to: (1) alleviate the confusion caused by Micro-PLANNER, CONNIVER, etc. by clarifying the embedding of non-recursive control structures in a recursive host language like LISP. (2) explain how to use these control structures, independent of such issues as pattern matching and data base manipulation. (3) have a simple concrete experimental domain for certain issues of programming semantics and style.},
  language = {en\_US},
  author = {Sussman, Gerald J. and Steele, Guy L.},
  month = dec,
  year = {1975},
  file = {/Users/luigi/work/zotero/storage/PJ7NT6PG/AIM-349.pdf;/Users/luigi/work/zotero/storage/J5WJR4IZ/5794.html}
}

@article{sestoftDerivingLazyAbstract1997,
  title = {Deriving a {{Lazy Abstract Machine}}},
  volume = {7},
  issn = {0956-7968},
  doi = {10.1017/S0956796897002712},
  abstract = {We derive a simple abstract machine for lazy evaluation of the lambda calculus, starting from Launchbury's natural semantics. Lazy evaluation here means non-strict evaluation with sharing of argument evaluation, i.e. call-by-need. The machine we derive is a lazy version of Krivine's abstract machine, which was originally designed for call-by-name evaluation. We extend it with datatype constructors and base values, so the final machine implements all dynamic aspects of a lazy functional language.},
  number = {3},
  journal = {J. Funct. Program.},
  author = {Sestoft, Peter},
  month = may,
  year = {1997},
  pages = {231--264},
  file = {/Users/luigi/work/zotero/storage/QXX2BSXD/amlazy5.pdf}
}

@inproceedings{whaleyUsingDatalogBinary2005,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Using {{Datalog}} with {{Binary Decision Diagrams}} for {{Program Analysis}}},
  isbn = {978-3-540-29735-2 978-3-540-32247-4},
  doi = {10.1007/11575467_8},
  abstract = {Many problems in program analysis can be expressed naturally and concisely in a declarative language like Datalog. This makes it easy to specify new analyses or extend or compose existing analyses. However, previous implementations of declarative languages perform poorly compared with traditional implementations. This paper describes bddbddb, a BDD-Based Deductive DataBase, which implements the declarative language Datalog with stratified negation, totally-ordered finite domains and comparison operators. bddbddb uses binary decision diagrams (BDDs) to efficiently represent large relations. BDD operations take time proportional to the size of the data structure, not the number of tuples in a relation, which leads to fast execution times. bddbddb is an effective tool for implementing a large class of program analyses. We show that a context-insensitive points-to analysis implemented with bddbddb is about twice as fast as a carefully hand-tuned version. The use of BDDs also allows us to solve heretofore unsolved problems, like context-sensitive pointer analysis for large programs.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Whaley, John and Avots, Dzintars and Carbin, Michael and Lam, Monica S.},
  month = nov,
  year = {2005},
  pages = {97-118},
  file = {/Users/luigi/work/zotero/storage/FJMFWB6Z/Whaley et al. - 2005 - Using Datalog with Binary Decision Diagrams for Pr.pdf;/Users/luigi/work/zotero/storage/F3F7AKAC/11575467_8.html}
}

@article{mcbrideApplicativeProgrammingEffects2008,
  title = {Applicative {{Programming}} with {{Effects}}},
  volume = {18},
  doi = {10.1017/S0956796807006326},
  abstract = {In this article, we introduce Applicative functors \textendash{} an abstract characterisation of an applicative style of effectful programming, weaker than Monads and hence more widespread. Indeed, it is the ubiquity of this programming pattern that drew us to the abstraction. We retrace our steps in this article, introducing the applicative pattern by diverse examples, then abstracting it to define the Applicative type class and introducing a bracket notation that interprets the normal application syntax in the idiom of an Applicative functor. Furthermore, we develop the properties of applicative functors and the generic operations they support. We close by identifying the categorical structure of applicative functors and examining their relationship both with Monads and with Arrow.},
  journal = {Journal of Functional Programming},
  author = {McBride, Conor and Paterson, Ross},
  month = jan,
  year = {2008},
  pages = {1-13},
  file = {/Users/luigi/work/zotero/storage/ITMTG5S7/Idiom.pdf}
}

@inproceedings{steuwerGeneratingPerformancePortable2015,
  address = {New York, NY, USA},
  series = {{{ICFP}} 2015},
  title = {Generating {{Performance Portable Code Using Rewrite Rules}}: {{From High}}-Level {{Functional Expressions}} to {{High}}-Performance {{OpenCL Code}}},
  isbn = {978-1-4503-3669-7},
  shorttitle = {Generating {{Performance Portable Code Using Rewrite Rules}}},
  doi = {10.1145/2784731.2784754},
  abstract = {Computers have become increasingly complex with the emergence of heterogeneous hardware combining multicore CPUs and GPUs. These parallel systems exhibit tremendous computational power at the cost of increased programming effort resulting in a tension between performance and code portability. Typically, code is either tuned in a low-level imperative language using hardware-specific optimizations to achieve maximum performance or is written in a high-level, possibly functional, language to achieve portability at the expense of performance. We propose a novel approach aiming to combine high-level programming, code portability, and high-performance. Starting from a high-level functional expression we apply a simple set of rewrite rules to transform it into a low-level functional representation, close to the OpenCL programming model, from which OpenCL code is generated. Our rewrite rules define a space of possible implementations which we automatically explore to generate hardware-specific OpenCL implementations. We formalize our system with a core dependently-typed lambda-calculus along with a denotational semantics which we use to prove the correctness of the rewrite rules. We test our design in practice by implementing a compiler which generates high performance imperative OpenCL code. Our experiments show that we can automatically derive hardware-specific implementations from simple functional high-level algorithmic expressions offering performance on a par with highly tuned code for multicore CPUs and GPUs written by experts.},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Steuwer, Michel and Fensch, Christian and Lindley, Sam and Dubach, Christophe},
  year = {2015},
  keywords = {GPU,code generation,OpenCL,Algorithmic patterns,performance portability,rewrite rules},
  pages = {205--217},
  file = {/Users/luigi/work/zotero/storage/FFJASUBM/Steuwer et al. - 2015 - Generating Performance Portable Code Using Rewrite.pdf}
}

@inproceedings{pradelAutomaticGenerationObject2009,
  address = {Washington, DC, USA},
  series = {{{ASE}} '09},
  title = {Automatic {{Generation}} of {{Object Usage Specifications}} from {{Large Method Traces}}},
  isbn = {978-0-7695-3891-4},
  doi = {10.1109/ASE.2009.60},
  abstract = {Formal specifications are used to identify programming errors, verify the correctness of programs, and as documentation. Unfortunately, producing them is error-prone and time-consuming, so they are rarely used in practice. Inferring specifications from a running application is a promising solution. However, to be practical, such an approach requires special techniques to treat large amounts of runtime data. We present a scalable dynamic analysis that infers specifications of correct method call sequences on multiple related objects. It preprocesses method traces to identify small sets of related objects and method calls which can be analyzed separately. We implemented our approach and applied the analysis to eleven real-world applications and more than 240 million runtime events. The experiments show the scalability of our approach. Moreover, the generated specifications describe correct and typical behavior, and match existing API usage documentation.},
  booktitle = {Proceedings of the 2009 {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  publisher = {{IEEE Computer Society}},
  author = {Pradel, Michael and Gross, Thomas R.},
  year = {2009},
  keywords = {formal specifications,dynamic analysis,Specification inference,temporal properties},
  pages = {371--382},
  file = {/Users/luigi/work/zotero/storage/GCIIW2W2/Pradel and Gross - 2009 - Automatic Generation of Object Usage Specification.pdf;/Users/luigi/work/zotero/storage/MJJ2NJF7/Pradel and Gross - 2009 - Automatic Generation of Object Usage Specification.pdf}
}

@inproceedings{tabareauAspectualSessionTypes2014,
  title = {Aspectual {{Session Types}}},
  doi = {10.1145/2577080.2577085},
  abstract = {Multiparty session types allow the definition of distributed processes with strong communication safety properties. A global type is a choreographic specification of the interactions between peers, which is then projected locally in each peer. Well-typed processes behave accordingly to the global protocol specification. Multiparty session types are however monolithic entities that are not amenable to modular extensions. Also, session types impose conservative requirements to prevent any race condition, which prohibit the uni- form application of extensions at different points in a protocol. In this paper, we describe a means to support modular extensions with aspectual session types, a static pointcut/advice mechanism at the session type level. To support the modular definition of crosscut- ting concerns, we augment the expressivity of session types to al- low harmless race conditions. We formally prove that well-formed aspectual session types entail communication safety. As a result, aspectual session types make multiparty session types more flexible, modular, and extensible.},
  language = {en},
  booktitle = {Modularity - 13th {{International Conference}} on {{Modularity}}},
  author = {Tabareau, Nicolas and S\"udholt, Mario and Tanter, \'Eric},
  month = apr,
  year = {2014},
  file = {/Users/luigi/work/zotero/storage/4FWIAJI6/Tabareau et al. - 2014 - Aspectual session types.pdf;/Users/luigi/work/zotero/storage/ABIG8C2I/Tabareau et al. - 2014 - Aspectual Session Types.pdf;/Users/luigi/work/zotero/storage/5WCTI5GS/en.html}
}

@article{anandOrchestratedSurveyMethodologies2013,
  title = {An {{Orchestrated Survey}} of {{Methodologies}} for {{Automated Software Test Case Generation}}},
  volume = {86},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2013.02.061},
  abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
  number = {8},
  journal = {J. Syst. Softw.},
  author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and Mcminn, Phil},
  month = aug,
  year = {2013},
  keywords = {Software testing,Adaptive random testing,Combinatorial testing,Model-based testing,Orchestrated survey,Search-based software testing,Symbolic execution,Test automation,Test case generation},
  pages = {1978--2001},
  file = {/Users/luigi/work/zotero/storage/JTBIH8N9/ASTJSS.pdf}
}

@inproceedings{chakravartyAssociatedTypeSynonyms2005,
  address = {New York, NY, USA},
  series = {{{ICFP}} '05},
  title = {Associated {{Type Synonyms}}},
  isbn = {978-1-59593-064-4},
  doi = {10.1145/1086365.1086397},
  abstract = {Haskell programmers often use a multi-parameter type class in which one or more type parameters are functionally dependent on the first. Although such functional dependencies have proved quite popular in practice, they express the programmer's intent somewhat indirectly. Developing earlier work on associated data types, we propose to add functionally dependent types as type synonyms to type-class bodies. These associated type synonyms constitute an interesting new alternative to explicit functional dependencies.},
  booktitle = {Proceedings of the {{Tenth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Chakravarty, Manuel M. T. and Keller, Gabriele and Jones, Simon Peyton},
  year = {2005},
  keywords = {type inference,associated types,generic programming,type classes,type functions},
  pages = {241--253},
  file = {/Users/luigi/work/zotero/storage/6VUM9SPU/Chakravarty et al. - 2005 - Associated Type Synonyms.pdf}
}

@article{baconKavaJavaDialect2003,
  title = {Kava: A {{Java}} Dialect with a Uniform Object Model for Lightweight Classes},
  volume = {15},
  issn = {1532-0634},
  shorttitle = {Kava},
  doi = {10.1002/cpe.653},
  abstract = {Object-oriented programming languages have always distinguished between `primitive' and `user-defined' data types, and in the case of languages like C++ and Java the primitives are not even treated as objects, further fragmenting the programming model. The distinction is especially problematic when a particular programming community requires primitive-level support for a new data type, as for complex, intervals, fixed-point numbers, and so on. We present Kava, a design for a backward-compatible version of Java that solves the problem of programmable lightweight objects in a much more aggressive and uniform manner than previous proposals. In Kava, there are no primitive types; instead, object-oriented programming is provided down to the level of single bits, and types such as int can be explicitly programmed within the language. While the language maintains a uniform object reference semantics, efficiency is obtained by making heavy use of unboxing and semantic expansion. We describe Kava as a dialect of the Java language, show how it can be used to define various primitive types, describe how it can be translated into Java, and compare it to other approaches to lightweight objects. Copyright \textcopyright{} 2003 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {3-5},
  journal = {Concurrency and Computation: Practice and Experience},
  author = {Bacon, David F.},
  month = mar,
  year = {2003},
  keywords = {Java,lightweight classes,object inlining,object models,unboxing},
  pages = {185-206},
  file = {/Users/luigi/work/zotero/storage/9DLREEXU/Bacon - 2003 - Kava a Java dialect with a uniform object model f.pdf;/Users/luigi/work/zotero/storage/SCQKGLMI/abstract.html}
}

@inproceedings{mitropoulosVulnerabilityDatasetLarge2014,
  title = {The {{Vulnerability Dataset}} of a {{Large Software Ecosystem}}},
  doi = {10.1109/BADGERS.2014.8},
  abstract = {Security bugs are critical programming errors that can lead to serious vulnerabilities in software. Examining their behaviour and characteristics within a software ecosystem can provide the research community with data regarding their evolution, persistence and others. We present a dataset that we produced by applying static analysis to the Maven Central Repository (approximately 265GB of data) in order to detect potential security bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics' results that FindBugs reports for every project version (a JAR) included in the ecosystem. For every version in our data repository, we also store specific metadata, such as the JAR's size, its dependencies and others. Our dataset can be used to produce interesting research results involving security bugs, as we show in specific examples.},
  booktitle = {2014 {{Third International Workshop}} on {{Building Analysis Datasets}} and {{Gathering Experience Returns}} for {{Security}} ({{BADGERS}})},
  author = {Mitropoulos, D. and Gousios, G. and Papadopoulos, P. and Karakoidas, V. and Louridas, P. and Spinellis, D.},
  month = sep,
  year = {2014},
  keywords = {Java,Java bytecode,Software,program debugging,Security,Static Analysis,Computer bugs,Ecosystems,static analysis,Software Evolution,security of data,Correlation,critical programming errors,data repository,FindBugs,large software ecosystem,Maven Central Repository,Maven Repository,metadata,Metadata,research community,security bugs,Security Bugs,Software Ecosystem,Software Security,vulnerability dataset},
  pages = {69-74},
  file = {/Users/luigi/work/zotero/storage/2GXECKR7/Mitropoulos et al. - 2014 - The Vulnerability Dataset of a Large Software Ecos.pdf;/Users/luigi/work/zotero/storage/ZJPC4GDR/7446036.html}
}

@article{barendregtIntroductionGeneralizedType1991,
  title = {Introduction to Generalized Type Systems},
  volume = {1},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796800020025},
  abstract = {Abstract
Programming languages often come with type systems. Some of these are simple, others are sophisticated. As a stylistic representation of types in programming languages several versions of typed lambda calculus are studied. During the last 20 years many of these systems have appeared, so there is some need of classification. Working towards a taxonomy, Barendregt (1991) gives a fine-structure of the theory of constructions (Coquand and Huet 1988) in the form of a canonical cube of eight type systems ordered by inclusion. Berardi (1988) and Terlouw (1988) have independently generalized the method of constructing systems in the {$\lambda$}-cube. Moreover, Berardi (1988, 1990) showed that the generalized type systems are flexible enough to describe many logical systems. In that way the well-known propositions-as-types interpretation obtains a nice canonical form.},
  number = {2},
  journal = {Journal of Functional Programming},
  author = {Barendregt, Henk},
  month = apr,
  year = {1991},
  pages = {125-154},
  file = {/Users/luigi/work/zotero/storage/I5YK4QH6/barendregt.pdf;/Users/luigi/work/zotero/storage/RAKPCYFW/869991BA6A99180BF96A616894C6D710.html}
}

@inproceedings{bastoulCodeGenerationPolyhedral2004,
  address = {Washington, DC, USA},
  series = {{{PACT}} '04},
  title = {Code {{Generation}} in the {{Polyhedral Model Is Easier Than You Think}}},
  isbn = {978-0-7695-2229-6},
  doi = {10.1109/PACT.2004.11},
  abstract = {Many advances in automatic parallelization and optimization have been achieved through the polyhedral model. It has been extensively shown that this computational model provides convenient abstractions to reason about and apply program transformations. Nevertheless, the complexity of code generation has long been a deterrent for using polyhedral representation in optimizing compilers. First, code generators have a hard time coping with generated code size and control overhead that may spoil theoretical benefits achieved by the transformations. Second, this step is usually time consuming, hampering the integration of the polyhedral framework in production compilers or feedback-directed, iterative optimization schemes. Moreover, current code generation algorithms only cover a restrictive set of possible transformation functions. This paper discusses a general transformation framework able to deal with non-unimodular, non-invertible, non-integral or even non-uniform functions. It presents several improvements to a state-of-the-art code generation algorithm. Two directions are explored: generated code size and code generator efficiency. Experimental evidence proves the ability of the improved method to handle real-life problems.},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  publisher = {{IEEE Computer Society}},
  author = {Bastoul, Cedric},
  year = {2004},
  pages = {7--16},
  file = {/Users/luigi/work/zotero/storage/BKIYUL87/Bastoul - 2004 - Code Generation in the Polyhedral Model Is Easier .pdf}
}

@inproceedings{gueheneucNoJavaCaffeine2002,
  title = {No {{Java}} without Caffeine: {{A}} Tool for Dynamic Analysis of {{Java}} Programs},
  shorttitle = {No {{Java}} without Caffeine},
  doi = {10.1109/ASE.2002.1115000},
  abstract = {To understand the behavior of a program, a maintainer reads some code, asks a question about this code, conjectures an answer, and searches the code and the documentation for confirmation of her conjecture. However, the confirmation of the conjecture can be error-prone and time-consuming because the maintainer has only static information at her disposal. She would benefit from dynamic information. In this paper, we present Caffeine, an assistant that helps the maintainer in checking her conjecture about the behavior of a Java program. Our assistant is a dynamic analysis tool that uses the Java platform debug architecture to generate a trace, i.e., an execution history, and a Prolog engine to perform queries over the trace. We present a usage scenario based on the n-queens problem, and two real-life examples based on the Singleton design pattern and on the composition relationship.},
  booktitle = {Proceedings 17th {{IEEE International Conference}} on {{Automated Software Engineering}},},
  author = {Gueheneuc, Y. G. and Douence, R. and Jussien, N.},
  year = {2002},
  keywords = {Java,reverse engineering,Software maintenance,History,Performance analysis,dynamic analysis,Electronic mail,program debugging,software tools,Documentation,Law,Caffeine,Engines,execution history,Java platform debug architecture,Java programs,Legal factors,n-queens problem,program behaviour understanding,Prolog engine,real-life examples,Singleton design pattern,Software algorithms,static information,usage scenario},
  pages = {117-126},
  file = {/Users/luigi/work/zotero/storage/UJT5THED/Gueheneuc et al. - 2002 - No Java without caffeine A tool for dynamic analy.pdf;/Users/luigi/work/zotero/storage/YT29I2A5/1115000.html}
}

@inproceedings{teytonMiningLibraryMigration2012,
  title = {Mining {{Library Migration Graphs}}},
  doi = {10.1109/WCRE.2012.38},
  abstract = {Software systems intensively depend on external libraries, chosen at conception time. However, relevance of any library irremediably changes during projects and/or library life cycle. As a consequence, projects developers must periodically reconsider the libraries they depend on, and must think about library migration. When they want to migrate their libraries, they then have to identify candidate libraries that offer similar facilities and thus can substitute to each other. They also have to compare candidates to choose the one that best fits their needs. Finding a relevant library replacement is a well known tedious and time-consuming task. In this paper, we propose an approach that identifies sets of similar libraries and that produces what we call library migration graphs that show how existing projects have performed migrations among them. These graphs, constructed from the observation of a large number of software projects, ease the discovery and selection of library replacements.},
  booktitle = {2012 19th {{Working Conference}} on {{Reverse Engineering}}},
  author = {Teyton, C. and Falleri, J. R. and Blanc, X.},
  month = oct,
  year = {2012},
  keywords = {Data mining,Libraries,software maintenance,software libraries,data mining,Software,software evolution,Google,Software algorithms,dependencies management,external software library,graph theory,library life cycle,library migration graph mining,library replacement,Manuals,project life cycle,Search engines,software project,software system},
  pages = {289-298},
  file = {/Users/luigi/work/zotero/storage/4EA6KSEF/Teyton et al. - 2012 - Mining Library Migration Graphs.pdf;/Users/luigi/work/zotero/storage/NB4CD2W3/Teyton et al. - 2012 - Mining Library Migration Graphs.pdf;/Users/luigi/work/zotero/storage/3UIEKISZ/6385124.html}
}

@inproceedings{xuMiningTestOracles2013,
  title = {Mining {{Test Oracles}} for {{Test Inputs Generated}} from {{Java Bytecode}}},
  doi = {10.1109/COMPSAC.2013.8},
  abstract = {Search-based test generation can automatically produce a large volume of test inputs. However, it is difficult to define the test oracle for each of the test inputs. This paper presents a mining approach to building a decision tree model according to the test inputs generated from Java bytecode. It converts Java bytecode into the Jimple representation, extracts predicates from the control flow graph of the Jimple code, and uses these predicates as attributes for organizing training data to build a decision tree. Our case studies show that the mining approach generated accurate behavioral models and that test oracles derived from these models were able to kill 94.67\% of the mutants with injected faults.},
  booktitle = {2013 {{IEEE}} 37th {{Annual Computer Software}} and {{Applications Conference}}},
  author = {Xu, W. and Ding, T. and Wang, H. and Xu, D.},
  month = jul,
  year = {2013},
  keywords = {Java,Data mining,Software testing,Buildings,data mining,Java bytecode,mining,program testing,Accuracy,control flow graph,decision tree,decision tree model,decision trees,Decision trees,Input variables,Jimple,Jimple representation,mining approach,search-based test generation,test oracle,test oracles,Training data},
  pages = {27-32},
  file = {/Users/luigi/work/zotero/storage/ALL76Y3V/Xu et al. - 2013 - Mining Test Oracles for Test Inputs Generated from.pdf;/Users/luigi/work/zotero/storage/P82S7F83/Xu et al. - 2013 - Mining Test Oracles for Test Inputs Generated from.pdf;/Users/luigi/work/zotero/storage/FS48L778/6649795.html}
}

@inproceedings{bergerHoardScalableMemory2000,
  address = {New York, NY, USA},
  series = {{{ASPLOS IX}}},
  title = {Hoard: {{A Scalable Memory Allocator}} for {{Multithreaded Applications}}},
  isbn = {978-1-58113-317-2},
  shorttitle = {Hoard},
  doi = {10.1145/378993.379232},
  abstract = {Parallel, multithreaded C and C++ programs such as web servers, database managers, news servers, and scientific applications are becoming increasingly prevalent. For these applications, the memory allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. Previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. Worse, many allocators exhibit a dramatic increase in memory consumption when confronted with a producer-consumer pattern of object allocation and freeing. This increase in memory consumption can range from a factor of P (the number of processors) to unbounded memory consumption.This paper introduces Hoard, a fast, highly scalable allocator that largely avoids false sharing and is memory efficient. Hoard is the first allocator to simultaneously solve the above problems. Hoard combines one global heap and per-processor heaps with a novel discipline that provably bounds memory consumption and has very low synchronization costs in the common case. Our results on eleven programs demonstrate that Hoard yields low average fragmentation and improves overall program performance over the standard Solaris allocator by up to a factor of 60 on 14 processors, and up to a factor of 18 over the next best allocator we tested.},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  publisher = {{ACM}},
  author = {Berger, Emery D. and McKinley, Kathryn S. and Blumofe, Robert D. and Wilson, Paul R.},
  year = {2000},
  pages = {117--128},
  file = {/Users/luigi/work/zotero/storage/JW3GKEPN/Berger et al. - 2000 - Hoard A Scalable Memory Allocator for Multithread.pdf}
}

@inproceedings{schultzDataMiningMethods2001,
  title = {Data Mining Methods for Detection of New Malicious Executables},
  doi = {10.1109/SECPRI.2001.924286},
  abstract = {A serious security threat today is malicious executables, especially new, unseen malicious executables often arriving as email attachments. These new malicious executables are created at the rate of thousands every year and pose a serious security threat. Current anti-virus systems attempt to detect these new malicious programs with heuristics generated by hand. This approach is costly and oftentimes ineffective. We present a data mining framework that detects new, previously unseen malicious executables accurately and automatically. The data mining framework automatically found patterns in our data set and used these patterns to detect a set of new malicious binaries. Comparing our detection methods with a traditional signature-based method, our method more than doubles the current detection rates for new malicious executables},
  booktitle = {Proceedings 2001 {{IEEE Symposium}} on {{Security}} and {{Privacy}}. {{S P}} 2001},
  author = {Schultz, M. G. and Eskin, E. and Zadok, F. and Stolfo, S. J.},
  year = {2001},
  keywords = {Protection,Data mining,Computer science,data mining,Testing,security of data,Training data,anti-virus systems,Computer security,data security,Data security,data set,electronic mail,email attachments,Face detection,heuristics,Information security,malicious binaries,malicious executable detection,pattern recognition,Permission,security threat,signature-based method},
  pages = {38-49},
  file = {/Users/luigi/work/zotero/storage/RYHDILPM/Schultz et al. - 2001 - Data mining methods for detection of new malicious.pdf;/Users/luigi/work/zotero/storage/NLVSTM8V/924286.html}
}

@incollection{sorensenIntroductionSupercompilation1999,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Introduction to {{Supercompilation}}},
  isbn = {978-3-540-66710-0 978-3-540-47018-2},
  abstract = {This paper gives an introduction to Turchin's supercompiler, a program transformer for functional programs which performs optimizations beyond partial evaluation and deforestation. More precisely, the paper presents positive supercompilation.},
  language = {en},
  booktitle = {Partial {{Evaluation}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {S\o{}rensen, Morten Heine B. and Gl\"uck, Robert},
  year = {1999},
  pages = {246-270},
  file = {/Users/luigi/work/zotero/storage/AG6L54ZU/Sørensen and Glück - 1999 - Introduction to Supercompilation.pdf;/Users/luigi/work/zotero/storage/JKLKDM28/3-540-47018-2_10.html},
  doi = {10.1007/3-540-47018-2_10}
}

@inproceedings{zhongMAPOMiningRecommending2009,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {{{MAPO}}: {{Mining}} and {{Recommending API Usage Patterns}}},
  isbn = {978-3-642-03012-3 978-3-642-03013-0},
  shorttitle = {{{MAPO}}},
  doi = {10.1007/978-3-642-03013-0_15},
  abstract = {To improve software productivity, when constructing new software systems, programmers often reuse existing libraries or frameworks by invoking methods provided in their APIs. Those API methods, however, are often complex and not well documented. To get familiar with how those API methods are used, programmers often exploit a source code search tool to search for code snippets that use the API methods of interest. However, the returned code snippets are often large in number, and the huge number of snippets places a barrier for programmers to locate useful ones. In order to help programmers overcome this barrier, we have developed an API usage mining framework and its supporting tool called MAPO (Mining API usage Pattern from Open source repositories) for mining API usage patterns automatically. A mined pattern describes that in a certain usage scenario, some API methods are frequently called together and their usages follow some sequential rules. MAPO further recommends the mined API usage patterns and their associated code snippets upon programmers' requests. Our experimental results show that with these patterns MAPO helps programmers locate useful code snippets more effectively than two state-of-the-art code search tools. To investigate whether MAPO can assist programmers in programming tasks, we further conducted an empirical study. The results show that using MAPO, programmers produce code with fewer bugs when facing relatively complex API usages, comparing with using the two state-of-the-art code search tools.},
  language = {en},
  booktitle = {{{ECOOP}} 2009 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Zhong, Hao and Xie, Tao and Zhang, Lu and Pei, Jian and Mei, Hong},
  month = jul,
  year = {2009},
  pages = {318-343},
  file = {/Users/luigi/work/zotero/storage/X4SELQIY/Zhong et al. - 2009 - MAPO Mining and Recommending API Usage Patterns.pdf;/Users/luigi/work/zotero/storage/7WG7CTBD/978-3-642-03013-0_15.html}
}

@article{hafizGrowingLanguageEmpirical2016,
  title = {Growing a Language: {{An}} Empirical Study on How (and Why) Developers Use Some Recently-Introduced and/or Recently-Evolving {{JavaScript}} Features},
  volume = {121},
  issn = {0164-1212},
  shorttitle = {Growing a Language},
  doi = {10.1016/j.jss.2016.04.045},
  abstract = {We describe an empirical study to understand how different language features in JavaScript are used by developers, with the goal of using this information to assist future extensions of JavaScript. We inspected more than one million unique scripts (over 80 MLOC) from various sources: JavaScript programs in the wild collected by a spider, (supposedly) better JavaScript programs collected from the top 100 URLs from the Alexa list, JavaScript programs with new language features used in Firefox Add-ons, widely used JavaScript libraries, and Node.js applications. Our corpus is larger and more diversified than those in prior studies. We also performed two explanatory studies to understand the reasons behind some of the language feature choices. One study was conducted on 107 JavaScript developers; the other was conducted on 45 developers of Node.js applications. Our study shows that there is a widespread confusion about newly introduced JavaScript features, a continuing misuse of existing problematic features, and a surprising lack of adoption of object-oriented features. It also hints at why developers choose to use language features this way. This information is valuable to the language designers and the stakeholders, e.g., IDE and tool builders, all of whom are responsible for growing a language.},
  number = {Supplement C},
  journal = {Journal of Systems and Software},
  author = {Hafiz, Munawar and Hasan, Samir and King, Zachary and {Wirfs-Brock}, Allen},
  month = nov,
  year = {2016},
  keywords = {Empirical study,JavaScript,Language evolution},
  pages = {191-208},
  file = {/Users/luigi/work/zotero/storage/ZDTE8LPZ/Hafiz et al. - 2016 - Growing a language An empirical study on how (and.pdf;/Users/luigi/work/zotero/storage/W6ZKT6RC/S0164121216300309.html}
}

@inproceedings{nguyenAPICodeRecommendation2016,
  address = {New York, NY, USA},
  series = {{{FSE}} 2016},
  title = {{{API Code Recommendation Using Statistical Learning}} from {{Fine}}-Grained {{Changes}}},
  isbn = {978-1-4503-4218-6},
  doi = {10.1145/2950290.2950333},
  abstract = {Learning and remembering how to use APIs is difficult. While code-completion tools can recommend API methods, browsing a long list of API method names and their documentation is tedious. Moreover, users can easily be overwhelmed with too much information. We present a novel API recommendation approach that taps into the predictive power of repetitive code changes to provide relevant API recommendations for developers. Our approach and tool, APIREC, is based on statistical learning from fine-grained code changes and from the context in which those changes were made. Our empirical evaluation shows that APIREC correctly recommends an API call in the first position 59\% of the time, and it recommends the correct API call in the top five positions 77\% of the time. This is a significant improvement over the state-of-the-art approaches by 30-160\% for top-1 accuracy, and 10-30\% for top-5 accuracy, respectively. Our result shows that APIREC performs well even with a one-time, minimal training dataset of 50 publicly available projects.},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Nguyen, Anh Tuan and Hilton, Michael and Codoban, Mihai and Nguyen, Hoan Anh and Mast, Lily and Rademacher, Eli and Nguyen, Tien N. and Dig, Danny},
  year = {2016},
  keywords = {API Recommendation,Fine-grained Code Changes,Statistical Learning},
  pages = {511--522},
  file = {/Users/luigi/work/zotero/storage/5XSUP35V/Nguyen et al. - 2016 - API Code Recommendation Using Statistical Learning.pdf}
}

@inproceedings{hillsEvolutionDynamicFeature2015,
  title = {Evolution of Dynamic Feature Usage in {{PHP}}},
  doi = {10.1109/SANER.2015.7081870},
  abstract = {PHP includes a number of dynamic features that, if used, make it challenging for both programmers and tools to reason about programs. In this paper we examine how usage of these features has changed over time, looking at usage trends for three categories of dynamic features across the release histories of two popular open-source PHP systems, WordPress and MediaWiki. Our initial results suggest that, while features such as eval are being removed over time, more constrained dynamic features such as variable properties are becoming more common. We believe the results of this analysis provide useful insights for researchers and tool developers into the evolving use of dynamic features in real PHP programs.},
  booktitle = {2015 {{IEEE}} 22nd {{International Conference}} on {{Software Analysis}}, {{Evolution}}, and {{Reengineering}} ({{SANER}})},
  author = {Hills, M.},
  month = mar,
  year = {2015},
  keywords = {Runtime,Maintenance engineering,History,program diagnostics,Software,programming languages,Web sites,Market research,public domain software,Arrays,dynamic feature usage,Feature extraction,MediaWiki,open-source PHP system,PHP program,reason about program,reasoning about programs,WordPress},
  pages = {525-529},
  file = {/Users/luigi/work/zotero/storage/K3TFK2FF/Hills - 2015 - Evolution of dynamic feature usage in PHP.pdf;/Users/luigi/work/zotero/storage/ML3VNEX6/7081870.html}
}

@article{landmanEmpiricalAnalysisRelationship2016,
  title = {Empirical Analysis of the Relationship between {{CC}} and {{SLOC}} in a Large Corpus of {{Java}} Methods and {{C}} Functions},
  volume = {28},
  issn = {2047-7481},
  doi = {10.1002/smr.1760},
  abstract = {Measuring the internal quality of source code is one of the traditional goals of making software development into an engineering discipline. Cyclomatic complexity (CC) is an often used source code quality...},
  language = {en},
  number = {7},
  journal = {Journal of Software: Evolution and Process},
  author = {Landman, Davy and Serebrenik, Alexander and Bouwers, Eric and Vinju, Jurgen J.},
  month = jul,
  year = {2016},
  pages = {589-618},
  file = {/Users/luigi/work/zotero/storage/FSE2WT5T/Landman2014-ccsloc-icsme2014-preprint.pdf;/Users/luigi/work/zotero/storage/4MHVVA37/full.html}
}

@inproceedings{vinjuHowMakeBridge2006,
  address = {Dagstuhl, Germany},
  series = {Dagstuhl {{Seminar Proceedings}}},
  title = {How to Make a Bridge between Transformation and Analysis Technologies?},
  booktitle = {Transformation {{Techniques}} in {{Software Engineering}}},
  publisher = {{Internationales Begegnungs- und Forschungszentrum f\"ur Informatik (IBFI), Schloss Dagstuhl, Germany}},
  author = {Vinju, Jurgen and Cordy, James R.},
  editor = {Cordy, James R. and L\"ammel, Ralf and Winter, Andreas},
  year = {2006},
  keywords = {analysis,fact extraction,middleware,source code representations,Transformation},
  file = {/Users/luigi/work/zotero/storage/HYN44RLV/Vinju and Cordy - 2006 - How to make a bridge between transformation and an.pdf;/Users/luigi/work/zotero/storage/KLJZF749/426.html}
}

@inproceedings{nagappanDiversitySoftwareEngineering2013,
  address = {New York, NY, USA},
  series = {{{ESEC}}/{{FSE}} 2013},
  title = {Diversity in {{Software Engineering Research}}},
  isbn = {978-1-4503-2237-9},
  doi = {10.1145/2491411.2491415},
  abstract = {One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space).},
  booktitle = {Proceedings of the 2013 9th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Nagappan, Meiyappan and Zimmermann, Thomas and Bird, Christian},
  year = {2013},
  keywords = {Coverage,Diversity,Representativeness,Sampling},
  pages = {466--476},
  file = {/Users/luigi/work/zotero/storage/ASKS9ZL8/Nagappan et al. - 2013 - Diversity in Software Engineering Research.pdf}
}

@inproceedings{meyerovichEmpiricalAnalysisProgramming2013,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '13},
  title = {Empirical {{Analysis}} of {{Programming Language Adoption}}},
  isbn = {978-1-4503-2374-1},
  doi = {10.1145/2509136.2509515},
  abstract = {Some programming languages become widely popular while others fail to grow beyond their niche or disappear altogether. This paper uses survey methodology to identify the factors that lead to language adoption. We analyze large datasets, including over 200,000 SourceForge projects, 590,000 projects tracked by Ohloh, and multiple surveys of 1,000-13,000 programmers. We report several prominent findings. First, language adoption follows a power law; a small number of languages account for most language use, but the programming market supports many languages with niche user bases. Second, intrinsic features have only secondary importance in adoption. Open source libraries, existing code, and experience strongly influence developers when selecting a language for a project. Language features such as performance, reliability, and simple semantics do not. Third, developers will steadily learn and forget languages. The overall number of languages developers are familiar with is independent of age. Finally, when considering intrinsic aspects of languages, developers prioritize expressivity over correctness. They perceive static types as primarily helping with the latter, hence partly explaining the popularity of dynamic languages.},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN International Conference}} on {{Object Oriented Programming Systems Languages}} \& {{Applications}}},
  publisher = {{ACM}},
  author = {Meyerovich, Leo A. and Rabkin, Ariel S.},
  year = {2013},
  keywords = {programming language adoption,survey research},
  pages = {1--18},
  file = {/Users/luigi/work/zotero/storage/WD7I8GW9/Meyerovich and Rabkin - 2013 - Empirical Analysis of Programming Language Adoptio.pdf}
}

@inproceedings{pradelGoodBadUgly2015,
  address = {Dagstuhl, Germany},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  title = {The {{Good}}, the {{Bad}}, and the {{Ugly}}: {{An Empirical Study}} of {{Implicit Type Conversions}} in {{JavaScript}}},
  volume = {37},
  isbn = {978-3-939897-86-6},
  shorttitle = {The {{Good}}, the {{Bad}}, and the {{Ugly}}},
  doi = {10.4230/LIPIcs.ECOOP.2015.519},
  booktitle = {29th {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2015)},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  author = {Pradel, Michael and Sen, Koushik},
  editor = {Boyland, John Tang},
  year = {2015},
  keywords = {JavaScript,Dynamically typed languages,Type coercions,Types},
  pages = {519--541},
  file = {/Users/luigi/work/zotero/storage/ZNSRZC2G/Pradel and Sen - 2015 - The Good, the Bad, and the Ugly An Empirical Stud.pdf;/Users/luigi/work/zotero/storage/3XQHDIDI/5236.html}
}

@inproceedings{shatnawiAnalyzingProgramDependencies2017,
  title = {Analyzing {{Program Dependencies}} in {{Java EE Applications}}},
  doi = {10.1109/MSR.2017.6},
  abstract = {Program dependency artifacts such as call graphs help support a number of software engineering tasks such as software mining, program understanding, debugging, feature location, software maintenance and evolution. Java Enterprise Edition (JEE) applications represent a significant part of the recent legacy applications, and we are interested in modernizing them. This modernization involves, among other things, analyzing dependencies between their various components/tiers. JEE applications tend to be multilanguage, rely on JEE container services, and make extensive use of late binding techniques-all of which makes finding such dependencies difficult. In this paper, we describe some of these difficulties and how we addressed them to build a dependency call graph. We developed our tool called DeJEE (Dependencies in JEE) as an Eclipse plug-in. We applied DeJEE on two open-source JEE applications: Java PetStore and JSP Blog. The results show that DeJEE is able to identify different types of JEE dependencies.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Shatnawi, A. and Mili, H. and Boussaidi, G. El and Boubaker, A. and Gu\'eh\'eneuc, Y. G. and Moha, N. and Privat, J. and Abdellatif, M.},
  month = may,
  year = {2017},
  keywords = {Java,Object oriented modeling,Data mining,program diagnostics,software maintenance,Software,Tools,code analysis,container services,Containers,DeJEE,dependencies in JEE,dependency call graph,Eclipse plug-in,Java EE application,Java EE applications,Java Enterprise Edition applications,Java PetStore,JEE applications,JEE container services,JSP Blog,late binding techniques,legacy applications,modernization,multilanguage,open-source JEE applications,Program dependency,program dependency analysis,program dependency artifacts,server pages,Servers,software engineering tasks},
  pages = {64-74},
  file = {/Users/luigi/work/zotero/storage/XRWXMDP5/Shatnawi et al. - 2017 - Analyzing Program Dependencies in Java EE Applicat.pdf;/Users/luigi/work/zotero/storage/RJFSIQK8/7962356.html}
}

@inproceedings{sainiDatasetMavenArtifacts2014,
  address = {New York, NY, USA},
  series = {{{MSR}} 2014},
  title = {A {{Dataset}} for {{Maven Artifacts}} and {{Bug Patterns Found}} in {{Them}}},
  isbn = {978-1-4503-2863-0},
  doi = {10.1145/2597073.2597134},
  abstract = {In this paper, we present data downloaded from Maven, one of the most popular component repositories. The data includes the binaries of 186,392 components, along with source code for 161,025. We identify and organize these components into groups where each group contains all the versions of a library. In order to asses the quality of these components, we make available report generated by the FindBugs tool on 64,574 components. The information is also made available in the form of a database which stores total number, type, and priority of bug patterns found in each component, along with its defect density. We also describe how this dataset can be useful in software engineering research.},
  booktitle = {Proceedings of the 11th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Saini, Vaibhav and Sajnani, Hitesh and Ossher, Joel and Lopes, Cristina V.},
  year = {2014},
  keywords = {FindBugs,Empirical Research,Empirical Software Engineering,Maven,Software Quality},
  pages = {416--419},
  file = {/Users/luigi/work/zotero/storage/3JGGWX4N/Saini et al. - 2014 - A dataset for maven artifacts and bug patterns fou.pdf;/Users/luigi/work/zotero/storage/SI94B8BU/Saini et al. - 2014 - A dataset for maven artifacts and bug patterns fou.pdf;/Users/luigi/work/zotero/storage/ZFL7UWCY/Saini et al. - 2014 - A Dataset for Maven Artifacts and Bug Patterns Fou.pdf}
}

@inproceedings{shieldsDynamicTypingStaged1998,
  address = {New York, NY, USA},
  series = {{{POPL}} '98},
  title = {Dynamic {{Typing As Staged Type Inference}}},
  isbn = {978-0-89791-979-1},
  doi = {10.1145/268946.268970},
  abstract = {Dynamic typing extends statically typed languages with a universal datatype, simplifying programs which must manipulate other programs as data, such as distributed, persistent, interpretive and generic programs. Current approaches, however, limit the use of polymorphism in dynamic values, and can be syntactically awkward.We introduce a new approach to dynamic typing, based on staged computation, which allows a single type-reconstruction algorithm to execute partly at compile time and partly at run-time. This approach seamlessly extends a single type system to accommodate types that are only known at run-time, while still supporting both type inference and polymorphism. The system is significantly more expressive than other approaches. Furthermore it can be implemented efficiently; most of the type inference is done at compile-time, leaving only some residual unification for run-time.We demonstrate our approach by examples in a small polymorphic functional language, and present its type system, type reconstruction algorithm, and operational semantics. Our proposal could also be readily adapted to many other programming languages.},
  booktitle = {Proceedings of the 25th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Shields, Mark and Sheard, Tim and Peyton Jones, Simon},
  year = {1998},
  pages = {289--302},
  file = {/Users/luigi/work/zotero/storage/DQ7NC5TI/Shields et al. - 1998 - Dynamic Typing As Staged Type Inference.pdf}
}

@inproceedings{lesterInformationFlowAnalysis2013,
  title = {Information {{Flow Analysis}} for a {{Dynamically Typed Language}} with {{Staged Metaprogramming}}},
  doi = {10.1109/CSF.2013.21},
  abstract = {Web applications written in JavaScript are regularly used for dealing with sensitive or personal data. Consequently, reasoning about their security properties has become an important problem, which is made very difficult by the highly dynamic nature of the language, particularly its support for runtime code generation. As a first step towards dealing with this, we propose to investigate security analyses for languages with more principled forms of dynamic code generation. To this end, we present a static information flow analysis for a dynamically typed functional language with prototype-based inheritance and staged metaprogramming. We prove its soundness, implement it and test it on various examples designed to show its relevance to proving security properties, such as noninterference, in JavaScript. To our knowledge, this is the first fully static information flow analysis for a language with staged metaprogramming, and the first formal soundness proof of a CFA-based information flow analysis for a functional programming language.},
  booktitle = {2013 {{IEEE}} 26th {{Computer Security Foundations Symposium}}},
  author = {Lester, M. and Ong, L. and Schaefer, M.},
  month = jun,
  year = {2013},
  keywords = {Java,program compilers,JavaScript,Security,Semantics,static analysis,Context,information flow,Educational institutions,data flow analysis,dynamically typed languages,CFA,CFA-based information flow analysis,Cognition,dynamic code generation,dynamically typed functional language,formal soundness proof,functional languages,functional programming language,metacomputing,noninterference,prototype-based inheritance,security analyses,security properties,Stability analysis,staged metaprogramming,static information flow analysis,Syntactics,theorem proving,type theory,Web applications},
  pages = {209-223},
  file = {/Users/luigi/work/zotero/storage/ZTTV453F/Lester et al. - 2013 - Information Flow Analysis for a Dynamically Typed .pdf;/Users/luigi/work/zotero/storage/VXZC2UDT/6595830.html}
}

@article{blackburnTruthWholeTruth2016,
  title = {The {{Truth}}, {{The Whole Truth}}, and {{Nothing But}} the {{Truth}}: {{A Pragmatic Guide}} to {{Assessing Empirical Evaluations}}},
  volume = {38},
  issn = {0164-0925},
  shorttitle = {The {{Truth}}, {{The Whole Truth}}, and {{Nothing But}} the {{Truth}}},
  doi = {10.1145/2983574},
  abstract = {An unsound claim can misdirect a field, encouraging the pursuit of unworthy ideas and the abandonment of promising ideas. An inadequate description of a claim can make it difficult to reason about the claim, for example, to determine whether the claim is sound. Many practitioners will acknowledge the threat of unsound claims or inadequate descriptions of claims to their field. We believe that this situation is exacerbated, and even encouraged, by the lack of a systematic approach to exploring, exposing, and addressing the source of unsound claims and poor exposition. This article proposes a framework that identifies three sins of reasoning that lead to unsound claims and two sins of exposition that lead to poorly described claims and evaluations. Sins of exposition obfuscate the objective of determining whether or not a claim is sound, while sins of reasoning lead directly to unsound claims. Our framework provides practitioners with a principled way of critiquing the integrity of their own work and the work of others. We hope that this will help individuals conduct better science and encourage a cultural shift in our research community to identify and promulgate sound claims.},
  number = {4},
  journal = {ACM Trans. Program. Lang. Syst.},
  author = {Blackburn, Stephen M. and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F. and Amaral, Jos\'e Nelson and Brecht, Tim and Bulej, Lubom\'ir and Click, Cliff and Eeckhout, Lieven and Fischmeister, Sebastian and Frampton, Daniel and Hendren, Laurie J. and Hind, Michael and Hosking, Antony L. and Jones, Richard E. and Kalibera, Tomas and Keynes, Nathan and Nystrom, Nathaniel and Zeller, Andreas},
  month = oct,
  year = {2016},
  keywords = {Experimental evaluation,experimentation,observation study},
  pages = {15:1--15:20},
  file = {/Users/luigi/work/zotero/storage/3X6UYR4P/Blackburn et al. - 2016 - The Truth, The Whole Truth, and Nothing But the Tr.pdf}
}

@inproceedings{holmesStrathconaExampleRecommendation2005,
  address = {New York, NY, USA},
  series = {{{ESEC}}/{{FSE}}-13},
  title = {Strathcona {{Example Recommendation Tool}}},
  isbn = {978-1-59593-014-9},
  doi = {10.1145/1081706.1081744},
  abstract = {Using the application programming interfaces (API) of large software systems requires developers to understand details about the interfaces that are often not explicitly defined. However, documentation about the API is often incomplete or out of date. Existing systems that make use of the API provide a form of implicit information on how to use that code. Manually searching through existing projects to find relevant source code is tedious and time consuming. We have created the Strathcona Example.Recommendation Tool to assist developers in finding relevant fragments of code, or examples, of an API's use. These examples can be used by developers to provide insight on how they are supposed to interact with the API.},
  booktitle = {Proceedings of the 10th {{European Software Engineering Conference Held Jointly}} with 13th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Holmes, Reid and Walker, Robert J. and Murphy, Gail C.},
  year = {2005},
  keywords = {examples,recommender,software structure},
  pages = {237--240},
  file = {/Users/luigi/work/zotero/storage/SPEKSBBV/Holmes et al. - 2005 - Strathcona Example Recommendation Tool.pdf}
}

@inproceedings{xieLoopsterStaticLoop2017,
  address = {New York, NY, USA},
  series = {{{ESEC}}/{{FSE}} 2017},
  title = {Loopster: {{Static Loop Termination Analysis}}},
  isbn = {978-1-4503-5105-8},
  shorttitle = {Loopster},
  doi = {10.1145/3106237.3106260},
  abstract = {Loop termination is an important problem for proving the correctness of a system and ensuring that the system always reacts. Existing loop termination analysis techniques mainly depend on the synthesis of ranking functions, which is often expensive. In this paper, we present a novel approach, named Loopster, which performs an efficient static analysis to decide the termination for loops based on path termination analysis and path dependency reasoning. Loopster adopts a divide-and-conquer approach: (1) we extract individual paths from a target multi-path loop and analyze the termination of each path, (2) analyze the dependencies between each two paths, and then (3) determine the overall termination of the target loop based on the relations among paths. We evaluate Loopster by applying it on the loop termination competition benchmark and three real-world projects. The results show that Loopster is effective in a majority of loops with better accuracy and 20 \texttimes{}+ performance improvement compared to the state-of-the-art tools.},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Xie, Xiaofei and Chen, Bihuan and Zou, Liang and Lin, Shang-Wei and Liu, Yang and Li, Xiaohong},
  year = {2017},
  keywords = {Loop Termination,Path Dependency Automaton,Reachability},
  pages = {84--94},
  file = {/Users/luigi/work/zotero/storage/HANYZ59V/Xie et al. - 2017 - Loopster Static Loop Termination Analysis.pdf}
}

@inproceedings{gopsteinUnderstandingMisunderstandingsSource2017,
  address = {New York, NY, USA},
  series = {{{ESEC}}/{{FSE}} 2017},
  title = {Understanding {{Misunderstandings}} in {{Source Code}}},
  isbn = {978-1-4503-5105-8},
  doi = {10.1145/3106237.3106264},
  abstract = {Humans often mistake the meaning of source code, and so misjudge a program's true behavior. These mistakes can be caused by extremely small, isolated patterns in code, which can lead to significant runtime errors. These patterns are used in large, popular software projects and even recommended in style guides. To identify code patterns that may confuse programmers we extracted a preliminary set of `atoms of confusion' from known confusing code. We show empirically in an experiment with 73 participants that these code patterns can lead to a significantly increased rate of misunderstanding versus equivalent code without the patterns. We then go on to take larger confusing programs and measure (in an experiment with 43 participants) the impact, in terms of programmer confusion, of removing these confusing patterns. All of our instruments, analysis code, and data are publicly available online for replication, experimentation, and feedback.},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Gopstein, Dan and Iannacone, Jake and Yan, Yu and DeLong, Lois and Zhuang, Yanyan and Yeh, Martin K.-C. and Cappos, Justin},
  year = {2017},
  keywords = {Program Understanding,Programming Languages},
  pages = {129--139},
  file = {/Users/luigi/work/zotero/storage/UWHGYDEZ/Gopstein et al. - 2017 - Understanding Misunderstandings in Source Code.pdf}
}

@article{bezemerEmpiricalStudyUnspecified2017,
  title = {An Empirical Study of Unspecified Dependencies in Make-Based Build Systems},
  volume = {22},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-017-9510-8},
  abstract = {Software developers rely on a build system to compile their source code changes and produce deliverables for testing and deployment. Since the full build of large software systems can take hours, the incremental build is a cornerstone of modern build systems. Incremental builds should only recompile deliverables whose dependencies have been changed by a developer. However, in many organizations, such dependencies still are identified by build rules that are specified and maintained (mostly) manually, typically using technologies like make. Incomplete rules lead to unspecified dependencies that can prevent certain deliverables from being rebuilt, yielding incomplete results, which leave sources and deliverables out-of-sync. In this paper, we present a case study on unspecified dependencies in the make-based build systems of the glib, openldap, linux and qt open source projects. To uncover unspecified dependencies in make-based build systems, we use an approach that combines a conceptual model of the dependencies specified in the build system with a concrete model of the files and processes that are actually exercised during the build. Our approach provides an overview of the dependencies that are used throughout the build system and reveals unspecified dependencies that are not yet expressed in the build system rules. During our analysis, we find that unspecified dependencies are common. We identify 6 common causes in more than 1.2 million unspecified dependencies.},
  language = {en},
  number = {6},
  journal = {Empirical Software Engineering},
  author = {Bezemer, Cor-Paul and McIntosh, Shane and Adams, Bram and German, Daniel M. and Hassan, Ahmed E.},
  month = dec,
  year = {2017},
  pages = {3117-3148},
  file = {/Users/luigi/work/zotero/storage/YM4DLHL8/Bezemer et al. - 2017 - An empirical study of unspecified dependencies in .pdf;/Users/luigi/work/zotero/storage/PNGDR36N/10.html}
}

@article{madsenModelReasoningJavaScript2017,
  title = {A {{Model}} for {{Reasoning About JavaScript Promises}}},
  volume = {1},
  issn = {2475-1421},
  doi = {10.1145/3133910},
  abstract = {In JavaScript programs, asynchrony arises in situations such as web-based user-interfaces, communicating with servers through HTTP requests, and non-blocking I/O. Event-based programming is the most popular approach for managing asynchrony, but suffers from problems such as lost events and event races, and results in code that is hard to understand and debug. Recently, ECMAScript 6 has added support for promises, an alternative mechanism for managing asynchrony that enables programmers to chain asynchronous computations while supporting proper error handling. However, promises are complex and error-prone in their own right, so programmers would benefit from techniques that can reason about the correctness of promise-based code. Since the ECMAScript 6 specification is informal and intended for implementers of JavaScript engines, it does not provide a suitable basis for formal reasoning. This paper presents \^I\guillemotright{}p, a core calculus that captures the essence of ECMAScript 6 promises. Based on \^I\guillemotright{}p, we introduce the promise graph, a program representation that can assist programmers with debugging of promise-based code. We then report on a case study in which we investigate how the promise graph can be helpful for debugging errors related to promises in code fragments posted to the StackOverflow website.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Madsen, Magnus and Lhot\'ak, Ond{\v r}ej and Tip, Frank},
  month = oct,
  year = {2017},
  keywords = {JavaScript,EcmaScript 6,Formal Semantics,Promise Graph,Promises},
  pages = {86:1--86:24},
  file = {/Users/luigi/work/zotero/storage/IG859BB8/Madsen et al. - 2017 - A Model for Reasoning About JavaScript Promises.pdf}
}

@article{ringerSolveraidedLanguageTest2017,
  title = {A {{Solver}}-Aided {{Language}} for {{Test Input Generation}}},
  volume = {1},
  issn = {2475-1421},
  doi = {10.1145/3133915},
  abstract = {Developing a small but useful set of inputs for tests is challenging. We show that a domain-specific language backed by a constraint solver can help the programmer with this process. The solver can generate a set of test inputs and guarantee that each input is different from other inputs in a way that is useful for testing. This paper presents Iorek: a tool that empowers the programmer with the ability to express to any SMT solver what it means for inputs to be different. The core of Iorek is a rich language for constraining the set of inputs, which includes a novel bounded enumeration mechanism that makes it easy to define and encode a flexible notion of difference over a recursive structure. We demonstrate the flexibility of this mechanism for generating strings. We use Iorek to test real services and find that it is effective at finding bugs. We also build Iorek into a random testing tool and show that it increases coverage.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Ringer, Talia and Grossman, Dan and {Schwartz-Narbonne}, Daniel and Tasiran, Serdar},
  month = oct,
  year = {2017},
  keywords = {generators,solver-aided languages,test input generation},
  pages = {91:1--91:24},
  file = {/Users/luigi/work/zotero/storage/FJN4967I/Ringer et al. - 2017 - A Solver-aided Language for Test Input Generation.pdf}
}

@article{rapoportSimpleSoundnessProof2017,
  title = {A {{Simple Soundness Proof}} for {{Dependent Object Types}}},
  volume = {1},
  issn = {2475-1421},
  doi = {10.1145/3133870},
  abstract = {Dependent Object Types (DOT) is intended to be a core calculus for modelling Scala. Its distinguishing feature is abstract type members, fields in objects that hold types rather than values. Proving soundness of DOT has been surprisingly challenging, and existing proofs are complicated, and reason about multiple concepts at the same time (e.g. types, values, evaluation). To serve as a core calculus for Scala, DOT should be easy to experiment with and extend, and therefore its soundness proof needs to be easy to modify.   This paper presents a simple and modular proof strategy for reasoning in DOT. The strategy separates reasoning about types from other concerns. It is centred around a theorem that connects the full DOT type system to a restricted variant in which the challenges and paradoxes caused by abstract type members are eliminated. Almost all reasoning in the proof is done in the intuitive world of this restricted type system. Once we have the necessary results about types, we observe that the other aspects of DOT are mostly standard and can be incorporated into a soundness proof using familiar techniques known from other calculi.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Rapoport, Marianna and Kabir, Ifaz and He, Paul and Lhot\'ak, Ond{\v r}ej},
  month = oct,
  year = {2017},
  keywords = {Scala,dependent object types,DOT calculus,type safety},
  pages = {46:1--46:27},
  file = {/Users/luigi/work/zotero/storage/GEXTI2XG/Rapoport et al. - 2017 - A Simple Soundness Proof for Dependent Object Type.pdf}
}

@article{jeongDatadrivenContextsensitivityPointsto2017,
  title = {Data-Driven {{Context}}-Sensitivity for {{Points}}-to {{Analysis}}},
  volume = {1},
  issn = {2475-1421},
  doi = {10.1145/3133924},
  abstract = {We present a new data-driven approach to achieve highly cost-effective context-sensitive points-to analysis for Java. While context-sensitivity has greater impact on the analysis precision and performance than any other precision-improving techniques, it is difficult to accurately identify the methods that would benefit the most from context-sensitivity and decide how much context-sensitivity should be used for them. Manually designing such rules is a nontrivial and laborious task that often delivers suboptimal results in practice. To overcome these challenges, we propose an automated and data-driven approach that learns to effectively apply context-sensitivity from codebases. In our approach, points-to analysis is equipped with a parameterized and heuristic rules, in disjunctive form of properties on program elements, that decide when and how much to apply context-sensitivity. We present a greedy algorithm that efficiently learns the parameter of the heuristic rules. We implemented our approach in the Doop framework and evaluated using three types of context-sensitive analyses: conventional object-sensitivity, selective hybrid object-sensitivity, and type-sensitivity. In all cases, experimental results show that our approach significantly outperforms existing techniques.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Jeong, Sehun and Jeon, Minseok and Cha, Sungdeok and Oh, Hakjoo},
  month = oct,
  year = {2017},
  keywords = {Context-sensitivity,Data-driven program analysis,Points-to analysis},
  pages = {100:1--100:28},
  file = {/Users/luigi/work/zotero/storage/4T776QN4/Jeong et al. - 2017 - Data-driven Context-sensitivity for Points-to Anal.pdf}
}

@article{grechTaintUnifiedPointsto2017,
  title = {P/{{Taint}}: {{Unified Points}}-to and {{Taint Analysis}}},
  volume = {1},
  issn = {2475-1421},
  shorttitle = {P/{{Taint}}},
  doi = {10.1145/3133926},
  abstract = {Static information-flow analysis (especially taint-analysis) is a key technique in software security, computing where sensitive or untrusted data can propagate in a program. Points-to analysis is a fundamental static program analysis, computing what abstract objects a program expression may point to. In this work, we propose a deep unification of information-flow and points-to analysis. We observe that information-flow analysis is not a mere high-level client of points-to information, but it is indeed identical to points-to analysis on artificial abstract objects that represent different information sources. The very same algorithm can compute, simultaneously, two interlinked but separate results (points-to and information-flow values) with changes only to its initial conditions. The benefits of such a unification are manifold. We can use existing points-to analysis implementations, with virtually no modification (only minor additions of extra logic for sanitization) to compute information flow concepts, such as value tainting. The algorithmic enhancements of points-to analysis (e.g., different flavors of context sensitivity) can be applied transparently to information-flow analysis. Heavy engineering work on points-to analysis (e.g., handling of the reflection API for Java) applies to information-flow analysis without extra effort. We demonstrate the benefits in a realistic implementation that leverages the Doop points-to analysis framework (including its context-sensitivity and reflection analysis features) to provide an information-flow analysis with excellent precision (over 91\%) and recall (over 99\%) for standard Java information-flow benchmarks. The analysis comfortably scales to large, real-world Android applications, analyzing the Facebook Messenger app with more than 55K classes in under 7 hours.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Grech, Neville and Smaragdakis, Yannis},
  month = oct,
  year = {2017},
  keywords = {Android,Pointer Analysis,Taint Analysis},
  pages = {102:1--102:28},
  file = {/Users/luigi/work/zotero/storage/HCPX6ZSP/Grech and Smaragdakis - 2017 - PTaint Unified Points-to and Taint Analysis.pdf}
}

@inproceedings{lamContextsensitiveProgramAnalysis2005,
  address = {New York, NY, USA},
  series = {{{PODS}} '05},
  title = {Context-Sensitive {{Program Analysis As Database Queries}}},
  isbn = {978-1-59593-062-0},
  doi = {10.1145/1065167.1065169},
  abstract = {Program analysis has been increasingly used in software
engineering tasks such as auditing programs for security
vulnerabilities and finding errors in general. Such tools often
require analyses much more sophisticated than those traditionally
used in compiler optimizations. In particular, context-sensitive
pointer alias information is a prerequisite for any sound and
precise analysis that reasons about uses of heap objects in a
program. Context-sensitive analysis is challenging because there
are over 1014 contexts in a typical large program, even
after recursive cycles are collapsed. Moreover, pointers cannot be
resolved in general without analyzing the entire program.

This paper presents a new framework, based on the concept of
deductive databases, for context-sensitive program analysis. In
this framework, all program information is stored as relations;
data access and analyses are written as Datalog queries. To handle
the large number of contexts in a program, the database represents
relations with binary decision diagrams (BDDs). The system we have
developed, called bddbddb, automatically translates database
queries into highly optimized BDD programs.

Our preliminary experiences suggest that a large class of
analyses involving heap objects can be described succinctly in
Datalog and implemented efficiently with BDDs. To make developing
application-specific analyses easy for programmers, we have also
created a language called PQL that makes a subset of Datalog
queries more intuitive to define. We have used the language to find
many security holes in Web applications.},
  booktitle = {Proceedings of the {{Twenty}}-Fourth {{ACM SIGMOD}}-{{SIGACT}}-{{SIGART Symposium}} on {{Principles}} of {{Database Systems}}},
  publisher = {{ACM}},
  author = {Lam, Monica S. and Whaley, John and Livshits, V. Benjamin and Martin, Michael C. and Avots, Dzintars and Carbin, Michael and Unkel, Christopher},
  year = {2005},
  pages = {1--12},
  file = {/Users/luigi/work/zotero/storage/5PIJYBXL/Lam et al. - 2005 - Context-sensitive program analysis as database que.pdf;/Users/luigi/work/zotero/storage/IQ5AK8FR/Lam et al. - 2005 - Context-sensitive Program Analysis As Database Que.pdf;/Users/luigi/work/zotero/storage/NIILFXSN/Lam et al. - 2005 - Context-sensitive program analysis as database que.pdf}
}

@phdthesis{prokschEnrichedEventStreams2017,
  address = {Darmstadt},
  type = {Ph.{{D}}. {{Thesis}}},
  title = {Enriched {{Event Streams}}: {{A General Platform For Empirical Studies On In}}-{{IDE Activities Of Software Developers}}},
  copyright = {CC-BY-SA 4.0 International - Creative Commons Attribution Share-alike, 4.0},
  shorttitle = {Enriched {{Event Streams}}},
  abstract = {Current studies on software development either focus on the change history of source code from version-control systems or on an analysis of simplistic in-IDE events without context information. Each of these approaches contains valuable information that is unavailable in the other case. This work proposes enriched event streams, a solution that combines the best of both worlds and provides a holistic view on the in-IDE software development process. Enriched event streams not only capture developer activities in the IDE, but also specialized context information, such as source-code snapshots for change events. To enable the storage of such code snapshots in an analyzable format, we introduce a new intermediate representation called Simplified Syntax Trees (SSTs) and build CARET, a platform that offers reusable components to conveniently work with enriched event streams. We implement FeedBaG++, an instrumentation for Visual Studio that collects enriched event streams with code snapshots in the form of SSTs and share a dataset of enriched event streams captured in an ongoing field study from 81 users and representing 15K hours of active development. We complement this with a dataset of 69M lines of released source code extracted from 360 GitHub repositories. To demonstrate the usefulness of our platform, we use it to conduct studies on the in-IDE development process that are both concerned with source-code evolution and the analysis of developer interactions. In addition, we build recommendation systems for software engineering and analyze and improve current evaluation techniques.},
  language = {en},
  school = {Technische Universit\"at},
  author = {Proksch, Sebastian},
  month = may,
  year = {2017},
  file = {/Users/luigi/work/zotero/storage/4QWSWZGS/Proksch - 2017 - Enriched Event Streams A General Platform For Emp.pdf;/Users/luigi/work/zotero/storage/ZYERRRPX/6971.html}
}

@inproceedings{dietrichConstructionSoundnessOracles2017,
  address = {New York, NY, USA},
  series = {{{SOAP}} 2017},
  title = {On the {{Construction}} of {{Soundness Oracles}}},
  isbn = {978-1-4503-5072-3},
  doi = {10.1145/3088515.3088520},
  abstract = {One of the inherent advantages of static analysis is that it can create and reason about models of an entire program. However, mainstream languages such as Java use numerous dynamic language features designed to boost programmer productivity, but these features are notoriously difficult to capture by static analysis, leading to unsoundness in practice. While existing research has focused on providing sound handling for selected language features (mostly reflection) based on anecdotal evidence and case studies, there is little empirical work to investigate the extent to which particular features cause unsoundness of static analysis in practice. In this paper, we (1) discuss language features that may cause unsoundness and (2) discuss a methodology that can be used to check the (un)soundness of a particular static analysis, call-graph construction, based on soundness oracles. These oracles can also be used for hybrid analyses.},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN International Workshop}} on {{State Of}} the {{Art}} in {{Program Analysis}}},
  publisher = {{ACM}},
  author = {Dietrich, Jens and Sui, Li and Rasheed, Shawn and Tahir, Amjed},
  year = {2017},
  keywords = {Static analysis,Dynamic analysis,Soundness},
  pages = {37--42},
  file = {/Users/luigi/work/zotero/storage/VQ66MXC9/Dietrich et al. - 2017 - On the Construction of Soundness Oracles.pdf}
}

@inproceedings{umemoriDesignImplementationBytecodebased2003,
  title = {Design and Implementation of Bytecode-Based {{Java}} Slicing System},
  doi = {10.1109/SCAM.2003.1238037},
  abstract = {A program slice is a set of statements that affect the value of a variable v in a statement s. In order to calculate a program slice, we must know the dependence relations between statements in the program. Program slicing techniques are roughly divided into two categories, static slicing and dynamic slicing, and we have proposed DC slicing technique which uses both static and dynamic information. We propose a method of constructing a DC slicing system for Java programs. Java programs have many elements which are dynamically determined at the time of execution, so the DC slicing technique is effective in the analysis of Java programs. To construct the system, we have extended a Java virtual machine for extraction of dynamic information. We have applied the system to several sample programs to evaluate our approach.},
  booktitle = {Proceedings {{Third IEEE International Workshop}} on {{Source Code Analysis}} and {{Manipulation}}},
  author = {Umemori, F. and Konda, K. and Yokomori, R. and Inoue, K.},
  month = sep,
  year = {2003},
  keywords = {Java,Debugging,Data mining,Java virtual machine,Performance analysis,virtual machines,Programming,Costs,Virtual machining,Testing,static information,graph theory,bytecode-based Java slicing system,DC slicing technique,dynamic information,dynamic slicing,dynamically determined elements,Information science,Java program,Merging,program slicing,program statement,sample program,static slicing},
  pages = {108-117},
  file = {/Users/luigi/work/zotero/storage/JAIIEDYK/Umemori et al. - 2003 - Design and implementation of bytecode-based Java s.pdf;/Users/luigi/work/zotero/storage/9IJCK7BI/1238037.html}
}

@inproceedings{rosaAccurateReificationComplete2017,
  address = {New York, NY, USA},
  series = {{{GPCE}} 2017},
  title = {Accurate {{Reification}} of {{Complete Supertype Information}} for {{Dynamic Analysis}} on the {{JVM}}},
  isbn = {978-1-4503-5524-7},
  doi = {10.1145/3136040.3136061},
  abstract = {Reflective supertype information (RSI) is useful for many instrumentation-based dynamic analyses on the Java Virtual Machine (JVM). On the one hand, while such information can be obtained when performing the instrumentation within the same JVM process executing the instrumented program, in-process instrumentation severely limits the code coverage of the analysis. On the other hand, performing the instrumentation in a separate process can achieve full code coverage, but complete RSI is generally not available, often requiring expensive runtime checks in the instrumented program. Providing accurate and complete RSI in the instrumentation process is challenging because of dynamic class loading and classloader namespaces. In this paper, we present a novel technique to accurately reify complete RSI in a separate instrumentation process. We implement our technique in the dynamic analysis framework DiSL and evaluate it on a task profiler, achieving speedups of up to 45\% for an analysis with full code coverage.},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  publisher = {{ACM}},
  author = {Ros\`a, Andrea and Rosales, Eduardo and Binder, Walter},
  year = {2017},
  keywords = {bytecode instrumentation,Dynamic analysis,Java Virtual Machine,reflective information},
  pages = {104--116},
  file = {/Users/luigi/work/zotero/storage/PDUHLTFF/Rosà et al. - 2017 - Accurate Reification of Complete Supertype Informa.pdf}
}

@inproceedings{zaytsevParserGenerationExample2017,
  address = {New York, NY, USA},
  series = {{{GPCE}} 2017},
  title = {Parser {{Generation}} by {{Example}} for {{Legacy Pattern Languages}}},
  isbn = {978-1-4503-5524-7},
  doi = {10.1145/3136040.3136058},
  abstract = {Most modern software languages enjoy relatively free and relaxed concrete syntax, with significant flexibility of formatting of the program/model/sheet text. Yet, in the dark legacy corners of software engineering there are still languages with a strict fixed column-based structure \textemdash{} the compromises of times long gone, attempting to combine some human readability with some ease of machine processing. In this paper, we consider an industrial case study for retirement of a legacy domain-specific language, completed under extreme circumstances: absolute lack of documentation, varying line structure, hierarchical blocks within one file, scalability demands for millions of lines of code, performance demands for manipulating tens of thousands multi-megabyte files, etc. However, the regularity of the language allowed to infer its structure from the available examples, automatically, and produce highly efficient parsers for it.},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  publisher = {{ACM}},
  author = {Zaytsev, Vadim},
  year = {2017},
  keywords = {parser generation,engineering by example,grammar inference,language acquisition,legacy software,pattern languages},
  pages = {212--218},
  file = {/Users/luigi/work/zotero/storage/XMAJJYFK/Zaytsev - 2017 - Parser Generation by Example for Legacy Pattern La.pdf}
}

@inproceedings{carlsonTypeQualifiersComposable2017,
  address = {New York, NY, USA},
  series = {{{GPCE}} 2017},
  title = {Type {{Qualifiers As Composable Language Extensions}}},
  isbn = {978-1-4503-5524-7},
  doi = {10.1145/3136040.3136055},
  abstract = {This paper reformulates type qualifiers as language extensions that can be automatically and reliably composed. Type qualifiers annotate type expressions to introduce new subtyping relations and are powerful enough to detect many kinds of errors. Type qualifiers, as illustrated in our ableC extensible language framework for C, can introduce rich forms of concrete syntax, can generate dynamic checks on data when static checks are infeasible or not appropriate, and inject code that affects the program's behavior, for example for conversions of data or logging.   ableC language extensions to C are implemented as attribute grammar fragments and provide an expressive mechanism for type qualifier implementations to check for additional errors, e.g. dereferences to pointers not qualified by a "nonnull" qualifier, and report custom error messages. Our approach distinguishes language extension users from developers and provides modular analyses to developers to ensure that when users select a set of extensions to use, they will automatically compose to form a working compiler.},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  publisher = {{ACM}},
  author = {Carlson, Travis and Van Wyk, Eric},
  year = {2017},
  keywords = {type systems,type qualifiers,dimensional analysis,extensible languages,pluggable types},
  pages = {91--103},
  file = {/Users/luigi/work/zotero/storage/KFS8YDPJ/Carlson and Van Wyk - 2017 - Type Qualifiers As Composable Language Extensions.pdf}
}

@inproceedings{wurthingerPracticalPartialEvaluation2017,
  address = {New York, NY, USA},
  series = {{{PLDI}} 2017},
  title = {Practical {{Partial Evaluation}} for {{High}}-Performance {{Dynamic Language Runtimes}}},
  isbn = {978-1-4503-4988-8},
  doi = {10.1145/3062341.3062381},
  abstract = {Most high-performance dynamic language virtual machines duplicate language semantics in the interpreter, compiler, and runtime system. This violates the principle to not repeat yourself. In contrast, we define languages solely by writing an interpreter. The interpreter performs specializations, e.g., augments the interpreted program with type information and profiling information. Compiled code is derived automatically using partial evaluation while incorporating these specializations. This makes partial evaluation practical in the context of dynamic languages: It reduces the size of the compiled code while still compiling all parts of an operation that are relevant for a particular program. When a speculation fails, execution transfers back to the interpreter, the program re-specializes in the interpreter, and later partial evaluation again transforms the new state of the interpreter to compiled code. We evaluate our approach by comparing our implementations of JavaScript, Ruby, and R with best-in-class specialized production implementations. Our general-purpose compilation system is competitive with production systems even when they have been heavily optimized for the one language they support. For our set of benchmarks, our speedup relative to the V8 JavaScript VM is 0.83x, relative to JRuby is 3.8x, and relative to GNU R is 5x.},
  booktitle = {Proceedings of the 38th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {W\"urthinger, Thomas and Wimmer, Christian and Humer, Christian and W\"o\ss, Andreas and Stadler, Lukas and Seaton, Chris and Duboscq, Gilles and Simon, Doug and Grimmer, Matthias},
  year = {2017},
  keywords = {dynamic languages,partial evaluation,virtual machine,optimization,language implementation},
  pages = {662--676},
  file = {/Users/luigi/work/zotero/storage/YVGBKN8X/Würthinger et al. - 2017 - Practical Partial Evaluation for High-performance .pdf}
}

@inproceedings{gallabaDonCallUs2015,
  title = {Don't {{Call Us}}, {{We}}'ll {{Call You}}: {{Characterizing Callbacks}} in {{Javascript}}},
  shorttitle = {Don't {{Call Us}}, {{We}}'ll {{Call You}}},
  doi = {10.1109/ESEM.2015.7321196},
  abstract = {JavaScript is a popular language for developing web applications and is increasingly used for both client-side and server-side application logic. The JavaScript runtime is inherently event-driven and callbacks are a key language feature. Unfortunately, callbacks induce a non-linear control flow and can be deferred to execute asynchronously, declared anonymously, and may be nested to arbitrary levels. All of these features make callbacks difficult to understand and maintain. We perform an empirical study to characterize JavaScript callback usage across a representative corpus of 138 JavaScript programs, with over 5 million lines of JavaScript code. We find that on average, every 10th function definition takes a callback argument, and that over 43\% of all callback-accepting function callsites are anonymous. Furthermore, the majority of callbacks are nested, more than half of all callbacks are asynchronous, and asynchronous callbacks, on average, appear more frequently in client-side code (72\%) than server-side (55\%). We also study three well-known solutions designed to help with the complexities associated with callbacks, including the error-first callback convention, Async.js library, and Promises. Our results inform the design of future JavaScript analysis and code comprehension tools.},
  booktitle = {2015 {{ACM}}/{{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}} ({{ESEM}})},
  author = {Gallaba, K. and Mesbah, A. and Beschastnikh, I.},
  month = oct,
  year = {2015},
  keywords = {Java,Protocols,Libraries,program diagnostics,Complexity theory,Web applications,Async.js library,Best practices,Browsers,callback argument,callback-accepting function,client-side application logic,code comprehension tool,error-first callback convention,Games,JavaScript analysis,JavaScript callback usage,JavaScript code,JavaScript language,JavaScript runtime feature,Reactive power,server-side application logic},
  pages = {1-10},
  file = {/Users/luigi/work/zotero/storage/DM8Z9KNP/Gallaba et al. - 2015 - Don't Call Us, We'll Call You Characterizing Call.pdf;/Users/luigi/work/zotero/storage/BCE9TETT/7321196.html}
}

@article{bullBenchmarkSuiteHigh2000,
  title = {A Benchmark Suite for High Performance {{Java}}},
  volume = {12},
  issn = {1096-9128},
  doi = {10.1002/1096-9128(200005)12:6<375::AID-CPE480>3.0.CO;2-M},
  abstract = {Increasing interest is being shown in the use of Java for large scale or Grande applications. This new use of Java places specific demands on the Java execution environments that could be tested and compared using a standard benchmark suite. We describe the design and implementation of such a suite, paying particular attention to Java-specific issues. Sample results are presented for a number of implementations of the Java Virtual Machine (JVM). Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {6},
  journal = {Concurrency: Practice and Experience},
  author = {Bull, J. M. and Smith, L. A. and Westhead, M. D. and Henty, D. S. and Davey, R. A.},
  month = may,
  year = {2000},
  keywords = {Java,JVM,benchmarking,high performance},
  pages = {375-388},
  file = {/Users/luigi/work/zotero/storage/CX69ELJS/Bull et al. - 2000 - A benchmark suite for high performance Java.pdf;/Users/luigi/work/zotero/storage/LWN2NES3/abstract.html}
}

@article{fitzgeraldMarmotOptimizingCompiler2000,
  title = {Marmot: An Optimizing Compiler for {{Java}}},
  volume = {30},
  issn = {1097-024X},
  shorttitle = {Marmot},
  doi = {10.1002/(SICI)1097-024X(200003)30:3<199::AID-SPE296>3.0.CO;2-2},
  abstract = {The Marmot system is a research platform for studying the implementation of high level programming languages. It currently comprises an optimizing native-code compiler, runtime system, and libraries for a large subset of Java. Marmot integrates well-known representation, optimization, code generation, and runtime techniques with a few Java-specific features to achieve competitive performance. This paper contains a description of the Marmot system design, along with highlights of our experience applying and adapting traditional implementation techniques to Java. A detailed performance evaluation assesses both Marmot's overall performance relative to other Java and C++ implementations, and the relative costs of various Java language features in Marmot-compiled code. Our experience with Marmot has demonstrated that well-known compilation techniques can produce very good performance for static Java applications \textendash{} comparable or superior to other Java systems, and approaching that of C++ in some cases. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {3},
  journal = {Software: Practice and Experience},
  author = {Fitzgerald, Robert and Knoblock, Todd B. and Ruf, Erik and Steensgaard, Bjarne and Tarditi, David},
  month = mar,
  year = {2000},
  keywords = {Java,compilers,optimization,language translation},
  pages = {199-232},
  file = {/Users/luigi/work/zotero/storage/ENLD6QS7/Fitzgerald et al. - 2000 - Marmot an optimizing compiler for Java.pdf;/Users/luigi/work/zotero/storage/63TCAEZU/abstract.html}
}

@inproceedings{calciatiHowAppsEvolve2017,
  title = {How {{Do Apps Evolve}} in {{Their Permission Requests}}? {{A Preliminary Study}}},
  shorttitle = {How {{Do Apps Evolve}} in {{Their Permission Requests}}?},
  doi = {10.1109/MSR.2017.64},
  abstract = {We present a preliminary study to understand how apps evolve in their permission requests across different releases. We analyze over 14K releases of 227 Android apps, and we see how permission requests change and how they are used. We find that apps tend to request more permissions in their evolution, and many of the newly requested permissions are initially overprivileged. Our qualitative analysis, however, shows that the results that popular tools report on overprivileged apps may be biased by incomplete information or by other factors. Finally, we observe that when apps no longer request a permission, it does not necessarily mean that the new release offers less in terms of functionalities.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Calciati, P. and Gorla, A.},
  month = may,
  year = {2017},
  keywords = {Software,Market research,Tools,Android (operating system),Androids,Google,Humanoid robots,Android application,Cameras,overprivileged application,permission requests,qualitative analysis},
  pages = {37-41},
  file = {/Users/luigi/work/zotero/storage/CU6RVINP/Calciati and Gorla - 2017 - How Do Apps Evolve in Their Permission Requests A.pdf;/Users/luigi/work/zotero/storage/HC68J3RD/7962353.html}
}

@inproceedings{corbelliniMiningSocialWeb2017,
  title = {Mining {{Social Web Service Repositories}} for {{Social Relationships}} to {{Aid Service Discovery}}},
  doi = {10.1109/MSR.2017.16},
  abstract = {The Service Oriented Computing (SOC) paradigm promotes building new applications by discovering and then invoking services, i.e., software components accessible through the Internet. Discovering services means inspecting registries where textual descriptions of services functional capabilities are stored. To automate this, existing approaches index descriptions and associate users' queries to relevant services. However, the massive adoption of Web-exposed API development practices, specially in large service ecosystems such as the IoT, is leading to evergrowing registries which challenge the accuracy and speed of such approaches. The recent notion of Social Web Services (SWS), where registries not only store service information but also sociallike relationships between users and services opens the door to new discovery schemes. We investigate an approach to discover SWSs that operates on graphs with user-service relationships and employs lightweight topological metrics to assess service similarity. Then, "socially" similar services, which are determined exploiting explicit relationships and mining implicit relationships in the graph, are clustered via exemplar-based clustering to ultimately aid discovery. Experiments performed with the ProgrammableWeb.com registry, which is at present the largest SWS repository with over 15k services and 140k user-service relationships, show that pure topology-based clustering may represent a promising complement to content-based approaches, which in fact are more time-consuming due to text processing operations.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Corbellini, A. and Godoy, D. and Mateos, C. and Zunino, A. and Lizarralde, I.},
  month = may,
  year = {2017},
  keywords = {Buildings,data mining,object-oriented programming,application program interfaces,Indexes,Measurement,Internet,Web services,Clustering algorithms,content-based approaches,Entropy,exemplar-based clustering,Exemplar-based clustering,index descriptions,IoT,large service ecosystems,pattern clustering,ProgrammableWeb.com registry,service discovery,Service discovery,service functional capabilities,service oriented computing,service-oriented architecture,Social network services,social networking (online),Social recommender systems,social relationships,Social Web Service,social Web service repositories mining,software components,text processing operations,topology-based clustering,user-service relationships,Web-exposed API development},
  pages = {75-79},
  file = {/Users/luigi/work/zotero/storage/XDUQTJS2/Corbellini et al. - 2017 - Mining Social Web Service Repositories for Social .pdf;/Users/luigi/work/zotero/storage/BZC4M79E/7962357.html}
}

@inproceedings{patilConceptBasedClassificationSoftware2017,
  title = {Concept-{{Based Classification}} of {{Software Defect Reports}}},
  doi = {10.1109/MSR.2017.20},
  abstract = {Automatic identification of the defect type from the textual description of a software defect can significantly speed-up as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects. In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the "semantic similarity" between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Patil, S.},
  month = may,
  year = {2017},
  keywords = {data mining,Software,software engineering,Internet,Semantics,Encyclopedias,Training data,concept-based classification,Electronic publishing,ESA,explicit semantic analysis,Explicit Semantic Analysis,labeled training data,learning (artificial intelligence),Mining Software Respositories,semantic similarity,software defect classification,Software Defect Classification,software defect management life-cycle,software defect reports,software management,supervised learning approach,Text Data Mining},
  pages = {182-186},
  file = {/Users/luigi/work/zotero/storage/QQDY87RC/Patil - 2017 - Concept-Based Classification of Software Defect Re.pdf;/Users/luigi/work/zotero/storage/KXYTBSH7/7962367.html}
}

@inproceedings{mantylaBootstrappingLexiconEmotional2017,
  address = {Piscataway, NJ, USA},
  series = {{{MSR}} '17},
  title = {Bootstrapping a {{Lexicon}} for {{Emotional Arousal}} in {{Software Engineering}}},
  isbn = {978-1-5386-1544-7},
  doi = {10.1109/MSR.2017.47},
  abstract = {Emotional arousal increases activation and performance but may also lead to burnout in software development. We present the first version of a Software Engineering Arousal lexicon (SEA) that is specifically designed to address the problem of emotional arousal in the software developer ecosystem. SEA is built using a bootstrapping approach that combines word embedding model trained on issue-tracking data and manual scoring of items in the lexicon. We show that our lexicon is able to differentiate between issue priorities, which are a source of emotional activation and then act as a proxy for arousal. The best performance is obtained by combining SEA (428 words) with a previously created general purpose lexicon by Warriner et al. (13,915 words) and it achieves Cohen's d effect sizes up to 0.5.},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Mining Software Repositories}}},
  publisher = {{IEEE Press}},
  author = {M\"antyl\"a, Mika V. and Novielli, Nicole and Lanubile, Filippo and Claes, Ma\"elick and Kuutila, Miikka},
  year = {2017},
  keywords = {empirical software engineering,emotional arousal,issue report,lexicon,sentiment analysis},
  pages = {198--202},
  file = {/Users/luigi/work/zotero/storage/HHNVQNW9/Mäntylä et al. - 2017 - Bootstrapping a Lexicon for Emotional Arousal in S.pdf}
}

@inproceedings{claesAbnormalWorkingHours2017,
  title = {Abnormal {{Working Hours}}: {{Effect}} of {{Rapid Releases}} and {{Implications}} to {{Work Content}}},
  shorttitle = {Abnormal {{Working Hours}}},
  doi = {10.1109/MSR.2017.3},
  abstract = {During the past years, overload at work leading to psychological diseases, such as burnouts, have drawn more public attention. This paper is a preliminary step toward an analysis of the work patterns and possible indicators of overload and time pressure on software developers with mining software repositories approach. We explore the working pattern of developers in the context of Mozilla Firefox, a large and long-lived open source project. To that end we investigate the impact of the move from traditional to rapid release cycle on work pattern. Moreover we compare Mozilla Firefox work pattern with another Mozilla product, Firefox OS, which has a different release cycle than Firefox. We find that both projects exhibit healthy working patterns, i.e. lower activity during the weekends and outside of office hours. Firefox experiences proportionally more activity on weekends than Firefox OS (Cohen's d = 0.94). We find that switching to rapid releases has reduced weekend work (Cohen's d = 1.43) and working during the night (Cohen's d = 0.45). This result holds even when we limit the analyzes on the hired resources, i.e. considering only individuals with Mozilla foundation email address, although, the effect sizes are smaller for weekends (Cohen's d = 0.64) and nights (Cohen's d = 0.23). Moreover, we use dissimilarity word clouds and find that work during the weekend is more technical while work during the week expresses more positive sentiment with words like "good" and "nice". Our results suggest that moving to rapid releases have positive impact on the work health and work-life-balance of software engineers. However, caution is needed as our results are based on a limited set of quantitative data from a single organization.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Claes, M. and M\"antyl\"a, M. and Kuutila, M. and Adams, B.},
  month = may,
  year = {2017},
  keywords = {Data mining,Software,software engineering,Electronic mail,public domain software,Computer bugs,software developers,empirical software engineering,human factors,Psychology,abnormal working hours,bugzilla,burnouts,diseases,Diseases,dissimilarity word clouds,firefox,Firefox OS,mozilla,Mozilla Firefox work pattern,open source project,psychological diseases,psychology,rapid releases,release,software engineer work health,software engineer work-life-balance,software repositories approach,Switches,time pressure,weekend,work content,working pattern},
  pages = {243-247},
  file = {/Users/luigi/work/zotero/storage/P4DJSA59/Claes et al. - 2017 - Abnormal Working Hours Effect of Rapid Releases a.pdf;/Users/luigi/work/zotero/storage/YKYA5S8L/7962374.html}
}

@inproceedings{cartaxoUsingWebsitesMethod2017,
  title = {Using {{Q A Websites}} as a {{Method}} for {{Assessing Systematic Reviews}}},
  doi = {10.1109/MSR.2017.5},
  abstract = {Questions and Answers (Q\&A) websites maintain a long history of needs, problems, and challenges that software developers face. In contrast to Q\&A websites, which are strongly tied to practitioners' needs, there are systematic reviews (SRs), which, according to recent studies, lack a connection with software engineering practice. In this paper, we investigate this claim by assessing to what extent systematic reviews help to solve questions posted on Q\&A websites. To achieve this goal, we propose and evaluate a coverage method. We applied this method to a set of more than 600 questions related to agile software development. Results suggest that 12\% of the related questions were covered. When considering specific agile methods, the majority of them have coverage below 50\% or were not covered at all. We also identified 27 recurrent questions.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Cartaxo, B. and Pinto, G. and Ribeiro, D. and Kamei, F. and Santos, R. E. S. and da Silva, F. Q. B. and Soares, S.},
  month = may,
  year = {2017},
  keywords = {Programming,Software,software engineering,Software engineering,Web sites,Systematics,software developers,agile software development,Encoding,Focusing,Q\&A Web sites,questions and answers Web sites,Scrum (Software development),software prototyping,software reviews,systematic review assessment},
  pages = {238-242},
  file = {/Users/luigi/work/zotero/storage/3R9CC42Q/Cartaxo et al. - 2017 - Using Q A Websites as a Method for Assessing Syste.pdf;/Users/luigi/work/zotero/storage/SNW887GQ/7962373.html}
}

@inproceedings{orlovGeneticProgrammingWild2009,
  address = {New York, NY, USA},
  series = {{{GECCO}} '09},
  title = {Genetic {{Programming}} in the {{Wild}}: {{Evolving Unrestricted Bytecode}}},
  isbn = {978-1-60558-325-9},
  shorttitle = {Genetic {{Programming}} in the {{Wild}}},
  doi = {10.1145/1569901.1570042},
  abstract = {We describe a methodology for evolving Java bytecode, enabling the evolution of extant, unrestricted Java programs, or programs in other languages that compile to Java bytecode. Bytecode is evolved directly, without any intermediate genomic representation. Our approach is based upon the notion of compatible crossover, which produces correct programs by performing operand stack-, local variables-, and control flow-based compatibility checks on source and destination bytecode sections. This is in contrast to existing work that uses restricted subsets of the Java bytecode instruction set as a representation language for individuals in genetic programming. Given the huge universe of unrestricted Java bytecode, as is programs, our work enables the applications of evolution within this realm. We experimentally validate our methodology by both extensively testing the correctness of compatible crossover on arbitrary bytecode, and by running evolution on a program that exploits the richness of the Java virtual machine architecture and type system.},
  booktitle = {Proceedings of the 11th {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  publisher = {{ACM}},
  author = {Orlov, Michael and Sipper, Moshe},
  year = {2009},
  keywords = {software evolution,java bytecode},
  pages = {1043--1050},
  file = {/Users/luigi/work/zotero/storage/KH84DVYT/Orlov and Sipper - 2009 - Genetic Programming in the Wild Evolving Unrestri.pdf}
}

@inproceedings{tronicekRefactoringNGFlexibleJava2012,
  address = {New York, NY, USA},
  series = {{{SAC}} '12},
  title = {{{RefactoringNG}}: {{A Flexible Java Refactoring Tool}}},
  isbn = {978-1-4503-0857-1},
  shorttitle = {{{RefactoringNG}}},
  doi = {10.1145/2245276.2231959},
  abstract = {The Java programming language and the Java API evolve and this evolution certainly will continue in future. Upgrade to a new version of programming language or API is nowadays usually done manually. We describe a new flexible refactoring tool for the Java programming language that can upgrade the code almost automatically. The tool performs refactoring rules described in the special language based on the abstract syntax trees. Each rule consists of two abstract syntax trees: the pattern and the rewrite. First, we search for the pattern and then replace each pattern occurrence with the rewrite. Searching and replacement is performed on the abstract syntax trees that are built and fully attributed by the Java compiler. Complete syntactic and semantic information about the source code and flexibility in refactoring rules give the tool competitive advantage over most similar tools.},
  booktitle = {Proceedings of the 27th {{Annual ACM Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  author = {Tron\'i{\v c}ek, Zden{\v e}k},
  year = {2012},
  keywords = {Java,refactoring,API evolution,software evolution},
  pages = {1165--1170},
  file = {/Users/luigi/work/zotero/storage/5H3AMBT9/Troníček - 2012 - RefactoringNG A Flexible Java Refactoring Tool.pdf}
}

@inproceedings{ernstFamilyPolymorphism2001,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Family {{Polymorphism}}},
  isbn = {978-3-540-42206-8 978-3-540-45337-6},
  doi = {10.1007/3-540-45337-7_17},
  abstract = {This paper takes polymorphism to the multi-object level. Traditional inheritance, polymorphism, and late binding interact nicely to provide both flexibility and safety \textemdash{} when a method is invoked on an object via a polymorphic reference, late binding ensures that we get the appropriate implementation of that method for the actual object. We are granted the flexibility of using different kinds of objects and different method implementations, and we are guaranteed the safety of the combination. Nested classes, polymorphism, and late binding of nested classes interact similarly to provide both safety and flexibility at the level of multi-object systems. We are granted the flexibility of using different families of kinds of objects, and we are guaranteed the safety of the combination. This paper highlights the inability of traditional polymorphism to handle multiple objects, and presents family polymorphism as a way to overcome this problem. Family polymorphism has been implemented in the programming language gbeta, a generalized version of Beta, and the source code of this implementation is available under GPL.1},
  language = {en},
  booktitle = {{{ECOOP}} 2001 \textemdash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Ernst, Erik},
  month = jun,
  year = {2001},
  pages = {303-326},
  file = {/Users/luigi/work/zotero/storage/NIALHHAA/Ernst - 2001 - Family Polymorphism.pdf;/Users/luigi/work/zotero/storage/JNEHCZHY/3-540-45337-7_17.html}
}

@article{wadlerPropositionsTypes2015,
  title = {Propositions {{As Types}}},
  volume = {58},
  issn = {0001-0782},
  doi = {10.1145/2699407},
  abstract = {Connecting mathematical logic and computation, it ensures that some aspects of programming are absolute.},
  number = {12},
  journal = {Commun. ACM},
  author = {Wadler, Philip},
  month = nov,
  year = {2015},
  pages = {75--84},
  file = {/Users/luigi/work/zotero/storage/SALIDGBQ/Wadler - 2015 - Propositions As Types.pdf}
}

@inproceedings{ploegPracticalPrincipledFRP2015,
  address = {New York, NY, USA},
  series = {{{ICFP}} 2015},
  title = {Practical {{Principled FRP}}: {{Forget}} the {{Past}}, {{Change}} the {{Future}}, {{FRPNow}}!},
  isbn = {978-1-4503-3669-7},
  shorttitle = {Practical {{Principled FRP}}},
  doi = {10.1145/2784731.2784752},
  abstract = {We present a new interface for practical Functional Reactive Programming (FRP) that (1) is close in spirit to the original FRP ideas, (2) does not have the original space-leak problems, without using arrows or advanced types, and (3) provides a simple and expressive way for performing IO actions from FRP code. We also provide a denotational semantics for this new interface, and a technique (using Kripke logical relations) for reasoning about which FRP functions may "forget their past", i.e. which functions do not have an inherent space-leak. Finally, we show how we have implemented this interface as a Haskell library called FRPNow.},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {van der Ploeg, Atze and Claessen, Koen},
  year = {2015},
  keywords = {Functional Reactive Programming,Kripke Logical Relations,Purely Functional IO,Space-leak},
  pages = {302--314},
  file = {/Users/luigi/work/zotero/storage/Y5IZU8LK/Ploeg and Claessen - 2015 - Practical Principled FRP Forget the Past, Change .pdf}
}

@inproceedings{najdEverythingOldNew2016,
  address = {New York, NY, USA},
  series = {{{PEPM}} '16},
  title = {Everything {{Old}} Is {{New Again}}: {{Quoted Domain}}-Specific {{Languages}}},
  isbn = {978-1-4503-4097-7},
  shorttitle = {Everything {{Old}} Is {{New Again}}},
  doi = {10.1145/2847538.2847541},
  abstract = {We describe a new approach to implementing Domain-Specific Languages(DSLs), called Quoted DSLs (QDSLs), that is inspired by two old ideas:quasi-quotation, from McCarthy's Lisp of 1960, and the subformula principle of normal proofs, from Gentzen's natural deduction of 1935. QDSLs reuse facilities provided for the host language, since host and quoted terms share the same syntax, type system, and normalisation rules. QDSL terms are normalised to a canonical form, inspired by the subformula principle, which guarantees that one can use higher-order types in the source while guaranteeing first-order types in the target, and enables using types to guide fusion. We test our ideas by re-implementing Feldspar, which was originally implemented as an Embedded DSL (EDSL), as a QDSL; and we compare the QDSL and EDSL variants. The two variants produce identical code.},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  publisher = {{ACM}},
  author = {Najd, Shayan and Lindley, Sam and Svenningsson, Josef and Wadler, Philip},
  year = {2016},
  keywords = {domain-specific language,DSL,EDSL,embedded language,normalisation,QDSL,quotation,subformula principle},
  pages = {25--36},
  file = {/Users/luigi/work/zotero/storage/6QX3S8NS/Najd et al. - 2016 - Everything Old is New Again Quoted Domain-specifi.pdf}
}

@article{gedikElasticScalingData2014,
  title = {Elastic {{Scaling}} for {{Data Stream Processing}}},
  volume = {25},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2013.295},
  abstract = {This article addresses the profitability problem associated with auto-parallelization of general-purpose distributed data stream processing applications. Auto-parallelization involves locating regions in the application's data flow graph that can be replicated at run-time to apply data partitioning, in order to achieve scale. In order to make auto-parallelization effective in practice, the profitability question needs to be answered: How many parallel channels provide the best throughput? The answer to this question changes depending on the workload dynamics and resource availability at run-time. In this article, we propose an elastic auto-parallelization solution that can dynamically adjust the number of channels used to achieve high throughput without unnecessarily wasting resources. Most importantly, our solution can handle partitioned stateful operators via run-time state migration, which is fully transparent to the application developers. We provide an implementation and evaluation of the system on an industrial-strength data stream processing platform to validate our solution.},
  number = {6},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  author = {Gedik, B. and Schneider, S. and Hirzel, M. and Wu, K. L.},
  month = jun,
  year = {2014},
  keywords = {Runtime,Parallel processing,Indexes,Measurement,data analysis,application data flow graph,auto-parallelization,Availability,data flow graphs,data partitioning,Data stream processing,elastic scaling,elasticity,general-purpose distributed data stream processing applications,industrial-strength data stream processing platform,parallel channels,parallel processing,parallelization,profitability,profitability problem,resource availability,run-time state migration,Safety,Throughput,workload dynamics},
  pages = {1447-1463},
  file = {/Users/luigi/work/zotero/storage/9PRB7ZJ2/Gedik et al. - 2014 - Elastic Scaling for Data Stream Processing.pdf;/Users/luigi/work/zotero/storage/EIKRYV75/Gedik et al. - 2014 - Elastic Scaling for Data Stream Processing.pdf;/Users/luigi/work/zotero/storage/ZSGTXRAX/6678504.html}
}

@book{rossbergTypeClasses2002,
  title = {Beyond {{Type Classes}}},
  abstract = {We discuss type classes in the context of the Chameleon language, a Haskell-style language where overloading resolution is expressed in terms of the meta-language of Constraint Handling Rules (CHRs). In a first step, we show how to encode Haskell's single-parameter type classes into Chameleon. The encoding works by providing an approrpriate set of CHRs which mimic the Haskell conditions. We also consider constructor classes, multi-parameter type classes and functional dependencies. Chameleon provides a testbed to experiment with new overloading features. We show how some novel features such as universal quantification in context can naturally be expressed in Chameleon.},
  author = {Rossberg, Andreas and Sulzmann, Martin},
  year = {2002},
  file = {/Users/luigi/work/zotero/storage/F4UY4QYZ/Rossberg and Sulzmann - 2002 - Beyond Type Classes.pdf;/Users/luigi/work/zotero/storage/UJZ2NRN9/summary.html}
}

@inproceedings{rompfFunctionalPearlSQL2015,
  address = {New York, NY, USA},
  series = {{{ICFP}} 2015},
  title = {Functional {{Pearl}}: {{A SQL}} to {{C Compiler}} in 500 {{Lines}} of {{Code}}},
  isbn = {978-1-4503-3669-7},
  shorttitle = {Functional {{Pearl}}},
  doi = {10.1145/2784731.2784760},
  abstract = {We present the design and implementation of a SQL query processor that outperforms existing database systems and is written in just about 500 lines of Scala code -- a convincing case study that high-level functional programming can handily beat C for systems-level programming where the last drop of performance matters. The key enabler is a shift in perspective towards generative programming. The core of the query engine is an interpreter for relational algebra operations, written in Scala. Using the open-source LMS Framework (Lightweight Modular Staging), we turn this interpreter into a query compiler with very low effort. To do so, we capitalize on an old and widely known result from partial evaluation known as Futamura projections, which state that a program that can specialize an interpreter to any given input program is equivalent to a compiler. In this pearl, we discuss LMS programming patterns such as mixed-stage data structures (e.g. data records with static schema and dynamic field components) and techniques to generate low-level C code, including specialized data structures and data loading primitives.},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Rompf, Tiark and Amin, Nada},
  year = {2015},
  keywords = {Futamura Projections,Generative Programming,Query Compilation,SQL,Staging},
  pages = {2--9},
  file = {/Users/luigi/work/zotero/storage/QMGANW9U/Rompf and Amin - 2015 - Functional Pearl A SQL to C Compiler in 500 Lines.pdf}
}

@inproceedings{nagarajApproximatingFlowsensitivePointer2015,
  address = {Washington, DC, USA},
  series = {{{CGO}} '15},
  title = {Approximating {{Flow}}-Sensitive {{Pointer Analysis Using Frequent Itemset Mining}}},
  isbn = {978-1-4799-8161-8},
  abstract = {Pointer alias analysis is a well researched problem in the area of compilers and program verification. Many recent works in this area have focused on flow-sensitivity due to the additional precision it offers. However, a flow-sensitive analysis is computationally expensive, thus, preventing its use in larger programs. In this work, we observe that a number of object sets, consisting of tens to hundreds of objects appear together and frequently in many points-to sets. By approximating each of these object sets by a single object, we can speedup computation of points-to sets. Although the proposed approach incurs a slight loss in precision, it is shown to be safe. We use a well known data mining technique called frequent itemset mining to find these frequently occurring objects. We compare our approximation to a fully flow-sensitive pointer analysis on a set of ten benchmarks. We measure precision loss using two common client analysis queries and report an average precision loss of 0.25\% on one measure and 1.40\% on the other. The proposed approach results in a speedup of upto 12.9x (and an average speedup of 6.2x) in computing the points-to sets.},
  booktitle = {Proceedings of the 13th {{Annual IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}}},
  publisher = {{IEEE Computer Society}},
  author = {Nagaraj, Vaivaswatha and Govindarajan, R.},
  year = {2015},
  pages = {225--234},
  file = {/Users/luigi/work/zotero/storage/LNJF5GH2/Nagaraj and Govindarajan - 2015 - Approximating Flow-sensitive Pointer Analysis Usin.pdf}
}

@inproceedings{bayneAlwaysavailableStaticDynamic2011,
  address = {New York, NY, USA},
  series = {{{ICSE}} '11},
  title = {Always-Available {{Static}} and {{Dynamic Feedback}}},
  isbn = {978-1-4503-0445-0},
  doi = {10.1145/1985793.1985864},
  abstract = {Developers who write code in a statically typed language are denied the ability to obtain dynamic feedback by executing their code during periods when it fails the static type checker. They are further confined to the static typing discipline during times in the development process where it does not yield the highest productivity. If they opt instead to use a dynamic language, they forgo the many benefits of static typing, including machine-checked documentation, improved correctness and reliability, tool support (such as for refactoring), and better runtime performance. We present a novel approach to giving developers the benefits of both static and dynamic typing, throughout the development process, and without the burden of manually separating their program into statically and dynamically-typed parts. Our approach, which is intended for temporary use during the development process, relaxes the static type system and provides a semantics for many type-incorrect programs. It defers type errors to run time, or suppresses them if they do not affect runtime semantics. We implemented our approach in a publicly available tool, DuctileJ, for the Java language. In case studies, DuctileJ conferred benefits both during prototyping and during the evolution of existing code.},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Bayne, Michael and Cook, Richard and Ernst, Michael D.},
  year = {2011},
  keywords = {refactoring,gradual typing,dynamic typing,hybrid typing,productivity,prototyping,static typing,type error},
  pages = {521--530},
  file = {/Users/luigi/work/zotero/storage/M4FXS9SQ/Bayne et al. - 2011 - Always-available Static and Dynamic Feedback.pdf}
}

@inproceedings{brownBlackboxLargeScale2014,
  address = {New York, NY, USA},
  series = {{{SIGCSE}} '14},
  title = {Blackbox: {{A Large Scale Repository}} of {{Novice Programmers}}' {{Activity}}},
  isbn = {978-1-4503-2605-6},
  shorttitle = {Blackbox},
  doi = {10.1145/2538862.2538924},
  abstract = {Automatically observing and recording the programming behaviour of novices is an established computing education research technique. However, prior studies have been conducted at a single institution on a small or medium scale, without the possibility of data re-use. Now, the widespread availability of always-on Internet access allows for data collection at a much larger, global scale. In this paper we report on the Blackbox project, begun in June 2013. Blackbox is a perpetual data collection project that collects data from worldwide users of the BlueJ IDE -- a programming environment designed for novice programmers. Over one hundred thousand users have already opted-in to Blackbox. The collected data is anonymous and is available to other researchers for use in their own studies, thus benefitting the larger research community. In this paper, we describe the data available via Blackbox, show some examples of analyses that can be performed using the collected data, and discuss some of the analysis challenges that lie ahead.},
  booktitle = {Proceedings of the 45th {{ACM Technical Symposium}} on {{Computer Science Education}}},
  publisher = {{ACM}},
  author = {Brown, Neil Christopher Charles and K\"olling, Michael and McCall, Davin and Utting, Ian},
  year = {2014},
  keywords = {blackbox,BlueJ,data collection,programming education},
  pages = {223--228},
  file = {/Users/luigi/work/zotero/storage/BEFNXL65/Brown et al. - 2014 - Blackbox A Large Scale Repository of Novice Progr.pdf}
}

@inproceedings{begelAnalyzeThis1452014,
  address = {New York, NY, USA},
  series = {{{ICSE}} 2014},
  title = {Analyze {{This}}! 145 {{Questions}} for {{Data Scientists}} in {{Software Engineering}}},
  isbn = {978-1-4503-2756-5},
  doi = {10.1145/2568225.2568233},
  abstract = {In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Begel, Andrew and Zimmermann, Thomas},
  year = {2014},
  keywords = {Analytics,Data Science,Software Engineering},
  pages = {12--23},
  file = {/Users/luigi/work/zotero/storage/RJZVMR5U/Begel and Zimmermann - 2014 - Analyze This! 145 Questions for Data Scientists in.pdf}
}

@article{myersProgrammersAreUsers2016,
  title = {Programmers {{Are Users Too}}: {{Human}}-{{Centered Methods}} for {{Improving Programming Tools}}},
  volume = {49},
  issn = {0018-9162},
  shorttitle = {Programmers {{Are Users Too}}},
  doi = {10.1109/MC.2016.200},
  abstract = {Human-centered methods can help researchers better understand and meet programmers' needs. Because programming is a human activity, many of these methods can be used without change. However, some programmer needs require new methods, which can also be applied to domains other than software engineering. This article features five Web extras. The video at https://youtu.be/4PH9-qi-yTQ demonstrates Azurite, an Eclipse plug-in with a selective undo feature that lets programmers more easily backtrack their code. The video at https://youtu.be/gOSlR62-rd8 describes Graphite, an Eclipse plug-in offering active code completion, a simple but powerful technique that integrates useful code-generation tools directly into the editor. The video at https://youtu.be/zyrqcYxqDtI describes HANDS, a new programming system that emphasizes usability by building on children's and beginning programmers' natural problem-solving tendencies. The video extra at https://youtu.be/80EctbI7PFc describes Whyline, a debugging tool that lets developers ask questions about their program's output and behavior. The video at https://youtu.be/3L4MK2dG\_6k demonstrates the prototype for Whyline, a debugging tool that lets developers pose questions about their program's output.},
  number = {7},
  journal = {Computer},
  author = {Myers, B. A. and Ko, A. J. and LaToza, T. D. and Yoon, Y.},
  month = jul,
  year = {2016},
  keywords = {Java,Data mining,data mining,Programming,software development,software engineering,Software engineering,program debugging,software tools,software psychology,Eclipse plug-in,A/B testing,active code completion,Azurite,contextual inquiry,debugging tool,end-user software engineering,evaluation studies,exploratory lab studies,Graphite,HANDS,HCI,human activity,Human computer interaction,human-centered computing,Human-centered computing,human-centered methods,human-computer interaction,Human-computer interaction,log analysis,Natural language processing,natural problem-solving tendencies,natural-programming elicitation,programming tools,rapid prototyping,studies of program constructs,think-aloud usability evaluation,undo feature,user centred design,user interfaces,Whyline},
  pages = {44-52},
  file = {/Users/luigi/work/zotero/storage/EVAECBD9/Myers et al. - 2016 - Programmers Are Users Too Human-Centered Methods .pdf;/Users/luigi/work/zotero/storage/S5NXT9GC/7503516.html}
}

@inproceedings{zhangContextsensitiveDatadependenceAnalysis2017,
  address = {New York, NY, USA},
  series = {{{POPL}} 2017},
  title = {Context-Sensitive {{Data}}-Dependence {{Analysis}} via {{Linear Conjunctive Language Reachability}}},
  isbn = {978-1-4503-4660-3},
  doi = {10.1145/3009837.3009848},
  abstract = {Many program analysis problems can be formulated as graph reachability problems. In the literature, context-free language (CFL) reachability has been the most popular formulation and can be computed in subcubic time. The context-sensitive data-dependence analysis is a fundamental abstraction that can express a broad range of program analysis problems. It essentially describes an interleaved matched-parenthesis language reachability problem. The language is not context-free, and the problem is well-known to be undecidable. In practice, many program analyses adopt CFL-reachability to exactly model the matched parentheses for either context-sensitivity or structure-transmitted data-dependence, but not both. Thus, the CFL-reachability formulation for context-sensitive data-dependence analysis is inherently an approximation. To support more precise and scalable analyses, this paper introduces linear conjunctive language (LCL) reachability, a new, expressive class of graph reachability. LCL not only contains the interleaved matched-parenthesis language, but is also closed under all set-theoretic operations. Given a graph with n nodes and m edges, we propose an O(mn) time approximation algorithm for solving all-pairs LCL-reachability, which is asymptotically better than known CFL-reachability algorithms. Our formulation and algorithm offer a new perspective on attacking the aforementioned undecidable problem \^a the LCL-reachability formulation is exact, while the LCL-reachability algorithm yields a sound approximation. We have applied the LCL-reachability framework to two existing client analyses. The experimental results show that the LCL-reachability framework is both more precise and scalable than the traditional CFL-reachability framework. This paper opens up the opportunity to exploit LCL-reachability in program analysis.},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Zhang, Qirun and Su, Zhendong},
  year = {2017},
  keywords = {program analysis,Context-free language reachability,linear conjunctive grammar,trellis automata},
  pages = {344--358},
  file = {/Users/luigi/work/zotero/storage/9LBZX4PJ/Zhang and Su - 2017 - Context-sensitive Data-dependence Analysis via Lin.pdf}
}

@inproceedings{bartmanSrcQLSyntaxawareQuery2017,
  title = {{{srcQL}}: {{A}} Syntax-Aware Query Language for Source Code},
  shorttitle = {{{srcQL}}},
  doi = {10.1109/SANER.2017.7884655},
  abstract = {A tool and domain specific language for querying source code is introduced and demonstrated. The tool, srcQL, allows for the querying of source code using the syntax of the language to identify patterns within source code documents. srcQL is built upon srcML, a widely used XML representation of source code, to identify the syntactic contexts being queried. srcML inserts XML tags into the source code to mark syntactic constructs. srcQL uses a combination of XPath on srcML, regular expressions, and syntactic patterns within a query. The syntactic patterns are snippets of source code that supports the use of logical variables which are unified during the query process. This allows for very complex patterns to be easily formulated and queried. The tool is implemented (in C++) and a number of queries are presented to demonstrate the approach. srcQL currently supports C++ and scales to large systems.},
  booktitle = {2017 {{IEEE}} 24th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Bartman, B. and Newman, C. D. and Collard, M. L. and Maletic, J. I.},
  month = feb,
  year = {2017},
  keywords = {computational linguistics,domain specific language,XML,source code (software),Context,Pattern matching,query languages,Syntactics,Reactive power,C++ languages,Database languages,language syntax,query process,source code documents,source code querying,Source code querying,srcML,srcQL,syntactic contexts,syntactic patterns,syntactic search,syntax-aware query language,XML representation,XML tags,XPath},
  pages = {467-471},
  file = {/Users/luigi/work/zotero/storage/XSGRTIDR/Bartman et al. - 2017 - srcQL A syntax-aware query language for source co.pdf;/Users/luigi/work/zotero/storage/KGJRY66L/7884655.html}
}

@inproceedings{collardSrcMLExploreAnalyze2016,
  title = {{{srcML}} 1.0: {{Explore}}, {{Analyze}}, and {{Manipulate Source Code}}},
  shorttitle = {{{srcML}} 1.0},
  doi = {10.1109/ICSME.2016.36},
  abstract = {Summary form only given. This technology briefing is intended for those interested in constructing custom software analysis and manipulation tools to support research or commercial applications. srcML (srcML.org) is an infrastructure consisting of an XML representation for C/C++/C\#/Java source code along with efficient parsing technology to convert source code to-and-from the srcML format. The briefing describes srcML, the toolkit, and the application of XPath and XSLT to query and modify source code. Additionally, a short tutorial of how to use srcML and XML tools to construct custom analysis and manipulation tools will be conducted.},
  booktitle = {2016 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Collard, M. L. and Maletic, J. I.},
  month = oct,
  year = {2016},
  keywords = {Software maintenance,program transformation,program diagnostics,source code,software engineering,XML,source code (software),static program analysis,srcML,XML representation,Conferences,software analysis tool,software manipulation tool,srcML format,Tutorials},
  pages = {649-649},
  file = {/Users/luigi/work/zotero/storage/VQFII8LF/ICSME16-srcML.pdf;/Users/luigi/work/zotero/storage/XNQ3PFUB/7816536.html}
}

@inproceedings{newmanSrcTypeToolEfficient2016,
  title = {{{srcType}}: {{A Tool}} for {{Efficient Static Type Resolution}}},
  shorttitle = {{{srcType}}},
  doi = {10.1109/ICSME.2016.38},
  abstract = {An efficient, static type resolution tool is presented. The tool is implemented on top of srcML, an XML representation of source code and abstract syntax. The approach computes the type of every identifier (i.e., function names and variable names) within the provided body of code. The result is a dictionary that can be used to lookup the type of each name. Type information includes metadata such as constness, class membership, aliasing, line number, file, and namespace. The approach is highly scalable and can generate a dictionary for Linux (13 MLOC) in less than 7 minutes. The tool is open source under a GPL license and available for download at srcML.org.},
  booktitle = {2016 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Newman, C. D. and Maletic, J. I. and Collard, M. L.},
  month = oct,
  year = {2016},
  keywords = {Runtime,computational linguistics,Software maintenance,program diagnostics,XML,static analysis tool,public domain software,software tools,source code (software),Context,metadata,Metadata,srcML,Conferences,abstract syntax,dictionaries,Dictionaries,GPL license,Linux,Linux dictionary,meta data,open source tool,source code representation,srcType,static type resolution,static type resolution tool,type information},
  pages = {604-606},
  file = {/Users/luigi/work/zotero/storage/UN3JHJAR/Newman et al. - 2016 - srcType A Tool for Efficient Static Type Resoluti.pdf;/Users/luigi/work/zotero/storage/783WUYZ7/7816517.html}
}

@inproceedings{guzmanSentimentAnalysisCommit2014,
  address = {New York, NY, USA},
  series = {{{MSR}} 2014},
  title = {Sentiment {{Analysis}} of {{Commit Comments}} in {{GitHub}}: {{An Empirical Study}}},
  isbn = {978-1-4503-2863-0},
  shorttitle = {Sentiment {{Analysis}} of {{Commit Comments}} in {{GitHub}}},
  doi = {10.1145/2597073.2597118},
  abstract = {Emotions have a high impact in productivity, task quality, creativity, group rapport and job satisfaction. In this work we use lexical sentiment analysis to study emotions expressed in commit comments of different open source projects and analyze their relationship with different factors such as used programming language, time and day of the week in which the commit was made, team distribution and project approval. Our results show that projects developed in Java tend to have more negative commit comments, and that projects that have more distributed teams tend to have a higher positive polarity in their emotional content. Additionally, we found that commit comments written on Mondays tend to a more negative emotion. While our results need to be confirmed by a more representative sample they are an initial step into the study of emotions and related factors in open source projects.},
  booktitle = {Proceedings of the 11th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Guzman, Emitza and Az\'ocar, David and Li, Yang},
  year = {2014},
  keywords = {Human Factors in Software Engineering,Sentiment Analysis},
  pages = {352--355},
  file = {/Users/luigi/work/zotero/storage/UDYDBBTJ/Guzman et al. - 2014 - Sentiment Analysis of Commit Comments in GitHub A.pdf}
}

@inproceedings{linRustLanguageHigh2016,
  address = {New York, NY, USA},
  series = {{{ISMM}} 2016},
  title = {Rust {{As}} a {{Language}} for {{High Performance GC Implementation}}},
  isbn = {978-1-4503-4317-6},
  doi = {10.1145/2926697.2926707},
  abstract = {High performance garbage collectors build upon performance-critical low-level code, typically exhibit multiple levels of concurrency, and are prone to subtle bugs. Implementing, debugging and maintaining such collectors can therefore be extremely challenging. The choice of implementation language is a crucial consideration when building a collector. Typically, the drive for performance and the need for efficient support of low-level memory operations leads to the use of low-level languages like C or C++, which offer little by way of safety and software engineering benefits. This risks undermining the robustness and flexibility of the collector design. Rust's ownership model, lifetime specification, and reference borrowing deliver safety guarantees through a powerful static checker with little runtime overhead. These features make Rust a compelling candidate for a collector implementation language, but they come with restrictions that threaten expressiveness and efficiency. We describe our experience implementing an Immix garbage collector in Rust and C. We discuss the benefits of Rust, the obstacles encountered, and how we overcame them. We show that our Immix implementation has almost identical performance on micro benchmarks, compared to its implementation in C, and outperforms the popular BDW collector on the gcbench micro benchmark. We find that Rust's safety features do not create significant barriers to implementing a high performance collector. Though memory managers are usually considered low-level, our high performance implementation relies on very little unsafe code, with the vast majority of the implementation benefiting from Rust's safety. We see our experience as a compelling proof-of-concept of Rust as an implementation language for high performance garbage collection.},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN International Symposium}} on {{Memory Management}}},
  publisher = {{ACM}},
  author = {Lin, Yi and Blackburn, Stephen M. and Hosking, Antony L. and Norrish, Michael},
  year = {2016},
  keywords = {garbage collection,memory management,Rust},
  pages = {89--98},
  file = {/Users/luigi/work/zotero/storage/WQTA3E67/Lin et al. - 2016 - Rust As a Language for High Performance GC Impleme.pdf}
}

@inproceedings{zhangAbstractionRefinementProgram2014,
  address = {New York, NY, USA},
  series = {{{PLDI}} '14},
  title = {On {{Abstraction Refinement}} for {{Program Analyses}} in {{Datalog}}},
  isbn = {978-1-4503-2784-8},
  doi = {10.1145/2594291.2594327},
  abstract = {A central task for a program analysis concerns how to efficiently find a program abstraction that keeps only information relevant for proving properties of interest. We present a new approach for finding such abstractions for program analyses written in Datalog. Our approach is based on counterexample-guided abstraction refinement: when a Datalog analysis run fails using an abstraction, it seeks to generalize the cause of the failure to other abstractions, and pick a new abstraction that avoids a similar failure. Our solution uses a boolean satisfiability formulation that is general, complete, and optimal: it is independent of the Datalog solver, it generalizes the failure of an abstraction to as many other abstractions as possible, and it identifies the cheapest refined abstraction to try next. We show the performance of our approach on a pointer analysis and a typestate analysis, on eight real-world Java benchmark programs.},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Zhang, Xin and Mangal, Ravi and Grigore, Radu and Naik, Mayur and Yang, Hongseok},
  year = {2014},
  pages = {239--248},
  file = {/Users/luigi/work/zotero/storage/CZST8AZI/Zhang et al. - 2014 - On Abstraction Refinement for Program Analyses in .pdf}
}

@article{kanvarHeapAbstractionsStatic2016,
  title = {Heap {{Abstractions}} for {{Static Analysis}}},
  volume = {49},
  issn = {0360-0300},
  doi = {10.1145/2931098},
  abstract = {Heap data is potentially unbounded and seemingly arbitrary. Hence, unlike stack and static data, heap data cannot be abstracted in terms of a fixed set of program variables. This makes it an interesting topic of study and there is an abundance of literature employing heap abstractions. Although most studies have addressed similar concerns, insights gained in one description of heap abstraction may not directly carry over to some other description. In our search of a unified theme, we view heap abstraction as consisting of two steps: (a) heap modelling, which is the process of representing a heap memory (i.e., an unbounded set of concrete locations) as a heap model (i.e., an unbounded set of abstract locations), and (b) summarization, which is the process of bounding the heap model by merging multiple abstract locations into summary locations. We classify the heap models as storeless, store based, and hybrid. We describe various summarization techniques based on k-limiting, allocation sites, patterns, variables, other generic instrumentation predicates, and higher-order logics. This approach allows us to compare the insights of a large number of seemingly dissimilar heap abstractions and also paves the way for creating new abstractions by mix and match of models and summarization techniques.},
  number = {2},
  journal = {ACM Comput. Surv.},
  author = {Kanvar, Vini and Khedker, Uday P.},
  month = jun,
  year = {2016},
  keywords = {heap,static analysis,Abstraction,pointers,shape analysis,store based,storeless,summarization},
  pages = {29:1--29:47},
  file = {/Users/luigi/work/zotero/storage/R6GQT9CW/Kanvar and Khedker - 2016 - Heap Abstractions for Static Analysis.pdf}
}

@book{pierceTypesProgrammingLanguages2002,
  edition = {1st},
  title = {Types and {{Programming Languages}}},
  isbn = {978-0-262-16209-8},
  abstract = {A type system is a syntactic method for automatically checking the absence of certain erroneous behaviors by classifying program phrases according to the kinds of values they compute. The study of type systems -- and of programming languages from a type-theoretic perspective -- has important applications in software engineering, language design, high-performance compilers, and security.This text provides a comprehensive introduction both to type systems in computer science and to the basic theory of programming languages. The approach is pragmatic and operational; each new concept is motivated by programming examples and the more theoretical sections are driven by the needs of implementations. Each chapter is accompanied by numerous exercises and solutions, as well as a running implementation, available via the Web. Dependencies between chapters are explicitly identified, allowing readers to choose a variety of paths through the material.The core topics include the untyped lambda-calculus, simple type systems, type reconstruction, universal and existential polymorphism, subtyping, bounded quantification, recursive types, kinds, and type operators. Extended case studies develop a variety of approaches to modeling the features of object-oriented languages.},
  publisher = {{The MIT Press}},
  author = {Pierce, Benjamin C.},
  year = {2002},
  keywords = {tapl},
  file = {/Users/luigi/work/zotero/storage/7PFLWK8V/Pierce - 2002 - Types and Programming Languages.pdf}
}

@article{stracheyFundamentalConceptsProgramming2000,
  title = {Fundamental {{Concepts}} in {{Programming Languages}}},
  volume = {13},
  issn = {1388-3690, 1573-0557},
  doi = {10.1023/A:1010000313106},
  abstract = {This paper forms the substance of a course of lectures given at the International Summer School in Computer Programming at Copenhagen in August, 1967. The lectures were originally given from notes and the paper was written after the course was finished. In spite of this, and only partly because of the shortage of time, the paper still retains many of the shortcomings of a lecture course. The chief of these are an uncertainty of aim\textemdash{}it is never quite clear what sort of audience there will be for such lectures\textemdash{}and an associated switching from formal to informal modes of presentation which may well be less acceptable in print than it is natural in the lecture room. For these (and other) faults, I apologise to the reader.There are numerous references throughout the course to CPL [1\textendash{}3]. This is a programming language which has been under development since 1962 at Cambridge and London and Oxford. It has served as a vehicle for research into both programming languages and the design of compilers. Partial implementations exist at Cambridge and London. The language is still evolving so that there is no definitive manual available yet. We hope to reach another resting point in its evolution quite soon and to produce a compiler and reference manuals for this version. The compiler will probably be written in such a way that it is relatively easyto transfer it to another machine, and in the first instance we hope to establish it on three or four machines more or less at the same time.The lack of a precise formulation for CPL should not cause much difficulty in this course, as we are primarily concerned with the ideas and concepts involved rather than with their precise representation in a programming language.},
  language = {en},
  number = {1-2},
  journal = {Higher-Order and Symbolic Computation},
  author = {Strachey, Christopher},
  month = apr,
  year = {2000},
  pages = {11-49},
  file = {/Users/luigi/work/zotero/storage/8TGV69IU/Strachey - 2000 - Fundamental Concepts in Programming Languages.pdf;/Users/luigi/work/zotero/storage/32ITHINJ/A1010000313106.html}
}

@inproceedings{altidorTamingWildcardsCombining2011,
  address = {New York, NY, USA},
  series = {{{PLDI}} '11},
  title = {Taming the {{Wildcards}}: {{Combining Definition}}- and {{Use}}-Site {{Variance}}},
  isbn = {978-1-4503-0663-8},
  shorttitle = {Taming the {{Wildcards}}},
  doi = {10.1145/1993498.1993569},
  abstract = {Variance allows the safe integration of parametric and subtype polymorphism. Two flavors of variance, definition-site versus use-site variance, have been studied and have had their merits hotly debated. Definition-site variance (as in Scala and C\#) offers simple type-instantiation rules, but causes fractured definitions of naturally invariant classes; Use-site variance (as in Java) offers simplicity in class definitions, yet complex type-instantiation rules that elude most programmers. We present a unifying framework for reasoning about variance. Our framework is quite simple and entirely denotational, that is, it evokes directly the definition of variance with a small core calculus that does not depend on specific type systems. This general framework can have multiple applications to combine the best of both worlds: for instance, it can be used to add use-site variance annotations to the Scala type system. We show one such application in detail: we extend the Java type system with a mechanism that modularly infers the definition-site variance of type parameters, while allowing use-site variance annotations on any type-instantiation. Applying our technique to six Java generic libraries (including the Java core library) shows that 20-58 (depending on the library) of generic definitions are inferred to have single-variance; 8-63\% of method signatures can be relaxed through this inference, and up to 91\% of existing wildcard annotations are unnecessary and can be elided.},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Altidor, John and Huang, Shan Shan and Smaragdakis, Yannis},
  year = {2011},
  keywords = {definition-site variance,language extensions,use-site variance,variance,wildcards},
  pages = {602--613},
  file = {/Users/luigi/work/zotero/storage/N3CWKXQ9/Altidor et al. - 2011 - Taming the Wildcards Combining Definition- and Us.pdf}
}

@inproceedings{dolanPolymorphismSubtypingType2017,
  address = {New York, NY, USA},
  series = {{{POPL}} 2017},
  title = {Polymorphism, {{Subtyping}}, and {{Type Inference}} in {{MLsub}}},
  isbn = {978-1-4503-4660-3},
  doi = {10.1145/3009837.3009882},
  abstract = {We present a type system combining subtyping and ML-style parametric polymorphism. Unlike previous work, our system supports type inference and has compact principal types. We demonstrate this system in the minimal language MLsub, which types a strict superset of core ML programs.   This is made possible by keeping a strict separation between the types used to describe inputs and those used to describe outputs, and extending the classical unification algorithm to handle subtyping constraints between these input and output types. Principal types are kept compact by type simplification, which exploits deep connections between subtyping and the algebra of regular languages. An implementation is available online.},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Dolan, Stephen and Mycroft, Alan},
  year = {2017},
  keywords = {Algebra,Polymorphism,Subtyping,Type Inference},
  pages = {60--72},
  file = {/Users/luigi/work/zotero/storage/2NERMPEG/Dolan and Mycroft - 2017 - Polymorphism, Subtyping, and Type Inference in MLs.pdf}
}

@inproceedings{vaziriDeclarativeObjectIdentity2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Declarative {{Object Identity Using Relation Types}}},
  isbn = {978-3-540-73588-5 978-3-540-73589-2},
  doi = {10.1007/978-3-540-73589-2_4},
  abstract = {Object-oriented languages define the identity of an object to be an address-based object identifier. The programmer may customize the notion of object identity by overriding the equals() and hashCode() methods following a specified contract. This customization often introduces latent errors, since the contract is unenforced and at times impossible to satisfy, and its implementation requires tedious and error-prone boilerplate code. Relation types are a programming model in which object identity is defined declaratively, obviating the need for equals() and hashCode() methods. This entails a stricter contract: identity never changes during an execution. We formalize the model as an adaptation of Featherweight Java, and implement it by extending Java with relation types. Experiments on a set of Java programs show that the majority of classes that override equals() can be refactored into relation types, and that most of the remainder are buggy or fragile.},
  language = {en},
  booktitle = {{{ECOOP}} 2007 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Vaziri, Mandana and Tip, Frank and Fink, Stephen and Dolby, Julian},
  month = jul,
  year = {2007},
  pages = {54-78},
  file = {/Users/luigi/work/zotero/storage/3SMW9WTT/Vaziri et al. - 2007 - Declarative Object Identity Using Relation Types.pdf;/Users/luigi/work/zotero/storage/HVIIFBLL/978-3-540-73589-2_4.html}
}

@inproceedings{kechagiaUndocumentedUncheckedExceptions2014,
  address = {New York, NY, USA},
  series = {{{MSR}} 2014},
  title = {Undocumented and {{Unchecked}}: {{Exceptions That Spell Trouble}}},
  isbn = {978-1-4503-2863-0},
  shorttitle = {Undocumented and {{Unchecked}}},
  doi = {10.1145/2597073.2597089},
  abstract = {Modern programs rely on large application programming interfaces (APIs). The Android framework comprises 231 core APIs, and is used by countless developers. We examine a sample of 4,900 distinct crash stack traces from 1,800 different Android applications, looking for Android API methods with undocumented exceptions that are part of application crashes. For the purposes of this study, we take as a reference the version 15 of the Android API, which matches our stack traces. Our results show that a significant number of crashes (19\%) might have been avoided if these methods had the corresponding exceptions documented as part of their interface.},
  booktitle = {Proceedings of the 11th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {Kechagia, Maria and Spinellis, Diomidis},
  year = {2014},
  keywords = {stack traces,exceptions,APIs,mobile applications},
  pages = {312--315},
  file = {/Users/luigi/work/zotero/storage/2QCM2NHZ/Kechagia and Spinellis - 2014 - Undocumented and Unchecked Exceptions That Spell .pdf}
}

@inproceedings{saiedVisualizationBasedAPI2015,
  title = {Visualization Based {{API}} Usage Patterns Refining},
  doi = {10.1109/VISSOFT.2015.7332428},
  abstract = {Learning to use existing or new software libraries is a difficult task for software developers, which would impede their productivity. Most of existing work provided different techniques to mine API usage patterns from client programs, in order to help developers to understand and use existing libraries. However, considering only client programs to identify API usage patterns, is a strong constraint as collecting several similar client programs for an API is not a trivial task. And even if these clients are available, all the usage scenarios of the API of interest may not be covered by those clients. In this paper, we propose a visualization based approach for the refinement of Client-based Usage Patterns. We first visualize the patterns structure. Then we enrich the patterns with API methods that are semantically related to them, and thus may contribute together to the implementation of a particular functionality for potential client programs.},
  booktitle = {2015 {{IEEE}} 3rd {{Working Conference}} on {{Software Visualization}} ({{VISSOFT}})},
  author = {Saied, M. A. and Benomar, O. and Sahraoui, H.},
  month = sep,
  year = {2015},
  keywords = {Libraries,application program interfaces,Software,Semantics,application programming interfaces,client programs,Documentation,API usage patterns refinement,data visualisation,Layout,Matrix decomposition,patterns structure visualization,Visualization},
  pages = {155-159},
  file = {/Users/luigi/work/zotero/storage/QV6CWF22/Saied et al. - 2015 - Visualization based API usage patterns refining.pdf;/Users/luigi/work/zotero/storage/647D5HGF/7332428.html}
}

@inproceedings{camposCommonBugFixPatterns2017,
  title = {Common {{Bug}}-{{Fix Patterns}}: {{A Large}}-{{Scale Observational Study}}},
  shorttitle = {Common {{Bug}}-{{Fix Patterns}}},
  doi = {10.1109/ESEM.2017.55},
  abstract = {[Background]: There are more bugs in real-world programs than human programmers can realistically address. Several approaches have been proposed to aid debugging. A recent research direction that has been increasingly gaining interest to address the reduction of costs associated with defect repair is automatic program repair. Recent work has shown that some kind of bugs are more suitable for automatic repair techniques. [Aim]: The detection and characterization of common bug-fix patterns in software repositories play an important role in advancing the field of automatic program repair. In this paper, we aim to characterize the occurrence of known bug-fix patterns in Java repositories at an unprecedented large scale. [Method]: The study was conducted for Java GitHub projects organized in two distinct data sets: the first one (i.e., Boa data set) contains more than 4 million bug-fix commits from 101,471 projects and the second one (i.e., Defects4J data set) contains 369 real bug fixes from five open-source projects. We used a domain-specific programming language called Boa in the first data set and conducted a manual analysis on the second data set in order to confront the results. [Results]: We characterized the prevalence of the five most common bug-fix patterns (identified in the work of Pan et al.) in those bug fixes. The combined results showed direct evidence that developers often forget to add IF preconditions in the code. Moreover, 76\% of bug-fix commits associated with the IF-APC bug-fix pattern are isolated from the other four bug-fix patterns analyzed. [Conclusion]: Targeting on bugs that miss preconditions is a feasible alternative in automatic repair techniques that would produce a relevant payback.},
  booktitle = {2017 {{ACM}}/{{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}} ({{ESEM}})},
  author = {Campos, E. C. and Maia, M. d A.},
  month = nov,
  year = {2017},
  keywords = {Java,Debugging,Maintenance engineering,software maintenance,Software,program debugging,public domain software,Computer bugs,Manuals,4 million bug,automatic program repair,Automatic program repair,automatic repair techniques,Boa data set,bug fixes,Bug-fix patterns,common bug-fix patterns,distinct data sets,domain-specific programming language,Human-written patches,IF-APC bug,Java GitHub projects,large-scale observational study,real-world programs,Software defects},
  pages = {404-413},
  file = {/Users/luigi/work/zotero/storage/A9QRD6UW/Campos and Maia - 2017 - Common Bug-Fix Patterns A Large-Scale Observation.pdf;/Users/luigi/work/zotero/storage/RYJ7DRZZ/8170127.html}
}

@article{panUnderstandingBugFix2009,
  title = {Toward an Understanding of Bug Fix Patterns},
  volume = {14},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-008-9077-5},
  abstract = {Twenty-seven automatically extractable bug fix patterns are defined using the syntax components and context of the source code involved in bug fix changes. Bug fix patterns are extracted from the configuration management repositories of seven open source projects, all written in Java (Eclipse, Columba, JEdit, Scarab, ArgoUML, Lucene, and MegaMek). Defined bug fix patterns cover 45.7\% to 63.3\% of the total bug fix hunk pairs in these projects. The frequency of occurrence of each bug fix pattern is computed across all projects. The most common individual patterns are MC-DAP (method call with different actual parameter values) at 14.9\textendash{}25.5\%, IF-CC (change in if conditional) at 5.6\textendash{}18.6\%, and AS-CE (change of assignment expression) at 6.0\textendash{}14.2\%. A correlation analysis on the extracted pattern instances on the seven projects shows that six have very similar bug fix pattern frequencies. Analysis of if conditional bug fix sub-patterns shows a trend towards increasing conditional complexity in if conditional fixes. Analysis of five developers in the Eclipse projects shows overall consistency with project-level bug fix pattern frequencies, as well as distinct variations among developers in their rates of producing various bug patterns. Overall, data in the paper suggest that developers have difficulty with specific code situations at surprisingly consistent rates. There appear to be broad mechanisms causing the injection of bugs that are largely independent of the type of software being produced.},
  language = {en},
  number = {3},
  journal = {Empirical Software Engineering},
  author = {Pan, Kai and Kim, Sunghun and Whitehead, E. James},
  month = jun,
  year = {2009},
  pages = {286-315},
  file = {/Users/luigi/work/zotero/storage/7QA3ESMD/Pan et al. - 2009 - Toward an understanding of bug fix patterns.pdf;/Users/luigi/work/zotero/storage/IZFDVILX/s10664-008-9077-5.html}
}

@article{dietrichWhatJavaDevelopers2016,
  title = {What {{Java}} Developers Know about Compatibility, and Why This Matters},
  volume = {21},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-015-9389-1},
  abstract = {Real-world programs are neither monolithic nor static\textemdash{}they are constructed using platform and third party libraries, and both programs and libraries continuously evolve in response to change pressure. In case of the Java language, rules defined in the Java Language and Java Virtual Machine Specifications define when library evolution is safe. These rules distinguish between three types of compatibility\textemdash{}binary, source and behavioural. We claim that some of these rules are counter intuitive and not well-understood by many developers. We present the results of a survey where we quizzed developers about their understanding of the various types of compatibility. 414 developers responded to our survey. We find that while most programmers are familiar with the rules of source compatibility, they generally lack knowledge about the rules of binary and behavioural compatibility. This can be problematic when organisations switch from integration builds to technologies that require dynamic linking, such as OSGi. We have assessed the gravity of the problem by studying how often linkage-related problems are referenced in issue tracking systems, and find that they are common.},
  language = {en},
  number = {3},
  journal = {Empirical Software Engineering},
  author = {Dietrich, Jens and Jezek, Kamil and Brada, Premek},
  month = jun,
  year = {2016},
  pages = {1371-1396},
  file = {/Users/luigi/work/zotero/storage/7YW6CVKH/Dietrich et al. - 2016 - What Java developers know about compatibility, and.pdf;/Users/luigi/work/zotero/storage/R7D7RMUN/s10664-015-9389-1.html}
}

@inproceedings{ahmedEmpiricalExaminationRelationship2017,
  address = {Piscataway, NJ, USA},
  series = {{{ESEM}} '17},
  title = {An {{Empirical Examination}} of the {{Relationship Between Code Smells}} and {{Merge Conflicts}}},
  isbn = {978-1-5090-4039-1},
  doi = {10.1109/ESEM.2017.12},
  abstract = {Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both "smelly" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.},
  booktitle = {Proceedings of the 11th {{ACM}}/{{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  publisher = {{IEEE Press}},
  author = {Ahmed, Iftekhar and Brindescu, Caius and Mannan, Umme Ayda and Jensen, Carlos and Sarma, Anita},
  year = {2017},
  keywords = {machine learning,code smell,empirical analysis,merge conflict},
  pages = {58--67},
  file = {/Users/luigi/work/zotero/storage/4AIU57MP/Ahmed et al. - 2017 - An Empirical Examination of the Relationship Betwe.pdf}
}

@inproceedings{torgersenAddingWildcardsJava2004,
  address = {New York, NY, USA},
  series = {{{SAC}} '04},
  title = {Adding {{Wildcards}} to the {{Java Programming Language}}},
  isbn = {978-1-58113-812-2},
  doi = {10.1145/967900.968162},
  abstract = {This paper describes wildcards, a new language construct designed to increase the flexibility of object-oriented type systems with parameterized classes. Based on the notion of use-site variance, wildcards provide a type safe abstraction over different instantiations of parameterized classes, by using '?' to denote unspecified type arguments. Thus they essentially unify the distinct families of classes often introduced by parametric polymorphism. Wildcards are implemented as part of the upcoming addition of generics to the Java\texttrademark{} programming language, and will thus be deployed world-wide as part of the reference implementation of the Java compiler javac available from Sun Microsystems, Inc. By providing a richer type system, wildcards allow for an improved type inference scheme for polymorphic method calls. Moreover, by means of a novel notion of wildcard capture, polymorphic methods can be used to give symbolic names to unspecified types, in a manner similar to the "open" construct known from existential types. Wildcards show up in numerous places in the Java Platform APIs of the upcoming release, and some of the examples in this paper are taken from these APIs.},
  booktitle = {Proceedings of the 2004 {{ACM Symposium}} on {{Applied Computing}}},
  publisher = {{ACM}},
  author = {Torgersen, Mads and Hansen, Christian Plesner and Ernst, Erik and {von der Ah\'e}, Peter and Bracha, Gilad and Gafter, Neal},
  year = {2004},
  keywords = {wildcards,genericity,parameterized types},
  pages = {1289--1296},
  file = {/Users/luigi/work/zotero/storage/KQLQ4MA9/Torgersen et al. - 2004 - Adding Wildcards to the Java Programming Language.pdf}
}

@inproceedings{tateTamingWildcardsJava2011,
  address = {New York, NY, USA},
  series = {{{PLDI}} '11},
  title = {Taming {{Wildcards}} in {{Java}}'s {{Type System}}},
  isbn = {978-1-4503-0663-8},
  doi = {10.1145/1993498.1993570},
  abstract = {Wildcards have become an important part of Java's type system since their introduction 7 years ago. Yet there are still many open problems with Java's wildcards. For example, there are no known sound and complete algorithms for subtyping (and consequently type checking) Java wildcards, and in fact subtyping is suspected to be undecidable because wildcards are a form of bounded existential types. Furthermore, some Java types with wildcards have no joins, making inference of type arguments for generic methods particularly difficult. Although there has been progress on these fronts, we have identified significant shortcomings of the current state of the art, along with new problems that have not been addressed. In this paper, we illustrate how these shortcomings reflect the subtle complexity of the problem domain, and then present major improvements to the current algorithms for wildcards by making slight restrictions on the usage of wildcards. Our survey of existing Java programs suggests that realistic code should already satisfy our restrictions without any modifications. We present a simple algorithm for subtyping which is both sound and complete with our restrictions, an algorithm for lazily joining types with wildcards which addresses some of the shortcomings of prior work, and techniques for improving the Java type system as a whole. Lastly, we describe various extensions to wildcards that would be compatible with our algorithms.},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Tate, Ross and Leung, Alan and Lerner, Sorin},
  year = {2011},
  keywords = {subtyping,type inference,existential types,wildcards,joins,parametric types,single-instantiation inheritance},
  pages = {614--627},
  file = {/Users/luigi/work/zotero/storage/VI9HQLLP/Tate et al. - 2011 - Taming Wildcards in Java's Type System.pdf}
}

@inproceedings{linares-vasquezMiningEnergygreedyAPI2014,
  address = {New York, NY, USA},
  series = {{{MSR}} 2014},
  title = {Mining {{Energy}}-Greedy {{API Usage Patterns}} in {{Android Apps}}: {{An Empirical Study}}},
  isbn = {978-1-4503-2863-0},
  shorttitle = {Mining {{Energy}}-Greedy {{API Usage Patterns}} in {{Android Apps}}},
  doi = {10.1145/2597073.2597085},
  abstract = {Energy consumption of mobile applications is nowadays a hot topic, given the widespread use of mobile devices. The high demand for features and improved user experience, given the available powerful hardware, tend to increase the apps' energy consumption. However, excessive energy consumption in mobile apps could also be a consequence of energy greedy hardware, bad programming practices, or particular API usage patterns. We present the largest to date quantitative and qualitative empirical investigation into the categories of API calls and usage patterns that\textemdash{}in the context of the Android development framework\textemdash{}exhibit particularly high energy consumption profiles. By using a hardware power monitor, we measure energy consumption of method calls when executing typical usage scenarios in 55 mobile apps from different domains. Based on the collected data, we mine and analyze energy-greedy APIs and usage patterns. We zoom in and discuss the cases where either the anomalous energy consumption is unavoidable or where it is due to suboptimal usage or choice of APIs. Finally, we synthesize our findings into actionable knowledge and recipes for developers on how to reduce energy consumption while using certain categories of Android APIs and patterns},
  booktitle = {Proceedings of the 11th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{ACM}},
  author = {{Linares-V\'asquez}, Mario and Bavota, Gabriele and {Bernal-C\'ardenas}, Carlos and Oliveto, Rocco and Di Penta, Massimiliano and Poshyvanyk, Denys},
  year = {2014},
  keywords = {Energy consumption,Empirical Study,Mobile applications},
  pages = {2--11},
  file = {/Users/luigi/work/zotero/storage/XLL3NVEK/Linares-Vásquez et al. - 2014 - Mining Energy-greedy API Usage Patterns in Android.pdf}
}

@inproceedings{dietrichGigascaleExhaustivePointsto2015,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} 2015},
  title = {Giga-Scale {{Exhaustive Points}}-to {{Analysis}} for {{Java}} in {{Under}} a {{Minute}}},
  isbn = {978-1-4503-3689-5},
  doi = {10.1145/2814270.2814307},
  abstract = {Computing a precise points-to analysis for very large Java programs remains challenging despite the large body of research on points-to analysis. Any approach must solve an underlying dynamic graph reachability problem, for which the best algorithms have near-cubic worst-case runtime complexity, and, hence, previous work does not scale to programs with millions of lines of code. In this work, we present a novel approach for solving the field-sensitive points-to problem for Java with the means of (1) a transitive-closure data-structure, and (2) a pre-computed set of potentially matching load/store pairs to accelerate the fix-point calculation. Experimentation on Java benchmarks validates the superior performance of our approach over the standard context-free language reachability implementations. Our approach computes a points-to index for the OpenJDK with over 1.5 billion tuples in under a minute.},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Dietrich, Jens and Hollingum, Nicholas and Scholz, Bernhard},
  year = {2015},
  keywords = {Java,Context-free Language,Points-to Analysis,Transitive Closure},
  pages = {535--551},
  file = {/Users/luigi/work/zotero/storage/E7LLWTKG/Dietrich et al. - 2015 - Giga-scale Exhaustive Points-to Analysis for Java .pdf}
}

@inproceedings{petricekUniversalDefinitionType2015,
  address = {New York, NY, USA},
  series = {Onward! 2015},
  title = {Against a {{Universal Definition}} of '{{Type}}'},
  isbn = {978-1-4503-3688-8},
  doi = {10.1145/2814228.2814249},
  abstract = {What is the definition of 'type'? Having a clear and precise answer to this question would avoid many misunderstandings and prevent meaningless discussions that arise from them. But having such clear and precise answer to this question would also hurt science, "hamper the growth of knowledge" and "deflect the course of investigation into narrow channels of things already understood". In this essay, I argue that not everything we work with needs to be precisely defined. There are many definitions used by different communities, but none of them applies universally. A brief excursion into philosophy of science shows that this is not just tolerable, but necessary for progress. Philosophy also suggests how we can think about this imprecise notion of type.},
  booktitle = {2015 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}} ({{Onward}}!)},
  publisher = {{ACM}},
  author = {Petricek, Tomas},
  year = {2015},
  keywords = {types,philosophy,science},
  pages = {254--266},
  file = {/Users/luigi/work/zotero/storage/PQ9NBBDC/Petricek - 2015 - Against a Universal Definition of 'Type'.pdf}
}

@article{zhangFamiliaUnifyingInterfaces2017,
  title = {Familia: {{Unifying Interfaces}}, {{Type Classes}}, and {{Family Polymorphism}}},
  volume = {1},
  issn = {2475-1421},
  shorttitle = {Familia},
  doi = {10.1145/3133894},
  abstract = {Parametric polymorphism and inheritance are both important, extensively explored language mechanisms for providing code reuse and extensibility. But harmoniously integrating these apparently distinct mechanisms\textemdash{}and powerful recent forms of them, including type classes and family polymorphism\textemdash{}in a single language remains an elusive goal. In this paper, we show that a deep unification can be achieved by generalizing the semantics of interfaces and classes. The payoff is a significant increase in expressive power with little increase in programmer-visible complexity. Salient features of the new programming language include retroactive constraint modeling, underpinning both object-oriented programming and generic programming, and module-level inheritance with further-binding, allowing family polymorphism to be deployed at large scale. The resulting mechanism is syntactically light, and the more advanced features are transparent to the novice programmer. We describe the design of a programming language that incorporates this mechanism; using a core calculus, we show that the type system is sound. We demonstrate that this language is highly expressive by illustrating how to use it to implement highly extensible software and by showing that it can not only concisely model state-of-the-art features for code reuse, but also go beyond them.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  author = {Zhang, Yizhou and Myers, Andrew C.},
  month = oct,
  year = {2017},
  keywords = {language design,type classes,genericity,extensibility,Familia,family polymorphism,type-safety},
  pages = {70:1--70:31},
  file = {/Users/luigi/work/zotero/storage/VKD39BKI/Zhang and Myers - 2017 - Familia Unifying Interfaces, Type Classes, and Fa.pdf}
}

@inproceedings{igarashiLightweightFamilyPolymorphism2005,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Lightweight {{Family Polymorphism}}},
  isbn = {978-3-540-29735-2 978-3-540-32247-4},
  doi = {10.1007/11575467_12},
  abstract = {Family polymorphism has been proposed for object-oriented languages as a solution to supporting reusable yet type-safe mutually recursive classes. A key idea of family polymorphism is the notion of families, which are used to group mutually recursive classes. In the original proposal, due to the design decision that families are represented by objects, dependent types had to be introduced, resulting in a rather complex type system. In this paper, we propose a simpler solution of lightweight family polymorphism, based on the idea that families are represented by classes rather than objects. This change makes the type system significantly simpler without losing much expressibility of the language. Moreover, ``family-polymorphic'' methods now take a form of parametric methods; thus it is easy to apply the Java-style type inference. To rigorously show that our approach is safe, we formalize the set of language features on top of Featherweight Java and prove the type system is sound. An algorithm of type inference for family-polymorphic method invocations is also formalized and proved to be correct.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Igarashi, Atsushi and Saito, Chieri and Viroli, Mirko},
  month = nov,
  year = {2005},
  pages = {161-177},
  file = {/Users/luigi/work/zotero/storage/C88ZL523/Igarashi et al. - 2005 - Lightweight Family Polymorphism.pdf;/Users/luigi/work/zotero/storage/7AXXJAE5/11575467_12.html}
}

@inproceedings{tufanoWhenWhyYour2015,
  address = {Piscataway, NJ, USA},
  series = {{{ICSE}} '15},
  title = {When and {{Why Your Code Starts}} to {{Smell Bad}}},
  isbn = {978-1-4799-1934-5},
  abstract = {In past and recent years, the issues related to managing technical debt received significant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To fill this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smell-introducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., those identified as smell-introducing). Our findings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Software Engineering}} - {{Volume}} 1},
  publisher = {{IEEE Press}},
  author = {Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Oliveto, Rocco and Di Penta, Massimiliano and De Lucia, Andrea and Poshyvanyk, Denys},
  year = {2015},
  pages = {403--414},
  file = {/Users/luigi/work/zotero/storage/DHHNNMEH/Tufano et al. - 2015 - When and Why Your Code Starts to Smell Bad.pdf}
}

@misc{schaferAlgebraicDataTypes2017,
  title = {Algebraic {{Data Types}} for {{Object}}-Oriented {{Datalog}}},
  abstract = {Value lookup(Context ctxt, Parameter p) \{ exists(Function f, int i | appliesTo(ctxt, f) and p = f.getParameter(i) and result = get(ctxt, i)) \} With these preparations out of the way, we now show three representative clauses of the eval predicate: analysis of numeric literals as an example of a simple base case (where we use appliesTo to restrict the range of the otherwise unused parameter ctxt), and the two crucial cases of parameter and return value passing. To analyse a use of a parameter p we look it up in the context; to analyse a function call, we construct the appropriate calling context, and use the auxiliary predicate retval to determine the possible return values of the callee in that context. That predicate, in turn, considers the return statements in the function to determine possible return values: AbstractValue eval(Context ctxt, Expr e) \{ e instanceof NumberLiteral and appliesTo(ctxt, e.getEnclosingFunction()) and result = Number() or exists(SimpleParameter p | e = p.getVariable().getAnAccess() and result = ctxt.lookup(p)) or exists(Function f | calls(ctxt, e, f) and result = retval(evalArgs(ctxt, e, f, 0), f)) or ... \} AbstractValue retval(Context ctxt, Function f) \{ exists(ReturnStmt ret | ret = f.getAReturnStmt() and result = eval(ctxt, ret.getExpr())) \}Value retval(Context ctxt, Function f) \{ exists(ReturnStmt ret | ret = f.getAReturnStmt() and result = eval(ctxt, ret.getExpr())) \} Extending these predicates to model all of ECMAScript 2016 is a non-trivial task, but takes no more than about 500 lines of QL. We want to emphasise, however, that we do not mean to claim that CPA is a silver bullet for the analysis of JavaScript, the challenges of which are manifold and extensively documented in the literature (Jensen et al. 2009; Kashyap et al. 2014; Park and Ryu 2015; Sch\"afer et al. 2013; Sridharan et al. 2012), we simply use it as an example of a non-trivial context sensitivity policy that nicely demonstrates the use of recursive algebraic data types: since contexts can be lists of arbitrary length, it is not clear how they could be represented in plain QL. 6.2 Constructing control flow graphs As our next example, we show how to construct intra-procedural control ow graphs for programs written in a very small subset of Java, comprising just four kinds of statements: expression statements, throw, try with catch (but no finally) and block statements. We ignore any control ow resulting from expression evaluation. The CFG contains one node for each statement, plus one entry node and one exit node for each callable (that is, method or constructor): newtype CfgNode = StmtNode(Stmt s) or EntryNode(Callable c) or ExitNode(Callable c) CFG edges are labelled with completions that indicate the reason for the ow. We have two kinds of completions: the normal completion indicating normal termination of a statement; and throw completions, indicating that a statement has thrown an exception: , Vol. 1, No. 1, Article 1. Publication date: April 2017.},
  howpublished = {/paper/Algebraic-Data-Types-for-Object-oriented-Datalog-Sch\%C3\%A4fer-Avgustinov/68bd04b8ad7aaf0e853a0091a620381da7ee2130},
  author = {Sch\"afer, Max and Avgustinov, Pavel and de Moor, Oege},
  year = {2017},
  file = {/Users/luigi/work/zotero/storage/AL78XXZQ/Schäfer et al. - 2017 - Algebraic Data Types for Object-oriented Datalog.pdf;/Users/luigi/work/zotero/storage/RQMMUZ85/68bd04b8ad7aaf0e853a0091a620381da7ee2130.html}
}

@inproceedings{arntzeniusDatafunFunctionalDatalog2016,
  address = {New York, NY, USA},
  series = {{{ICFP}} 2016},
  title = {Datafun: {{A Functional Datalog}}},
  isbn = {978-1-4503-4219-3},
  shorttitle = {Datafun},
  doi = {10.1145/2951913.2951948},
  abstract = {Loading...},
  booktitle = {Proceedings of the 21st {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Arntzenius, Michael and Krishnaswami, Neelakantan R.},
  year = {2016},
  keywords = {functional programming,domain-specific languages,Datalog,logic programming,type theory,adjoint logic,denotational semantics,operational semantics,Prolog},
  pages = {214--227},
  file = {/Users/luigi/work/zotero/storage/LZC6MTKH/Arntzenius and Krishnaswami - 2016 - Datafun A Functional Datalog.pdf}
}

@book{saraswatJavaNotTypeSafe1997,
  title = {Java Is {{Not Type}}-{{Safe}}},
  abstract = {A language is type-safe if the only operations that can be performed on data in the language are those sanctioned by the type of the data. Java is not type-safe, though it was intended to be. A Java object may read and modify fields (and invoke methods) private to another object. It may read and modify internal Java Virtual Machine (JVM) data-structures. It may invoke operations not even defined for that object, causing completely unpredictable results, including JVM crashes (core dumps). Thus Java security, which depends strongly on type-safety, is completely compromised.},
  author = {Saraswat, Vijay},
  year = {1997},
  file = {/Users/luigi/work/zotero/storage/4BQU235R/Saraswat - 1997 - Java is Not Type-Safe.pdf;/Users/luigi/work/zotero/storage/RR3XXSNB/summary.html}
}

@article{pollLectureNotesLanguageBased,
  title = {Lecture {{Notes}} on {{Language}}-{{Based Security}}},
  language = {en},
  author = {Poll, Erik},
  pages = {46},
  file = {/Users/luigi/work/zotero/storage/5L2GKFL8/Poll - Lecture Notes on Language-Based Security.pdf}
}

@inproceedings{lammelScrapMoreBoilerplate2004,
  address = {New York, NY, USA},
  series = {{{ICFP}} '04},
  title = {Scrap {{More Boilerplate}}: {{Reflection}}, {{Zips}}, and {{Generalised Casts}}},
  isbn = {978-1-58113-905-1},
  shorttitle = {Scrap {{More Boilerplate}}},
  doi = {10.1145/1016850.1016883},
  abstract = {Writing boilerplate code is a royal pain. Generic programming promises to alleviate this pain by allowing the programmer to write a generic "recipe" for boilerplate code, and use that recipe in many places. In earlier work we introduced the "Scrap your boilerplate" approach to generic programming, which exploits Haskell's existing type-class mechanism to support generic transformations and queries.This paper completes the picture. We add a few extra "introspective" or "reflective" facilities, that together support a rich variety of serialisation and de-serialisation. We also show how to perform generic "zips", which at first appear to be somewhat tricky in our framework. Lastly, we generalise the ability to over-ride a generic function with a type-specific one.All of this can be supported in Haskell with independently-useful extensions: higher-rank types and type-safe cast. The GHC implementation of Haskell readily derives the required type classes for user-defined data types.},
  booktitle = {Proceedings of the {{Ninth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {L\"ammel, Ralf and Jones, Simon Peyton},
  year = {2004},
  keywords = {type cast,generic programming,reflection,zippers},
  pages = {244--255},
  file = {/Users/luigi/work/zotero/storage/9926G9XL/Lämmel and Jones - 2004 - Scrap More Boilerplate Reflection, Zips, and Gene.pdf}
}

@inproceedings{dewaelForkJoinParallelism2014,
  address = {New York, NY, USA},
  series = {{{PPPJ}} '14},
  title = {Fork/{{Join Parallelism}} in the {{Wild}}: {{Documenting Patterns}} and {{Anti}}-Patterns in {{Java Programs Using}} the {{Fork}}/{{Join Framework}}},
  isbn = {978-1-4503-2926-2},
  shorttitle = {Fork/{{Join Parallelism}} in the {{Wild}}},
  doi = {10.1145/2647508.2647511},
  abstract = {Now that multicore processors are commonplace, developing parallel software has escaped the confines of high-performance computing and enters the mainstream. The Fork/Join framework, for instance, is part of the standard Java platform since version 7. Fork/Join is a high-level parallel programming model advocated to make parallelizing recursive divide-and-conquer algorithms particularly easy. While, in theory, Fork/Join is a simple and effective technique to expose parallelism in applications, it has not been investigated before whether and how the technique is applied in practice. We therefore performed an empirical study on a corpus of 120 open source Java projects that use the framework for roughly 362 different tasks. On the one hand, we confirm the frequent use of four best-practice patterns (Sequential Cutoff, Linked Subtasks, Leaf Tasks, and avoiding unnecessary forking) in actual projects. On the other hand, we also discovered three recurring anti-patterns that potentially limit parallel performance: sub-optimal use of Java collections when splitting tasks into subtasks as well as when merging the results of subtasks, and finally the inappropriate sharing of resources between tasks. We document these anti-patterns and study their impact on performance.},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform}}: {{Virtual Machines}}, {{Languages}}, and {{Tools}}},
  publisher = {{ACM}},
  author = {De Wael, Mattias and Marr, Stefan and Van Cutsem, Tom},
  year = {2014},
  keywords = {Java,empirical study,patterns,anti-patterns,fork/join,open source projects},
  pages = {39--50},
  file = {/Users/luigi/work/zotero/storage/DC5TXM2X/De Wael et al. - 2014 - ForkJoin Parallelism in the Wild Documenting Pat.pdf}
}

@misc{hanVerificationJavaPrograms2010,
  title = {Verification of {{Java}} Programs in Type Theory with Dependent Record Types and Coercive Subtyping},
  abstract = {This thesis describes research on a functional interpretation of object-oriented programs in the intensional type theory UTT with dependent record types and coercive subtyping. In other words, it presents a functional analysis of constructing Java programs and their precise specifications in the Coq system. Here, we are using Coq only for the simulation of modeling Java programs in UTT, because Coq supports coercive subtyping and a macro for dependent record types. Representing a class and its interface-type, which declares a set of methods and their signatures for code reuse, as dependent record types, the type-theoretic encoding of Java programs enjoys desirable subtyping relationships that correctly capture the important object-oriented features such as encapsulation, inheritance, reuse, subtype polymorphism and dynamic dispatch. Furthermore, since the model is given in the intensional type theory, machine-supported verification of Java programs can be done by proving properties of Java programs in Coq. The current object-oriented languages are closely linked to the underlying hardware, in the sense that programming is based on the idea of changing stored values. In contrast, functional programming languages promote a more abstract style of programming, based on the idea of applying functions to arguments. Moving to this higher-level leads to considerably simpler programs, and support a number of powerful new ways to structure and reason about programs. So, research on a functional interpretation of object-oriented programming languages can be in interest. Most importantly, we will model Java programs by dwelling on intensional type theories without extensional features, and lead to the machine-supported verification of Java programs. When we have a suitable intensional notion of computational equality, it is easier to obtain a good operational understanding of the language. We shall use dependent record types and structural subtyping for them in the framework of coercive subtyping with the notion of intensional},
  howpublished = {/paper/Verification-of-Java-programs-in-type-theory-with-Han/5f9d97e1da8399b45f83b962b477b4a1827401da},
  author = {Han, Seokhyun},
  year = {2010},
  file = {/Users/luigi/work/zotero/storage/KUHE7NB3/Han - 2010 - Verification of Java programs in type theory with .pdf;/Users/luigi/work/zotero/storage/W3SL7YEP/5f9d97e1da8399b45f83b962b477b4a1827401da.html}
}

@article{nordstromProgrammingMartinLoType,
  title = {Programming in {{Martin}}-{{Lo}}\textasciidieresis{}f 's {{Type Theory}}},
  language = {en},
  author = {Nordstrom, Bengt and Petersson, Kent and Smith, Jan M},
  pages = {211},
  file = {/Users/luigi/work/zotero/storage/6VRGHNFG/Nordstrom et al. - Programming in Martin-Lo¨f ’s Type Theory.pdf}
}

@article{adveSharedMemoryConsistency1996,
  title = {Shared Memory Consistency Models: A Tutorial},
  volume = {29},
  issn = {0018-9162},
  shorttitle = {Shared Memory Consistency Models},
  doi = {10.1109/2.546611},
  abstract = {The memory consistency model of a system affects performance, programmability, and portability. We aim to describe memory consistency models in a way that most computer professionals would understand. This is important if the performance-enhancing features being incorporated by system designers are to be correctly and widely used by programmers. Our focus is consistency models proposed for hardware-based shared memory systems. Most of these models emphasize the system optimizations they support, and we retain this system-centric emphasis. We also describe an alternative, programmer-centric view of relaxed consistency models that describes them in terms of program behavior, not system optimizations.},
  number = {12},
  journal = {Computer},
  author = {Adve, S. V. and Gharachorloo, K.},
  month = dec,
  year = {1996},
  keywords = {Programming profession,Computer architecture,parallel programming,Software design,Hardware,software performance evaluation,performance,High level languages,Read-write memory,computer professionals,data integrity,hardware-based shared memory systems,Magnetic heads,Message passing,Optimizing compilers,performance-enhancing features,portability,program behavior,programmability,programmer-centric,relaxed consistency models,shared memory consistency models,shared memory systems,software portability,system designers,system optimizations,system-centric,Tutorial},
  pages = {66-76},
  file = {/Users/luigi/work/zotero/storage/WKPDRDU7/Adve and Gharachorloo - 1996 - Shared memory consistency models a tutorial.pdf;/Users/luigi/work/zotero/storage/ESM3EWI6/546611.html}
}

@inproceedings{shahamHeapProfilingSpaceefficient2001,
  address = {New York, NY, USA},
  series = {{{PLDI}} '01},
  title = {Heap {{Profiling}} for {{Space}}-Efficient {{Java}}},
  isbn = {978-1-58113-414-8},
  doi = {10.1145/378795.378820},
  abstract = {We present a heap-profiling tool for exploring the potential for space savings in Java programs. The output of the tool is used to direct rewriting of application source code in a way that allows more timely garbage collection (GC) of objects, thus saving space. The rewriting can also avoid allocating some objects that are never used.
The tool measures the difference between the actual collection time and the potential earliest collection time of objects for a Java application. This time difference indicates potential savings. Then the tool sorts the allocation sites in the application source code according to the accumulated potential space saving for the objects allocated at the sites. A programmer can investigate the source code surrounding the sites with the highest savings to find opportunities for code rewriting that could save space. Our experience shows that in many cases simple code rewriting leads to actual space savings and in some cases also to improvements in program runtime.
Experimental results using the tool and manually rewriting code show average space savings of 18\% for the SPECjvm98 benchmark suite. Results for other benchmarks are also promising. We have also classified the program transformations that we have used and argue that in many cases improvements can be achieved by an optimizing compiler.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2001 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Shaham, Ran and Kolodner, Elliot K. and Sagiv, Mooly},
  year = {2001},
  pages = {104--113},
  file = {/Users/luigi/work/zotero/storage/RVE4WJSD/Shaham et al. - 2001 - Heap Profiling for Space-efficient Java.pdf}
}

@article{ballSLAMProjectDebugging,
  title = {The {{SLAM}} Project: Debugging System Software via Static Analysis},
  shorttitle = {The {{SLAM}} Project},
  abstract = {Abstract. The goal of the SLAM project is to check whether or not a program obeys "API usage rules " that specif[y what it means to be a good client of an API. The SLAM toolkit statically analyzes a C program to determine whether or not it violates given usage rules. The toolkit has two unique aspects: it does not require the programmer to annotate the source program (invariants are inferred); it minimizes noise (false error messages) through a process known as "counterexample-driven refinement". SLAM exploits and extends results fi'om program analysis, model checking and automated deduction.\vphantom\{\}V \textasciitilde{} have successfully applied the SLAM toolkit to Windows XP device drivers, to both validate behavior and find defects in their usage of kernel APIs. Context. Today, many programmers are realizing the benefits of using languages with static type systems. By providing},
  journal = {SIGPLAN Not},
  author = {Ball, Thomas and Rajamani, Sriram K.},
  pages = {2002},
  file = {/Users/luigi/work/zotero/storage/5CSD7XF7/Ball and Rajamani - The SLAM project debugging system software via st.pdf;/Users/luigi/work/zotero/storage/FACJBKP9/summary.html}
}

@inproceedings{brunetonRecursiveDynamicSoftware2002,
  title = {Recursive and {{Dynamic Software Composition}} with {{Sharing}}},
  abstract = {null},
  author = {Bruneton, E. and Coupaye, T. and Stefani, J. B.},
  year = {2002},
  file = {/Users/luigi/work/zotero/storage/97MBU96Q/Bruneton et al. - 2002 - Recursive and Dynamic Software Composition with Sh.pdf;/Users/luigi/work/zotero/storage/5RUGNATL/summary.html}
}

@inproceedings{sagivParametricShapeAnalysis1999,
  address = {New York, NY, USA},
  series = {{{POPL}} '99},
  title = {Parametric {{Shape Analysis}} via 3-Valued {{Logic}}},
  isbn = {978-1-58113-095-9},
  doi = {10.1145/292540.292552},
  abstract = {We present a family of abstract-interpretation algorithms that are capable of determining "shape invariants" of programs that perform destructive updating on dynamically allocated storage. The main idea is to represent the stores that can passibly arise during execution using three-valued logical structures.Questions about properties of stores can be answered by evaluating predicate-logic formulae using Kleene's semantics of three-valued logic: \textquestiondown{}If a formula evaluates to true, then the formula holds in every store represented by the three-valued structure. \textquestiondown{}If a formula evaluates to false, then the formula does not hold in any store represented by the three-valued structure. \textquestiondown{}If a formula evaluates to unknown, then we do not know if this formula always holds, never holds, or sometimes holds and sometimes does not hold in the stores represented by the three-valued structure. Three-valued logical structures are thus a conservative representation of memory stores.The approach described is a parametric framework: It provides the basis for generating a family of shape-analysis algorithms by varying the vocabulary used in the three-valued logic.},
  booktitle = {Proceedings of the 26th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Sagiv, Mooly and Reps, Thomas and Wilhelm, Reinhard},
  year = {1999},
  pages = {105--118},
  file = {/Users/luigi/work/zotero/storage/UWVSVTCL/Sagiv et al. - 1999 - Parametric Shape Analysis via 3-valued Logic.pdf}
}

@inproceedings{xiDependentTypesProgram2001,
  title = {Dependent Types for Program Termination Verification},
  doi = {10.1109/LICS.2001.932500},
  abstract = {Program termination verification is a challenging research subject of significant practical importance. While there is already a rich body of literature on this subject, it is still undeniably a difficult task to design a termination checker for a realistic programming language that supports general recursion. In this paper, we present an approach to program termination verification that makes use of a form of dependent types developed in Dependent ML (DML), demonstrating a novel application of such dependent types to establishing a liveness property. We design a type system that enables the programmer to supply metrics for verifying program termination and prove that every well-typed program in this type system is terminating. We also provide realistic examples, which are all verified in a prototype implementation, to support the effectiveness of our approach to program termination verification as well as its unobtrusiveness to programming. The main contribution of the paper lies in the design of an approach to program termination verification that smoothly combines types with metrics, yielding a type system capable of guaranteeing program termination that supports a general form of recursion (including mutual recursion), higher-order functions, algebraic data types and polymorphism.},
  booktitle = {Proceedings 16th {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Xi, Hongwei},
  month = jun,
  year = {2001},
  keywords = {Programming profession,program verification,programming languages,software metrics,polymorphism,dependent types,type theory,algebraic data types,Dependent ML,Error correction,general recursion,higher-order functions,liveness property,ML language,mutual recursion,Polynomials,program termination verification,termination checker,type system,unobtrusiveness,well-typed programs},
  pages = {231-242},
  file = {/Users/luigi/work/zotero/storage/TTIK4JTH/Xi - 2001 - Dependent types for program termination verificati.pdf;/Users/luigi/work/zotero/storage/3NRQX3I5/932500.html}
}

@inproceedings{chibaEasytoUseToolkitEfficient2003,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {An {{Easy}}-to-{{Use Toolkit}} for {{Efficient Java Bytecode Translators}}},
  isbn = {978-3-540-39815-8},
  abstract = {This paper presents our toolkit for developing a Java-bytecode translator. Bytecode translation is getting important in various domains such as generative programming and aspect-oriented programming. To help the users easily develop a translator, the design of our toolkit is based on the reflective architecture. However, the previous implementations of this architecture involved serious runtime penalties. To address this problem, our toolkit uses a custom compiler so that the runtime penalties are minimized. Since the previous version of our toolkit named Javassist has been presented in another paper, this paper focuses on this new compiler support for performance improvement. This feature was not included in the previous version.},
  language = {en},
  booktitle = {Generative {{Programming}} and {{Component Engineering}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Chiba, Shigeru and Nishizawa, Muga},
  editor = {Pfenning, Frank and Smaragdakis, Yannis},
  year = {2003},
  keywords = {Application Programming Interface,Java Class,Meta Variable,Method Call,Source Text},
  pages = {364-376},
  file = {/Users/luigi/work/zotero/storage/ADWBHXB3/Chiba and Nishizawa - 2003 - An Easy-to-Use Toolkit for Efficient Java Bytecode.pdf}
}

@inproceedings{grossmanTypesafeMultithreadingCyclone2003,
  address = {New York, NY, USA},
  series = {{{TLDI}} '03},
  title = {Type-Safe {{Multithreading}} in {{Cyclone}}},
  isbn = {978-1-58113-649-4},
  doi = {10.1145/604174.604177},
  abstract = {We extend Cyclone, a type-safe polymorphic language at the C level of abstraction, with threads and locks. Data races can violate type safety in Cyclone. An extended type system statically guarantees their absence by enforcing that thread-shared data is protected via locking and that thread-local data does not escape the thread that creates it. The extensions interact smoothly with parametric polymorphism and region-based memory management. We present a formal abstract machine that models the need to prevent races, a polymorphic type system for the machine that supports thread-local data, and a corresponding type-safety result.},
  booktitle = {Proceedings of the 2003 {{ACM SIGPLAN International Workshop}} on {{Types}} in {{Languages Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Grossman, Dan},
  year = {2003},
  keywords = {types,cyclone,data races},
  pages = {13--25},
  file = {/Users/luigi/work/zotero/storage/ZEFC7ZG9/Grossman - 2003 - Type-safe Multithreading in Cyclone.pdf}
}

@misc{meghaniDeterminingLegalMethod2003,
  title = {Determining Legal Method Call Sequences in Object Interfaces},
  abstract = {The permitted sequences of method calls in an objectoriented component interface summarize how to correctly use the component. Many components lack such documentation: even if the documentation specifies the behavior of each of the component's methods, it may not state the order in which the methods should be invoked. This paper presents a dynamic technique for automatically extracting the legal method call sequences in a component interface, expressed as a finite state machine. Compared to previous techniques, it increases accuracy and reduces dependence on the test suite. It also identifies certain programming errors.},
  howpublished = {/paper/Determining-legal-method-call-sequences-in-object-Meghani-Ernst/b8f358a275f1bdbdce1c78142d80936b89551e79},
  author = {Meghani, Samir V. and Ernst, Michael D.},
  year = {2003},
  file = {/Users/luigi/work/zotero/storage/K8295UDP/Meghani and Ernst - 2003 - Determining legal method call sequences in object .pdf;/Users/luigi/work/zotero/storage/HNKWY4HI/b8f358a275f1bdbdce1c78142d80936b89551e79.html}
}

@article{roseLightweightBytecodeVerification2003,
  title = {Lightweight {{Bytecode Verification}}},
  volume = {31},
  issn = {1573-0670},
  doi = {10.1023/B:JARS.0000021015.15794.82},
  abstract = {In this paper, we provide a theoretical foundation for and improvements to the existing bytecode verification technology, a critical component of the Java security model, for mobile code used with the Java ``micro edition'' (J2ME), which is intended for embedded computing devices. In Java, remotely loaded ``bytecode'' class files are required to be bytecode verified before execution, that is, to undergo a static type analysis that protects the platform's Java run-time system from so-called type confusion attacks such as pointer manipulation. The data flow analysis that performs the verification, however, is beyond the capacity of most embedded devices because of the memory requirements that the typical algorithm will need. We propose to take a proof-carrying code approach to data flow analysis in defining an alternative technique called ``lightweight analysis'' that uses the notion of a ``certificate'' to reanalyze a previously analyzed data flow problem, even on poorly resourced platforms. We formally prove that the technique provides the same guarantees as standard bytecode safety verification analysis, in particular that it is ``tamper proof'' in the sense that the guarantees provided by the analysis cannot be broken by crafting a ``false'' certificate or by altering the analyzed code. We show how the Java bytecode verifier fits into this framework for an important subset of the Java Virtual Machine; we also show how the resulting ``lightweight bytecode verification'' technique generalizes and simulates the J2ME verifier (to be expected as Sun's J2ME ``K-Virtual machine'' verifier was directly based on an early version of this work), as well as Leroy's ``on-card bytecode verifier,'' which is specifically targeted for Java Cards.},
  language = {en},
  number = {3},
  journal = {Journal of Automated Reasoning},
  author = {Rose, Eva},
  month = nov,
  year = {2003},
  keywords = {data flow analysis,bytecode verification,proof-carrying code},
  pages = {303-334},
  file = {/Users/luigi/work/zotero/storage/7SJVGHPE/Rose - 2003 - Lightweight Bytecode Verification.pdf;/Users/luigi/work/zotero/storage/ILMK7FB5/Rose - 2003 - Lightweight Bytecode Verification.pdf}
}

@inproceedings{xiGuardedRecursiveDatatype2003,
  address = {New York, NY, USA},
  series = {{{POPL}} '03},
  title = {Guarded {{Recursive Datatype Constructors}}},
  isbn = {978-1-58113-628-9},
  doi = {10.1145/604131.604150},
  abstract = {We introduce a notion of guarded recursive (g.r.) datatype constructors, generalizing the notion of recursive datatypes in functional programming languages such as ML and Haskell. We address both theoretical and practical issues resulted from this generalization. On one hand, we design a type system to formalize the notion of g.r. datatype constructors and then prove the soundness of the type system. On the other hand, we present some significant applications (e.g., implementing objects, implementing staged computation, etc.) of g.r. datatype constructors, arguing that g.r. datatype constructors can have far-reaching consequences in programming. The main contribution of the paper lies in the recognition and then the formalization of a programming notion that is of both theoretical interest and practical use.},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Xi, Hongwei and Chen, Chiyan and Chen, Gang},
  year = {2003},
  keywords = {constructors,datatype,guarded,recursive},
  pages = {224--235},
  file = {/Users/luigi/work/zotero/storage/XQJ7F6MH/Xi et al. - 2003 - Guarded Recursive Datatype Constructors.pdf}
}

@incollection{czarneckiDSLImplementationMetaOCaml2004,
  address = {Berlin, Heidelberg},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {{{DSL Implementation}} in {{MetaOCaml}}, {{Template Haskell}}, and {{C}}++},
  isbn = {978-3-540-25935-0},
  abstract = {A wide range of domain-specific languages (DSLs) has been implemented successfully by embedding them in general purpose languages. This paper reviews embedding, and summarizes how two alternative techniques \textendash{} staged interpreters and templates \textendash{} can be used to overcome the limitations of embedding. Both techniques involve a form of generative programming. The paper reviews and compares three programming languages that have special support for generative programming. Two of these languages (MetaOCaml and Template Haskell) are research languages, while the third (C++) is already in wide industrial use. The paper identifies several dimensions that can serve as a basis for comparing generative languages.},
  language = {en},
  booktitle = {Domain-{{Specific Program Generation}}: {{International Seminar}}, {{Dagstuhl Castle}}, {{Germany}}, {{March}} 23-28, 2003. {{Revised Papers}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Czarnecki, Krzysztof and O'Donnell, John T. and Striegnitz, J\"org and Taha, Walid},
  editor = {Lengauer, Christian and Batory, Don and Consel, Charles and Odersky, Martin},
  year = {2004},
  keywords = {Functional Programming,Abstract Syntax Tree,Concrete Syntax,Function Template,Parse Tree},
  pages = {51-72},
  file = {/Users/luigi/work/zotero/storage/TWS8CZYT/Czarnecki et al. - 2004 - DSL Implementation in MetaOCaml, Template Haskell,.pdf},
  doi = {10.1007/978-3-540-25935-0_4}
}

@inproceedings{arthoJNukeEfficientDynamic2004,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {{{JNuke}}: {{Efficient Dynamic Analysis}} for {{Java}}},
  isbn = {978-3-540-27813-9},
  shorttitle = {{{JNuke}}},
  abstract = {JNuke is a framework for verification and model checking of Java programs. It is a novel combination of run-time verification, explicit-state model checking, and counter-example exploration. Efficiency is crucial in dynamic verification. Therefore JNuke has been written from scratch in C, improving performance and memory usage by an order of magnitude compared to competing approaches and tools.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Artho, Cyrille and Schuppan, Viktor and Biere, Armin and Eugster, Pascal and Baur, Marcel and Zweim\"uller, Boris},
  editor = {Alur, Rajeev and Peled, Doron A.},
  year = {2004},
  keywords = {Virtual Machine,Java Virtual Machine,Code Instrumentation,Java Program,Model Check},
  pages = {462-465},
  file = {/Users/luigi/work/zotero/storage/U7FK9JJE/Artho et al. - 2004 - JNuke Efficient Dynamic Analysis for Java.pdf}
}

@inproceedings{boehmThreadsCannotBe2005,
  address = {New York, NY, USA},
  series = {{{PLDI}} '05},
  title = {Threads {{Cannot Be Implemented As}} a {{Library}}},
  isbn = {978-1-59593-056-9},
  doi = {10.1145/1065010.1065042},
  abstract = {In many environments, multi-threaded code is written in a language that was originally designed without thread support (e.g. C), to which a library of threading primitives was subsequently added. There appears to be a general understanding that this is not the right approach. We provide specific arguments that a pure library approach, in which the compiler is designed independently of threading issues, cannot guarantee correctness of the resulting code.We first review why the approach almost works, and then examine some of the surprising behavior it may entail. We further illustrate that there are very simple cases in which a pure library-based approach seems incapable of expressing an efficient parallel algorithm.Our discussion takes place in the context of C with Pthreads, since it is commonly used, reasonably well specified, and does not attempt to ensure type-safety, which would entail even stronger constraints. The issues we raise are not specific to that context.},
  booktitle = {Proceedings of the 2005 {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Boehm, Hans-J.},
  year = {2005},
  keywords = {optimization,data race,pthreads,register promotion,threads},
  pages = {261--268},
  file = {/Users/luigi/work/zotero/storage/8ML7T9N5/Boehm - 2005 - Threads Cannot Be Implemented As a Library.pdf}
}

@techreport{goldsmithLightWeightInstrumentationRelational2004,
  address = {Fort Belvoir, VA},
  title = {Light-{{Weight Instrumentation From Relational Queries Over Program Traces}}:},
  shorttitle = {Light-{{Weight Instrumentation From Relational Queries Over Program Traces}}},
  abstract = {Instrumenting programs with code to monitor their dynamic behaviour is a technique as old as computing. Today, most instrumentation is either inserted manually by programmers, which is tedious, or automatically by specialized tools, which are nontrivial to build and monitor particular properties. We introduce Program Trace Query Language (PTQL), a general language in which programmers can write expressive, declarative queries about program behaviour. PTQL is based on relational queries over program traces. We argue that PTQL is more amenable to human and machine understanding than competing languages. We also describe a compiler, Partiqle, that takes a PTQL query and a Java program and produces an instrumented program. This instrumented program runs normally but also evaluates the PTQL query on-line. We explain some novel optimizations required to compile relational queries into efficient instrumentation. To help evaluate our work, we present the results of applying a variety of PTQL queries to a set of benchmark programs, including the Apache Tomcat Web server. The results show that our prototype system already has usable performance, and that our optimizations are critical to obtaining this performance. Our queries also revealed significant (and apparently unknown) performance bugs in the jack SpecJVM98 benchmark, in Tomcat, and in the IBM Java class library, and some uncomfortably clever code in the Xerces XML parser.},
  language = {en},
  institution = {{Defense Technical Information Center}},
  author = {Goldsmith, Simon and O'Callahan, Robert and Aiken, Alex},
  month = mar,
  year = {2004},
  file = {/Users/luigi/work/zotero/storage/J7WXUSSF/Goldsmith et al. - 2004 - Light-Weight Instrumentation From Relational Queri.pdf},
  doi = {10.21236/ADA603315}
}

@article{hirzelConnectivityBasedGarbageCollection,
  title = {Connectivity-{{Based Garbage Collection}}},
  language = {en},
  author = {Hirzel, Martin},
  pages = {203},
  file = {/Users/luigi/work/zotero/storage/R8I2I99Q/Hirzel - Connectivity-Based Garbage Collection.pdf}
}

@article{lengletComposingTransformationsCompiled2004,
  title = {Composing Transformations of Compiled {{Java}} Programs with {{Jabyce}}},
  volume = {1},
  abstract = {This article introduces Jabyce, a software framework for the implementation and composition of transformations of compiled Java programs. Most distinguishing features of Jabyce are 1) its interaction orientation, i.e. it represents elements of transformed programs as interactions (method calls) which generally consumes less memory and CPU time than representing programs as graphs of objects; and 2) its component orientation, i.e. it allows for the design and composition of transformers as software components based on the Fractal component model. This latter point is strongly connected to infra-structural and architectural issues, and software engineering aspects such as composing, scaling, maintaining and evolving transformers. Jabyce is compared with other existing compiled Java programs transformation systems using an extension of a previous well-known categorization of program transformation systems.},
  number = {2},
  journal = {Computer Science and Information Systems},
  author = {Lenglet, Romain and Coupaye, Thierry and Bruneton, Eric},
  year = {2004},
  pages = {83-125},
  file = {/Users/luigi/work/zotero/storage/HWEK2URB/Lenglet et al. - 2004 - Composing transformations of compiled Java program.pdf;/Users/luigi/work/zotero/storage/U5YRNBT7/Article.html}
}

@inproceedings{saffMockObjectCreation2004,
  address = {New York, NY, USA},
  series = {{{PASTE}} '04},
  title = {Mock {{Object Creation}} for {{Test Factoring}}},
  isbn = {978-1-58113-910-5},
  doi = {10.1145/996821.996838},
  abstract = {Test factoring creates fast, focused unit tests from slow system-wide tests; each new unit test exercises only a subset of the functionality exercised by the system tests. Augmenting a test suite with factored unit tests, and prioritizing the tests, should catch errors earlier in a test run.One way to factor a test is to introduce mock objects. If a test exercises a component A, which is designed to issue queries against or mutate another component B, the implementation of B can be replaced by a mock. The mock has two purposes: it checks that A's calls to B are as expected, and it simulates B's behavior in response. Given a system test for A and B, and a record of A's and B's behavior when the system test is run, we would like to automatically generate unit tests for A in which B is mocked. The factored tests can isolate bugs in A from bugs in B and, if B is slow or expensive, improve test performance or cost.This paper motivates test factoring with an illustrative example, proposes a simple procedure for automatically generating mock objects for factored tests, and gives examples of how the procedure can be extended to produce more robust factored tests.},
  booktitle = {Proceedings of the 5th {{ACM SIGPLAN}}-{{SIGSOFT Workshop}} on {{Program Analysis}} for {{Software Tools}} and {{Engineering}}},
  publisher = {{ACM}},
  author = {Saff, David and Ernst, Michael D.},
  year = {2004},
  keywords = {mock objects,test factoring,unit testing},
  pages = {49--51},
  file = {/Users/luigi/work/zotero/storage/IJVK2JYD/Saff and Ernst - 2004 - Mock Object Creation for Test Factoring.pdf}
}

@article{harperPracticalFoundationsProgramming,
  title = {Practical {{Foundations}} for {{Programming Languages}}},
  language = {en},
  author = {Harper, Robert},
  pages = {590},
  file = {/Users/luigi/work/zotero/storage/J7KCGSV2/Harper - Practical Foundations for Programming Languages.pdf;/Users/luigi/work/zotero/storage/KLSF45DY/Harper - Practical Foundations for Programming Languages.pdf}
}

@article{jhalaLazyAbstraction,
  title = {Lazy {{Abstraction}}},
  language = {en},
  author = {Jhala, Ranjit},
  pages = {169},
  file = {/Users/luigi/work/zotero/storage/84MWEYQJ/Jhala - Lazy Abstraction.pdf;/Users/luigi/work/zotero/storage/RDSIMDMT/Jhala - Lazy Abstraction.pdf}
}

@article{arthoSubroutineInliningBytecode2005,
  series = {Proceedings of the {{First Workshop}} on {{Bytecode Semantics}}, {{Verification}}, {{Analysis}} and {{Transformation}} ({{Bytecode}} 2005)},
  title = {Subroutine {{Inlining}} and {{Bytecode Abstraction}} to {{Simplify Static}} and {{Dynamic Analysis}}},
  volume = {141},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2005.02.034},
  abstract = {In Java bytecode, intra-method subroutines are employed to represent code in ``finally'' blocks. The use of such polymorphic subroutines within a method makes bytecode analysis very difficult. Fortunately, such subroutines can be eliminated through recompilation or inlining. Inlining is the obvious choice since it does not require changing compilers or access to the source code. It also allows transformation of legacy bytecode. However, the combination of nested, non-contiguous subroutines with overlapping exception handlers poses a difficult challenge. This paper presents an algorithm that successfully solves all these problems without producing superfluous instructions. Furthermore, inlining can be combined with bytecode simplification, using abstract bytecode. We show how this abstration is extended to the full set of instructions and how it simplifies static and dynamic analysis.},
  number = {1},
  journal = {Electronic Notes in Theoretical Computer Science},
  author = {Artho, Cyrille and Biere, Armin},
  month = dec,
  year = {2005},
  keywords = {dynamic analysis,static analysis,inlining,Java bytecode analysis},
  pages = {109-128},
  file = {/Users/luigi/work/zotero/storage/U6C4ECQ9/Artho and Biere - 2005 - Subroutine Inlining and Bytecode Abstraction to Si.pdf;/Users/luigi/work/zotero/storage/KCV6W3US/S1571066105051467.html}
}

@inproceedings{chenCombiningProgrammingTheorem2005,
  address = {New York, NY, USA},
  series = {{{ICFP}} '05},
  title = {Combining {{Programming}} with {{Theorem Proving}}},
  isbn = {978-1-59593-064-4},
  doi = {10.1145/1086365.1086375},
  abstract = {Applied Type System (ATS) is recently proposed as a framework for designing and formalizing (advanced) type systems in support of practical programming. In ATS, the definition of type equality involves a constraint relation, which may or may not be algorithmically decidable. To support practical programming, we adopted a design in the past that imposes certain restrictions on the syntactic form of constraints so that some effective means can be found for solving constraints automatically. Evidently, this is a rather em ad hoc design in its nature. In this design, which we claim to be both novel and practical. Instead of imposing syntactical restrictions on constraints, we provide a means for the programmer to construct proofs that attest to the validity of constraints. In particular, we are to accommodate a programming paradigm that enables the programmer to combine programming with theorem proving. Also we present some concrete examples in support of the practicality of this design.},
  booktitle = {Proceedings of the {{Tenth ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Chen, Chiyan and Xi, Hongwei},
  year = {2005},
  keywords = {dependent types,theorem proving,applied type system,ATS,proof erasure},
  pages = {66--77},
  file = {/Users/luigi/work/zotero/storage/7A9DNWL8/Chen and Xi - 2005 - Combining Programming with Theorem Proving.pdf}
}

@inproceedings{chapmanCorrectnessConstructionManifesto2006,
  address = {Darlinghurst, Australia, Australia},
  series = {{{SCS}} '05},
  title = {Correctness by {{Construction}}: {{A Manifesto}} for {{High Integrity Software}}},
  isbn = {978-1-920682-37-8},
  shorttitle = {Correctness by {{Construction}}},
  abstract = {High integrity software systems are often so large that conventional development processes cannot get anywhere near achieving tolerable defect rates. This paper presents Correctness by Construction (CbyC)--an approach that has delivered very low defect rate software cost-effectively. We describe the main principles of CbyC and the results achieved to date. We also touch on some of the barriers that we have encountered in trying to field CbyC within our own and other organisations.},
  booktitle = {Proceedings of the 10th {{Australian Workshop}} on {{Safety Critical Systems}} and {{Software}} - {{Volume}} 55},
  publisher = {{Australian Computer Society, Inc.}},
  author = {Chapman, Roderick},
  year = {2006},
  keywords = {software engineering,correctness-by-construction,safety-critical,security-critical,SPARK},
  pages = {43--46},
  file = {/Users/luigi/work/zotero/storage/QJCC5NPP/Chapman - 2006 - Correctness by Construction A Manifesto for High .pdf}
}

@article{garbervetskyProgramInstrumentationRunTime2005,
  series = {Proceedings of the {{Fourth Workshop}} on {{Runtime Verification}} ({{RV}} 2004)},
  title = {Program {{Instrumentation}} and {{Run}}-{{Time Analysis}} of {{Scoped Memory}} in {{Java}}},
  volume = {113},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2004.01.031},
  abstract = {We present a method to analyze, monitor and control dynamic memory allocation in Java. It first consists in performing pointer and escape analysis to detect memory scopes. This information is used to automatically instrument Java programs in such a way memory is allocated and freed by a region-based memory manager. Our source code instrumentation fully exploits the result of scope analysis by dynamically mapping allocation places to the region stack at runtime via a registering mechanism. Moreover, it allows executing the same transformed program with different implementations of scoped-memory managers and perform different run-time analysis without changing the transformed code. In particular, we consider a class of managers that handle variable-size regions composed of fixed-size memory blocks for which we provide analytical models for the intra- and inter-region fragmentation. These models can be used to observe and control fragmentation at run-time with negligible overhead. We describe a prototype tool that implements our approach.},
  journal = {Electronic Notes in Theoretical Computer Science},
  author = {Garbervetsky, D. and Nakhli, C. and Yovine, S. and Zorgati, H.},
  month = jan,
  year = {2005},
  keywords = {Java,Memory management,Real-time and embedded systems,Run-time analysis},
  pages = {105-121},
  file = {/Users/luigi/work/zotero/storage/ZQMN7H8B/Garbervetsky et al. - 2005 - Program Instrumentation and Run-Time Analysis of S.pdf;/Users/luigi/work/zotero/storage/CIXAXQJH/S1571066104052557.html}
}

@inproceedings{goldsmithRelationalQueriesProgram2005,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '05},
  title = {Relational {{Queries}} over {{Program Traces}}},
  isbn = {978-1-59593-031-6},
  doi = {10.1145/1094811.1094841},
  abstract = {Instrumenting programs with code to monitor runtime behavior is a common technique for profiling and debugging. In practice, instrumentation is either inserted manually by programmers, or automatically by specialized tools that monitor particular properties. We propose Program Trace Query Language (PTQL), a language based on relational queries over program traces, in which programmers can write expressive, declarative queries about program behavior. We also describe our compiler, Partiqle. Given a PTQL query and a Java program, Partiqle instruments the program to execute the query on-line. We apply several PTQL queries to a set of benchmark programs, including the Apache Tomcat Web server. Our queries reveal significant performance bugs in the jack SpecJVM98 benchmark, in Tomcat, and in the IBM Java class library, as well as some correct though uncomfortably subtle code in the Xerces XML parser. We present performance measurements demonstrating that our prototype system has usable performance.},
  booktitle = {Proceedings of the 20th {{Annual ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Goldsmith, Simon F. and O'Callahan, Robert and Aiken, Alex},
  year = {2005},
  keywords = {partiqle,program trace query language,PTQL,relational},
  pages = {385--402},
  file = {/Users/luigi/work/zotero/storage/I82ZEDNI/Goldsmith et al. - 2005 - Relational Queries over Program Traces.pdf}
}

@article{leaJavaUtilConcurrent2005,
  series = {Special {{Issue}} on {{Concurrency}} and Synchonization in {{Java}} Programs},
  title = {The Java.Util.Concurrent Synchronizer Framework},
  volume = {58},
  issn = {0167-6423},
  doi = {10.1016/j.scico.2005.03.007},
  abstract = {Most synchronizers (locks, barriers, etc.) in the J2SE 5.0 java.util.concurrent package are constructed using a small framework based on class AbstractQueuedSynchronizer. This framework provides common mechanics for atomically managing synchronization state, blocking and unblocking threads, and queuing. The paper describes the rationale, design, implementation, usage, and performance of this framework.},
  number = {3},
  journal = {Science of Computer Programming},
  author = {Lea, Doug},
  month = dec,
  year = {2005},
  pages = {293-309},
  file = {/Users/luigi/work/zotero/storage/Z2EFZBIL/Lea - 2005 - The java.util.concurrent synchronizer framework.pdf;/Users/luigi/work/zotero/storage/XIDL5AIG/S0167642305000663.html}
}

@article{martinFindingApplicationErrors,
  title = {Finding {{Application Errors}} and {{Security Flaws Using PQL}}: A {{Program Query Language}}},
  abstract = {A number of effective error detection tools have been built in recent years to check if a program conforms to certain design rules. An important class of design rules deals with sequences of events associated with a set of related objects. This paper presents a language called PQL (Program Query Language) that allows programmers to express such questions easily in an application-specific context. A query looks like a code excerpt corresponding to the shortest amount of code that would violate a design rule. Details of the target application's precise implementation are abstracted away. The programmer may also specify actions to perform when a match is found, such as recording relevant information or even correcting an erroneous execution on the fly.},
  language = {en},
  author = {Martin, Michael and Livshits, Benjamin and Lam, Monica S},
  pages = {19},
  file = {/Users/luigi/work/zotero/storage/95T7C352/Martin et al. - Finding Application Errors and Security Flaws Usin.pdf;/Users/luigi/work/zotero/storage/WA46RQS3/Martin et al. - Finding Application Errors and Security Flaws Usin.pdf}
}

@article{streitz100TFlopSolidification,
  title = {100+ {{TFlop Solidification Simulations}} on {{BlueGene}}/{{L}}},
  language = {en},
  author = {Streitz, Frederick H and Glosli, James N and Patel, Mehul V and Chan, Bor and Yates, Robert K and {de Supinski}, Bronis R and Sexton, James and Gunnels, John A},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/GKKTHVNN/Streitz et al. - 100+ TFlop Solidiﬁcation Simulations on BlueGeneL.pdf}
}

@inproceedings{zhangMatchingExecutionHistories2005,
  address = {New York, NY, USA},
  series = {{{ESEC}}/{{FSE}}-13},
  title = {Matching {{Execution Histories}} of {{Program Versions}}},
  isbn = {978-1-59593-014-9},
  doi = {10.1145/1081706.1081738},
  abstract = {We develop a method for matching dynamic histories of program executions of two program versions. The matches produced can be useful in many applications including software piracy detection and several debugging scenarios. Unlike some static approaches for matching program versions, our approach does not require access to source code of the two program versions because dynamic histories can be collected by running instrumented versions of program binaries. We base our matching algorithm on comparison of rich program execution histories which include: control flow taken, values produced, addresses referenced, as well as data dependences exercised. In developing a matching algorithm we had two goals: producing an accurate match and producing it quickly. By using rich execution history, we are able to compare the program versions across many behavioral dimensions. The result is a fast and highly precise matching algorithm. Our algorithm first uses individual histories of instructions to identify multiple potential matches and then it refines the set of matches by matching the data dependence structure established by the matching instructions. To test our algorithm we attempted matching of execution histories of unoptimized and optimized program versions. Our results show that our algorithm produces highly accurate matches which are highly effective when used in comparison checking approach to debugging optimized code.},
  booktitle = {Proceedings of the 10th {{European Software Engineering Conference Held Jointly}} with 13th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Zhang, Xiangyu and Gupta, Rajiv},
  year = {2005},
  keywords = {debugging,dynamic analysis,execution traces,piracy detection},
  pages = {197--206},
  file = {/Users/luigi/work/zotero/storage/MAD6MFTD/Zhang and Gupta - 2005 - Matching Execution Histories of Program Versions.pdf}
}

@article{zhuSafeProgrammingPointers,
  title = {Safe {{Programming}} with {{Pointers}} through {{Stateful Views}}},
  abstract = {The need for direct memory manipulation through pointers is essential in many applications. However, it is also commonly understood that the use (or probably misuse) of pointers is often a rich source of program errors. Therefore, approaches that can effectively enforce safe use of pointers in programming are highly sought after. ATS is a programming language with a type system rooted in a recently developed framework Applied Type System, and a novel and desirable feature in ATS lies in its support for safe programming with pointers through a novel notion of stateful views. In particular, even pointer arithmetic is allowed in ATS and guaranteed to be safe by the type system of ATS. In this paper, we give an overview of this feature in ATS, presenting some interesting examples based on a prototype implementation of ATS to demonstrate the practicality of safe programming with pointer through stateful views.},
  language = {en},
  author = {Zhu, Dengping and Xi, Hongwei},
  pages = {15},
  file = {/Users/luigi/work/zotero/storage/6GTAR6V4/Zhu and Xi - Safe Programming with Pointers through Stateful Vi.pdf}
}

@article{asanovicLandscapeParallelComputing,
  title = {The {{Landscape}} of {{Parallel Computing Research}}: {{A View}} from {{Berkeley}}},
  abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation.},
  language = {en},
  author = {Asanov\'ic, Krste and Bodik, Rastislav and Catanzaro, Bryan and Gebis, Joseph and Husbands, Parry and Keutzer, Kurt and Patterson, David and Plishker, William and Shalf, John and Williams, Samuel and Yelick, Katherine},
  pages = {54},
  file = {/Users/luigi/work/zotero/storage/W83ERAQP/Asanovíc et al. - The Landscape of Parallel Computing Research A Vi.pdf}
}

@inproceedings{dwyerAdaptiveOnlineProgram2007,
  address = {Washington, DC, USA},
  series = {{{ICSE}} '07},
  title = {Adaptive {{Online Program Analysis}}},
  isbn = {978-0-7695-2828-1},
  doi = {10.1109/ICSE.2007.12},
  abstract = {Analyzing a program run can provide important insights about its correctness. Dynamic analysis of complex correctness properties, however, usually results in significant run-time overhead and, consequently, it is rarely used in practice. In this paper, we present an approach for exploiting properties of stateful program specifications to reduce the cost of their dynamic analysis. With our approach, analysis results are guaranteed to be identical to those of a traditional expensive dynamic analyses, while analysis cost is very low -- between 23\% and 33\% more than the un-instrumented program for the analyses we studied. We describe the principles behind our adaptive online program analysis technique, extensions to our Java run-time analysis framework that support such analyses, and report on the performance and capabilities of two different families of adaptive online program analyses.},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Software Engineering}}},
  publisher = {{IEEE Computer Society}},
  author = {Dwyer, Matthew B. and Kinneer, Alex and Elbaum, Sebastian},
  year = {2007},
  pages = {220--229},
  file = {/Users/luigi/work/zotero/storage/PKZTIDY9/Dwyer et al. - 2007 - Adaptive Online Program Analysis.pdf}
}

@article{aftandilianGCAssertionsUsing,
  title = {{{GC Assertions}}: {{Using}} the {{Garbage Collector}} to {{Check Heap Properties}}},
  abstract = {This paper introduces GC assertions, a system interface that programmers can use to check for errors, such as data structure invariant violations, and to diagnose performance problems, such as memory leaks. GC assertions are checked by the garbage collector, which is in a unique position to gather information and answer questions about the lifetime and connectivity of objects in the heap. We introduce several kinds of GC assertions, and we describe how they are implemented in the collector. We also describe our reporting mechanism, which provides a complete path through the heap to the offending objects. We show results for one type of assertion that allows the programmer to indicate that an object should be reclaimed at the next GC. We find that using this assertion we can quickly identify a memory leak and its cause with negligible overhead.},
  language = {en},
  author = {Aftandilian, Edward and Guyer, Samuel Z},
  pages = {5},
  file = {/Users/luigi/work/zotero/storage/4NSJMX7F/Aftandilian and Guyer - GC Assertions Using the Garbage Collector to Chec.pdf;/Users/luigi/work/zotero/storage/FKXDQHKQ/Aftandilian and Guyer - GC Assertions Using the Garbage Collector to Chec.pdf;/Users/luigi/work/zotero/storage/NYAWFKCC/Aftandilian and Guyer - GC Assertions Using the Garbage Collector to Chec.pdf;/Users/luigi/work/zotero/storage/QMLRMTMZ/Aftandilian and Guyer - GC Assertions Using the Garbage Collector to Chec.pdf;/Users/luigi/work/zotero/storage/TD7IQELR/Aftandilian and Guyer - GC Assertions Using the Garbage Collector to Chec.pdf}
}

@inproceedings{bettenburgWhatMakesGood2008,
  address = {New York, NY, USA},
  series = {{{SIGSOFT}} '08/{{FSE}}-16},
  title = {What {{Makes}} a {{Good Bug Report}}?},
  isbn = {978-1-59593-995-1},
  doi = {10.1145/1453101.1453146},
  abstract = {In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are at the same time most difficult to provide for users. Such insight is helpful to design new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. In our experiments, CUEZILLA was able to predict the quality of 31--48\% of bug reports accurately.},
  booktitle = {Proceedings of the 16th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Bettenburg, Nicolas and Just, Sascha and Schr\"oter, Adrian and Weiss, Cathrin and Premraj, Rahul and Zimmermann, Thomas},
  year = {2008},
  pages = {308--318},
  file = {/Users/luigi/work/zotero/storage/4KYYEEIG/Bettenburg et al. - 2008 - What Makes a Good Bug Report.pdf}
}

@inproceedings{lindermanMergeProgrammingModel2008,
  address = {New York, NY, USA},
  series = {{{ASPLOS XIII}}},
  title = {Merge: {{A Programming Model}} for {{Heterogeneous Multi}}-Core {{Systems}}},
  isbn = {978-1-59593-958-6},
  shorttitle = {Merge},
  doi = {10.1145/1346281.1346318},
  abstract = {In this paper we propose the Merge framework, a general purpose programming model for heterogeneous multi-core systems. The Merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. The Merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. Using a generic sequencer architecture interface for heterogeneous accelerators, the Merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. The Merge framework has been prototyped on a heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core 32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous 32-way Unisys SMP system with Intel Xeon processors. We implemented a set of benchmarks using the Merge framework and enhanced the library with X3000 specific implementations, achieving speedups of 3.6x -- 8.5x using the X3000 and 5.2x -- 22x using the 32-way system relative to the straight C reference implementation on a single IA32 core.},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  publisher = {{ACM}},
  author = {Linderman, Michael D. and Collins, Jamison D. and Wang, Hong and Meng, Teresa H.},
  year = {2008},
  keywords = {GPGPU,heterogeneous multi-core,predicate dispatch},
  pages = {287--296},
  file = {/Users/luigi/work/zotero/storage/WZZY6Q2C/Linderman et al. - 2008 - Merge A Programming Model for Heterogeneous Multi.pdf}
}

@article{musuvathiFindingReproducingHeisenbugs2008,
  title = {Finding and {{Reproducing Heisenbugs}} in {{Concurrent Programs}}},
  abstract = {Concurrency is pervasive in large systems. Unexpected interference among threads often results in ``Heisenbugs'' that are extremely difficult to reproduce and eliminate. We have implemented a tool called Chess for finding and reproducing such bugs. When attached to a program, Chess takes control of thread scheduling and uses efficient search techniques to drive the program \ldots{}},
  language = {en-US},
  journal = {Microsoft Research},
  author = {Musuvathi, Madan and Qadeer, Shaz and Ball, Tom and Basler, Gerard and Nainar, Piramanayakam Arumuga and Neamtiu, Iulian},
  month = dec,
  year = {2008},
  file = {/Users/luigi/work/zotero/storage/32VK2887/Musuvathi et al. - 2008 - Finding and Reproducing Heisenbugs in Concurrent P.pdf;/Users/luigi/work/zotero/storage/55KC37KC/finding-and-reproducing-heisenbugs-in-concurrent-programs.html}
}

@inproceedings{nanzModalAbstractionsConcurrent2008,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Modal {{Abstractions}} of {{Concurrent Behaviour}}},
  isbn = {978-3-540-69166-2},
  abstract = {We present a novel algorithm for the automatic construction of modal transition systems as abstractions of concurrent processes. Modal transition systems are recognised as valuable abstractions for model checking because they allow for the deduction of safety as well as liveness properties. However, the issue of effectively creating these abstractions from specification languages such as process algebras is a missing link that prevents their more widespread usage for model checking of concurrent systems. Our algorithm is based on static analysis and uses a lattice of intervals to express simultaneous over- and under-approximations to the set of process actions available in a particular state. We obtain an abstraction that is 3-valued in both states and transitions and that naturally integrates with model checking approaches for modal transition systems.},
  language = {en},
  booktitle = {Static {{Analysis}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Nanz, Sebastian and Nielson, Flemming and Riis Nielson, Hanne},
  editor = {Alpuente, Mar\'ia and Vidal, Germ\'an},
  year = {2008},
  pages = {159-173},
  file = {/Users/luigi/work/zotero/storage/J5UIZHGG/Nanz et al. - 2008 - Modal Abstractions of Concurrent Behaviour.pdf}
}

@article{emanuelssonComparativeStudyIndustrial2008,
  series = {Proceedings of the 3rd {{International Workshop}} on {{Systems Software Verification}} ({{SSV}} 2008)},
  title = {A {{Comparative Study}} of {{Industrial Static Analysis Tools}}},
  volume = {217},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2008.06.039},
  abstract = {Tools based on static analysis can be used to find defects in programs. Tools that do shallow analyses based on pattern matching have existed since the 1980's and although they can analyze large programs they have the drawback of producing a massive amount of warnings that have to be manually analyzed to see if they are real defects or not. Recent technology advances has brought forward tools that do deeper analyses that discover more defects and produce a limited amount of false warnings. These tools can still handle large industrial applications with millions lines of code. This article surveys the underlying supporting technology of three state-of-the-art static analysis tools. The survey relies on information in research articles and manuals, and includes the types of defects checked for (such as memory management, arithmetics, security vulnerabilities), soundness, value and aliasing analyses, incrementality and IDE integration. This survey is complemented by practical experiences from evaluations at the Ericsson telecom company.},
  journal = {Electronic Notes in Theoretical Computer Science},
  author = {Emanuelsson, P\"ar and Nilsson, Ulf},
  month = jul,
  year = {2008},
  keywords = {Static analysis,dataflow analysis,defects,security vulnerabilities},
  pages = {5-21},
  file = {/Users/luigi/work/zotero/storage/KVR9X5SK/Emanuelsson and Nilsson - 2008 - A Comparative Study of Industrial Static Analysis .pdf;/Users/luigi/work/zotero/storage/XABNXAKV/S1571066108003824.html}
}

@inproceedings{papiPracticalPluggableTypes2008,
  address = {New York, NY, USA},
  series = {{{ISSTA}} '08},
  title = {Practical {{Pluggable Types}} for {{Java}}},
  isbn = {978-1-60558-050-0},
  doi = {10.1145/1390630.1390656},
  abstract = {This paper introduces the Checker Framework, which supports adding pluggable type systems to the Java language in a backward-compatible way. A type system designer defines type qualifiers and their semantics, and a compiler plug-in enforces the semantics. Programmers can write the type qualifiers in their programs and use the plug-in to detect or prevent errors. The Checker Framework is useful both to programmers who wish to write error-free code, and to type system designers who wish to evaluate and deploy their type systems. The Checker Framework includes new Java syntax for expressing type qualifiers; declarative and procedural mechanisms for writing type-checking rules; and support for flow-sensitive local type qualifier inference and for polymorphism over types and qualifiers. The Checker Framework is well-integrated with the Java language and toolset. We have evaluated the Checker Framework by writing 5 checkers and running them on over 600K lines of existing code. The checkers found real errors, then confirmed the absence of further errors in the fixed code. The case studies also shed light on the type systems themselves.},
  booktitle = {Proceedings of the 2008 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  publisher = {{ACM}},
  author = {Papi, Matthew M. and Ali, Mahmood and Correa, Jr., Telmo Luis and Perkins, Jeff H. and Ernst, Michael D.},
  year = {2008},
  keywords = {verification,java,compiler,polymorphism,type system,annotation,bug finding,case study,flow sensitivity,igj,immutable,intern,javac,javari,nonnull,pluggable type,readonly,type qualifier},
  pages = {201--212},
  file = {/Users/luigi/work/zotero/storage/MI5UAXFQ/Papi et al. - 2008 - Practical Pluggable Types for Java.pdf}
}

@inproceedings{rondonLiquidTypes2008,
  address = {New York, NY, USA},
  series = {{{PLDI}} '08},
  title = {Liquid {{Types}}},
  isbn = {978-1-59593-860-2},
  doi = {10.1145/1375581.1375602},
  abstract = {We present Logically Qualified Data Types, abbreviated to Liquid Types, a system that combines Hindley-Milner type inference with Predicate Abstraction to automatically infer dependent types precise enough to prove a variety of safety properties. Liquid types allow programmers to reap many of the benefits of dependent types, namely static verification of critical properties and the elimination of expensive run-time checks, without the heavy price of manual annotation. We have implemented liquid type inference in DSOLVE, which takes as input an OCAML program and a set of logical qualifiers and infers dependent types for the expressions in the OCAML program. To demonstrate the utility of our approach, we describe experiments using DSOLVE to statically verify the safety of array accesses on a set of OCAML benchmarks that were previously annotated with dependent types as part of the DML project. We show that when used in conjunction with a fixed set of array bounds checking qualifiers, DSOLVE reduces the amount of manual annotation required for proving safety from 31\% of program text to under 1\%.},
  booktitle = {Proceedings of the 29th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Rondon, Patrick M. and Kawaguci, Ming and Jhala, Ranjit},
  year = {2008},
  keywords = {type inference,dependent types,hindley-milner,predicate abstraction},
  pages = {159--169},
  file = {/Users/luigi/work/zotero/storage/334H55DD/Rondon et al. - 2008 - Liquid Types.pdf}
}

@article{wilkinsonSpeculativeMultithreadingObjectDriven,
  title = {Speculative {{Multithreading}}: {{An Object}}-{{Driven Approach}}},
  abstract = {Speculative multithreading (SpMT) is a parallelizing execution model for single-threaded programs on multi-core architectures. In this paper, we introduce a new SpMT model, Object-Driven Speculative Multithreading, which exploits the structure and semantics of object-oriented programs to generate speculative parallelism. Within our technique, individual program objects take the responsibility to predict their own future behavior, and to speculatively mutate their own state and that of other objects. We present a detailed description of the Object-Driven Speculative Multithreading model, discuss our current progress and challenges, and include some preliminary results.},
  language = {en},
  author = {Wilkinson, Simon and Watson, Ian},
  pages = {8},
  file = {/Users/luigi/work/zotero/storage/KJRBZVHT/Wilkinson and Watson - Speculative Multithreading An Object-Driven Appro.pdf}
}

@inproceedings{xuPreciseMemoryLeak2008,
  title = {Precise Memory Leak Detection for Java Software Using Container Profiling},
  doi = {10.1145/1368088.1368110},
  abstract = {A memory leak in a Java program occurs when object references that are no longer needed are unnecessarily maintained. Such leaks are difficult to understand because static analyses typically cannot precisely identify these redundant references, and existing dynamic analyses for leak detection track and report fine-grained information about individual objects, producing results that are usually hard to interpret and lack precision. We introduce a novel container-based heap-tracking technique, based on the observation that many memory leaks in Java programs occur due to containers that keep references to unused data entries. The novelty of the described work is two-fold: (1) instead of tracking arbitrary objects and finding leaks by analyzing references to unused objects, the technique tracks only containers and directly identifies the source of the leak, and (2) the approach computes a confidence value for each container based on a combination of its memory consumption and its elements' staleness (time since last retrieval), while previous approaches do not consider such combined metrics. Our experimental results show that the reports generated by the proposed technique can be very precise: for two bugs reported by Sun and for a known bug in SPECjbb, the top containers in the reports include the containers that leak memory.},
  booktitle = {2008 {{ACM}}/{{IEEE}} 30th {{International Conference}} on {{Software Engineering}}},
  author = {Xu, G. and Rountev, A.},
  month = may,
  year = {2008},
  keywords = {Java,Programming profession,Runtime,Software maintenance,software engineering,Software engineering,Computer bugs,memory leaks,Java programs,Containers,container profiling,Data structures,Information analysis,Java software,Leak detection,leaking confidence,precise memory leak detection},
  pages = {151-160},
  file = {/Users/luigi/work/zotero/storage/5Y2T3YW5/Xu and Rountev - 2008 - Precise memory leak detection for java software us.pdf;/Users/luigi/work/zotero/storage/7MG35ZP2/4814126.html}
}

@article{cadarKLEEUnassistedAutomatic,
  title = {{{KLEE}}: {{Unassisted}} and {{Automatic Generation}} of {{High}}-{{Coverage Tests}} for {{Complex Systems Programs}}},
  abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We applied KLEE to all 90 programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on almost all Unix systems and, as such, represent some of the most heavily used and tested open-source programs in existence. For 84\% of these utilities, KLEE's automatically generated tests covered 80\textendash{}100\% of executable statements and, in aggregate, significantly beat the coverage of the developers' own hand-written test suites. KLEE also found nine serious bugs (including three that had been missed for over 15 years!) and produced concrete inputs that triggered the errors when run on the uninstrumented code. When applied to MINIX's versions of a small selection of the same applications, KLEE achieved similar coverage (along with two bugs). In addition, we also used KLEE to automatically find numerous incorrect differences between several MINIX and COREUTILS tools. Finally, we checked the kernel of the HISTAR operating system, generating tests that achieved 76.4\% (without paging enabled) and 67.1\% coverage (with paging) and found one important security bug.},
  language = {en},
  author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/9JTVHY4X/Cadar et al. - KLEE Unassisted and Automatic Generation of High-.pdf;/Users/luigi/work/zotero/storage/FEKC7PZQ/Cadar et al. - KLEE Unassisted and Automatic Generation of High-.pdf}
}

@inproceedings{wadlerViewsWayPattern1987,
  address = {New York, NY, USA},
  series = {{{POPL}} '87},
  title = {Views: {{A Way}} for {{Pattern Matching}} to {{Cohabit}} with {{Data Abstraction}}},
  isbn = {978-0-89791-215-0},
  shorttitle = {Views},
  doi = {10.1145/41625.41653},
  abstract = {Pattern matching and data abstraction are important concepts in designing programs, but they do not fit well together. Pattern matching depends on making public a free data type representation, while data abstraction depends on hiding the representation. This paper proposes the views mechanism as a means of reconciling this conflict. A view allows any type to be viewed as a free data type, thus combining the clarity of pattern matching with the efficiency of data abstraction.},
  booktitle = {Proceedings of the 14th {{ACM SIGACT}}-{{SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Wadler, P.},
  year = {1987},
  pages = {307--313},
  file = {/Users/luigi/work/zotero/storage/7I4SKPY6/Wadler - 1987 - Views A Way for Pattern Matching to Cohabit with .pdf}
}

@inproceedings{vogelsangSoftwareMetricsStatic2010,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Software {{Metrics}} in {{Static Program Analysis}}},
  isbn = {978-3-642-16901-4},
  abstract = {Software metrics play an important role in the management of professional software projects. Metrics are used, e.g., to track development progress, to measure restructuring impact and to estimate code quality. They are most beneficial if they can be computed continuously at development time. This work presents a framework and an implementation for integrating metric computations into static program analysis. The contributions are a language and formal semantics for user-definable metrics, an implementation and integration in the existing static analysis tool , and a user-definable visualization approach to display metrics results. Moreover, we report our experiences on a case study of a popular open source code base.},
  language = {en},
  booktitle = {Formal {{Methods}} and {{Software Engineering}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Vogelsang, Andreas and Fehnker, Ansgar and Huuck, Ralf and Reif, Wolfgang},
  editor = {Dong, Jin Song and Zhu, Huibiao},
  year = {2010},
  keywords = {software maintenance,software quality,software metrics,static program analysis},
  pages = {485-500},
  file = {/Users/luigi/work/zotero/storage/NUI55MAS/Vogelsang et al. - 2010 - Software Metrics in Static Program Analysis.pdf}
}

@inproceedings{dyerMiningSourceCode2013,
  address = {New York, NY, USA},
  series = {{{SPLASH}} '13},
  title = {Mining {{Source Code Repositories}} with {{Boa}}},
  isbn = {978-1-4503-1995-9},
  doi = {10.1145/2508075.2514570},
  abstract = {Mining source code has become a common task for researchers and yielded significant benefits for the software engineering community. Mining source code however is a very difficult and time consuming task. The Boa language and infrastructure was designed to ease mining of project and revision metadata. Recently Boa was extended to support mining source code and currently contains source code for over 23k Java projects, including full revision histories. In this demonstration we pose source code mining tasks and give solutions using Boa. We then execute these programs via our web-based infrastructure and show how to easily make the results available for future researchers.},
  booktitle = {Proceedings of the 2013 {{Companion Publication}} for {{Conference}} on {{Systems}}, {{Programming}}, \& {{Applications}}: {{Software}} for {{Humanity}}},
  publisher = {{ACM}},
  author = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
  year = {2013},
  keywords = {mapreduce,software repository mining},
  pages = {13--14},
  file = {/Users/luigi/work/zotero/storage/3TRV4BYQ/Dyer et al. - 2013 - Mining source code repositories with boa.pdf;/Users/luigi/work/zotero/storage/ASNCK4FU/Dyer et al. - 2013 - Mining Source Code Repositories with Boa.pdf}
}

@article{mandelinJungloidMiningHelping,
  title = {Jungloid {{Mining}}: {{Helping}} to {{Navigate}} the {{API Jungle}}},
  abstract = {Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.},
  language = {en},
  author = {Mandelin, David and Xu, Lin and Bod\i{}k, Rastislav and Kimelman, Doug},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/BGIBTEXA/Mandelin et al. - Jungloid Mining Helping to Navigate the API Jungl.pdf}
}

@inproceedings{cockxPatternMatching2014,
  address = {New York, NY, USA},
  series = {{{ICFP}} '14},
  title = {Pattern {{Matching Without K}}},
  isbn = {978-1-4503-2873-9},
  doi = {10.1145/2628136.2628139},
  abstract = {Dependent pattern matching is an intuitive way to write programs and proofs in dependently typed languages. It is reminiscent of both pattern matching in functional languages and case analysis in on-paper mathematics. However, in general it is incompatible with new type theories such as homotopy type theory (HoTT). As a consequence, proofs in such theories are typically harder to write and to understand. The source of this incompatibility is the reliance of dependent pattern matching on the so-called K axiom - also known as the uniqueness of identity proofs - which is inadmissible in HoTT. The Agda language supports an experimental criterion to detect definitions by pattern matching that make use of the K axiom, but so far it lacked a formal correctness proof. In this paper, we propose a new criterion for dependent pattern matching without K, and prove it correct by a translation to eliminators in the style of Goguen et al. (2006). Our criterion both allows more good definitions than existing proposals, and solves a previously undetected problem in the criterion offered by Agda. It has been implemented in Agda and is the first to be supported by a formal proof. Thus it brings the benefits of dependent pattern matching to contexts where we cannot assume K, such as HoTT. It also points the way to new forms of dependent pattern matching, for example on higher inductive types.},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Cockx, Jesper and Devriese, Dominique and Piessens, Frank},
  year = {2014},
  keywords = {agda,dependent pattern matching,homotopy type theory,k axiom},
  pages = {257--268},
  file = {/Users/luigi/work/zotero/storage/LCM5JBF3/Cockx et al. - 2014 - Pattern Matching Without K.pdf}
}

@inproceedings{grechanikInductiveProverBased2015,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Inductive {{Prover Based}} on {{Equality Saturation}} for a {{Lazy Functional Language}}},
  isbn = {978-3-662-46823-4},
  abstract = {The present paper shows how the idea of equality saturation can be used to build an inductive prover for a non-total first-order lazy functional language. We adapt equality saturation approach to a functional language by using transformations borrowed from supercompilation. A special transformation called merging by bisimilarity is used to perform proof by induction of equivalence between nodes of the E-graph. Equalities proved this way are just added to the E-graph. We also experimentally compare our prover with HOSC and HipSpec.},
  language = {en},
  booktitle = {Perspectives of {{System Informatics}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Grechanik, Sergei},
  editor = {Voronkov, Andrei and Virbitskaite, Irina},
  year = {2015},
  keywords = {Equivalence Class,Outgoing Edge,Pattern Match,Reduction Rule,Variable Node},
  pages = {127-141},
  file = {/Users/luigi/work/zotero/storage/A2AQ4XIA/Grechanik - 2015 - Inductive Prover Based on Equality Saturation for .pdf}
}

@article{grechanikStagedMultiResultSupercompilation,
  title = {Staged {{Multi}}-{{Result Supercompilation}}: {{Filtering}} by {{Transformation}}},
  abstract = {When applying supercompilation to problem-solving, multiresult supercompilation enables us to find the best solutions by generating a set of possible residual graphs of configurations that are then filtered according to some criteria. Unfortunately, the search space may be rather large. However, we show that the search can be drastically reduced by decomposing multi-result supercompilation into two stages. The first stage produces a compact representation for the set of residual graphs by delaying some graph-building operation. These operations are performed at the second stage, when the representation is interpreted, to actually produce the set of graphs. The main idea of our approach is that, instead of filtering a collection of graphs, we can analyze and clean its compact representation. In some cases of practical importance (such as selecting graphs of minimal size and removing graphs containing unsafe configurations) cleaning can be performed in linear time.},
  language = {en},
  author = {Grechanik, Sergei A and Klyuchnikov, Ilya G and Romanenko, Sergei A},
  pages = {25},
  file = {/Users/luigi/work/zotero/storage/VG2BAPEN/Grechanik et al. - Staged Multi-Result Supercompilation Filtering by.pdf}
}

@article{grechanikUnificationSupercompilationEquality,
  title = {Towards {{Unification}} of {{Supercompilation}} and {{Equality Saturation}}},
  language = {en},
  author = {Grechanik, Sergei A},
  pages = {4},
  file = {/Users/luigi/work/zotero/storage/CHGYGDZJ/Grechanik - Towards Unification of Supercompilation and Equali.pdf}
}

@article{meyerApplyingDesignContract1992,
  title = {Applying 'Design by Contract'},
  volume = {25},
  issn = {0018-9162},
  doi = {10.1109/2.161279},
  language = {en},
  number = {10},
  journal = {Computer},
  author = {Meyer, B.},
  month = oct,
  year = {1992},
  pages = {40-51},
  file = {/Users/luigi/work/zotero/storage/7ZBKHUGY/Meyer - 1992 - Applying 'design by contract'.pdf;/Users/luigi/work/zotero/storage/EDPXN6XH/Meyer - 1992 - Applying 'design by contract'.pdf}
}

@misc{daiCompositionalMiningMultiple2013,
  title = {Compositional {{Mining}} of {{Multiple Object API Protocols}} through {{State Abstraction}}},
  abstract = {API protocols specify correct sequences of method invocations. Despite their usefulness, API protocols are often unavailable in practice because writing them is cumbersome and error prone. Multiple object API protocols are more expressive than single object API protocols. However, the huge number of objects of typical object-oriented programs poses a major challenge to the automatic mining of multiple object API protocols: besides maintaining scalability, it is important to capture various object interactions. Current approaches utilize various heuristics to focus on small sets of methods. In this paper, we present a general, scalable, multiple object API protocols mining approach that can capture all object interactions. Our approach uses abstract field values to label object states during the mining process. We first mine single object typestates as finite state automata whose transitions are annotated with states of interacting objects before and after the execution of the corresponding method and then construct multiple object API protocols by composing these annotated single object typestates. We implement our approach for Java and evaluate it through a series of experiments.},
  journal = {undefined},
  howpublished = {/paper/Compositional-Mining-of-Multiple-Object-API-through-Dai-Mao/8212944a347f434c52919a06e04bf38b0ddd971f},
  author = {Dai, Ziying and Mao, Xiaoguang and Lei, Yan and Qi, Yuhua and Wang, Rui and Gu, Bin},
  year = {2013},
  file = {/Users/luigi/work/zotero/storage/VBNPYKZT/Dai et al. - 2013 - Compositional Mining of Multiple Object API Protoc.pdf;/Users/luigi/work/zotero/storage/M97MZH7B/8212944a347f434c52919a06e04bf38b0ddd971f.html}
}

@inproceedings{roseBytecodesMeetCombinators2009,
  address = {New York, NY, USA},
  series = {{{VMIL}} '09},
  title = {Bytecodes {{Meet Combinators}}: {{Invokedynamic}} on the {{JVM}}},
  isbn = {978-1-60558-874-2},
  shorttitle = {Bytecodes {{Meet Combinators}}},
  doi = {10.1145/1711506.1711508},
  abstract = {The Java Virtual Machine (JVM) has been widely adopted in part because of its classfile format, which is portable, compact, modular, verifiable, and reasonably easy to work with. However, it was designed for just one language---Java---and so when it is used to express programs in other source languages, there are often "pain points" which retard both development and execution. The most salient pain points show up at a familiar place, the method call site. To generalize method calls on the JVM, the JSR 292 Expert Group has designed a new invokedynamic instruction that provides user-defined call site semantics. In the chosen design, invokedynamic serves as a hinge-point between two coexisting kinds of intermediate language: bytecode containing dynamic call sites, and combinator graphs specifying call targets. A dynamic compiler can traverse both representations simultaneously, producing optimized machine code which is the seamless union of both kinds of input. As a final twist, the user-defined linkage of a call site may change, allowing the code to adapt as the application evolves over time. The result is a system balancing the conciseness of bytecode with the dynamic flexibility of function pointers.},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Virtual Machines}} and {{Intermediate Languages}}},
  publisher = {{ACM}},
  author = {Rose, John R.},
  year = {2009},
  keywords = {bytecode,combinator,dynamic compilation,invokedynamic,method invocation},
  pages = {2:1--2:11},
  file = {/Users/luigi/work/zotero/storage/ZTRMZFG4/Rose - 2009 - Bytecodes Meet Combinators Invokedynamic on the J.pdf}
}

@article{nakamotoBitcoinPeertoPeerElectronic,
  title = {Bitcoin: {{A Peer}}-to-{{Peer Electronic Cash System}}},
  abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
  language = {en},
  author = {Nakamoto, Satoshi},
  pages = {9},
  file = {/Users/luigi/work/zotero/storage/UR48NSAN/Nakamoto - Bitcoin A Peer-to-Peer Electronic Cash System.pdf}
}

@inproceedings{wadlerWellTypedProgramsCan2009,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Well-{{Typed Programs Can}}'t {{Be Blamed}}},
  isbn = {978-3-642-00590-9},
  abstract = {We introduce the blame calculus, which adds the notion of blame from Findler and Felleisen's contracts to a system similar to Siek and Taha's gradual types and Flanagan's hybrid types. We characterise where positive and negative blame can arise by decomposing the usual notion of subtype into positive and negative subtypes, and show that these recombine to yield naive subtypes. Naive subtypes previously appeared in type systems that are unsound, but we believe this is the first time naive subtypes play a role in establishing type soundness.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Wadler, Philip and Findler, Robert Bruce},
  editor = {Castagna, Giuseppe},
  year = {2009},
  keywords = {Functional Programming,Dynamic Typing,Function Type,Ground Type,Hybrid Type},
  pages = {1-16},
  file = {/Users/luigi/work/zotero/storage/LTRMRQTF/Wadler and Findler - 2009 - Well-Typed Programs Can’t Be Blamed.pdf}
}

@article{harperProgrammingStandardML,
  title = {Programming in {{Standard ML}}},
  language = {en},
  author = {Harper, Robert},
  pages = {297},
  file = {/Users/luigi/work/zotero/storage/4MB8AQ79/Harper - Programming in Standard ML.pdf}
}

@misc{bradelTwoApproachesShowing2008,
  title = {Two {{Approaches}} of {{Showing Program Equivalence CSC}} 2108 {{Automated Verification Final Report}}},
  abstract = {We describe two approaches to showing program equivalence. Program equivalence is useful for showing that a program created by an optimizing compi ler is correct. The optimized version of the program can be compared to a program that is created from the same source code by a verified compiler that performs no optimizati ons. Our first approach consists of using a theorem prover, ACL2, to prove that two program s re equivalent. For this approach to work we must transform the instructions of the progra ms into a representation that ACL2 can work with. Our second approach consists of calculat ing the weakest pre-condition needed for two programs to be correct, and then using ACL2 to prove t hat the pre-condition is met. We use both approaches to show the equivalence of severa l programs.},
  howpublished = {/paper/Two-Approaches-of-Showing-Program-Equivalence-CSC-Bradel/327261d1c1d2461643db37d5e67b16a31e57ee55},
  author = {Bradel, Borys},
  year = {2008},
  file = {/Users/luigi/work/zotero/storage/ICLDLHQR/Bradel - 2008 - Two Approaches of Showing Program Equivalence CSC .pdf;/Users/luigi/work/zotero/storage/DTLHADY9/327261d1c1d2461643db37d5e67b16a31e57ee55.html}
}

@inproceedings{balabonskiTypeSoundnessRace2014,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Type {{Soundness}} and {{Race Freedom}} for {{Mezzo}}},
  isbn = {978-3-319-07151-0},
  abstract = {The programming language Mezzo is equipped with a rich type system that controls aliasing and access to mutable memory. We incorporate shared-memory concurrency into Mezzo and present a modular formalization of its core type system, in the form of a concurrent {$\lambda$}-calculus, which we extend with references and locks. We prove that well-typed programs do not go wrong and are data-race free. Our definitions and proofs are machine-checked.},
  language = {en},
  booktitle = {Functional and {{Logic Programming}}},
  publisher = {{Springer International Publishing}},
  author = {Balabonski, Thibaut and Pottier, Fran{\c c}ois and Protzenko, Jonathan},
  editor = {Codish, Michael and Sumii, Eijiro},
  year = {2014},
  keywords = {Machine State,Operational Semantic,Separation Logic,Type Soundness,Typing Rule},
  pages = {253-269},
  file = {/Users/luigi/work/zotero/storage/GEYU7DQC/Balabonski et al. - 2014 - Type Soundness and Race Freedom for Mezzo.pdf}
}

@article{fridlenderWeNeedDependent,
  title = {Do We Need Dependent Types?},
  abstract = {Inspired by [1], we describe a technique for defining, within the Hindley-Milner type system, some functions which seem to require a language with dependent types. We illustrate this by giving a general definition of zipWith for which the Haskell library provides a family of functions, each member of the family having a different type and arity. Our technique consists in introducing ad hoc codings for natural numbers which resemble numerals in {$\lambda$}-calculus.},
  language = {en},
  author = {Fridlender, Daniel},
  pages = {9},
  file = {/Users/luigi/work/zotero/storage/IENPA7RB/Fridlender - Do we need dependent types.pdf}
}

@inproceedings{brainAutomatableFormalSemantics2015,
  title = {An {{Automatable Formal Semantics}} for {{IEEE}}-754 {{Floating}}-{{Point Arithmetic}}},
  doi = {10.1109/ARITH.2015.26},
  abstract = {Automated reasoning tools often provide little or no support to reason accurately and efficiently about floating-point arithmetic. As a consequence, software verification systems that use these tools are unable to reason reliably about programs containing floating-point calculations or may give unsound results. These deficiencies are in stark contrast to the increasing awareness that the improper use of floating-point arithmetic in programs can lead to unintuitive and harmful defects in software. To promote coordinated efforts towards building efficient and accurate floating-point reasoning engines, this paper presents a formalization of the IEEE-754 standard for floating-point arithmetic as a theory in many-sorted first-order logic. Benefits include a standardized syntax and unambiguous semantics, allowing tool interoperability and sharing of benchmarks, and providing a basis for automated, formal analysis of programs that process floating-point data.},
  booktitle = {2015 {{IEEE}} 22nd {{Symposium}} on {{Computer Arithmetic}}},
  author = {Brain, M. and Tinelli, C. and Ruemmer, P. and Wahl, T.},
  month = jun,
  year = {2015},
  keywords = {formal verification,Hardware,Software,Semantics,Standards,Cognition,automatable formal semantics,automated reasoning tools,Computers,floating point arithmetic,Floating point aritihmetic,Floating-point arithmetic,floating-point calculation,floating-point data processing,floating-point reasoning engine,formal logic,IEEE-754 floating-point arithmetic,many-sorted first-order logic,SMT,software verification systems,tool interoperability},
  pages = {160-167},
  file = {/Users/luigi/work/zotero/storage/55LJCZHR/Brain et al. - 2015 - An Automatable Formal Semantics for IEEE-754 Float.pdf;/Users/luigi/work/zotero/storage/WFXCGP5T/7203811.html}
}

@book{burstallTransformationSystemDeveloping1977,
  title = {A {{Transformation System}} for {{Developing Recursive Programs}}},
  abstract = {A system of rules for transforming programs is described, with the programs in the form of recursion equations An initially very simple, lucid. and hopefully correct program IS transformed into a more efficient one by altering the recursion structure Illustrative examples of program transformations are given, and a tentative implementation !s described Alternative structures for programs are shown, and a possible initial phase for an automatic or semiautomatic program manipulation system is lndmated  KEY WORDS AND PHRASES program transformation, program mampulatlon, optimization, recursion  CR CATEGORIES' 3 69, 4 12, 4 22, 5 24, 5 25  1.},
  author = {Burstall, R. M. and Darlington, John},
  year = {1977},
  file = {/Users/luigi/work/zotero/storage/3B6ZLVFS/Burstall and Darlington - 1977 - A Transformation System for Developing Recursive P.pdf;/Users/luigi/work/zotero/storage/VICJV4HB/summary.html}
}

@article{burstallProvingPropertiesPrograms1969,
  title = {Proving {{Properties}} of {{Programs}} by {{Structural Induction}}},
  volume = {12},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/12.1.41},
  language = {en},
  number = {1},
  journal = {The Computer Journal},
  author = {Burstall, R. M.},
  month = feb,
  year = {1969},
  pages = {41-48},
  file = {/Users/luigi/work/zotero/storage/FECHLULX/Burstall - 1969 - Proving Properties of Programs by Structural Induc.pdf}
}

@inproceedings{urbanBarendregtVariableConvention2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Barendregt's {{Variable Convention}} in {{Rule Inductions}}},
  isbn = {978-3-540-73595-3},
  abstract = {Inductive definitions and rule inductions are two fundamental reasoning tools in logic and computer science. When inductive definitions involve binders, then Barendregt's variable convention is nearly always employed (explicitly or implicitly) in order to obtain simple proofs. Using this convention, one does not consider truly arbitrary bound names, as required by the rule induction principle, but rather bound names about which various freshness assumptions are made. Unfortunately, neither Barendregt nor others give a formal justification for the variable convention, which makes it hard to formalise such proofs. In this paper we identify conditions an inductive definition has to satisfy so that a form of the variable convention can be built into the rule induction principle. In practice this means we come quite close to the informal reasoning of ``pencil-and-paper'' proofs, while remaining completely formal. Our conditions also reveal circumstances in which Barendregt's variable convention is not applicable, and can even lead to faulty reasoning.},
  language = {en},
  booktitle = {Automated {{Deduction}} \textendash{} {{CADE}}-21},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Urban, Christian and Berghofer, Stefan and Norrish, Michael},
  editor = {Pfenning, Frank},
  year = {2007},
  pages = {35-50},
  file = {/Users/luigi/work/zotero/storage/L4JJKYET/Urban et al. - 2007 - Barendregt’s Variable Convention in Rule Induction.pdf}
}

@inproceedings{bryantModelingVerifyingSystems2002,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Modeling and {{Verifying Systems Using}} a {{Logic}} of {{Counter Arithmetic}} with {{Lambda Expressions}} and {{Uninterpreted Functions}}},
  isbn = {978-3-540-45657-5},
  abstract = {In this paper, we present the logic of Counter Arithmetic with Lambda Expressions and Uninterpreted Functions (CLU). CLU generalizes the logic of equality with uninterpreted functions (EUF) with constrained lambda expressions, ordering, and successor and predecessor functions. In addition to modeling pipelined processors that EUF has proved useful for, CLU can be used to model many infinite-state systems including those with infinite memories, finite and infinite queues including lossy channels, and networks of identical processes. Even with this richer expressive power, the validity of a CLU formula can be efficiently decided by translating it to a propositional formula, and then using Boolean methods to check validity. We give theoretical and empirical evidence for the efficiency of our decision procedure. We also describe verification techniques that we have used on a variety of systems, including an out-of-order execution unit and the load-store unit of an industrial microprocessor.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bryant, Randal E. and Lahiri, Shuvendu K. and Seshia, Sanjit A.},
  editor = {Brinksma, Ed and Larsen, Kim Guldstrand},
  year = {2002},
  keywords = {Decision Procedure,Predicate Symbol,Propositional Formula,Symbolic Constant,Verify System},
  pages = {78-92},
  file = {/Users/luigi/work/zotero/storage/YC26C94C/Bryant et al. - 2002 - Modeling and Verifying Systems Using a Logic of Co.pdf}
}

@inproceedings{joshiCalFuzzerExtensibleActive2009,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {{{CalFuzzer}}: {{An Extensible Active Testing Framework}} for {{Concurrent Programs}}},
  isbn = {978-3-642-02658-4},
  shorttitle = {{{CalFuzzer}}},
  abstract = {Active testing has recently been introduced to effectively test concurrent programs. Active testing works in two phases. It first uses predictive off-the-shelf static or dynamic program analyses to identify potential concurrency bugs, such as data races, deadlocks, and atomicity violations. In the second phase, active testing uses the reports from these predictive analyses to explicitly control the underlying scheduler of the concurrent program to accurately and quickly discover real concurrency bugs, if any, with very high probability and little overhead. In this paper, we present an extensible framework for active testing of Java programs. The framework currently implements three active testers based on data races, atomic blocks, and deadlocks.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Joshi, Pallavi and Naik, Mayur and Park, Chang-Seo and Sen, Koushik},
  editor = {Bouajjani, Ahmed and Maler, Oded},
  year = {2009},
  keywords = {Callback Function,Check Method,Concurrent Program,Data Race,Multithreaded Program},
  pages = {675-681},
  file = {/Users/luigi/work/zotero/storage/SNGCPF3I/Joshi et al. - 2009 - CalFuzzer An Extensible Active Testing Framework .pdf}
}

@inproceedings{kastrinisEfficientEffectiveHandling2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Efficient and {{Effective Handling}} of {{Exceptions}} in {{Java Points}}-to {{Analysis}}},
  isbn = {978-3-642-37051-9},
  abstract = {A joint points-to and exception analysis has been shown to yield benefits in both precision and performance. Treating exceptions as regular objects, however, incurs significant and rather unexpected overhead. We show that in a typical joint analysis most of the objects computed to flow in and out of a method are due to exceptional control-flow and not normal call-return control-flow. For instance, a context-insensitive analysis of the Antlr benchmark from the DaCapo suite computes 4-5 times more objects going in or out of a method due to exceptional control-flow than due to normal control-flow. As a consequence, the analysis spends a large amount of its time considering exceptions.We show that the problem can be addressed both effectively and elegantly by coarsening the representation of exception objects. An interesting find is that, instead of recording each distinct exception object, we can collapse all exceptions of the same type, and use one representative object per type, to yield nearly identical precision (loss of less than 0.1\%) but with a boost in performance of at least 50\% for most analyses and benchmarks and large space savings (usually 40\% or more).},
  language = {en},
  booktitle = {Compiler {{Construction}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Kastrinis, George and Smaragdakis, Yannis},
  editor = {Jhala, Ranjit and De Bosschere, Koen},
  year = {2013},
  keywords = {Binary Decision Diagram,Exception Analysis,Exception Handler,Input Relation,Intermediate Language},
  pages = {41-60},
  file = {/Users/luigi/work/zotero/storage/YIBNGVKG/Kastrinis and Smaragdakis - 2013 - Efficient and Effective Handling of Exceptions in .pdf}
}

@inproceedings{devrieseExplicitlyRecursiveGrammar2011,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Explicitly {{Recursive Grammar Combinators}}},
  isbn = {978-3-642-18378-2},
  abstract = {We propose a novel context-free grammar representation for parsing libraries in a pure programming language. Our representation explicitizes the recursion in the grammar, thus avoiding fundamental limitations of the grammar model currently employed by parser combinator libraries. Additionally, we decouple the grammar from its semantic actions using techniques from the Multirec generic programming library. The look and feel of the grammar and semantic actions remain close to traditional EBNF and syntax-directed definitions respectively.In an accompanying technical report, we demonstrate that our representation supports more declarative implementations of grammar transformations than other work. The ideas described in this paper form the basis for our freely available grammar-combinators parsing library.},
  language = {en},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Devriese, Dominique and Piessens, Frank},
  editor = {Rocha, Ricardo and Launchbury, John},
  year = {2011},
  keywords = {Abstract Syntax Tree,Attribute Grammar,Pattern Functor,Production Rule,Semantic Action},
  pages = {84-98},
  file = {/Users/luigi/work/zotero/storage/WDG8Q4LT/Devriese and Piessens - 2011 - Explicitly Recursive Grammar Combinators.pdf}
}

@inproceedings{walkerAlgebraicProofsProperties1994,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Algebraic Proofs of Properties of Objects},
  isbn = {978-3-540-48376-2},
  abstract = {A semantics by translation to a process calculus is used as the basis for an investigation of transformations to programs expressed in a parallel object-oriented language. Two concrete examples are studied. In one it is shown that transformations introducing concurrency into the design of a priority queue class do not alter the observable behaviour. In the other, a more delicate relationship between two symbol table classes is established.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}} \textemdash{} {{ESOP}} '94},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Walker, David},
  editor = {Sannella, Donald},
  year = {1994},
  keywords = {Operational Semantic,Abstract Description,Agent Expression,Denotational Semantic,Method Invocation},
  pages = {501-516},
  file = {/Users/luigi/work/zotero/storage/CJ7KB6JJ/Walker - 1994 - Algebraic proofs of properties of objects.pdf}
}

@article{breitnerSafeZerocostCoercions,
  title = {Safe {{Zero}}-Cost {{Coercions}} for {{Haskell}}},
  abstract = {Generative type abstractions \textendash{} present in Haskell, OCaml, and other languages \textendash{} are useful concepts to help prevent programmer errors. They serve to create new types that are distinct at compile time but share a run-time representation with some base type. We present a new mechanism that allows for zero-cost conversions between generative type abstractions and their representations, even when such types are deeply nested. We prove type safety in the presence of these conversions and have implemented our work in GHC.},
  language = {en},
  author = {Breitner, Joachim and Eisenberg, Richard A and Weirich, Stephanie and Jones, Simon Peyton},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/WM9GAX4W/Breitner et al. - Safe Zero-cost Coercions for Haskell.pdf}
}

@article{bradyCrossplatformCompilersFunctional,
  title = {Cross-Platform {{Compilers}} for {{Functional Languages}}},
  abstract = {Modern software is often designed to run on a virtual machine, such as the JVM or .NET's CLR. Increasingly, even the web browser is considered a target platform with the Javascript engine as its virtual machine. The choice of programming language for a project is therefore restricted to the languages which target the desired platform. As a result, an important consideration for a programming language designer is the platform to be targetted. For a language to be truly cross-platform, it must not only support different operating systems (e.g. Windows, OSX and Linux) but it must also target different virtual machine environments such as JVM, .NET, Javascript and others. In this paper, I describe how this problem is addressed in the Idris programming language. The overall compilation process involves a number of intermediate representations and Idris exposes an interface to each of these representations, allowing back ends for different target platforms to decide which is most appropriate. I show how to use these representations to retarget Idris for multiple platforms, and further show how to build a generic foreign function interface supporting multiple platforms.},
  language = {en},
  author = {Brady, Edwin},
  pages = {20},
  file = {/Users/luigi/work/zotero/storage/INCHLY6U/Brady - Cross-platform Compilers for Functional Languages.pdf}
}

@article{kennedyCompilingContinuationsContinued2007,
  title = {Compiling with {{Continuations}}, {{Continued}}},
  abstract = {We present a series of CPS-based intermediate languages suitable for functional language compilation, arguing that they have practical benefits over direct-style languages based on A-normal form (ANF) or monads. Inlining of functions demonstrates the benefits most clearly: in ANF-based languages, inlining involves a re-normalization step that rearranges let expressions and possibly introduces a new `join \ldots{}},
  language = {en-US},
  journal = {Microsoft Research},
  author = {Kennedy, Andrew},
  month = oct,
  year = {2007},
  file = {/Users/luigi/work/zotero/storage/VHQYP9NC/Kennedy - 2007 - Compiling with Continuations, Continued.pdf;/Users/luigi/work/zotero/storage/KS8GYLM3/compiling-with-continuations-continued.html}
}

@article{wadlerComplementBlame2015,
  title = {A {{Complement}} to {{Blame}}},
  doi = {10.4230/lipics.snapl.2015.309},
  abstract = {Contracts, gradual typing, and hybrid typing all permit less-precisely typed and more-precisely typed code to interact. Blame calculus encompasses these, and guarantees blame safety: blame for type errors always lays with less-precisely typed code. This paper serves as a complement to the literature on blame calculus: it elaborates on motivation, comments on the reception of the work, critiques some work for not properly attending to blame, and looks forward to applications. No knowledge of contracts, gradual typing, hybrid typing, or blame calculus is assumed.},
  language = {en},
  author = {Wadler, Philip},
  editor = {Herbstritt, Marc},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/L7P3548Y/Wadler - 2015 - A Complement to Blame.pdf}
}

@article{sarna-starostaRelatingConstraintHandling,
  title = {Relating {{Constraint Handling Rules}} to {{Datalog}}},
  abstract = {DatalogLB is an extension of Datalog supporting global stratification of negation and functional dependencies, designed for use in industrial-scale decision automation applications. Constraint Handling Rules (CHR) is a declarative rule-based programming language, particularly suitable for specifying custom constraint solvers at a high level. Our goal is to enhance DatalogLB with CHR-like capabilities in order to improve its expressive power and open it to specification of general-purpose constraint solvers for industrial applications. In this paper we relate the two formalisms and define a translation of a significant class of CHR programs into DatalogLB. It turns out that the translation enables reasoning about the properties of CHR programs at a high level of Datalog logic.},
  language = {en},
  author = {{Sarna-Starosta}, Beata and Zook, David and Pasalic, Emir and Aref, Molham},
  pages = {15},
  file = {/Users/luigi/work/zotero/storage/QJBLJMTB/Sarna-Starosta et al. - Relating Constraint Handling Rules to Datalog.pdf}
}

@misc{garnock-jonesCoordinatedConcurrentProgramming2016,
  title = {Coordinated {{Concurrent Programming}} in {{Syndicate}}},
  abstract = {Most programs interact with the world: via graphical user interfaces, networks, etc. This form of interactivity entails concurrency, and concurrent program components must coordinate their computations. This paper presents Syndicate, a novel design for a coordinated, concurrent programming language. Each concurrent component in Syndicate is a functional actor that participates in scoped conversations. The medium of conversation arranges for message exchanges and coordinates access to common knowledge. As such, Syndicate occupies a novel point in this design space, halfway between actors and threads. 1 From Interaction to Concurrency and Coordination Most programs must interact with their context. Interactions often start as reactions to external events, such as a user's gesture or the arrival of a message. Because nobody coordinates the multitude of external events, a program must notice and react to events in a concurrent manner. Thus, a sequential program must de-multiplex the sequence of events and launch the appropriate concurrent component for each event. Put differently, these interacting programs consist of concurrent components, even in sequential languages. Concurrent program components must coordinate their computations to realize the overall goals of the program. This coordination takes two forms: the exchange of knowledge and the establishment of frame conditions. In addition, coordination must take into account that reactions to events may call for the creation of new concurrent components or that existing components may disappear due to exceptions or partial failures. In short, coordination poses a major problem to the proper design of effective communicating, concurrent components. This paper presents Syndicate, a novel language for coordinated concurrent programming. A Syndicate program consists of functional actors that participate in precisely scoped conversations. So-called networks coordinate these conversations. When needed, they apply a functional actor to an event and its current state; in turn, they receive a new state plus descriptions of actions. These actions may represent messages for other participants in the conversations or assertions for a common space of knowledge. Precise scoping implies a separation of distinct conversations, and hence existence of multiple networks. At the same time, an actor in one network may have to communicate with an actor in a different network. To accommodate such situations, Syndicate allows the embedding of one network into another as if Programs P {$\in$} P ::= actor f u \#'' a | net \#'' P Leaf functions f {$\in$} F = E\texttimes{} V -{$\rightarrow$}total \#'' A \texttimes{} V+Err Values u, v {$\in$} V (first-order data; numbers, strings, lists, trees, sets, etc.) Events e {$\in$} E ::= {$\langle$}c{$\rangle$} | {$\pi$} Actions a {$\in$} A ::= {$\langle$}c{$\rangle$} | {$\pi$} | P Assertions c, d {$\in$} S ::= u | ?c | c Assertion sets {$\pi$} {$\in$} = P(S) Fig. 1: Syntax of Syndicate Programs the first were just an actor within the second. In other words, networks simultaneously scope and compose conversations. The resulting tree-structured shape of networked conversations corresponds both to tree-like arrangements of containers and processes in modern operating systems and to the nesting of layers in network protocols [1]. Syndicate thus unifies the programming techniques of distributed programming with those of coordinated concurrent programming. By construction, Syndicate networks also manage resources. When a new actor appears in a conversation, a network allocates the necessary resources. When an actor fails, it deallocates the associated resources. In particular, it retracts all shared state associated with the actor, thereby making the failure visible to interested participants. Syndicate thus solves notorious problems of service discovery and resource management in the coordination of communicating components. In sum, Syndicate occupies a novel point in the design space of coordinated concurrent (functional) components (sec. 2), sitting firmly between a threadbased world with sharing and local-state-only, message-passing actors. Our design for Syndicate includes two additional contributions: an efficient protocol for incrementally maintaining the common knowledge base and a trie-based data structure for efficiently indexing into it (sec. 3). Finally, our paper presents evaluations concerning the fundamental performance characteristics of Syndicate as well as its pragmatics (secs. 4 and 5).},
  journal = {undefined},
  howpublished = {/paper/Coordinated-Concurrent-Programming-in-Syndicate-Garnock-Jones-Felleisen/dcd23d21c95380f98f452dc04310fb3155850f72},
  author = {{Garnock-Jones}, Tony and Felleisen, Matthias},
  year = {2016},
  file = {/Users/luigi/work/zotero/storage/C75222ZC/Garnock-Jones and Felleisen - 2016 - Coordinated Concurrent Programming in Syndicate.pdf;/Users/luigi/work/zotero/storage/Y2NFAJYQ/dcd23d21c95380f98f452dc04310fb3155850f72.html}
}

@article{eisenbergSystemFCImplemented,
  title = {System {{FC}}, as Implemented in {{GHC}}},
  language = {en},
  author = {Eisenberg, Richard A},
  pages = {24},
  file = {/Users/luigi/work/zotero/storage/NEQDIPVX/Eisenberg - System FC, as implemented in GHC.pdf}
}

@article{tolmachExternalRepresentationGHC,
  title = {An {{External Representation}} for the {{GHC Core Language}} ({{For GHC}} 6.10)},
  abstract = {This document provides a precise definition for the GHC Core language, so that it can be used to communicate between GHC and new stand-alone compilation tools such as back-ends or optimizers.1 The definition includes a formal grammar and an informal semantics. An executable typechecker and interpreter (in Haskell), which formally embody the static and dynamic semantics, are available separately.},
  language = {en},
  author = {Tolmach, Andrew and Chevalier, Tim},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/246JMZHS/Tolmach and Chevalier - An External Representation for the GHC Core Langua.pdf}
}

@inproceedings{fosterIntegratingAutomatedTheorem2011,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Integrating an {{Automated Theorem Prover}} into {{Agda}}},
  isbn = {978-3-642-20398-5},
  abstract = {Agda is a dependently typed functional programming language and a proof assistant in which developing programs and proving their correctness is one activity. We show how this process can be enhanced by integrating external automated theorem provers, provide a prototypical integration of the equational theorem prover Waldmeister, and give examples of how this proof automation works in practice.},
  language = {en},
  booktitle = {{{NASA Formal Methods}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Foster, Simon and Struth, Georg},
  editor = {Bobaru, Mihaela and Havelund, Klaus and Holzmann, Gerard J. and Joshi, Rajeev},
  year = {2011},
  keywords = {Automate Theorem Prove,Equational Logic,Functional Programming Language,Proof Assistant,Proof Obligation},
  pages = {116-130},
  file = {/Users/luigi/work/zotero/storage/7QDZHXLQ/Foster and Struth - 2011 - Integrating an Automated Theorem Prover into Agda.pdf}
}

@incollection{rondonCSolveVerifyingLiquid2012,
  address = {Berlin, Heidelberg},
  title = {{{CSolve}}: {{Verifying C}} with {{Liquid Types}}},
  volume = {7358},
  isbn = {978-3-642-31423-0 978-3-642-31424-7},
  shorttitle = {{{CSolve}}},
  abstract = {We present CSolve, an automated verifier for C programs based on Liquid Type inference. We show how CSolve verifies memory safety through an example and describe its architecture and interface.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Rondon, Patrick and Bakst, Alexander and Kawaguchi, Ming and Jhala, Ranjit},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Madhusudan, P. and Seshia, Sanjit A.},
  year = {2012},
  pages = {744-750},
  file = {/Users/luigi/work/zotero/storage/IWJUS3UQ/Rondon et al. - 2012 - CSolve Verifying C with Liquid Types.pdf;/Users/luigi/work/zotero/storage/KZDDSH4H/Rondon et al. - 2012 - CSolve Verifying C with Liquid Types.pdf},
  doi = {10.1007/978-3-642-31424-7_59}
}

@inproceedings{danielssonTotalParserCombinators2010,
  address = {New York, NY, USA},
  series = {{{ICFP}} '10},
  title = {Total {{Parser Combinators}}},
  isbn = {978-1-60558-794-3},
  doi = {10.1145/1863543.1863585},
  abstract = {A monadic parser combinator library which guarantees termination of parsing, while still allowing many forms of left recursion, is described. The library's interface is similar to those of many other parser combinator libraries, with two important differences: one is that the interface clearly specifies which parts of the constructed parsers may be infinite, and which parts have to be finite, using dependent types and a combination of induction and coinduction; and the other is that the parser type is unusually informative. The library comes with a formal semantics, using which it is proved that the parser combinators are as expressive as possible. The implementation is supported by a machine-checked correctness proof.},
  booktitle = {Proceedings of the 15th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Danielsson, Nils Anders},
  year = {2010},
  keywords = {dependent types,productivity,mixed induction and coinduction,parser combinators,termination},
  pages = {285--296},
  file = {/Users/luigi/work/zotero/storage/WN8CPDTZ/Danielsson - 2010 - Total Parser Combinators.pdf}
}

@inproceedings{greenLogicBloxPlatformLanguage2012,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {{{LogicBlox}}, {{Platform}} and {{Language}}: {{A Tutorial}}},
  isbn = {978-3-642-32925-8},
  shorttitle = {{{LogicBlox}}, {{Platform}} and {{Language}}},
  abstract = {The modern enterprise software stack\textemdash{}a collection of applications supporting bookkeeping, analytics, planning, and forecasting for enterprise data\textemdash{}is in danger of collapsing under its own weight. The task of building and maintaining enterprise software is tedious and laborious; applications are cumbersome for end-users; and adapting to new computing hardware and infrastructures is difficult. We believe that much of the complexity in today's architecture is accidental, rather than inherent. This tutorial provides an overview of the LogicBlox platform, a ambitious redesign of the enterprise software stack centered around a unified declarative programming model, based on an extended version of Datalog.},
  language = {en},
  booktitle = {Datalog in {{Academia}} and {{Industry}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Green, Todd J. and Aref, Molham and Karvounarakis, Grigoris},
  editor = {Barcel\'o, Pablo and Pichler, Reinhard},
  year = {2012},
  keywords = {Business Intelligence,Conjunctive Query,Datalog Program,Declarative Language,Enterprise Application},
  pages = {1-8},
  file = {/Users/luigi/work/zotero/storage/4LW54QE8/Green et al. - 2012 - LogicBlox, Platform and Language A Tutorial.pdf}
}

@inproceedings{ansaloniEnablingModularityReuse2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Enabling {{Modularity}} and {{Re}}-Use in {{Dynamic Program Analysis Tools}} for the {{Java Virtual Machine}}},
  isbn = {978-3-642-39038-8},
  abstract = {Dynamic program analysis tools based on code instrumentation serve many important software engineering tasks such as profiling, debugging, testing, program comprehension, and reverse engineering. Unfortunately, constructing new analysis tools is unduly difficult, because existing frameworks offer little or no support to the programmer beyond the incidental task of instrumentation. We observe that existing dynamic analysis tools re-address recurring requirements in their essential task: maintaining state which captures some property of the analysed program. This paper presents a general architecture for dynamic program analysis tools which treats the maintenance of analysis state in a modular fashion, consisting of mappers decomposing input events spatially, and updaters aggregating them over time. We show that this architecture captures the requirements of a wide variety of existing analysis tools.},
  language = {en},
  booktitle = {{{ECOOP}} 2013 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ansaloni, Danilo and Kell, Stephen and Zheng, Yudi and Bulej, Lubom\'ir and Binder, Walter and T\r{u}ma, Petr},
  editor = {Castagna, Giuseppe},
  year = {2013},
  keywords = {Basic Block,Context Accessor,Context Information,Pipeline Stage,Shadow State},
  pages = {352-377},
  file = {/Users/luigi/work/zotero/storage/ICSU3DYW/Ansaloni et al. - 2013 - Enabling Modularity and Re-use in Dynamic Program .pdf}
}

@book{ExploitingVirtualMachine2012,
  title = {Exploiting {{Virtual Machine Infrastructure To Implement Low}}-{{Overhead Error Checking Tools}}},
  abstract = {Program-specific bugs are a growing problem with modern software. General bugs\textemdash{}language-level bugs that would be errors in any program, such as memory leaks and buffer overflows\textemdash{}have mostly been solved by modern programming languages and tools. However, program-specific bugs, such as violating data structure invariants, remain. In addition, modern trends such as construction of large programs, use of large standard libraries and thirdparty frameworks, and increasingly higher-level languages conspire to make program-specific bugs even more common in the future. Static analysis tools struggle with the size of these programs and language features such as dynamic classloading. Current dynamic analysis tools are too slow, often incurring a slowdown of 1-2 orders of magnitude, and thus can only be used in a debugging environment. In this thesis, we introduce a set of dynamic analysis tools that help programmers find program-specific bugs in their software. These dynamic},
  year = {2012},
  file = {/Users/luigi/work/zotero/storage/MSRX7U3V/2012 - Exploiting Virtual Machine Infrastructure To Imple.pdf;/Users/luigi/work/zotero/storage/IJSR7FV9/summary.html}
}

@article{augustssonFunctionalPearlGenerating1994,
  title = {Functional {{Pearl}}: {{On}} Generating Unique Names},
  volume = {4},
  issn = {1469-7653, 0956-7968},
  shorttitle = {Functional {{Pearl}}},
  doi = {10.1017/S0956796800000988},
  abstract = {And Joktan begat Almodad, and Sheleph, and Hazarmaveth, and Jerah, and Handoram, and Uzal, and Diklah, and Obal, and Abimael, and Sheba, and Ophir, and Havilah, and Jobab: all these were the sons of Joktan.\textemdash{} Genesis 10:26\textendash{}29},
  language = {en},
  number = {1},
  journal = {Journal of Functional Programming},
  author = {Augustsson, Lennart and Rittri, Mikael and Synek, Dan},
  month = jan,
  year = {1994},
  pages = {117-123},
  file = {/Users/luigi/work/zotero/storage/VHA742DR/Augustsson et al. - 1994 - Functional Pearl On generating unique names.pdf;/Users/luigi/work/zotero/storage/DY8QW9YD/763DE73EB4761FDF681A613BE0E98443.html}
}

@article{repsInterproceduralDataflowAnalysis,
  title = {Interprocedural {{Dataflow Analysis}} via {{Graph Reachability}}},
  language = {en},
  author = {Reps, Thomas and Sagiv, Mooly and Horwitz, Susan},
  pages = {51},
  file = {/Users/luigi/work/zotero/storage/K3I5CIHR/Reps et al. - Interprocedural Dataﬂow Analysis via Graph Reachab.pdf}
}

@article{cooperSimpleFastDominance,
  title = {A {{Simple}}, {{Fast Dominance Algorithm}}},
  abstract = {The problem of finding the dominators in a control-flow graph has a long history in the literature. The original algorithms suffered from a large asymptotic complexity but were easy to understand. Subsequent work improved the time bound, but generally sacrificed both simplicity and ease of implementation. This paper returns to a simple formulation of dominance as a global data-flow problem. Some insights into the nature of dominance lead to an implementation of an O(N 2) algorithm that runs faster, in practice, than the classic Lengauer-Tarjan algorithm, which has a timebound of O(E {${_\ast}$} log(N )). We compare the algorithm to Lengauer-Tarjan because it is the best known and most widely used of the fast algorithms for dominance. Working from the same implementation insights, we also rederive (from earlier work on control dependence by Ferrante, et al.) a method for calculating dominance frontiers that we show is faster than the original algorithm by Cytron, et al. The aim of this paper is not to present a new algorithm, but, rather, to make an argument based on empirical evidence that algorithms with discouraging asymptotic complexities can be faster in practice than those more commonly employed. We show that, in some cases, careful engineering of simple algorithms can overcome theoretical advantages, even when problems grow beyond realistic sizes. Further, we argue that the algorithms presented herein are intuitive and easily implemented, making them excellent teaching tools.},
  language = {en},
  author = {Cooper, Keith D and Harvey, Timothy J and Kennedy, Ken},
  pages = {15},
  file = {/Users/luigi/work/zotero/storage/ZTKNMISL/Cooper et al. - A Simple, Fast Dominance Algorithm.pdf}
}

@incollection{leeMarcoSafeExpressive2012,
  address = {Berlin, Heidelberg},
  title = {Marco: {{Safe}}, {{Expressive Macros}} for {{Any Language}}},
  volume = {7313},
  isbn = {978-3-642-31056-0 978-3-642-31057-7},
  shorttitle = {Marco},
  abstract = {Macros improve expressiveness, concision, abstraction, and language interoperability without changing the programming language itself. They are indispensable for building increasingly prevalent multilingual applications. Unfortunately, existing macro systems are wellencapsulated but unsafe (e.g., the C preprocessor) or are safe but tightlyintegrated with the language implementation (e.g., Scheme macros). This paper introduces Marco, the first macro system that seeks both encapsulation and safety. Marco is based on the observation that the macro system need not know all the syntactic and semantic rules of the target language but must only directly enforce some rules, such as variable name binding. Using this observation, Marco off-loads most rule checking to unmodified target-language compilers and interpreters and thus becomes language-scalable. We describe the Marco language, its languageindependent safety analysis, and how it uses two example target-language analysis plug-ins, one for C++ and one for SQL. This approach opens the door to safe and expressive macros for any language.},
  language = {en},
  booktitle = {{{ECOOP}} 2012 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Lee, Byeongcheol and Grimm, Robert and Hirzel, Martin and McKinley, Kathryn S.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Noble, James},
  year = {2012},
  pages = {589-613},
  file = {/Users/luigi/work/zotero/storage/BXIG5DSU/Lee et al. - 2012 - Marco Safe, Expressive Macros for Any Language.pdf;/Users/luigi/work/zotero/storage/G78FEPWP/Lee et al. - 2012 - Marco Safe, Expressive Macros for Any Language.pdf},
  doi = {10.1007/978-3-642-31057-7_26}
}

@article{leungVerifyingGPUKernels,
  title = {Verifying {{GPU Kernels}} by {{Test Amplification}}},
  abstract = {We present a novel technique for verifying properties of data parallel GPU programs via test amplification. The key insight behind our work is that we can use the technique of static information flow to amplify the result of a single test execution over the set of all inputs and interleavings that affect the property being verified. We empirically demonstrate the effectiveness of test amplification for verifying race-freedom and determinism over a large number of standard GPU kernels, by showing that the result of verifying a single dynamic execution can be amplified over the massive space of possible data inputs and thread interleavings.},
  language = {en},
  author = {Leung, Alan and Gupta, Manish and Agarwal, Yuvraj and Gupta, Rajesh and Jhala, Ranjit and Lerner, Sorin},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/HZVFZYYY/Leung et al. - Verifying GPU Kernels by Test Ampliﬁcation.pdf;/Users/luigi/work/zotero/storage/K662D7YQ/Leung et al. - Verifying GPU Kernels by Test Ampliﬁcation.pdf}
}

@article{guptaDOCTORPHILOSOPHY,
  title = {{{DOCTOR OF PHILOSOPHY}}},
  language = {en},
  author = {Gupta, Varun},
  pages = {30},
  file = {/Users/luigi/work/zotero/storage/ZGHSPZKR/Gupta - DOCTOR OF PHILOSOPHY.pdf}
}

@inproceedings{chlipalaOptimizingCompilerPurely2015,
  address = {Vancouver, BC, Canada},
  title = {An Optimizing Compiler for a Purely Functional Web-Application Language},
  isbn = {978-1-4503-3669-7},
  doi = {10.1145/2784731.2784741},
  abstract = {High-level scripting languages have become tremendously popular for development of dynamic Web applications. Many programmers appreciate the productivity benefits of automatic storage management, freedom from verbose type annotations, and so on. While it is often possible to improve performance substantially by rewriting an application in C or a similar language, very few programmers bother to do so, because of the consequences for human development effort. This paper describes a compiler that makes it possible to have most of the best of both worlds, coding Web applications in a high-level language but compiling to native code with performance comparable to handwritten C code. The source language is Ur/Web, a domain-specific, purely functional, statically typed language for the Web. Through a coordinated suite of relatively straightforward program analyses and algebraic optimizations, we transform Ur/Web programs into almost-idiomatic C code, with no garbage collection, little unnecessary memory allocation for intermediate values, etc. Our compiler is in production use for commercial Web sites supporting thousands of users, and microbenchmarks demonstrate very competitive performance versus mainstream tools.},
  language = {en},
  booktitle = {Proceedings of the 20th {{ACM SIGPLAN International Conference}} on {{Functional Programming}} - {{ICFP}} 2015},
  publisher = {{ACM Press}},
  author = {Chlipala, Adam},
  year = {2015},
  pages = {10-21},
  file = {/Users/luigi/work/zotero/storage/IB5PMT96/Chlipala - 2015 - An optimizing compiler for a purely functional web.pdf}
}

@inproceedings{zhengComprehensiveMultiplatformDynamic2014,
  address = {Cracow, Poland},
  title = {Comprehensive Multi-Platform Dynamic Program Analysis for the {{Java}} and {{Dalvik}} Virtual Machines},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2655186},
  abstract = {Despite its importance for many software engineering tasks, dynamic program analysis is only insufficiently supported on the Java platform [2]. Existing Java Virtual Machines (JVMs) as well as Android's Dalvik Virtual Machine (DVM) lack dedicated mechanisms for expressing arbitrary dynamic program analysis tasks at a high abstraction level, for ensuring complete code coverage of the analysis, and for isolating analysis tasks from the observed program to prevent interference. For example, the JVM Tool Interface requires analysis tasks to be written in low-level native code, and some virtual machines (e.g., DVM) do not support it. As a consequence, dynamic program analysis tools are often implemented using lowlevel mechanisms, resulting in error-prone code that is difficult to maintain, and support only a particular virtual machine. Moreover, many analysis tools produce unsound profiles (due to interference of the analysis with the observed program) or incomplete profiles (due to limited code coverage).},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Zheng, Yudi and Sun, Haiyang and Bulej, Lubomir and T\r{u}ma, Petr and Binder, Walter},
  year = {2014},
  pages = {4-4},
  file = {/Users/luigi/work/zotero/storage/XFTPHHUK/Zheng et al. - 2014 - Comprehensive multi-platform dynamic program analy.pdf}
}

@article{vazouRefinementTypesHaskell,
  title = {Refinement {{Types For Haskell}}},
  abstract = {SMT-based checking of refinement types for call-by-value languages is a well-studied subject. Unfortunately, the classical translation of refinement types to verification conditions is unsound under lazy evaluation. When checking an expression, such systems implicitly assume that all the free variables in the expression are bound to values. This property is trivially guaranteed by eager, but does not hold under lazy, evaluation. Thus, to be sound and precise, a refinement type system for Haskell and the corresponding verification conditions must take into account which subset of binders actually reduces to values. We present a stratified type system that labels binders as potentially diverging or not, and that (circularly) uses refinement types to verify the labeling. We have implemented our system in LIQUIDHASKELL and present an experimental evaluation of our approach on more than 10,000 lines of widely used Haskell libraries. We show that LIQUIDHASKELL is able to prove 96\% of all recursive functions terminating, while requiring a modest 1.7 lines of termination-annotations per 100 lines of code.},
  language = {en},
  author = {Vazou, Niki and Seidel, Eric L and Jhala, Ranjit and Vytiniotis, Dimitrios and {Peyton-Jones}, Simon},
  pages = {15},
  file = {/Users/luigi/work/zotero/storage/K9N79YUV/Vazou et al. - Reﬁnement Types For Haskell.pdf;/Users/luigi/work/zotero/storage/LBEQ38JS/Vazou et al. - Reﬁnement Types For Haskell.pdf}
}

@inproceedings{vazouLiquidHaskellExperienceRefinement2014,
  address = {Gothenburg, Sweden},
  title = {{{LiquidHaskell}}: Experience with Refinement Types in the Real World},
  isbn = {978-1-4503-3041-1},
  shorttitle = {{{LiquidHaskell}}},
  doi = {10.1145/2633357.2633366},
  abstract = {Haskell has many delightful features. Perhaps the one most beloved by its users is its type system that allows developers to specify and verify a variety of program properties at compile time. However, many properties, typically those that depend on relationships between program values are impossible, or at the very least, cumbersome to encode within the existing type system. Many such properties can be verified using a combination of Refinement Types and external SMT solvers. We describe the refinement type checker LIQUIDHASKELL, which we have used to specify and verify a variety of properties of over 10,000 lines of Haskell code from various popular libraries, including containers, hscolour, bytestring, text, vector-algorithms and xmonad. First, we present a high-level overview of LIQUIDHASKELL, through a tour of its features. Second, we present a qualitative discussion of the kinds of properties that can be checked \textendash{} ranging from generic application independent criteria like totality and termination, to application specific concerns like memory safety and data structure correctness invariants. Finally, we present a quantitative evaluation of the approach, with a view towards measuring the efficiency and programmer effort required for verification, and discuss the limitations of the approach.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{ACM SIGPLAN}} Symposium on {{Haskell}} - {{Haskell}} '14},
  publisher = {{ACM Press}},
  author = {Vazou, Niki and Seidel, Eric L. and Jhala, Ranjit},
  year = {2014},
  pages = {39-51},
  file = {/Users/luigi/work/zotero/storage/JZVLTQG8/Vazou et al. - 2014 - LiquidHaskell experience with refinement types in.pdf;/Users/luigi/work/zotero/storage/LWR6ATYN/Vazou et al. - 2014 - LiquidHaskell experience with refinement types in.pdf}
}

@inproceedings{cousotAbstractInterpretationUnified1977,
  address = {Los Angeles, California},
  title = {Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints},
  shorttitle = {Abstract Interpretation},
  doi = {10.1145/512950.512973},
  language = {en},
  booktitle = {Proceedings of the 4th {{ACM SIGACT}}-{{SIGPLAN}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '77},
  publisher = {{ACM Press}},
  author = {Cousot, Patrick and Cousot, Radhia},
  year = {1977},
  pages = {238-252},
  file = {/Users/luigi/work/zotero/storage/S9HPZHMQ/Cousot and Cousot - 1977 - Abstract interpretation a unified lattice model f.pdf;/Users/luigi/work/zotero/storage/VAL45EDD/Cousot and Cousot - 1977 - Abstract interpretation a unified lattice model f.pdf}
}

@article{goslingJavaLanguageSpecification,
  title = {The {{Java}}\textregistered{} {{Language Specification}}},
  language = {en},
  author = {Gosling, James and Joy, Bill and Steele, Guy and Bracha, Gilad and Buckley, Alex},
  pages = {670},
  file = {/Users/luigi/work/zotero/storage/JV2A86LU/Gosling et al. - The Java® Language Specification.pdf;/Users/luigi/work/zotero/storage/K5UHI5UF/Gosling et al. - The Java® Language Specification.pdf}
}

@incollection{stroderProvingTerminationMemory2014,
  address = {Cham},
  title = {Proving {{Termination}} and {{Memory Safety}} for {{Programs}} with {{Pointer Arithmetic}}},
  volume = {8562},
  isbn = {978-3-319-08586-9 978-3-319-08587-6},
  abstract = {Proving termination automatically for programs with explicit pointer arithmetic is still an open problem. To close this gap, we introduce a novel abstract domain that can track allocated memory in detail. We use it to automatically construct a symbolic execution graph that represents all possible runs of the program and that can be used to prove memory safety. This graph is then transformed into an integer transition system, whose termination can be proved by standard techniques. We implemented this approach in the automated termination prover AProVE and demonstrate its capability of analyzing C programs with pointer arithmetic that existing tools cannot handle.},
  language = {en},
  booktitle = {Automated {{Reasoning}}},
  publisher = {{Springer International Publishing}},
  author = {Str\"oder, Thomas and Giesl, J\"urgen and Brockschmidt, Marc and Frohn, Florian and Fuhs, Carsten and Hensel, Jera and {Schneider-Kamp}, Peter},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Demri, St\'ephane and Kapur, Deepak and Weidenbach, Christoph},
  year = {2014},
  pages = {208-223},
  file = {/Users/luigi/work/zotero/storage/UXA43DHN/Ströder et al. - 2014 - Proving Termination and Memory Safety for Programs.pdf;/Users/luigi/work/zotero/storage/WXVZRFUI/Ströder et al. - 2014 - Proving Termination and Memory Safety for Programs.pdf},
  doi = {10.1007/978-3-319-08587-6_15}
}

@article{shieldsFirstClassModulesHaskell,
  title = {First-{{Class Modules}} for {{Haskell}}},
  abstract = {Though Haskell's module language is quite weak, its core language is highly expressive. Indeed, it is tantalisingly close to being able to express much of the structure traditionally delegated to a seperate module language. However, the encodings are awkward, and some situations can't be encoded at all.},
  language = {en},
  author = {Shields, Mark and Jones, Simon Peyton},
  pages = {20},
  file = {/Users/luigi/work/zotero/storage/CGZXVVQ8/Shields and Jones - First-Class Modules for Haskell.pdf;/Users/luigi/work/zotero/storage/VJMVHTIK/Shields and Jones - First-Class Modules for Haskell.pdf}
}

@article{schwartzFINALIZATIONOPERATIONABSTRACT,
  title = {{{THE FINALIZATION OPERATION FOR ABSTRACT TYPES}}},
  abstract = {In this paper we argue the importance e r a finalization capability in a programming language abstract type facility. Finalization, the dual of initialization, is crucial for applications involving the allocation of, and access to, abstract resources. A semantic model for finalization is given, defining both statically and dynamically allocated abstract objects, in the presence or" exception handling. For illustration, we incorporate finalization in an abstract data type facility designed as an extension of Ada.},
  language = {en},
  author = {Schwartz, Richard L and {Melliar-Smfth}, P M},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/H8U8GDPC/Schwartz and Melliar-Smfth - THE FINALIZATION OPERATION FOR ABSTRACT TYPES.pdf;/Users/luigi/work/zotero/storage/U6IU7KFK/Schwartz and Melliar-Smfth - THE FINALIZATION OPERATION FOR ABSTRACT TYPES.pdf}
}

@inproceedings{roemersAdapterawareNonintrusiveDependency2013,
  address = {Stuttgart, Germany},
  title = {An Adapter-Aware, Non-Intrusive Dependency Injection Framework for {{Java}}},
  isbn = {978-1-4503-2111-2},
  doi = {10.1145/2500828.2500834},
  abstract = {In strongly typed Object-Oriented Programming languages, it is common to encounter type incompatibilities between separately developed software components one desires to compose. Using the Adapter pattern to overcome these type incompatibilities is only an option if changing the source code of the software components is feasible, as references from objects to other objects are oftentimes hard-coded. The concept of Dependency Injection (DI) is aimed at mitigating the issue of hard-coded references. However, current implementations of DI are intrusive in ways that component developers need to foresee future use cases. To increase the reusability of components we propose an approach and a tool to configure interoperations between components externally, without the need for intrusive code changes. This approach is based on a new dependency injection mechanism that is combined with the Adapter pattern. If necessary, the most appropriate adapter to inject is selected automatically, thereby making the specifications of dependency injection very flexible.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform Virtual Machines}}, {{Languages}}, and {{Tools}} - {{PPPJ}} '13},
  publisher = {{ACM Press}},
  author = {Roemers, Arnout and Hatun, Kardelen and Bockisch, Christoph},
  year = {2013},
  pages = {57},
  file = {/Users/luigi/work/zotero/storage/88A2SMC5/Roemers et al. - 2013 - An adapter-aware, non-intrusive dependency injecti.pdf;/Users/luigi/work/zotero/storage/SGWAF8EC/Roemers et al. - 2013 - An adapter-aware, non-intrusive dependency injecti.pdf}
}

@article{reichenbachWhatCanGC,
  title = {What {{Can}} the {{GC Compute Efficiently}}? {{A Language}} for {{Heap Assertions}} at {{GC Time}}},
  abstract = {We present the DeAL language for heap assertions that are efficiently evaluated during garbage collection time. DeAL is a rich, declarative, logic-based language whose programs are guaranteed to be executable with good whole-heap locality, i.e., within a single traversal over every live object on the heap and a finite neighborhood around each object. As a result, evaluating DeAL programs incurs negligible cost: for simple assertion checking at each garbage collection, the end-to-end execution slowdown is below 2\%. DeAL is integrated into Java as a VM extension and we demonstrate its efficiency and expressiveness with several applications and properties from the past literature.},
  language = {en},
  author = {Reichenbach, Christoph and Immerman, Neil and Smaragdakis, Yannis and Aftandilian, Edward E and Guyer, Samuel Z},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/3CKTYTF4/Reichenbach et al. - What Can the GC Compute Eﬃciently A Language for .pdf;/Users/luigi/work/zotero/storage/TF6PWE7S/Reichenbach et al. - What Can the GC Compute Eﬃciently A Language for .pdf}
}

@inproceedings{koscielnyDeltaJDeltaorientedProgramming2014,
  address = {Cracow, Poland},
  title = {{{DeltaJ}} 1.5: Delta-Oriented Programming for {{Java}} 1.5},
  isbn = {978-1-4503-2926-2},
  shorttitle = {{{DeltaJ}} 1.5},
  doi = {10.1145/2647508.2647512},
  abstract = {Delta-oriented programming (DOP) is a modular, yet flexible approach to implement software product lines. In DOP, a product line is implemented by a set of deltas, which are containers of modifications to a program. A delta-oriented product line is specified by its code base, i.e., the set of delta modules, and a product line declaration specifying the set of possible product variants. In this paper, we present DOP for JAVA 1.5 extending previous proof-of-concept realizations of DOP for simple core JAVA-like languages. The novel prototypical implementation DELTAJ 1.5 provides full integrated access to the object-oriented features of JAVA. The extensions include delta operations to fully integrate the JAVA package system, to declare and modify interfaces, to explicitly change the inheritance hierarchy, to access nested types and enum types, to alter field declarations, and to unambiguously remove overloaded methods. Furthermore, we improve the specification of the product line declaration by providing a separate language. We have evaluated DELTAJ 1.5 using a case study.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Koscielny, Jonathan and Holthusen, S\"onke and Schaefer, Ina and Schulze, Sandro and Bettini, Lorenzo and Damiani, Ferruccio},
  year = {2014},
  pages = {63-74},
  file = {/Users/luigi/work/zotero/storage/7V3AJV3S/Koscielny et al. - 2014 - DeltaJ 1.5 delta-oriented programming for Java 1..pdf}
}

@inproceedings{bastoulCodeGenerationPolyhedral2004a,
  address = {Antibes Juan-les-Pins, France},
  title = {Code Generation in the Polyhedral Model Is Easier than You Think},
  isbn = {978-0-7695-2229-6},
  doi = {10.1109/PACT.2004.1342537},
  abstract = {Many advances in automatic parallelization and optimization have been achieved through the polyhedral model. It has been extensively shown that this computational model provides convenient abstractions to reason about and apply program transformations. Nevertheless, the complexity of code generation has long been a deterrent for using polyhedral representation in optimizing compilers. First, code generators have a hard time coping with generated code size and control overhead that may spoil theoretical benefits achieved by the transformations. Second, this step is usually time consuming, hampering the integration of the polyhedral framework in production compilers or feedback-directed, iterative optimization schemes. Moreover, current code generation algorithms only cover a restrictive set of possible transformation functions. This paper discusses a general transformation framework able to deal with non-unimodular, non-invertible, non-integral or even non-uniform functions. It presents several improvements to a state-of-the-art code generation algorithm. Two directions are explored: generated code size and code generator efficiency. Experimental evidence proves the ability of the improved method to handle real-life problems.},
  language = {en},
  booktitle = {Proceedings. 13th {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}}, 2004. {{PACT}} 2004.},
  publisher = {{IEEE}},
  author = {Bastoul, C.},
  year = {2004},
  pages = {7-16},
  file = {/Users/luigi/work/zotero/storage/NE8UDXJU/Bastoul - 2004 - Code generation in the polyhedral model is easier .pdf}
}

@inproceedings{oneyInterStateLanguageEnvironment2014,
  address = {Honolulu, Hawaii, USA},
  title = {{{InterState}}: A Language and Environment for Expressing Interface Behavior},
  isbn = {978-1-4503-3069-5},
  shorttitle = {{{InterState}}},
  doi = {10.1145/2642918.2647358},
  abstract = {InterState is a new programming language and environment that addresses the challenges of writing and reusing user interface code. InterState represents interactive behaviors clearly and concisely using a combination of novel forms of state machines and constraints. It also introduces new language features that allow programmers to easily modularize and reuse behaviors. InterState uses a new visual notation that allows programmers to better understand and navigate their code. InterState also includes a live editor that immediately updates the running application in response to changes in the editor and vice versa to help programmers understand the state of their program. Finally, InterState can interface with code and widgets written in other languages, for example to create a user interface in InterState that communicates with a database. We evaluated the understandability of InterState's programming primitives in a comparative laboratory study. We found that participants were twice as fast at understanding and modifying GUI components when they were implemented with InterState than when they were implemented in a conventional textual event-callback style. We evaluated InterState's scalability with a series of benchmarks and example applications and found that it can scale to implement complex behaviors involving thousands of objects and constraints.},
  language = {en},
  booktitle = {Proceedings of the 27th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology - {{UIST}} '14},
  publisher = {{ACM Press}},
  author = {Oney, Stephen and Myers, Brad and Brandt, Joel},
  year = {2014},
  pages = {263-272},
  file = {/Users/luigi/work/zotero/storage/YXFB52SW/Oney et al. - 2014 - InterState a language and environment for express.pdf;/Users/luigi/work/zotero/storage/ZUB47BJF/Oney et al. - 2014 - InterState a language and environment for express.pdf}
}

@incollection{oliveiraExtensibilityMasses2012,
  address = {Berlin, Heidelberg},
  title = {Extensibility for the {{Masses}}},
  volume = {7313},
  isbn = {978-3-642-31056-0 978-3-642-31057-7},
  abstract = {This paper presents a new solution to the expression problem (EP) that works in OO languages with simple generics (including Java or C\#). A key novelty of this solution is that advanced typing features, including F-bounded quantification, wildcards and variance annotations, are not needed. The solution is based on object algebras, which are an abstraction closely related to algebraic datatypes and Church encodings. Object algebras also have much in common with the traditional forms of the Visitor pattern, but without many of its drawbacks: they are extensible, remove the need for accept methods, and do not compromise encapsulation. We show applications of object algebras that go beyond toy examples usually presented in solutions for the expression problem. In the paper we develop an increasingly more complex set of features for a mini-imperative language, and we discuss a real-world application of object algebras in an implementation of remote batches. We believe that object algebras bring extensibility to the masses: object algebras work in mainstream OO languages, and they significantly reduce the conceptual overhead by using only features that are used by everyday programmers.},
  language = {en},
  booktitle = {{{ECOOP}} 2012 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Oliveira, Bruno C. d. S. and Cook, William R.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Noble, James},
  year = {2012},
  pages = {2-27},
  file = {/Users/luigi/work/zotero/storage/LHTKF9MF/Oliveira and Cook - 2012 - Extensibility for the Masses.pdf;/Users/luigi/work/zotero/storage/N7MEGDVA/Oliveira and Cook - 2012 - Extensibility for the Masses.pdf},
  doi = {10.1007/978-3-642-31057-7_2}
}

@inproceedings{octeauRetargetingAndroidApplications2012,
  address = {Cary, North Carolina},
  title = {Retargeting {{Android}} Applications to {{Java}} Bytecode},
  isbn = {978-1-4503-1614-9},
  doi = {10.1145/2393596.2393600},
  abstract = {The Android OS has emerged as the leading platform for SmartPhone applications. However, because Android applications are compiled from Java source into platform-specific Dalvik bytecode, existing program analysis tools cannot be used to evaluate their behavior. This paper develops and evaluates algorithms for retargeting Android applications received from markets to Java class files. The resulting Dare tool uses a new intermediate representation to enable fast and accurate retargeting. Dare further applies strong constraint solving to infer typing information and translates the 257 DVM opcodes using only 9 translation rules. It also handles cases where the input Dalvik bytecode is unverifiable. We evaluate Dare on 1,100 of the top applications found in the free section of the Android market and successfully retarget 99.99\% of the 262,110 associated classes. Further, whereas existing tools can only fully retarget about half of these applications, Dare can recover over 99\% of them. In this way, we open the door to users, developers and markets to use the vast array of program analysis tools to ensure the correct operation of Android applications.},
  language = {en},
  booktitle = {Proceedings of the {{ACM SIGSOFT}} 20th {{International Symposium}} on the {{Foundations}} of {{Software Engineering}} - {{FSE}} '12},
  publisher = {{ACM Press}},
  author = {Octeau, Damien and Jha, Somesh and McDaniel, Patrick},
  year = {2012},
  pages = {1},
  file = {/Users/luigi/work/zotero/storage/948CB6DS/Octeau et al. - 2012 - Retargeting Android applications to Java bytecode.pdf;/Users/luigi/work/zotero/storage/L57BB37W/Octeau et al. - 2012 - Retargeting Android applications to Java bytecode.pdf}
}

@inproceedings{milanovaCFLreachabilityContextsensitiveIntegrity2014,
  address = {Cracow, Poland},
  title = {{{CFL}}-Reachability and Context-Sensitive Integrity Types},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647522},
  abstract = {Integrity types can help detect information flow vulnerabilities in web applications and Android apps. We study DFlow, a contextsensitive integrity type system and we give an interpretation of DFlow in terms of CFL-reachability. We propose DFlowCFL, a new, more precise integrity type system, and DFlowCFL-Infer, the corresponding type inference analysis, which is equivalent to CFL-reachability. DFlowCFL-Infer is an effective taint analysis for Android. It scales well and detects numerous privacy leaks in popular Android apps.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Milanova, Ana and Huang, Wei and Dong, Yao},
  year = {2014},
  pages = {99-109},
  file = {/Users/luigi/work/zotero/storage/WGXTMGPQ/Milanova et al. - 2014 - CFL-reachability and context-sensitive integrity t.pdf}
}

@article{mcbrideFakingItSimulating2002,
  title = {Faking It {{Simulating}} Dependent Types in {{Haskell}}},
  volume = {12},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796802004355},
  abstract = {Dependent types re ect the fact that validity of data is often a relative notion by allowing prior data to a ect the types of subsequent data. Not only does this make for a precise type system, but also a highly generic one: both the type and the program for each instance of a family of operations can be computed from the data which codes for that instance.},
  language = {en},
  number = {4-5},
  journal = {Journal of Functional Programming},
  author = {McBRIDE, Conor},
  month = jul,
  year = {2002},
  file = {/Users/luigi/work/zotero/storage/DKCS2EXB/McBRIDE - 2002 - Faking it Simulating dependent types in Haskell.pdf}
}

@inproceedings{lukQilinExploitingParallelism2009,
  address = {New York, New York},
  title = {Qilin: Exploiting Parallelism on Heterogeneous Multiprocessors with Adaptive Mapping},
  isbn = {978-1-60558-798-1},
  shorttitle = {Qilin},
  doi = {10.1145/1669112.1669121},
  abstract = {Heterogeneous multiprocessors are increasingly important in the multi-core era due to their potential for high performance and energy efficiency. In order for software to fully realize this potential, the step that maps computations to processing elements must be as automated as possible. However, the state-of-the-art approach is to rely on the programmer to specify this mapping manually and statically. This approach is not only labor intensive but also not adaptable to changes in runtime environments like problem sizes and hardware/software configurations. In this study, we propose adaptive mapping, a fully automatic technique to map computations to processing elements on a CPU+GPU machine. We have implemented it in our experimental heterogeneous programming system called Qilin. Our results show that, by judiciously distributing works over the CPU and GPU, automatic adaptive mapping achieves a 25\% reduction in execution time and a 20\% reduction in energy consumption than static mappings on average for a set of important computation benchmarks. We also demonstrate that our technique is able to adapt to changes in the input problem size and system configuration.},
  language = {en},
  booktitle = {Proceedings of the 42nd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} - {{Micro}}-42},
  publisher = {{ACM Press}},
  author = {Luk, Chi-Keung and Hong, Sunpyo and Kim, Hyesoon},
  year = {2009},
  pages = {45},
  file = {/Users/luigi/work/zotero/storage/ELNY9AZ6/Luk et al. - 2009 - Qilin exploiting parallelism on heterogeneous mul.pdf}
}

@article{leeDebugAllYour,
  title = {Debug All Your Code: Portable Mixed-Environment Debugging},
  abstract = {Programmers build large-scale systems with multiple languages to reuse legacy code and leverage languages best suited to their problems. For instance, the same program may use Java for ease-of-programming and C to interface with the operating system. These programs pose significant debugging challenges, because programmers need to understand and control code across languages, which may execute in different environments. Unfortunately, traditional multilingual debuggers require a single execution environment.},
  language = {en},
  author = {Lee, Byeongcheol and Hirzel, Martin and Grimm, Robert and McKinley, Kathryn S},
  pages = {19},
  file = {/Users/luigi/work/zotero/storage/IDATSBY8/Lee et al. - Debug all your code portable mixed-environment de.pdf;/Users/luigi/work/zotero/storage/WK22YUKN/Lee et al. - Debug all your code portable mixed-environment de.pdf}
}

@article{leeAutomatedPredicateSelection,
  title = {Automated {{Predicate Selection}} for {{Staleness Based Memory Leak Detection}}},
  abstract = {State-of-the-art memory leak detectors leverage the staleness of allocated objects. Staleness predicate, which represents a condition upon which an object is regarded as a potential leak, is an important parameter of such memory leak detectors. Despite the fact, prior works lack an in-depth analysis on the staleness predicates. Their evaluation rather focus on the influence of sampling methods with which the staleness is estimated.},
  language = {en},
  author = {Lee, Sangho and Jung, Changhee and Pande, Santosh},
  pages = {15},
  file = {/Users/luigi/work/zotero/storage/QL7IIDR7/Lee et al. - Automated Predicate Selection for Staleness Based .pdf}
}

@article{leaAbstractionFailuresConcurrent,
  title = {Abstraction Failures in Concurrent Programming},
  language = {en},
  author = {Lea, Doug},
  pages = {1},
  file = {/Users/luigi/work/zotero/storage/DZKY22BN/Lea - Abstraction failures in concurrent programming.pdf;/Users/luigi/work/zotero/storage/SW6CN49T/Lea - Abstraction failures in concurrent programming.pdf}
}

@incollection{kawaguchiDsolveSafetyVerification2010,
  address = {Berlin, Heidelberg},
  title = {Dsolve: {{Safety Verification}} via {{Liquid Types}}},
  volume = {6174},
  isbn = {978-3-642-14294-9 978-3-642-14295-6},
  shorttitle = {Dsolve},
  abstract = {We present Dsolve, a verification tool for OCaml. Dsolve automates verification by inferring ``Liquid'' refinement types that are expressive enough to verify a variety of complex safety properties.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Kawaguchi, Ming and Rondon, Patrick M. and Jhala, Ranjit},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Touili, Tayssir and Cook, Byron and Jackson, Paul},
  year = {2010},
  pages = {123-126},
  file = {/Users/luigi/work/zotero/storage/6AANNI4N/Kawaguchi et al. - 2010 - Dsolve Safety Verification via Liquid Types.pdf;/Users/luigi/work/zotero/storage/BIDNVIRU/Kawaguchi et al. - 2010 - Dsolve Safety Verification via Liquid Types.pdf},
  doi = {10.1007/978-3-642-14295-6_12}
}

@article{juegaEvaluationStateoftheartPolyhedral,
  title = {Evaluation of State-of-the-Art Polyhedral Tools for Automatic Code Generation on {{GPUs}}},
  abstract = {At present, multi-core and manycore platforms lead the computer industry, forcing software developers to adopt new programming paradigms, in order to fully exploit their computing capabilities. Nowadays, Graphics Processing Units (GPUs) are one of representatives of many-core architectures, and certainly the most widespread.},
  language = {en},
  author = {Juega, J C and Gomez, J I and Tenllado, C and Verdoolaege, S and Cohen, A and Catthoor, F},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/4DTP3WSG/Juega et al. - Evaluation of state-of-the-art polyhedral tools fo.pdf;/Users/luigi/work/zotero/storage/YDPW2BZ3/Juega et al. - Evaluation of state-of-the-art polyhedral tools fo.pdf}
}

@article{jonesHowGiveGood,
  title = {How to Give a Good Research Talk},
  language = {en},
  author = {Jones, Simon L Peyton and Hughes, John and Launchbury, John},
  pages = {4},
  file = {/Users/luigi/work/zotero/storage/E3Q4WR36/Jones et al. - How to give a good research talk.pdf;/Users/luigi/work/zotero/storage/M78MN67N/Jones et al. - How to give a good research talk.pdf}
}

@incollection{jhalaHMCVerifyingFunctional2011,
  address = {Berlin, Heidelberg},
  title = {{{HMC}}: {{Verifying Functional Programs Using Abstract Interpreters}}},
  volume = {6806},
  isbn = {978-3-642-22109-5 978-3-642-22110-1},
  shorttitle = {{{HMC}}},
  abstract = {We present Hindley-Milner-Cousots (HMC), an algorithm that allows any interprocedural analysis for first-order imperative programs to be used to verify safety properties of typed higher-order functional programs. HMC works as follows. First, it uses the type structure of the functional program to generate a set of logical refinement constraints whose satisfaction implies the safety of the source program. Next, it transforms the logical refinement constraints into a simple first-order imperative program that is safe iff the constraints are satisfiable. Thus, in one swoop, HMC makes tools for invariant generation, e.g., based on abstract domains, predicate abstraction, counterexample-guided refinement, and Craig interpolation be directly applicable to verify safety properties of modern functional languages in a fully automatic manner. We have implemented HMC and describe preliminary experimental results using two imperative checkers \textendash{} ARMC and INTERPROC\textendash{} to verify OCAML programs. Thus, by composing type-based reasoning grounded in program syntax and state-based reasoning grounded in abstract interpretation, HMC opens the door to automatic verification of programs written in modern programming languages.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jhala, Ranjit and Majumdar, Rupak and Rybalchenko, Andrey},
  editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
  year = {2011},
  pages = {470-485},
  file = {/Users/luigi/work/zotero/storage/MNU4F7RJ/Jhala et al. - 2011 - HMC Verifying Functional Programs Using Abstract .pdf;/Users/luigi/work/zotero/storage/MPMHU3PN/Jhala et al. - 2011 - HMC Verifying Functional Programs Using Abstract .pdf},
  doi = {10.1007/978-3-642-22110-1_38}
}

@inproceedings{imamHabaneroJavaLibraryJava2014,
  address = {Cracow, Poland},
  title = {Habanero-{{Java}} Library: A {{Java}} 8 Framework for Multicore Programming},
  isbn = {978-1-4503-2926-2},
  shorttitle = {Habanero-{{Java}} Library},
  doi = {10.1145/2647508.2647514},
  abstract = {With the advent of the multicore era, it is clear that future growth in application performance will primarily come from increased parallelism. We believe parallelism should be introduced early into the Computer Science curriculum to educate students on the fundamentals of parallel computation. In this paper, we introduce the newly-created Habanero-Java library (HJlib), a pure Java 8 library implementation of the pedagogic parallel programming model [12]. HJlib has been used in teaching a sophomore-level course titled ``Fundamentals of Parallel Programming'' at Rice University.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Imam, Shams and Sarkar, Vivek},
  year = {2014},
  pages = {75-86},
  file = {/Users/luigi/work/zotero/storage/PF5GHAJZ/Imam and Sarkar - 2014 - Habanero-Java library a Java 8 framework for mult.pdf}
}

@inproceedings{hoferFastJavaProfiling2014,
  address = {Cracow, Poland},
  title = {Fast {{Java}} Profiling with Scheduling-Aware Stack Fragment Sampling and Asynchronous Analysis},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647509},
  abstract = {Sampling is a popular approach to profiling because it typically has only a small impact on performance and does not modify the profiled application. Common sampling profilers collect data about an application by pausing the application threads, walking the stacks to create stack traces, and then adding the traces to their profile. Waiting threads are often sampled as well, even when they have not been active since their last sample. Sampling profilers for Java commonly rely on safepoints, which are locations in the Java code where a thread can pause to be sampled. However, restricting profiling to these locations affects the accuracy of the profile, and the safepoint mechanism itself imposes significant pause times on the application.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Hofer, Peter and M\"ossenb\"ock, Hanspeter},
  year = {2014},
  pages = {145-156},
  file = {/Users/luigi/work/zotero/storage/X3MVHGYL/Hofer and Mössenböck - 2014 - Fast Java profiling with scheduling-aware stack fr.pdf}
}

@inproceedings{hirzelSelectiveRegressionTesting2014,
  address = {Cracow, Poland},
  title = {Selective Regression Testing for Web Applications Created with Google Web Toolkit},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647527},
  abstract = {Today's web applications are highly dynamic and powerful software components that may change often. Mostly, they are based on JavaScript or AJAX. A common way to ensure correct behaviour is to use selective regression tests. Nevertheless, especially on the client side, testing is hard. One way to ease the development and the testing process of dynamic web applications is to use the Google Web Toolkit (GWT). This framework enables the development in Java and transfers the code via a compiler into JavaScript. However, it does not support regression testing with test selection. As far as we know, this paper presents the first selective regression testing technique for GWT-based web applications. In order to determine test cases that have to be rerun, it compares the Java code of two versions of the application, localizes and classifies changes in the code, and traces the mapping of Java source code to JavaScript code. We have implemented our technique as a prototype Eclipse plug-in and have conducted an evaluation of the tool.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Hirzel, Matthias},
  year = {2014},
  pages = {110-121},
  file = {/Users/luigi/work/zotero/storage/NTFVZ9ZF/Hirzel - 2014 - Selective regression testing for web applications .pdf}
}

@inproceedings{glewTypesafeLinkingModular1999,
  address = {San Antonio, Texas, United States},
  title = {Type-Safe Linking and Modular Assembly Language},
  isbn = {978-1-58113-095-9},
  doi = {10.1145/292540.292563},
  abstract = {Linking is a low-level task that is usually vaguely specified, if at all, by language definitions. However, the security of web browsers and other extensible systems depends crucially upon a set of checks that must be performed at link time. Building upon the simple, but elegant ideas of Cardelli, and module constructs from high-level languages, we present a formal model of typed object files and a set of inference rules that are sufficient to guarantee that type safety is preserved by the linking process.},
  language = {en},
  booktitle = {Proceedings of the 26th {{ACM SIGPLAN}}-{{SIGACT}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '99},
  publisher = {{ACM Press}},
  author = {Glew, Neal and Morrisett, Greg},
  year = {1999},
  pages = {250-261},
  file = {/Users/luigi/work/zotero/storage/F959RSCH/Glew and Morrisett - 1999 - Type-safe linking and modular assembly language.pdf;/Users/luigi/work/zotero/storage/JBN5LGP4/Glew and Morrisett - 1999 - Type-safe linking and modular assembly language.pdf}
}

@inproceedings{geffkenSideEffectMonitoring2014,
  address = {Cracow, Poland},
  title = {Side Effect Monitoring for {{Java}} Using Bytecode Rewriting},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647515},
  abstract = {A side effect of a method in Java is a read or write operation that the method may perform on an object in the heap. Methods with side effects are more difficult to understand and to reason about than pure methods, in particular in the presence of aliasing. While both, Java and the underlying Java Virtual Machine (JVM), support specifying and checking types statically, neither supports ways of specifying and checking side effects statically or at run time.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Geffken, Manuel and Thiemann, Peter},
  year = {2014},
  pages = {87-98},
  file = {/Users/luigi/work/zotero/storage/JBQN9WPF/Geffken and Thiemann - 2014 - Side effect monitoring for Java using bytecode rew.pdf}
}

@article{dufourToolDynamicAnalysis,
  title = {*{{J}}: {{A Tool}} for {{Dynamic Analysis}} of {{Java Programs}}},
  abstract = {We describe a complete system for gathering, computing and presenting dynamic metrics from Java programs. The system itself was motivated from our real goals in understanding program behaviour as compiler/runtime developers, and so solves a number of practical and difficult problems related to metric gathering and analysis.},
  language = {en},
  author = {Dufour, Bruno and Hendren, Laurie and Verbrugge, Clark},
  pages = {2},
  file = {/Users/luigi/work/zotero/storage/2U43GING/Dufour et al. - J A Tool for Dynamic Analysis of Java Programs.pdf;/Users/luigi/work/zotero/storage/M4QRMRFJ/Dufour et al. - J A Tool for Dynamic Analysis of Java Programs.pdf}
}

@incollection{delineTypestatesObjects2004,
  address = {Berlin, Heidelberg},
  title = {Typestates for {{Objects}}},
  volume = {3086},
  isbn = {978-3-540-22159-3 978-3-540-24851-4},
  abstract = {Today's mainstream object-oriented compilers and tools do not support declaring and statically checking simple pre- and postconditions on methods and invariants on object representations. The main technical problem preventing static verification is reasoning about the sharing relationships among objects as well as where object invariants should hold. We have developed a programming model of typestates for objects with a sound modular checking algorithm. The programming model handles typical aspects of object-oriented programs such as downcasting, virtual dispatch, direct calls, and subclassing. The model also permits subclasses to extend the interpretation of typestates and to introduce additional typestates. We handle aliasing by adapting our previous work on practical linear types developed in the context of the Vault system. We have implemented these ideas in a tool called Fugue for specifying and checking typestates on Microsoft .NET-based programs.},
  language = {en},
  booktitle = {{{ECOOP}} 2004 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {DeLine, Robert and F\"ahndrich, Manuel},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Odersky, Martin},
  year = {2004},
  pages = {465-490},
  file = {/Users/luigi/work/zotero/storage/EGZ76PEI/DeLine and Fähndrich - 2004 - Typestates for Objects.pdf;/Users/luigi/work/zotero/storage/XT8FZFEC/DeLine and Fähndrich - 2004 - Typestates for Objects.pdf},
  doi = {10.1007/978-3-540-24851-4_21}
}

@article{wagnerPerspectiveComplexAdaptations1996,
  title = {Perspective: {{Complex Adaptations}} and the {{Evolution}} of {{Evolvability}}},
  volume = {50},
  copyright = {\textcopyright{} 1996 The Society for the Study of Evolution},
  issn = {1558-5646},
  shorttitle = {Perspective},
  doi = {10.1111/j.1558-5646.1996.tb02339.x},
  abstract = {The problem of complex adaptations is studied in two largely disconnected research traditions: evolutionary biology and evolutionary computer science. This paper summarizes the results from both areas and compares their implications. In evolutionary computer science it was found that the Darwinian process of mutation, recombination and selection is not universally effective in improving complex systems like computer programs or chip designs. For adaptation to occur, these systems must possess ``evolvability,'' i.e., the ability of random variations to sometimes produce improvement. It was found that evolvability critically depends on the way genetic variation maps onto phenotypic variation, an issue known as the representation problem. The genotype-phenotype map determines the variability of characters, which is the propensity to vary. Variability needs to be distinguished from variations, which are the actually realized differences between individuals. The genotype-phenotype map is the common theme underlying such varied biological phenomena as genetic canalization, developmental constraints, biological versatility, developmental dissociability, and morphological integration. For evolutionary biology the representation problem has important implications: how is it that extant species acquired a genotype-phenotype map which allows improvement by mutation and selection? Is the genotype-phenotype map able to change in evolution? What are the selective forces, if any, that shape the genotype-phenotype map? We propose that the genotype-phenotype map can evolve by two main routes: epistatic mutations, or the creation of new genes. A common result for organismic design is modularity. By modularity we mean a genotype-phenotype map in which there are few pleiotropic effects among characters serving different functions, with pleiotropic effects falling mainly among characters that are part of a single functional complex. Such a design is expected to improve evolvability by limiting the interference between the adaptation of different functions. Several population genetic models are reviewed that are intended to explain the evolutionary origin of a modular design. While our current knowledge is insufficient to assess the plausibility of these models, they form the beginning of a framework for understanding the evolution of the genotype-phenotype map.},
  language = {en},
  number = {3},
  journal = {Evolution},
  author = {Wagner, G\"unter P. and Altenberg, Lee},
  month = jun,
  year = {1996},
  keywords = {Adaptation,evolution of development,evolutionary computation,genetic representations,modularity,pleiotropy,quantitative genetics},
  pages = {967-976},
  file = {/Users/luigi/work/zotero/storage/EMAVLI3M/Wagner and Altenberg - 1996 - Perspective Complex Adaptations and the Evolution.pdf;/Users/luigi/work/zotero/storage/UCGW7U63/j.1558-5646.1996.tb02339.html}
}

@article{stromTypestateProgrammingLanguage1986,
  title = {Typestate: {{A}} Programming Language Concept for Enhancing Software Reliability},
  volume = {SE-12},
  issn = {0098-5589},
  shorttitle = {Typestate},
  doi = {10.1109/TSE.1986.6312929},
  abstract = {The authors introduce a new programming language concept, called typestate, which is a refinement of the concept of type. Whereas the type of a data object determines the set of operations over permitted on the object, typestate determines the subset of these operations which is permitted in a particular context. Typestate tracking is a program analysis technique which enhances program reliability by detecting at compile-time syntactically legal but semantically undefined execution sequences. These include reading a variable before it has been initialized and dereferencing a pointer after the dynamic object has been deallocated. The authors define typestate, give examples of its application, and show how typestate checking may be embedded into a compiler. They discuss the consequences of typestate checking for software reliability and software structure, and summarize their experience in using a high-level language incorporating typestate checking.},
  number = {1},
  journal = {IEEE Transactions on Software Engineering},
  author = {Strom, R. E. and Yemini, S.},
  month = jan,
  year = {1986},
  keywords = {Computer languages,program compilers,program verification,type checking,programming language,Program processors,Context,compiler,data structures,software reliability,compile-time,data object,dynamic object,high-level language,Law,Program analysis,program analysis technique,security,Software reliability,typestate,undefined execution sequences},
  pages = {157-171},
  file = {/Users/luigi/work/zotero/storage/FY7CH6NC/Strom and Yemini - 1986 - Typestate A programming language concept for enha.pdf;/Users/luigi/work/zotero/storage/WQ6KM8JT/6312929.html}
}

@misc{zhangNonblockingSynchronizationAlgorithms2003,
  title = {Non-Blocking {{Synchronization}}: {{Algorithms}} and {{Performance Evaluation}}},
  shorttitle = {Non-Blocking {{Synchronization}}},
  abstract = {Semantic Scholar extracted view of \&quot;Non-blocking Synchronization: Algorithms and Performance Evaluation\&quot; by Yi Zhang},
  howpublished = {/paper/Non-blocking-Synchronization\%3A-Algorithms-and-Zhang/f14e817755a2e1b60d761b5680cd4eb0021fc03c},
  author = {Zhang, Yi},
  year = {2003},
  file = {/Users/luigi/work/zotero/storage/7TJ569SU/Zhang - 2003 - Non-blocking Synchronization Algorithms and Perfo.pdf;/Users/luigi/work/zotero/storage/V2KPL9ZW/f14e817755a2e1b60d761b5680cd4eb0021fc03c.html}
}

@inproceedings{liskovKeynoteAddressData1987,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '87},
  title = {Keynote {{Address}} - {{Data Abstraction}} and {{Hierarchy}}},
  isbn = {978-0-89791-266-2},
  doi = {10.1145/62138.62141},
  booktitle = {Addendum to the {{Proceedings}} on {{Object}}-Oriented {{Programming Systems}}, {{Languages}} and {{Applications}} ({{Addendum}})},
  publisher = {{ACM}},
  author = {Liskov, Barbara},
  year = {1987},
  pages = {17--34},
  file = {/Users/luigi/work/zotero/storage/YPUU5XSS/Liskov - 1987 - Keynote Address - Data Abstraction and Hierarchy.pdf}
}

@article{vandelftDependentTypesInheritable,
  title = {Dependent {{Types}} and {{Inheritable Constructors}} in {{Java}}},
  abstract = {This paper presents a small extension to the Java language with dependent types and inheritable constructors. A dependent type is the most specific type of a given object. In essence it is already known in other languages such as Eiffel. There the definition was unfortunately flawed. Also existing theory does not adequately cover dependent types. This work presents a simple definition, and shows how a type hierarchy based on class types can be extended with dependent types in a sound way. This leads Dependent to rules that compilers can apply.},
  language = {en},
  author = {{van Delft}, Andr\'e},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/53P5G6DV/van Delft - Dependent Types and Inheritable Constructors in Ja.pdf;/Users/luigi/work/zotero/storage/UAT96TYU/van Delft - Dependent Types and Inheritable Constructors in Ja.pdf}
}

@article{saTypeSystemSafe,
  title = {A {{Type System}} for {{Safe Region}}-{{Based Memory Management}} in {{Real}}-{{Time Java}}},
  abstract = {The Real-Time Specification for Java (RTSJ) allows a program to create real-time threads with hard real-time constraints. Real-time threads use immortal memory and region-based memory management to avoid unbounded pauses caused by interference from the garbage collector. The RTSJ uses runtime checks to ensure that deleting a region does not create dangling references and that real-time threads do not access references to objects allocated in the garbage-collected heap. This paper presents a static type system that guarantees that these runtime checks will never fail for well-typed programs. Our type system therefore 1) provides an important safety guarantee for real-time programs and 2) makes it possible to eliminate the runtime checks and their associated overhead.},
  language = {en},
  author = {Sa, Alexandru},
  pages = {29},
  file = {/Users/luigi/work/zotero/storage/GII9UTVH/Sa - A Type System for Safe Region-Based Memory Managem.pdf;/Users/luigi/work/zotero/storage/UAT2AUWL/Sa - A Type System for Safe Region-Based Memory Managem.pdf}
}

@article{dwyerAdaptiveOnlineProgram,
  title = {Adaptive {{Online Program Analysis}}: {{Concepts}}, {{Infrastructure}}, and {{Applications}}},
  abstract = {Dynamic analysis of state-based properties is being applied to problems such as validation, intrusion detection, and program steering and reconfiguration. Dynamic analysis of such properties, however, is used rarely in practice due to its associated run-time overhead that causes multiple orders of magnitude slowdown of program execution. In this paper, we present an approach for exploiting the state-fullness of specifications to reduce the cost of dynamic program analysis. With our approach, the results of the analysis are guaranteed to be identical to those of the traditional, expensive dynamic analyses, yet with overheads between 23\% and 33\% relative to the un-instrumented application, for a range of non-trivial analyses. We describe the principles behind our adaptive online program analysis technique, extensions to our Java run-time analysis framework that support such analyses, and report on the performance and capabilities of two different families of adaptive online program analyses.},
  language = {en},
  author = {Dwyer, Matthew B and Kinneer, Alex and Elbaum, Sebastian},
  pages = {26},
  file = {/Users/luigi/work/zotero/storage/9ZPHU2IX/Dwyer et al. - Adaptive Online Program Analysis Concepts, Infras.pdf}
}

@article{garciaFoundationsTypestateOrientedProgramming2014,
  title = {Foundations of {{Typestate}}-{{Oriented Programming}}},
  volume = {36},
  issn = {01640925},
  doi = {10.1145/2629609},
  language = {en},
  number = {4},
  journal = {ACM Transactions on Programming Languages and Systems},
  author = {Garcia, Ronald and Tanter, \'Eric and Wolff, Roger and Aldrich, Jonathan},
  month = oct,
  year = {2014},
  pages = {1-44},
  file = {/Users/luigi/work/zotero/storage/BRWITKWL/Garcia et al. - 2014 - Foundations of Typestate-Oriented Programming.pdf;/Users/luigi/work/zotero/storage/V598JNMW/Garcia et al. - 2014 - Foundations of Typestate-Oriented Programming.pdf}
}

@article{seoSociaLiteEfficientGraph2015,
  title = {{{SociaLite}}: {{An Efficient Graph Query Language Based}} on {{Datalog}}},
  volume = {27},
  issn = {1041-4347},
  shorttitle = {{{SociaLite}}},
  doi = {10.1109/TKDE.2015.2405562},
  abstract = {With the rise of social networks, large-scale graph analysis becomes increasingly important. Because SQL lacks the expressiveness and performance needed for graph algorithms, lower-level, general-purpose languages are often used instead. For greater ease of use and efficiency, we propose SociaLite, a high-level graph query language based on Datalog. As a logic programming language, Datalog allows many graph algorithms to be expressed succinctly. However, its performance has not been competitive when compared to low-level languages. With SociaLite, users can provide high-level hints on the data layout and evaluation order; they can also define recursive aggregate functions which, as long as they are meet operations, can be evaluated incrementally and efficiently. Moreover, recursive aggregate functions make it possible to implement more graph algorithms that cannot be implemented in Datalog.},
  language = {en},
  number = {7},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  author = {Seo, Jiwon and Guo, Stephen and Lam, Monica S.},
  month = jul,
  year = {2015},
  pages = {1824-1837},
  file = {/Users/luigi/work/zotero/storage/8FCYNI47/Seo et al. - 2015 - SociaLite An Efficient Graph Query Language Based.pdf}
}

@article{norellPracticalProgrammingLanguage,
  title = {Towards a Practical Programming Language Based on Dependent Type Theory},
  abstract = {Dependent type theories [ML72] have a long history of being used for theorem proving. One aspect of type theory which makes it very powerful as a proof language is that it mixes deduction with computation. This also makes type theory a good candidate for programming\textemdash{}the strength of the type system allows properties of programs to be stated and established, and the computational properties provide semantics for the programs.},
  language = {en},
  author = {Norell, Ulf},
  pages = {166},
  file = {/Users/luigi/work/zotero/storage/SPZW7MH8/Norell - Towards a practical programming language based on .pdf}
}

@article{couttsStreamFusionPractical,
  title = {Stream {{Fusion}}: {{Practical}} Shortcut Fusion for Coinductive Sequence Types},
  language = {en},
  author = {Coutts, Duncan},
  pages = {281},
  file = {/Users/luigi/work/zotero/storage/IYGMA6SU/Coutts - Stream Fusion Practical shortcut fusion for coind.pdf}
}

@article{RecursiveFunctionalHardware,
  title = {Recursive {{Functional Hardware Descriptions}} Using {{C$\lambda$aSH}}},
  language = {en},
  pages = {84},
  file = {/Users/luigi/work/zotero/storage/N24AZEHQ/Recursive Functional Hardware Descriptions using C.pdf}
}

@article{bradyPracticalImplementationDependently,
  title = {Practical {{Implementation}} of a {{Dependently Typed Functional Programming Language}}},
  language = {en},
  author = {Brady, Edwin C},
  pages = {270},
  file = {/Users/luigi/work/zotero/storage/HN77CX7N/Brady - Practical Implementation of a Dependently Typed Fu.pdf}
}

@article{jonssonTimeSizeEfficientSupercompilation,
  title = {Time- and {{Size}}-{{Efficient Supercompilation}}},
  language = {en},
  author = {Jonsson, Peter A},
  pages = {116},
  file = {/Users/luigi/work/zotero/storage/5GMCMVXN/Jonsson - Time- and Size-Efficient Supercompilation.pdf}
}

@article{baxterBranchCoverageArbitrary,
  title = {Branch {{Coverage}} for {{Arbitrary Languages Made Easy}}},
  abstract = {Branch coverage is an important measure of the thoroughness of testing. One can easily get tools that collect this information for mainstream languages (C, Ada) on mainstream platforms (Solaris, UNIX). Such tools are difficult to find for less widely used or interpretive languages (JavaScript) or languages used on nonstandard platforms (C in embedded systems).},
  language = {en},
  author = {Baxter, Ira D},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/DBFX8BVI/Baxter - Branch Coverage for Arbitrary Languages Made Easy.pdf}
}

@incollection{albertTypeBasedHomeomorphicEmbedding2008,
  address = {Berlin, Heidelberg},
  title = {Type-{{Based Homeomorphic Embedding}} and {{Its Applications}} to {{Online Partial Evaluation}}},
  volume = {4915},
  isbn = {978-3-540-78768-6 978-3-540-78769-3},
  abstract = {Homeomorphic Embedding (HEm) has proven to be very powerful for supervising termination of computations, provided that such computations are performed over a finite signature, i.e., the number of constants and function symbols involved is finite. However, there are situations, for example numeric computations, which involve an infinite (or too large) signature, in which HEm does not guarantee termination. Some extensions to HEm for the case of infinite signatures have been proposed which guarantee termination, but they either do not provide systematic means for generating such extensions or the extensions are too simplistic and do not produce the expected results in practice. We introduce Type-based Homeomorphic Embedding (TbHEm) as an extension of the standard, untyped HEm to deal with infinite signatures. In the paper, we show how TbHEm can be used to improve the accuracy of online partial evaluation. For this purpose, we propose an approach to constructing suitable types for partial evaluation automatically based on existing analysis tools for constraint logic programs. We also present useful properties of types which allow us to take full advantage of TbHEm in practice. Experimental results are reported which show that our work improves the state of the practice of online partial evaluation.},
  language = {en},
  booktitle = {Logic-{{Based Program Synthesis}} and {{Transformation}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Albert, Elvira and Gallagher, John and {G\'omez-Zamalloa}, Miguel and Puebla, Germ\'an},
  editor = {King, Andy},
  year = {2008},
  pages = {23-42},
  file = {/Users/luigi/work/zotero/storage/T5MX264N/Albert et al. - 2008 - Type-Based Homeomorphic Embedding and Its Applicat.pdf},
  doi = {10.1007/978-3-540-78769-3_3}
}

@incollection{demouraLeanTheoremProver2015,
  address = {Cham},
  title = {The {{Lean Theorem Prover}} ({{System Description}})},
  volume = {9195},
  isbn = {978-3-319-21400-9 978-3-319-21401-6},
  abstract = {Lean is a new open source theorem prover being developed at Microsoft Research and Carnegie Mellon University, with a small trusted kernel based on dependent type theory. It aims to bridge the gap between interactive and automated theorem proving, by situating automated tools and methods in a framework that supports user interaction and the construction of fully specified axiomatic proofs. Lean is an ongoing and long-term effort, but it already provides many useful components, integrated development environments, and a rich API which can be used to embed it into other systems. It is currently being used to formalize category theory, homotopy type theory, and abstract algebra. We describe the project goals, system architecture, and main features, and we discuss applications and continuing work.},
  language = {en},
  booktitle = {Automated {{Deduction}} - {{CADE}}-25},
  publisher = {{Springer International Publishing}},
  author = {{de Moura}, Leonardo and Kong, Soonho and Avigad, Jeremy and {van Doorn}, Floris and {von Raumer}, Jakob},
  editor = {Felty, Amy P. and Middeldorp, Aart},
  year = {2015},
  pages = {378-388},
  file = {/Users/luigi/work/zotero/storage/ITA7VJC5/de Moura et al. - 2015 - The Lean Theorem Prover (System Description).pdf},
  doi = {10.1007/978-3-319-21401-6_26}
}

@inproceedings{choiGuidedGUITesting2013,
  address = {Indianapolis, Indiana, USA},
  title = {Guided {{GUI}} Testing of Android Apps with Minimal Restart and Approximate Learning},
  isbn = {978-1-4503-2374-1},
  doi = {10.1145/2509136.2509552},
  abstract = {Smartphones and tablets with rich graphical user interfaces (GUI) are becoming increasingly popular. Hundreds of thousands of specialized applications, called apps, are available for such mobile platforms. Manual testing is the most popular technique for testing graphical user interfaces of such apps. Manual testing is often tedious and error-prone. In this paper, we propose an automated technique, called SwiftHand, for generating sequences of test inputs for Android apps. The technique uses machine learning to learn a model of the app during testing, uses the learned model to generate user inputs that visit unexplored states of the app, and uses the execution of the app on the generated inputs to refine the model. A key feature of the testing algorithm is that it avoids restarting the app, which is a significantly more expensive operation than executing the app on a sequence of inputs. An important insight behind our testing algorithm is that we do not need to learn a precise model of an app, which is often computationally intensive, if our goal is to simply guide test execution into unexplored parts of the state space. We have implemented our testing algorithm in a publicly available tool for Android apps written in Java. Our experimental results show that we can achieve significantly better coverage than traditional random testing and L{${_\ast}$}-based testing in a given time budget. Our algorithm also reaches peak coverage faster than both random and L{${_\ast}$}-based testing.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN}} International Conference on {{Object}} Oriented Programming Systems Languages \& Applications - {{OOPSLA}} '13},
  publisher = {{ACM Press}},
  author = {Choi, Wontae and Necula, George and Sen, Koushik},
  year = {2013},
  pages = {623-640},
  file = {/Users/luigi/work/zotero/storage/XFNZ4AP6/Choi et al. - 2013 - Guided GUI testing of android apps with minimal re.pdf}
}

@article{erdmann15212Principles,
  title = {15\textendash{}212: {{Principles}} of {{Programming}}},
  language = {en},
  author = {Erdmann, Michael},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/ZTUQ8CYY/Erdmann - 15–212 Principles of Programming.pdf}
}

@inproceedings{steensgaardPointstoAnalysisAlmost1996,
  address = {St. Petersburg Beach, Florida, United States},
  title = {Points-to Analysis in Almost Linear Time},
  isbn = {978-0-89791-769-8},
  doi = {10.1145/237721.237727},
  abstract = {We present an interprocedural flow-insensitive points-to analysis based on type inference methods with an almost linear time cost complexity. To our knowledge, this is the asymptotically fastest non-trivial interprocedural points-to analysis algorithm yet described. The algorithm is based on a non-standard type system. The type inferred for any variable represents a set of locations and includes a type which in turn represents a set of locations possibly pointed to by the variable. The type inferred for a function variable represents a set of functions it may point to and includes a type signature for these functions. The results are equivalent to those of a flowinsensitive alias analysis (and control flow analysis) that assumes alias relations are reflexive and transitive.},
  language = {en},
  booktitle = {Proceedings of the 23rd {{ACM SIGPLAN}}-{{SIGACT}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '96},
  publisher = {{ACM Press}},
  author = {Steensgaard, Bjarne},
  year = {1996},
  pages = {32-41},
  file = {/Users/luigi/work/zotero/storage/ZJQZEJ3S/Steensgaard - 1996 - Points-to analysis in almost linear time.pdf}
}

@article{olinskyStagedAllocationCompositional,
  title = {Staged {{Allocation}}: {{A Compositional Technique}} for {{Specifying}} and {{Implementing Procedure Calling Conventions}}},
  abstract = {We present staged allocation, a technique for specifying calling conventions by composing tiny allocators called stages. A specification written using staged allocation has a precise, formal semantics, and it can be executed directly inside a compiler. Specifications of nine standard C calling conventions range in size from 15 to 30 lines each. An implementation of staged allocation takes about 250 lines of ML or 650 lines of C++. Each specification can be used not only to help a compiler implement the calling convention but also to generate a test suite.},
  language = {en},
  author = {Olinsky, Reuben and Lindig, Christian and Ramsey, Norman},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/87UH6PQ6/Olinsky et al. - Staged Allocation A Compositional Technique for S.pdf}
}

@inproceedings{wangOptimizationsafeSystemsAnalyzing2013,
  address = {Farminton, Pennsylvania},
  title = {Towards Optimization-Safe Systems: Analyzing the Impact of Undefined Behavior},
  isbn = {978-1-4503-2388-8},
  shorttitle = {Towards Optimization-Safe Systems},
  doi = {10.1145/2517349.2522728},
  abstract = {This paper studies an emerging class of software bugs called optimization-unstable code: code that is unexpectedly discarded by compiler optimizations due to undefined behavior in the program. Unstable code is present in many systems, including the Linux kernel and the Postgres database. The consequences of unstable code range from incorrect functionality to missing security checks. To reason about unstable code, this paper proposes a novel model, which views unstable code in terms of optimizations that leverage undefined behavior. Using this model, we introduce a new static checker called Stack that precisely identifies unstable code. Applying Stack to widely used systems has uncovered 160 new bugs that have been confirmed and fixed by developers.},
  language = {en},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth ACM Symposium}} on {{Operating Systems Principles}} - {{SOSP}} '13},
  publisher = {{ACM Press}},
  author = {Wang, Xi and Zeldovich, Nickolai and Kaashoek, M. Frans and {Solar-Lezama}, Armando},
  year = {2013},
  pages = {260-275},
  file = {/Users/luigi/work/zotero/storage/KL87RKG5/Wang et al. - 2013 - Towards optimization-safe systems analyzing the i.pdf}
}

@article{pukallFlexibleDynamicSoftware,
  title = {Flexible {{Dynamic Software Updates}} of {{Java Applications}}: {{Tool Support}} and {{Case Study}}},
  abstract = {Software is changed frequently during its life cycle. New requirements come and bugs must be fixed. To update an application it usually must be stopped, patched, and restarted. This causes time periods of unavailability which is always a problem for highly available applications. Even for the development of complex applications restarts to test new program parts can be time consuming and annoying. Thus, we aim at dynamic software updates to update programs at runtime. There is a large body of research on dynamic software updates, but so far, existing approaches have shortcomings either in terms of flexibility or performance. In addition, some of them depend on specific runtime environments and dictate the program's architecture. We present JavAdaptor, the first runtime update approach based on Java that (a) offers flexible dynamic software updates, (b) is platform independent, (c) introduces only minimal performance overhead, and (d) does not dictate the program architecture. JavAdaptor combines schema changing class replacements by class renaming and caller updates with Java HotSwap using containers and proxies. It runs on top of all major standard Java virtual machines. We evaluate our approach's applicability and performance in a nontrivial case study and compare it to existing dynamic software update approaches.},
  language = {en},
  author = {Pukall, M and Kaestner, C and Cazzola, W and Goetz, S and Grebhahn, A and Schroeter, R},
  pages = {39},
  file = {/Users/luigi/work/zotero/storage/KXUB79UJ/Pukall et al. - Flexible Dynamic Software Updates of Java Applicat.pdf}
}

@book{workshoponprogrammingformobiletouchPROMOTO15Proceedings2015,
  title = {{{PROMOTO}}'15: Proceedings of the 3rd {{International Workshop}} on {{Programming}} for {{Mobile}} and {{Touch}} : {{October}} 27, 2015, {{Pittsburgh}}, {{PA}}, {{USA}}},
  shorttitle = {{{PROMOTO}}'15},
  language = {en},
  author = {{Workshop on Programming for Mobile \& Touch} and Fraser, Steven D and Sillitti, Alberto and {ACM Special Interest Group on Programming Languages} and {Association for Computing Machinery} and {SPLASH (Conference)}},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/VI9IT4QB/Workshop on Programming for Mobile & Touch et al. - 2015 - PROMOTO'15 proceedings of the 3rd International W.pdf},
  note = {OCLC: 961216634}
}

@book{acmsigplanworkshoponprogrammingbasedonactorsAGERE2015Proceedings2015,
  title = {{{AGERE}}! 2015: Proceedings of the 5th {{International Workshop}} on {{Programming Based}} on {{Actors}}, {{Agents}}, and {{Decentralized Control}} : {{October}} 26, 2015, {{Pittsburgh}}, {{PA}}, {{USA}}},
  shorttitle = {{{AGERE}}! 2015},
  language = {en},
  author = {{ACM SIGPLAN Workshop on Programming Based on Actors}, {and} Decentralized Control, Agents and Boix, Elisa Gonzalez and {ACM Special Interest Group on Programming Languages} and {Association for Computing Machinery} and {SPLASH (Conference)}},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/GEF9AXQ2/ACM SIGPLAN Workshop on Programming Based on Actors et al. - 2015 - AGERE! 2015 proceedings of the 5th International .pdf},
  note = {OCLC: 956875779}
}

@book{splashconferenceSPLASHCompanion152015,
  title = {{{SPLASH Companion}}'15: Companion Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Systems}}, {{Programming}}, {{Languages}} and {{Applications}}: {{Software}} for {{Humanity}} : {{October}} 25-30, 2015, {{Pittsburgh}}, {{PA}}, {{USA}}},
  shorttitle = {{{SPLASH Companion}}'15},
  language = {en},
  author = {{SPLASH (Conference)} and Aldrich, Jonathan and Eugster, Patrick and {ACM Special Interest Group on Programming Languages} and {Association for Computing Machinery}},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/DK48DLRV/SPLASH (Conference) et al. - 2015 - SPLASH Companion'15 companion proceedings of the .pdf},
  note = {OCLC: 962030088}
}

@article{matsakisTimeAwareTypeSystem,
  title = {A {{Time}}-{{Aware Type System For Data}}-{{Race Protection}} and {{Guaranteed Initialization}}},
  abstract = {We introduce a type system based on intervals, objects representing the time in which a block of code will execute. The type system can verify time-based properties such as when a field will be accessed or a method will be invoked.},
  language = {en},
  author = {Matsakis, Nicholas D and Gross, Thomas R},
  pages = {18},
  file = {/Users/luigi/work/zotero/storage/738XG38M/Matsakis and Gross - A Time-Aware Type System For Data-Race Protection .pdf;/Users/luigi/work/zotero/storage/TRD8LIJ2/Matsakis and Gross - A Time-Aware Type System For Data-Race Protection .pdf}
}

@article{jonesCallpatternSpecialisationHaskell,
  title = {Call-Pattern {{Specialisation}} for {{Haskell Programs}}},
  abstract = {User-defined data types, pattern-matching, and recursion are ubiquitous features of Haskell programs. Sometimes a function is called with arguments that are statically known to be in constructor form, so that the work of pattern-matching is wasted. Even worse, the argument is sometimes freshly-allocated, only to be immediately decomposed by the function.},
  language = {en},
  author = {Jones, Simon Peyton},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/QV5V9SXG/Jones - Call-pattern Specialisation for Haskell Programs.pdf}
}

@article{greenbergTrackingFlowIdeas,
  title = {Tracking the {{Flow}} of {{Ideas}} through the {{Programming Languages Literature}}},
  abstract = {How have conferences like ICFP, OOPSLA, PLDI, and POPL evolved over the last 20 years? Did generalizing the Call for Papers for OOPSLA in 2007 or changing the name of the umbrella conference to SPLASH in 2010 have any effect on the kinds of papers published there? How do POPL and PLDI papers compare, topic-wise? Is there related work that I am missing? Have the ideas in O'Hearn's classic paper on separation logic shifted the kinds of papers that appear in POPL? Does a proposed program committee cover the range of submissions expected for the conference? If we had better tools for analyzing the programming language literature, we might be able to answer these questions and others like them in a data-driven way. In this paper, we explore how topic modeling, a branch of machine learning, might help the programming language community better understand our literature.},
  language = {en},
  author = {Greenberg, Michael and Fisher, Kathleen and Walker, David},
  pages = {16},
  file = {/Users/luigi/work/zotero/storage/6JZ7KHJX/Greenberg et al. - Tracking the Flow of Ideas through the Programming.pdf}
}

@book{sleconferenceSLE15Proceedings2015,
  title = {{{SLE}}'15: Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Software Language Engineering}} : {{October}} 26-27, 2015, {{Pittsburgh}}, {{PA}}, {{USA}}},
  shorttitle = {{{SLE}}'15},
  language = {en},
  author = {{SLE (Conference)} and Paige, Richard F and Di Ruscio, Davide and V\"olter, Markus and {ACM Special Interest Group on Programming Languages} and {Association for Computing Machinery} and {SPLASH (Conference)}},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/KXTP6F8S/SLE (Conference) et al. - 2015 - SLE'15 proceedings of the 2015 ACM SIGPLAN Intern.pdf},
  note = {OCLC: 962027920}
}

@article{sjobergDEPENDENTLYTYPEDLANGUAGE,
  title = {A {{DEPENDENTLY TYPED LANGUAGE WITH NONTERMINATION}}},
  language = {en},
  author = {Sjoberg, Vilhelm},
  pages = {288},
  file = {/Users/luigi/work/zotero/storage/F6TNNFX6/Sjoberg - A DEPENDENTLY TYPED LANGUAGE WITH NONTERMINATION.pdf}
}

@article{InstitutNationalRecherche1992,
  title = {Institut National de Recherche En Informatique et En Automatique},
  volume = {37},
  issn = {0759-1063, 2070-2779},
  doi = {10.1177/075910639203700105},
  abstract = {Guarded algebraic data types subsume the concepts known in the literature as indexed types, guarded recursive datatype constructors, and first-class phantom types, and are closely related to inductive types. They have the distinguishing feature that, when typechecking a function defined by cases, every branch may be checked under different assumptions about the type variables in scope. This mechanism allows exploiting the presence of dynamic tests in the code to produce extra static type information.},
  language = {en},
  number = {1},
  journal = {Bulletin of Sociological Methodology/Bulletin de M\'ethodologie Sociologique},
  month = dec,
  year = {1992},
  pages = {55-57},
  file = {/Users/luigi/work/zotero/storage/VNSKYHEE/1992 - Institut national de recherche en informatique et .pdf}
}

@article{ungarSELFPowerSimplicity,
  title = {{{SELF}}: {{The Power}} of {{Simplicity}}},
  abstract = {SELF is an object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because SELF does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. SELF's simplicity and expressiveness offer new insights into objectoriented computation.},
  language = {en},
  author = {Ungar, David},
  pages = {20},
  file = {/Users/luigi/work/zotero/storage/3W3U4T5A/Ungar - SELF The Power of Simplicity.pdf}
}

@article{leijenExtensibleRecordsScoped,
  title = {Extensible Records with Scoped Labels},
  abstract = {Records provide a safe and flexible way to construct data structures. We describe a natural approach to typing polymorphic and extensible records that is simple, easy to use in practice, and straightforward to implement. A novel aspect of this work is that records can contain duplicate labels, effectively introducing a form of scoping over the labels. Furthermore, it is a fully orthogonal extension to existing type systems and programming languages. In particular, we show how it can be used conveniently with standard HindleyMilner, qualified types, and MLF.},
  language = {en},
  author = {Leijen, Daan},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/GQHZITXE/Leijen - Extensible records with scoped labels.pdf}
}

@article{knowlesSageUnifiedHybrid,
  title = {Sage: {{Unified Hybrid Checking}} for {{First}}-{{Class Types}}, {{General Refinement Types}}, and {{Dynamic}} ({{Extended Report}})},
  abstract = {This paper presents Sage, a functional programming language with a rich type system that supports a broad range of typing paradigms, from dynamically-typed Scheme-like programming, to decidable ML-like types, to precise refinement types. This type system is a synthesis of three general concepts \textemdash{} first-class types, general refinement types, and the type Dynamic \textemdash{} that add expressive power in orthogonal and complementary ways.},
  language = {en},
  author = {Knowles, Kenneth and Tomb, Aaron and Gronski, Jessica and Freund, Stephen N and Flanagan, Cormac},
  pages = {29},
  file = {/Users/luigi/work/zotero/storage/VRCCE9PL/Knowles et al. - Sage Uniﬁed Hybrid Checking for First-Class Types.pdf}
}

@article{volanschiDeclarativeSpecializationObjectOriented,
  title = {Declarative {{Specialization}} of {{Object}}-{{Oriented Programs}}},
  language = {en},
  author = {Volanschi, Eugen-Nicolae and Consel, Charles and Muller, Gilles and Cowan, Crispin},
  pages = {28},
  file = {/Users/luigi/work/zotero/storage/ZG5ZCW8F/Volanschi et al. - Declarative Specialization of Object-Oriented Prog.pdf}
}

@article{jonesImplementationGoferFunctional,
  title = {The Implementation of the {{Gofer}} Functional Programming System},
  abstract = {The Gofer system is a functional programming environment for a small, Haskell-like language. Supporting a wide range of different machines, including home computers, the system is widely used, both for teaching and research.},
  language = {en},
  author = {Jones, Mark P and Box, P O},
  pages = {52},
  file = {/Users/luigi/work/zotero/storage/MALJNVVN/Jones and Box - The implementation of the Gofer functional program.pdf}
}

@inproceedings{repsPreciseInterproceduralDataflow1995,
  address = {San Francisco, California, United States},
  title = {Precise Interprocedural Dataflow Analysis via Graph Reachability},
  isbn = {978-0-89791-692-9},
  doi = {10.1145/199448.199462},
  abstract = {The paper shows how a large class of interprocedural dataflow-analysis problems can be solved precisely in polynomial time by transforming them into a special kind of graph-reachability problem. The only restrictions are that the set of dataflow facts must be a finite set, and that the dataflow functions must distribute over the confluence operator (either union or intersection). This class of problems includes\textemdash{}but is not limited to\textemdash{}the classical separable problems (also known as ``gen/kill'' or ``bit-vector'' problems)\textemdash{}e.g., reaching definitions, available expressions, and live variables. In addition, the class of problems that our techniques handle includes many non-separable problems, including truly-live variables, copy constant propagation, and possibly-uninitialized variables.},
  language = {en},
  booktitle = {Proceedings of the 22nd {{ACM SIGPLAN}}-{{SIGACT}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '95},
  publisher = {{ACM Press}},
  author = {Reps, Thomas and Horwitz, Susan and Sagiv, Mooly},
  year = {1995},
  pages = {49-61},
  file = {/Users/luigi/work/zotero/storage/CG3VDZPQ/Reps et al. - 1995 - Precise interprocedural dataflow analysis via grap.pdf}
}

@inproceedings{mainlandWhyItNice2007,
  address = {Freiburg, Germany},
  title = {Why It's Nice to Be Quoted: Quasiquoting for Haskell},
  isbn = {978-1-59593-674-5},
  shorttitle = {Why It's Nice to Be Quoted},
  doi = {10.1145/1291201.1291211},
  abstract = {Quasiquoting allows programmers to use domain specific syntax to construct program fragments. By providing concrete syntax for complex data types, programs become easier to read, easier to write, and easier to reason about and maintain. Haskell is an excellent host language for embedded domain specific languages, and quasiquoting ideally complements the language features that make Haskell perform so well in this area. Unfortunately, until now no Haskell compiler has provided support for quasiquoting. We present an implementation in GHC and demonstrate that by leveraging existing compiler capabilities, building a full quasiquoter requires little more work than writing a parser. Furthermore, we provide a compile-time guarantee that all quasiquoted data is typecorrect.},
  language = {en},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} Workshop on {{Haskell}} Workshop  - {{Haskell}} '07},
  publisher = {{ACM Press}},
  author = {Mainland, Geoffrey},
  year = {2007},
  pages = {73},
  file = {/Users/luigi/work/zotero/storage/25RPXL8N/Mainland - 2007 - Why it's nice to be quoted quasiquoting for haske.pdf}
}

@incollection{leuschelPowerHomeomorphicEmbedding1998,
  address = {Berlin, Heidelberg},
  title = {On the {{Power}} of {{Homeomorphic Embedding}} for {{Online Termination}}},
  volume = {1503},
  isbn = {978-3-540-65014-0 978-3-540-49727-1},
  abstract = {Recently well-quasi orders in general, and homeomorphic embedding in particular, have gained popularity to ensure the termination of program analysis, specialisation and transformation techniques. In this paper we investigate and clarify for the first time, both intuitively and formally, the advantages of such an approach over one using wellfounded orders. Notably we show that the homeomorphic embedding relation is strictly more powerful than a large class of involved well-founded approaches.},
  language = {en},
  booktitle = {Static {{Analysis}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Leuschel, Michael},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Levi, Giorgio},
  year = {1998},
  pages = {230-245},
  file = {/Users/luigi/work/zotero/storage/ITC422L9/Leuschel - 1998 - On the Power of Homeomorphic Embedding for Online .pdf},
  doi = {10.1007/3-540-49727-7_14}
}

@article{jonssonPositiveSupercompilationHigherOrder,
  title = {Positive {{Supercompilation}} for a {{Higher}}-{{Order Call}}-{{By}}-{{Value Language}}},
  language = {en},
  author = {Jonsson, Peter A},
  pages = {77},
  file = {/Users/luigi/work/zotero/storage/4KRYLLRT/Jonsson and Nordlander - Positive Supercompilation for a Higher Order Call-.pdf;/Users/luigi/work/zotero/storage/CETA3UFV/Jonsson - Positive Supercompilation for a Higher-Order Call-.pdf}
}

@article{klyuchnikovKELDYSHINSTITUTEAPPLIED,
  title = {{{KELDYSH INSTITUTE OF APPLIED MATHEMATICS}}},
  language = {en},
  author = {Klyuchnikov, Ilya G and Romanenko, Sergei A},
  pages = {29},
  file = {/Users/luigi/work/zotero/storage/4MCURTZT/Klyuchnikov - KELDYSH INSTITUTE OF APPLIED MATHEMATICS.pdf;/Users/luigi/work/zotero/storage/KAXMT8IG/Klyuchnikov and Romanenko - KELDYSH INSTITUTE OF APPLIED MATHEMATICS.pdf}
}

@article{grechanikKELDYSHINSTITUTEAPPLIED,
  title = {{{KELDYSH INSTITUTE OF APPLIED MATHEMATICS Russian Academy}} of {{Sciences}}},
  language = {en},
  author = {Grechanik, Sergei A and Klyuchnikov, Ilya G and Romanenko, Sergei A},
  pages = {29},
  file = {/Users/luigi/work/zotero/storage/RFFYT6MR/Klimov et al. - KELDYSH INSTITUTE OF APPLIED MATHEMATICS Russian A.pdf;/Users/luigi/work/zotero/storage/VQZ54R6P/Grechanik et al. - KELDYSH INSTITUTE OF APPLIED MATHEMATICS Russian A.pdf}
}

@inproceedings{fumeroRuntimeCodeGeneration2015,
  address = {Melbourne, FL, USA},
  title = {Runtime {{Code Generation}} and {{Data Management}} for {{Heterogeneous Computing}} in {{Java}}},
  isbn = {978-1-4503-3712-0},
  doi = {10.1145/2807426.2807428},
  abstract = {GPUs (Graphics Processing Unit) and other accelerators are nowadays commonly found in desktop machines, mobile devices and even data centres. While these highly parallel processors offer high raw performance, they also dramatically increase program complexity, requiring extra effort from programmers. This results in difficult-to-maintain and non-portable code due to the low-level nature of the languages used to program these devices.},
  language = {en},
  booktitle = {Proceedings of the {{Principles}} and {{Practices}} of {{Programming}} on {{The Java Platform}} - {{PPPJ}} '15},
  publisher = {{ACM Press}},
  author = {Fumero, Juan Jos\'e and Remmelg, Toomas and Steuwer, Michel and Dubach, Christophe},
  year = {2015},
  pages = {16-26},
  file = {/Users/luigi/work/zotero/storage/45BVS6LI/Fumero et al. - 2015 - Runtime Code Generation and Data Management for He.pdf}
}

@inproceedings{turcuHyflow2HighPerformance2013,
  address = {Stuttgart, Germany},
  title = {Hyflow2: A High Performance Distributed Transactional Memory Framework in Scala},
  isbn = {978-1-4503-2111-2},
  shorttitle = {Hyflow2},
  doi = {10.1145/2500828.2500836},
  abstract = {Distributed Transactional Memory (DTM) is a recent but promising model for programming distributed systems. It aims to present programmers with a simple to use distributed concurrency control abstraction (transactions), while maintaining performance and scalability similar to distributed fine-grained locks. Any complications usually associated with such locks (e.g., distributed deadlocks) are avoided. We propose a new DTM framework for the Java Virtual Machine named Hyflow2. We implement Hyflow2 in Scala and base it on the existing ScalaSTM API soon to be included in the Scala standard library. We thus aim to create a smooth transition from multiprocessor STM programs to DTM.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform Virtual Machines}}, {{Languages}}, and {{Tools}} - {{PPPJ}} '13},
  publisher = {{ACM Press}},
  author = {Turcu, Alexandru and Ravindran, Binoy and Palmieri, Roberto},
  year = {2013},
  pages = {79},
  file = {/Users/luigi/work/zotero/storage/6MHSU2WL/Turcu et al. - 2013 - Hyflow2 a high performance distributed transactio.pdf;/Users/luigi/work/zotero/storage/F87DVBTA/Turcu et al. - 2013 - Hyflow2 a high performance distributed transactio.pdf}
}

@article{poswolskyFunctionalProgrammingLogical,
  title = {Functional {{Programming}} with {{Logical Frameworks}}},
  language = {en},
  author = {Poswolsky, Adam Brett},
  pages = {548},
  file = {/Users/luigi/work/zotero/storage/C6HLRWRH/Poswolsky - Functional Programming with Logical Frameworks.pdf}
}

@article{smaragdakisPickYourContexts,
  title = {Pick {{Your Contexts Well}}: {{Understanding Object}}-{{Sensitivity}}},
  abstract = {Object-sensitivity has emerged as an excellent context abstraction for points-to analysis in object-oriented languages. Despite its practical success, however, object-sensitivity is poorly understood. For instance, for a context depth of 2 or higher, past scalable implementations deviate significantly from the original definition of an object-sensitive analysis. The reason is that the analysis has many degrees of freedom, relating to which context elements are picked at every method call and object creation. We offer a clean model for the analysis design space, and discuss a formal and informal understanding of object-sensitivity and of how to create good objectsensitive analyses. The results are surprising in their extent. We find that past implementations have made a sub-optimal choice of contexts, to the severe detriment of precision and performance. We define a ``full-object-sensitive'' analysis that results in significantly higher precision, and often performance, for the exact same context depth. We also introduce ``type-sensitivity'' as an explicit approximation of object-sensitivity that preserves high context quality at substantially reduced cost. A type-sensitive points-to analysis makes an unconventional use of types as context: the context types are not dynamic types of objects involved in the analysis, but instead upper bounds on the dynamic types of their allocator objects. Our results expose the influence of context choice on the quality of points-to analysis and demonstrate type-sensitivity to be an idea with major impact: It decisively advances the state-of-the-art with a spectrum of analyses that simultaneously enjoy speed (several times faster than an analogous object-sensitive analysis), scalability (comparable to analyses with much less context-sensitivity), and precision (comparable to the best object-sensitive analysis with the same context depth).},
  language = {en},
  author = {Smaragdakis, Yannis and Bravenboer, Martin and Lhotak, Ond\textasciicaron{}rej},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/AFSR3HFF/Smaragdakis et al. - Pick Your Contexts Well Understanding Object-Sens.pdf}
}

@article{fosterTheoryTypeQualifiers,
  title = {A {{Theory}} of {{Type Qualifiers}}},
  abstract = {We describe a framework for adding type qualifiers to a language. Type qualifiers encode a simple but highly useful form of subtyping. Our framework extends standard type rules to model the flow of qualifiers through a program, where each qualifier or set of qualifiers comes with additional rules that capture its semantics. Our framework allows types to be polymorphic in the type qualifiers. We present a const-inference system for C as an example application of the framework. We show that for a set of real C programs, many more consts can be used than are actually present in the original code.},
  language = {en},
  author = {Foster, Jeffrey S and Fahndrich, Manuel and Aiken, Alexander},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/3W64M6FR/Foster et al. - A Theory of Type Qualiﬁers.pdf}
}

@article{ammonsExploitingHardwarePerformance,
  title = {Exploiting {{Hardware Performance Counters}} with {{Flow}} and {{Context Sensitive Pro}} Ling},
  abstract = {A program pro le attributes run-time costs to portions of a program's execution. Most pro ling systems su er from two major de ciencies: rst, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking di erences in program behavior.},
  language = {en},
  author = {Ammons, Glenn and Ball, Thomas and Larus, James R},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/Y27X8HZL/Ammons et al. - Exploiting Hardware Performance Counters with Flow.pdf}
}

@article{flanaganEssenceCompilingContinuations,
  title = {The {{Essence}} of {{Compiling}} with {{Continuations}}},
  abstract = {In order to simplify the compilation process, many compilers for higher-order languages use the continuationpassing style (CPS) transformation in a rst phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the \textbackslash{}continuation"). Since the na ve CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.},
  language = {en},
  author = {Flanagan, Cormac and Sabry, Amr and Duba, Bruce F and Felleisen, Matthias},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/TZNXH2ZH/Flanagan et al. - The Essence of Compiling with Continuations.pdf}
}

@article{whaleyCloningBasedContextSensitivePointer,
  title = {Cloning-{{Based Context}}-{{Sensitive Pointer Alias Analysis Using Binary Decision Diagrams}}},
  abstract = {This paper presents the first scalable context-sensitive, inclusionbased pointer alias analysis for Java programs. Our approach to context sensitivity is to create a clone of a method for every context of interest, and run a context-insensitive algorithm over the expanded call graph to get context-sensitive results. For precision, we generate a clone for every acyclic path through a program's call graph, treating methods in a strongly connected component as a single node. Normally, this formulation is hopelessly intractable as a call graph often has 1014 acyclic paths or more. We show that these exponential relations can be computed efficiently using binary decision diagrams (BDDs). Key to the scalability of the technique is a context numbering scheme that exposes the commonalities across contexts. We applied our algorithm to the most popular applications available on Sourceforge, and found that the largest programs, with hundreds of thousands of Java bytecodes, can be analyzed in under 20 minutes.},
  language = {en},
  author = {Whaley, John and Lam, Monica S},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/ZTP9ZBQM/Whaley and Lam - Cloning-Based Context-Sensitive Pointer Alias Anal.pdf}
}

@article{fosterFlowSensitiveTypeQualifiers,
  title = {Flow-{{Sensitive Type Qualifiers}}},
  abstract = {We present a system for extending standard type systems with flow-sensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flow-sensitively\textemdash{}the underlying standard types are unchanged, which allows us to obtain an efficient constraint-based inference algorithm that integrates flowinsensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flow-sensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.},
  language = {en},
  author = {Foster, Jeffrey S and Terauchi, Tachio and Aiken, Alex},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/A9B8GHMH/Foster et al. - Flow-Sensitive Type Qualiﬁers.pdf;/Users/luigi/work/zotero/storage/L9Y96UXR/Foster et al. - Flow-Sensitive Type Qualiﬁers.pdf}
}

@incollection{bierhoffPracticalAPIProtocol2009,
  address = {Berlin, Heidelberg},
  title = {Practical {{API Protocol Checking}} with {{Access Permissions}}},
  volume = {5653},
  isbn = {978-3-642-03012-3 978-3-642-03013-0},
  abstract = {Reusable APIs often de ne usage protocols. We previously developed a sound modular type system that checks compliance with typestate-based protocols while a ording a great deal of aliasing  exibility. We also developed Plural, a prototype tool that embodies our approach as an automated static analysis and includes several extensions we found useful in practice. This paper evaluates our approach along the following dimensions: (1) We report on experience in specifying relevant usage rules for a large Java standard API with our approach. We also specify several other Java APIs and identify recurring patterns. (2) We summarize two case studies in verifying third-party open-source code bases with few false positives using our tool. We discuss how tool shortcomings can be addressed either with code refactorings or extensions to the tool itself. These results indicate that our approach can be used to specify and enforce real API protocols in practice.},
  language = {en},
  booktitle = {{{ECOOP}} 2009 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bierhoff, Kevin and Beckman, Nels E. and Aldrich, Jonathan},
  editor = {Drossopoulou, Sophia},
  year = {2009},
  pages = {195-219},
  file = {/Users/luigi/work/zotero/storage/J4ET53RQ/Bierhoff et al. - 2009 - Practical API Protocol Checking with Access Permis.pdf},
  doi = {10.1007/978-3-642-03013-0_10}
}

@article{birdPearlsFunctionalAlgorithm,
  title = {Pearls of {{Functional Algorithm Design}}},
  language = {en},
  author = {Bird, Richard},
  pages = {291},
  file = {/Users/luigi/work/zotero/storage/QV8HLZD6/Bird - Pearls of Functional Algorithm Design.pdf}
}

@article{phothilimthanaPortablePerformanceHeterogeneous,
  title = {Portable {{Performance}} on {{Heterogeneous Architectures}}},
  abstract = {Trends in both consumer and high performance computing are bringing not only more cores, but also increased heterogeneity among the computational resources within a single machine. In many machines, one of the greatest computational resources is now their graphics coprocessors (GPUs), not just their primary CPUs. But GPU programming and memory models differ dramatically from conventional CPUs, and the relative performance characteristics of the different processors vary widely between machines. Different processors within a system often perform best with different algorithms and memory usage patterns, and achieving the best overall performance may require mapping portions of programs across all types of resources in the machine.},
  language = {en},
  author = {Phothilimthana, Phitchaya Mangpo and Ansel, Jason and {Ragan-Kelley}, Jonathan and Amarasinghe, Saman},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/2NH7MD8H/Phothilimthana et al. - Portable Performance on Heterogeneous Architecture.pdf;/Users/luigi/work/zotero/storage/9J2PFSJJ/Phothilimthana et al. - Portable performance on heterogeneous architecture.pdf;/Users/luigi/work/zotero/storage/BSCBCPC5/Phothilimthana et al. - Portable Performance on Heterogeneous Architecture.pdf}
}

@article{leijenParsecFastCombinator,
  title = {Parsec, a Fast Combinator Parser},
  language = {en},
  author = {Leijen, Daan},
  pages = {51},
  file = {/Users/luigi/work/zotero/storage/4S5WWDAR/Leijen - Parsec, a fast combinator parser.pdf}
}

@inproceedings{marlowParallelGenerationalcopyingGarbage2008,
  address = {Tucson, AZ, USA},
  title = {Parallel Generational-Copying Garbage Collection with a Block-Structured Heap},
  isbn = {978-1-60558-134-7},
  doi = {10.1145/1375634.1375637},
  abstract = {We present a parallel generational-copying garbage collector implemented for the Glasgow Haskell Compiler. We use a blockstructured memory allocator, which provides a natural granularity for dividing the work of GC between many threads, leading to a simple yet effective method for parallelising copying GC. The results are encouraging: we demonstrate wall-clock speedups of on average a factor of 2 in GC time on a commodity 4-core machine with no programmer intervention, compared to our best sequential GC.},
  language = {en},
  booktitle = {Proceedings of the 7th International Symposium on {{Memory}} Management  - {{ISMM}} '08},
  publisher = {{ACM Press}},
  author = {Marlow, Simon and Harris, Tim and James, Roshan P. and Peyton Jones, Simon},
  year = {2008},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/QJF4BZFE/Marlow et al. - 2008 - Parallel generational-copying garbage collection w.pdf}
}

@incollection{madsenStringAnalysisDynamic2014,
  address = {Berlin, Heidelberg},
  title = {String {{Analysis}} for {{Dynamic Field Access}}},
  volume = {8409},
  isbn = {978-3-642-54806-2 978-3-642-54807-9},
  abstract = {In JavaScript, and scripting languages in general, dynamic field access is a commonly used feature. Unfortunately, current static analysis tools either completely ignore dynamic field access or use overly conservative approximations that lead to poor precision and scalability. We present new string domains to reason about dynamic field access in a static analysis tool. A key feature of the domains is that the equal, concatenate and join operations take O(1) time.},
  language = {en},
  booktitle = {Compiler {{Construction}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Madsen, Magnus and Andreasen, Esben},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Cohen, Albert},
  year = {2014},
  pages = {197-217},
  file = {/Users/luigi/work/zotero/storage/ZN7KC2CV/Madsen and Andreasen - 2014 - String Analysis for Dynamic Field Access.pdf},
  doi = {10.1007/978-3-642-54807-9_12}
}

@article{mitchellTransformationAnalysisFunctional,
  title = {Transformation and {{Analysis}} of {{Functional Programs}}},
  language = {en},
  author = {Mitchell, Neil},
  pages = {225},
  file = {/Users/luigi/work/zotero/storage/ASSAAWAU/Mitchell - Transformation and Analysis of Functional Programs.pdf}
}

@article{mitchellSuperoMakingHaskell,
  title = {Supero: {{Making Haskell Faster}}},
  abstract = {Haskell is a functional language, with features such as higher order functions and lazy evaluation, which allow succinct programs. These high-level features are difficult for fast execution, but GHC is a mature and widely used optimising compiler. This paper presents a wholeprogram approach to optimisation, which produces speed improvements of between 10\% and 60\% when used with GHC, on eight benchmarks.},
  language = {en},
  author = {Mitchell, Neil and Runciman, Colin},
  pages = {16},
  file = {/Users/luigi/work/zotero/storage/L22ZE2SB/Mitchell and Runciman - Supero Making Haskell Faster.pdf}
}

@inproceedings{mitchellNotAllPatterns2008,
  address = {Victoria, BC, Canada},
  title = {Not All Patterns, but Enough: An Automatic Verifier for Partial but Sufficient Pattern Matching},
  isbn = {978-1-60558-064-7},
  shorttitle = {Not All Patterns, but Enough},
  doi = {10.1145/1411286.1411293},
  abstract = {We describe an automated analysis of Haskell 98 programs to check statically that, despite the possible use of partial (or nonexhaustive) pattern matching, no pattern-match failure can occur. Our method is an iterative backward analysis using a novel form of pattern-constraint to represent sets of data values. The analysis is defined for a core first-order language to which Haskell 98 programs are reduced. Our analysis tool has been successfully applied to a range of programs, and our techniques seem to scale well. Throughout the paper, methods are represented much as we have implemented them in practice, again in Haskell.},
  language = {en},
  booktitle = {Proceedings of the First {{ACM SIGPLAN}} Symposium on {{Haskell}} - {{Haskell}} '08},
  publisher = {{ACM Press}},
  author = {Mitchell, Neil and Runciman, Colin},
  year = {2008},
  pages = {49},
  file = {/Users/luigi/work/zotero/storage/YT83A73P/Mitchell and Runciman - 2008 - Not all patterns, but enough an automatic verifie.pdf}
}

@incollection{geuversIntroductionTypeTheory2009,
  address = {Berlin, Heidelberg},
  title = {Introduction to {{Type Theory}}},
  volume = {5520},
  isbn = {978-3-642-03152-6 978-3-642-03153-3},
  language = {en},
  booktitle = {Language {{Engineering}} and {{Rigorous Software Development}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Geuvers, Herman},
  editor = {Bove, Ana and Barbosa, Lu\'is Soares and Pardo, Alberto and Pinto, Jorge Sousa},
  year = {2009},
  pages = {1-56},
  file = {/Users/luigi/work/zotero/storage/B2NLB5X9/Geuvers - 2009 - Introduction to Type Theory.pdf},
  doi = {10.1007/978-3-642-03153-3_1}
}

@incollection{bockischOverviewALIA4J2011,
  address = {Berlin, Heidelberg},
  title = {An {{Overview}} of {{ALIA4J}}},
  volume = {6705},
  isbn = {978-3-642-21951-1 978-3-642-21952-8},
  abstract = {New programming languages that allow to reduce the complexity of software solutions are frequently developed, often as extensions of existing languages. Many implementations thus resort to transforming the extension's source code to the imperative intermediate representation of the parent language. But approaches like compiler frameworks only allow for re-use of code transformations for syntactically-related languages; they do not allow for re-use across language families. In this paper, we present the ALIA4J approach to bring such re-use to language families with advanced dispatching mechanisms like pointcut-advice or predicate dispatching. ALIA4J introduces a meta-model of dispatching as a rich, extensible intermediate language. By implementing language constructs from four languages as refinements of this meta-model, we show that a significant amount of them can be re-used across language families. Another building block of ALIA4J is a framework for execution environments that automatically derives an execution model of the program's dispatching from representations in our intermediate language. This model enables different execution strategies for dispatching; we have validated this by implementing three execution environments whose strategies range from interpretation to optimizing code generation.},
  language = {en},
  booktitle = {Objects, {{Models}}, {{Components}}, {{Patterns}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bockisch, Christoph and Sewe, Andreas and Mezini, Mira and Ak{\c s}it, Mehmet},
  editor = {Bishop, Judith and Vallecillo, Antonio},
  year = {2011},
  pages = {131-146},
  file = {/Users/luigi/work/zotero/storage/9UL9PEIS/Bockisch et al. - 2011 - An Overview of ALIA4J.pdf},
  doi = {10.1007/978-3-642-21952-8_11}
}

@misc{veldhuizenTriejoinSimpleWorstCase2014,
  title = {Triejoin: {{A Simple}}, {{Worst}}-{{Case Optimal Join Algorithm}}},
  shorttitle = {Triejoin},
  abstract = {Recent years have seen exciting developments in join algorithms. In 2008, Atserias, Grohe and Marx (henceforth AGM) proved a tight bound on the maximum result size of a full conjunctive query, given constraints on the input relation sizes. In 2012, Ngo, Porat, R\textasciiacute{}e and Rudra (henceforth NPRR) devised a join algorithm with worst-case running time proportional to the AGM bound [8]. Our commercial database system LogicBlox employs a novel join algorithm, leapfrog triejoin, which compared conspicuously well to the NPRR algorithm in preliminary benchmarks. This spurred us to analyze the complexity of leapfrog triejoin. In this paper we establish that leapfrog triejoin is also worst-case optimal, up to a log factor, in the sense of NPRR. We improve on the results of NPRR by proving that leapfrog triejoin achieves worst-case optimality for finer-grained classes of database instances, such as those defined by constraints on projection cardinalities. We show that NPRR is not worstcase optimal for such classes, giving a counterexample where leapfrog triejoin runs in O(n log n) time and NPRR runs in {$\Theta$}(n1.375) time. On a practical note, leapfrog triejoin can be implemented using conventional data structures such as B-trees, and extends naturally to {$\exists$}1 queries. We believe our algorithm offers a useful addition to the existing toolbox of join algorithms, being easy to absorb, simple to implement, and having a concise optimality proof.},
  language = {en},
  publisher = {{OpenProceedings.org}},
  author = {Veldhuizen, Todd},
  year = {2014},
  file = {/Users/luigi/work/zotero/storage/J8IWCTXX/Veldhuizen - 2014 - Triejoin A Simple, Worst-Case Optimal Join Algori.pdf},
  doi = {10.5441/002/icdt.2014.13}
}

@article{andreasenDeterminacyStaticAnalysis,
  title = {Determinacy in {{Static Analysis}} for {{jQuery}}},
  abstract = {Static analysis for JavaScript can potentially help programmers find errors early during development. Although much progress has been made on analysis techniques, a major obstacle is the prevalence of libraries, in particular jQuery, which apply programming patterns that have detrimental consequences on the analysis precision and performance.},
  language = {en},
  author = {Andreasen, Esben and M\o{}ller, Anders},
  pages = {15},
  file = {/Users/luigi/work/zotero/storage/EUJB6LS8/Andreasen and Møller - Determinacy in Static Analysis for jQuery.pdf;/Users/luigi/work/zotero/storage/IQP7XSVQ/Andreasen and Møller - Determinacy in Static Analysis for jQuery.pdf}
}

@incollection{frostParserCombinatorsAmbiguous2008,
  address = {Berlin, Heidelberg},
  title = {Parser {{Combinators}} for {{Ambiguous Left}}-{{Recursive Grammars}}},
  volume = {4902},
  isbn = {978-3-540-77441-9 978-3-540-77442-6},
  abstract = {Parser combinators are higher-order functions used to build parsers as executable specifications of grammars. Some existing implementations are only able to handle limited ambiguity, some have exponential time and/or space complexity for ambiguous input, most cannot accommodate left-recursive grammars. This paper describes combinators, implemented in Haskell, which overcome all of these limitations.},
  language = {en},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Frost, Richard A. and Hafiz, Rahmatullah and Callaghan, Paul},
  editor = {Hudak, Paul and Warren, David S.},
  year = {2008},
  pages = {167-181},
  file = {/Users/luigi/work/zotero/storage/5KXST2BV/Frost et al. - 2008 - Parser Combinators for Ambiguous Left-Recursive Gr.pdf},
  doi = {10.1007/978-3-540-77442-6_12}
}

@inproceedings{smaragdakisIntrospectiveAnalysisContextsensitivity2013,
  address = {Edinburgh, United Kingdom},
  title = {Introspective Analysis: Context-Sensitivity, across the Board},
  isbn = {978-1-4503-2784-8},
  shorttitle = {Introspective Analysis},
  doi = {10.1145/2594291.2594320},
  abstract = {Context-sensitivity is the primary approach for adding more precision to a points-to analysis, while hopefully also maintaining scalability. An oft-reported problem with context-sensitive analyses, however, is that they are bi-modal: either the analysis is precise enough that it manipulates only manageable sets of data, and thus scales impressively well, or the analysis gets quickly derailed at the first sign of imprecision and becomes orders-of-magnitude more expensive than would be expected given the program's size. There is currently no approach that makes precise context-sensitive analyses (of any flavor: call-site-, object-, or type-sensitive) scale across the board at a level comparable to that of a context-insensitive analysis. To address this issue, we propose introspective analysis: a technique for uniformly scaling context-sensitive analysis by eliminating its performance-detrimental behavior, at a small precision expense. Introspective analysis consists of a common adaptivity pattern: first perform a context-insensitive analysis, then use the results to selectively refine (i.e., analyze context-sensitively) program elements that will not cause explosion in the running time or space. The technical challenge is to appropriately identify such program elements. We show that a simple but principled approach can be remarkably effective, achieving scalability (often with dramatic speedup) for benchmarks previously completely out-of-reach for deep context-sensitive analyses.},
  language = {en},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}} - {{PLDI}} '14},
  publisher = {{ACM Press}},
  author = {Smaragdakis, Yannis and Kastrinis, George and Balatsouras, George},
  year = {2013},
  pages = {485-495},
  file = {/Users/luigi/work/zotero/storage/VRH8MQIQ/Smaragdakis et al. - 2013 - Introspective analysis context-sensitivity, acros.pdf}
}

@article{kastrinisHybridContextsensitivityPointsto,
  title = {Hybrid Context-Sensitivity for Points-to Analysis},
  abstract = {Context-sensitive points-to analysis is valuable for achieving high precision with good performance. The standard flavors of contextsensitivity are call-site-sensitivity (kCFA) and object-sensitivity. Combining both flavors of context-sensitivity increases precision but at an infeasibly high cost. We show that a selective combination of call-site- and object-sensitivity for Java points-to analysis is highly profitable. Namely, by keeping a combined context only when analyzing selected language features, we can closely approximate the precision of an analysis that keeps both contexts at all times. In terms of speed, the selective combination of both kinds of context not only vastly outperforms non-selective combinations but is also faster than a mere object-sensitive analysis. This result holds for a large array of analyses (e.g., 1-object-sensitive, 2-object-sensitive with a context-sensitive heap, type-sensitive) establishing a new set of performance/precision sweet spots.},
  language = {en},
  author = {Kastrinis, George and Smaragdakis, Yannis},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/T2QA79WJ/Kastrinis and Smaragdakis - Hybrid context-sensitivity for points-to analysis.pdf}
}

@inproceedings{mitropoulosBugCatalogMaven2014,
  address = {Hyderabad, India},
  title = {The Bug Catalog of the Maven Ecosystem},
  isbn = {978-1-4503-2863-0},
  doi = {10.1145/2597073.2597123},
  abstract = {Examining software ecosystems can provide the research community with data regarding artifacts, processes, and communities. We present a dataset obtained from the Maven central repository ecosystem (approximately 265gb of data) by statically analyzing the repository to detect potential software bugs. For our analysis we used FindBugs, a tool that examines Java bytecode to detect numerous types of bugs. The dataset contains the metrics results that FindBugs reports for every project version (a jar) included in the ecosystem. For every version we also stored specific metadata such as the jar's size, its dependencies and others. Our dataset can be used to produce interesting research results, as we show in specific examples.},
  language = {en},
  booktitle = {Proceedings of the 11th {{Working Conference}} on {{Mining Software Repositories}} - {{MSR}} 2014},
  publisher = {{ACM Press}},
  author = {Mitropoulos, Dimitris and Karakoidas, Vassilios and Louridas, Panos and Gousios, Georgios and Spinellis, Diomidis},
  year = {2014},
  pages = {372-375},
  file = {/Users/luigi/work/zotero/storage/I6V87X6H/Mitropoulos et al. - 2014 - The bug catalog of the maven ecosystem.pdf;/Users/luigi/work/zotero/storage/TNWTV8QF/Mitropoulos et al. - 2014 - The bug catalog of the maven ecosystem.pdf}
}

@article{turchinConceptSupercompiler1986,
  title = {The Concept of a Supercompiler},
  volume = {8},
  issn = {01640925},
  doi = {10.1145/5956.5957},
  language = {en},
  number = {3},
  journal = {ACM Transactions on Programming Languages and Systems},
  author = {Turchin, Valentin F.},
  month = jun,
  year = {1986},
  pages = {292-325},
  file = {/Users/luigi/work/zotero/storage/589TSIUF/Turchin - 1986 - The concept of a supercompiler.pdf}
}

@inproceedings{pradelLeveragingTestGeneration2012,
  address = {Zurich},
  title = {Leveraging Test Generation and Specification Mining for Automated Bug Detection without False Positives},
  isbn = {978-1-4673-1066-6 978-1-4673-1067-3},
  doi = {10.1109/ICSE.2012.6227185},
  abstract = {Mining specifications and using them for bug detection is a promising way to reveal bugs in programs. Existing approaches suffer from two problems. First, dynamic specification miners require input that drives a program to generate common usage patterns. Second, existing approaches report false positives, that is, spurious warnings that mislead developers and reduce the practicability of the approach. We present a novel technique for dynamically mining and checking specifications without relying on existing input to drive a program and without reporting false positives. Our technique leverages automatically generated tests in two ways: Passing tests drive the program during specification mining, and failing test executions are checked against the mined specifications. The output are warnings that show with concrete test cases how the program violates commonly accepted specifications. Our implementation reports no false positives and 54 true positives in ten well-tested Java programs.},
  language = {en},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  publisher = {{IEEE}},
  author = {Pradel, Michael and Gross, Thomas R.},
  month = jun,
  year = {2012},
  pages = {288-298},
  file = {/Users/luigi/work/zotero/storage/28NYH45U/Pradel and Gross - 2012 - Leveraging test generation and specification minin.pdf;/Users/luigi/work/zotero/storage/MB6QRK6Z/Pradel and Gross - 2012 - Leveraging test generation and specification minin.pdf}
}

@article{jiRSVMRegionbasedSoftware,
  title = {{{RSVM}}: A {{Region}}-Based {{Software Virtual Memory}} for {{GPU}}},
  abstract = {While Graphics Processing Units (GPU) have gained much success in general purpose computing in recent years, their programming is still difficult, due to, particularly, explicitly managed GPU memory and manual CPU-GPU data transfer. Despite recent calls for managing GPU resources as first-class citizens in the operating system, a mature GPU memory management mechanism is still missing, which leads to reinventing the wheels in various GPU system software. Meanwhile, due to ever enlarging problem sizes, we urgently need a system-level mechanism for unified CPU-GPU memory management.},
  language = {en},
  author = {Ji, Feng and Lin, Heshan and Ma, Xiaosong},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/FRVD2X8C/Ji et al. - RSVM a Region-based Software Virtual Memory for G.pdf;/Users/luigi/work/zotero/storage/H38QZD5Q/Ji et al. - RSVM a Region-based Software Virtual Memory for G.pdf}
}

@inproceedings{smaragdakisSetbasedPreprocessingPointsto2013,
  address = {Indianapolis, Indiana, USA},
  title = {Set-Based Pre-Processing for Points-to Analysis},
  isbn = {978-1-4503-2374-1},
  doi = {10.1145/2509136.2509524},
  abstract = {We present set-based pre-analysis: a virtually universal optimization technique for flow-insensitive points-to analysis. Points-to analysis computes a static abstraction of how object values flow through a program's variables. Set-based pre-analysis relies on the observation that much of this reasoning can take place at the set level rather than the value level. Computing constraints at the set level results in significant optimization opportunities: we can rewrite the input program into a simplified form with the same essential points-to properties. This rewrite results in removing both local variables and instructions, thus simplifying the subsequent value-based points-to computation. Effectively, setbased pre-analysis puts the program in a normal form optimized for points-to analysis.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN}} International Conference on {{Object}} Oriented Programming Systems Languages \& Applications - {{OOPSLA}} '13},
  publisher = {{ACM Press}},
  author = {Smaragdakis, Yannis and Balatsouras, George and Kastrinis, George},
  year = {2013},
  pages = {253-270},
  file = {/Users/luigi/work/zotero/storage/AUDDSAVM/Smaragdakis et al. - 2013 - Set-based pre-processing for points-to analysis.pdf}
}

@article{bravenboerStrictlyDeclarativeSpecification,
  title = {Strictly Declarative Specification of Sophisticated Points-to Analyses},
  abstract = {We present the Doop framework for points-to analysis of Java programs. Doop builds on the idea of specifying pointer analysis algorithms declaratively, using Datalog: a logicbased language for defining (recursive) relations. We carry the declarative approach further than past work by describing the full end-to-end analysis in Datalog and optimizing aggressively using a novel technique specifically targeting highly recursive Datalog programs.},
  language = {en},
  author = {Bravenboer, Martin and Smaragdakis, Yannis},
  pages = {19},
  file = {/Users/luigi/work/zotero/storage/TBKBV3H6/Bravenboer and Smaragdakis - Strictly declarative specification of sophisticate.pdf}
}

@inproceedings{makarovJikesRDBDebugger2013,
  address = {Stuttgart, Germany},
  title = {Jikes {{RDB}}: A Debugger for the {{Jikes RVM}}},
  isbn = {978-1-4503-2111-2},
  shorttitle = {Jikes {{RDB}}},
  doi = {10.1145/2500828.2500847},
  abstract = {The Jikes RVM is a meta-circular JVM serving as a leading platform for virtual machine research. Despite its popularity, the Jikes RVM still provides almost no debugging support. In this paper we present Jikes RDB, the Jikes RVM Debugger. RDB is non-invasive: it does not require any changes to the Jikes RVM. It is vertical: it integrates low-level and Java-level information to provide a complete view and complete control over the execution of the RVM. RDB's implementation is minimally redundant: as much as possible it uses the RVM's own code base for interpreting the state of the RVM. RDB is object-centric: it is centered around inspectors for the many different kinds of objects and data structures existing in a running RVM. This paper introduces RDB and demonstrates its use with a brief example.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform Virtual Machines}}, {{Languages}}, and {{Tools}} - {{PPPJ}} '13},
  publisher = {{ACM Press}},
  author = {Makarov, Dmitri and Hauswirth, Matthias},
  year = {2013},
  pages = {169},
  file = {/Users/luigi/work/zotero/storage/8YN35P5N/Makarov and Hauswirth - 2013 - Jikes RDB a debugger for the Jikes RVM.pdf;/Users/luigi/work/zotero/storage/QPLE8AKZ/Makarov and Hauswirth - 2013 - Jikes RDB a debugger for the Jikes RVM.pdf}
}

@inproceedings{xuMiningMethodHandle2015,
  address = {Melbourne, FL, USA},
  title = {Mining {{Method Handle Graphs}} for {{Efficient Dynamic JVM Languages}}},
  isbn = {978-1-4503-3712-0},
  doi = {10.1145/2807426.2807440},
  abstract = {The Java Virtual Machine (JVM) has been used as an execution platform by many dynamically-typed programming languages such as Ruby, Python, and Groovy. The main challenge to compile such dynamic JVM languages is choosing the most appropriate implementation of a method for various types of an object at runtime. To address this challenge, a new Java bytecode instruction, invokedynamic, has been introduced, allowing users to control the linkage between a call site and a method implementation. With this instruction, a method handle that refers to a method is linked to the call site and then potentially transforms the invocation to a real implementation. As a referenced method of a method handle might in turn refer to other method handles, multiple method handles constitute a Method Handle Graph (MHG).},
  language = {en},
  booktitle = {Proceedings of the {{Principles}} and {{Practices}} of {{Programming}} on {{The Java Platform}} - {{PPPJ}} '15},
  publisher = {{ACM Press}},
  author = {Xu, Shijie and Bremner, David and Heidinga, Daniel},
  year = {2015},
  pages = {159-169},
  file = {/Users/luigi/work/zotero/storage/NQIQUVDY/Xu et al. - 2015 - Mining Method Handle Graphs for Efficient Dynamic .pdf}
}

@inproceedings{ramseyTeachingHowDesign2014,
  address = {Gothenburg, Sweden},
  title = {On Teaching *how to Design Programs*: Observations from a Newcomer},
  isbn = {978-1-4503-2873-9},
  shorttitle = {On Teaching *how to Design Programs*},
  doi = {10.1145/2628136.2628137},
  abstract = {This paper presents a personal, qualitative case study of a first course using How to Design Programs and its functional teaching languages. The paper reconceptualizes the book's six-step design process as an eight-step design process ending in a new ``review and refactor'' step. It recommends specific approaches to students' difficulties with function descriptions, function templates, data examples, and other parts of the design process. It connects the process to interactive ``world programs.'' It recounts significant, informative missteps in course design and delivery. Finally, it identifies some unsolved teaching problems and some potential solutions.},
  language = {en},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming - {{ICFP}} '14},
  publisher = {{ACM Press}},
  author = {Ramsey, Norman},
  year = {2014},
  pages = {153-166},
  file = {/Users/luigi/work/zotero/storage/LCW2TK2C/Ramsey - 2014 - On teaching how to design programs observations.pdf}
}

@article{lameedModularApproachOnStack,
  title = {A {{Modular Approach}} to {{On}}-{{Stack Replacement}} in {{LLVM}}},
  abstract = {On-stack replacement (OSR) is a technique that allows a virtual machine to interrupt running code during the execution of a function/method, to re-optimize the function on-the-fly using an optimizing JIT compiler, and then to resume the interrupted function at the point and state at which it was interrupted. OSR is particularly useful for programs with potentially long-running loops, as it allows dynamic optimization of those loops as soon as they become hot.},
  language = {en},
  author = {Lameed, Nurudeen and Hendren, Laurie},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/W3YU8RQQ/Lameed and Hendren - A Modular Approach to On-Stack Replacement in LLVM.pdf;/Users/luigi/work/zotero/storage/ZW3NN92H/Lameed and Hendren - A Modular Approach to On-Stack Replacement in LLVM.pdf}
}

@inproceedings{morrisTypesAreNot1973,
  address = {Boston, Massachusetts},
  title = {Types Are Not Sets},
  doi = {10.1145/512927.512938},
  language = {en},
  booktitle = {Proceedings of the 1st Annual {{ACM SIGACT}}-{{SIGPLAN}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '73},
  publisher = {{ACM Press}},
  author = {Morris, James H.},
  year = {1973},
  pages = {120-124},
  file = {/Users/luigi/work/zotero/storage/HG2J4WD6/Morris - 1973 - Types are not sets.pdf}
}

@article{goslingJavaIntermediateBytecodes,
  title = {Java {{Intermediate Bytecodes}}},
  language = {en},
  author = {Gosling, James},
  pages = {8},
  file = {/Users/luigi/work/zotero/storage/T6NZXKR9/Gosling - Java Intermediate Bytecodes.pdf}
}

@inproceedings{mccabeFeelDifferentJava2013,
  address = {Stuttgart, Germany},
  title = {Feel Different on the {{Java}} Platform: The Star Programming Language},
  isbn = {978-1-4503-2111-2},
  shorttitle = {Feel Different on the {{Java}} Platform},
  doi = {10.1145/2500828.2500837},
  abstract = {Star is a functional, multi-paradigm and extensible programming language that runs on the Java platform. Starview Inc developed the language as an integral part of the Starview Enterprise Platform, a framework for real-time business applications such as factory scheduling and data analytics. Star borrows from many languages, with obvious heritage from Haskell, ML, and April, but also contains new approaches to some design aspects, such as syntax and syntactic extensibility, actors, and queries. Its texture is quite different from that of other languages on the Java platform. Despite this, the learning curve for Java programmers is surprisingly shallow. The combination of a powerful type system (which includes type inference, constrained polymorphism, and existentials) and syntactic extensibility make the Star well-suited to producing embedded domain-specific languages. This paper gives an overview of the language, and reports on some aspects of its design process, on our experience on using it in industrial projects, and on our experience implementing Star on the Java platform.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform Virtual Machines}}, {{Languages}}, and {{Tools}} - {{PPPJ}} '13},
  publisher = {{ACM Press}},
  author = {McCabe, Frank and Sperber, Michael},
  year = {2013},
  pages = {89},
  file = {/Users/luigi/work/zotero/storage/85489MX6/McCabe and Sperber - 2013 - Feel different on the Java platform the star prog.pdf;/Users/luigi/work/zotero/storage/F7NNCFJ4/McCabe and Sperber - 2013 - Feel different on the Java platform the star prog.pdf}
}

@inproceedings{bettiniPureTraitbasedProgramming2013,
  address = {Stuttgart, Germany},
  title = {Pure Trait-Based Programming on the {{Java}} Platform},
  isbn = {978-1-4503-2111-2},
  doi = {10.1145/2500828.2500835},
  abstract = {Traits provide a mechanism for fine-grained code reuse to overcome the limitations of class-based inheritance. A trait is a set of methods which is completely independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. We present: (i) a formulation of traits which aims to achieve complete compatibility and interoperability with the Java platform without reducing the flexibility of traits, and (ii) an integration with Eclipse which aims to support an incremental adoption of traits in existing Java projects. Indeed, the proposed trait language can coexist with Java code. Single parts of a project can be refactored to use traits, without requiring a complete rewrite of the whole existing code-base.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java Platform Virtual Machines}}, {{Languages}}, and {{Tools}} - {{PPPJ}} '13},
  publisher = {{ACM Press}},
  author = {Bettini, Lorenzo and Damiani, Ferruccio},
  year = {2013},
  pages = {67},
  file = {/Users/luigi/work/zotero/storage/UCJKY4GL/Bettini and Damiani - 2013 - Pure trait-based programming on the Java platform.pdf;/Users/luigi/work/zotero/storage/YENR3VDI/Bettini and Damiani - 2013 - Pure trait-based programming on the Java platform.pdf}
}

@article{xieMAPOMiningAPI,
  title = {{{MAPO}}: {{Mining API Usages}} from {{Open Source Repositories}}},
  abstract = {To improve software productivity, when constructing new software systems, developers often reuse existing class libraries or frameworks by invoking their APIs. Those APIs, however, are often complex and not well documented, posing barriers for developers to use them in new client code. To get familiar with how those APIs are used, developers may search the Web using a general search engine to find relevant documents or code examples. Developers can also use a source code search engine to search open source repositories for source files that use the same APIs. Nevertheless, the number of returned source files is often large. It is difficult for developers to learn API usages from a large number of returned results. In order to help developers understand API usages and write API client code more effectively, we have developed an API usage mining framework and its supporting tool called MAPO (for Mining API usages from Open source repositories). Given a query that describes a method, class, or package for an API, MAPO leverages the existing source code search engines to gather relevant source files and conducts data mining. The mining leads to a short list of frequent API usages for developers to inspect. MAPO currently consists of five components: a code search engine, a source code analyzer, a sequence preprocessor, a frequent sequence miner, and a frequent sequence postprocessor. We have examined the effectiveness of MAPO using a set of various queries. The preliminary results show that the framework is practical for providing informative and succinct API usage patterns.},
  language = {en},
  author = {Xie, Tao and Pei, Jian},
  pages = {4},
  file = {/Users/luigi/work/zotero/storage/CFHZ9GDM/Xie and Pei - MAPO Mining API Usages from Open Source Repositor.pdf;/Users/luigi/work/zotero/storage/GSYZQLQB/Xie and Pei - MAPO Mining API Usages from Open Source Repositor.pdf}
}

@article{meyerViewpointResearchEvaluationComputer2009,
  title = {{{ViewpointResearch}} Evaluation for Computer Science},
  volume = {52},
  issn = {00010782},
  doi = {10.1145/1498765.1498780},
  language = {en},
  number = {4},
  journal = {Communications of the ACM},
  author = {Meyer, Bertrand and Choppy, Christine and Staunstrup, J\o{}rgen and {van Leeuwen}, Jan},
  month = apr,
  year = {2009},
  pages = {31},
  file = {/Users/luigi/work/zotero/storage/GFSFMGYY/Meyer et al. - 2009 - ViewpointResearch evaluation for computer science.pdf;/Users/luigi/work/zotero/storage/NC4CRCVZ/Meyer et al. - 2009 - ViewpointResearch evaluation for computer science.pdf}
}

@inproceedings{farmerReasoningHERMITTool2015,
  address = {Vancouver, BC, Canada},
  title = {Reasoning with the {{HERMIT}}: Tool Support for Equational Reasoning on {{GHC}} Core Programs},
  isbn = {978-1-4503-3808-0},
  shorttitle = {Reasoning with the {{HERMIT}}},
  doi = {10.1145/2804302.2804303},
  abstract = {A benefit of pure functional programming is that it encourages equational reasoning. However, the Haskell language has lacked direct tool support for such reasoning. Consequently, reasoning about Haskell programs is either performed manually, or in another language that does provide tool support (e.g. Agda or Coq).},
  language = {en},
  booktitle = {Proceedings of the 8th {{ACM SIGPLAN Symposium}} on {{Haskell}} - {{Haskell}} 2015},
  publisher = {{ACM Press}},
  author = {Farmer, Andrew and Sculthorpe, Neil and Gill, Andy},
  year = {2015},
  pages = {23-34},
  file = {/Users/luigi/work/zotero/storage/2EVCWM8E/Farmer et al. - 2015 - Reasoning with the HERMIT tool support for equati.pdf}
}

@inproceedings{beguetAcceleratingParserCombinators2014,
  address = {Uppsala, Sweden},
  title = {Accelerating Parser Combinators with Macros},
  isbn = {978-1-4503-2868-5},
  doi = {10.1145/2637647.2637653},
  abstract = {Parser combinators provide an elegant way of writing parsers: parser implementations closely follow the structure of the underlying grammar, while accommodating interleaved host language code for data processing. However, the host language features used for composition introduce substantial overhead, which leads to poor performance.},
  language = {en},
  booktitle = {Proceedings of the {{Fifth Anuual Scala Workshop}} on - {{SCALA}} '14},
  publisher = {{ACM Press}},
  author = {B\'eguet, Eric and Jonnalagedda, Manohar},
  year = {2014},
  pages = {7-17},
  file = {/Users/luigi/work/zotero/storage/UJBM2KTF/Béguet and Jonnalagedda - 2014 - Accelerating parser combinators with macros.pdf}
}

@inproceedings{liQuarantineFrameworkMitigate2011,
  address = {Kongens Lyngby, Denmark},
  title = {Quarantine: A Framework to Mitigate Memory Errors in {{JNI}} Applications},
  isbn = {978-1-4503-0935-6},
  shorttitle = {Quarantine},
  doi = {10.1145/2093157.2093159},
  abstract = {By using Java Native Interface (JNI), programmers can integrate Java programs with legacy applications or third-party libraries written in other languages (e.g., C, C++, and Pascal). However, the use of JNI can bypass the Java boundary checking and exceptionhandling mechanisms. Furthermore, its use can violate Java's typesafety feature because of the type-mismatches between native programs and Java programs. As a result, such integration can cause various security issues including heap errors that can be dangerous and difficult to detect.},
  language = {en},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Principles}} and {{Practice}} of {{Programming}} in {{Java}} - {{PPPJ}} '11},
  publisher = {{ACM Press}},
  author = {Li, Du and {Srisa-an}, Witawas},
  year = {2011},
  pages = {1},
  file = {/Users/luigi/work/zotero/storage/GZQU2RC6/Li and Srisa-an - 2011 - Quarantine a framework to mitigate memory errors .pdf}
}

@article{bravenboerExceptionAnalysisPointsto,
  title = {Exception Analysis and Points-to Analysis: Better Together},
  abstract = {Exception analysis and points-to analysis are typically done in complete separation. Past algorithms for precise exception analysis (e.g., pairing throw clauses with catch statements) use precomputed points-to information. Past points-to analyses either unsoundly ignore exceptions, or conservatively compute a crude approximation of exception throwing (e.g., considering an exception throw as an assignment to a global variable, accessible from any catch clause). We show that this separation results in significant slowdowns or vast imprecision. The two kinds of analyses are interdependent: neither can be performed accurately without the other. The interdependency leads us to propose a joint handling for performance and precision. We show that our exception analysis is expressible highly elegantly in a declarative form, and can apply to points-to analyses of varying precision. In fact, our specification of exception analysis is ``fully precise'', as it models closely the Java exception handling semantics. The necessary approximation is provided only through whichever abstractions are used for contexts and objects in the base points-to analysis. Our combined approach achieves similar precision relative to exceptions (exception-catch links) as the best past precise exception analysis, with a runtime of seconds instead of tens of minutes. At the same time, our analysis achieves much higher precision of points-to information (an average of half as many values for each reachable variable for most of the DaCapo benchmarks) than points-to analyses that treat exceptions conservatively, all at a fraction of the execution time.},
  language = {en},
  author = {Bravenboer, Martin and Smaragdakis, Yannis},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/JDMSAJRZ/Bravenboer and Smaragdakis - Exception analysis and points-to analysis better .pdf;/Users/luigi/work/zotero/storage/ZZEFULTU/Bravenboer and Smaragdakis - Exception Analysis and Points-to Analysis Better .pdf}
}

@book{associationforcomputingmachineryConferenceRecordPOPL2002,
  address = {New York, N.Y},
  title = {Conference Record of {{POPL}} 2002: The 29th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}: Papers Presented at the {{Symposium}}, {{Portland}}, {{Oregon}}, 16-18 {{January}} 2002},
  isbn = {978-1-58113-450-6},
  lccn = {QA76.7 .A26 2002},
  shorttitle = {Conference Record of {{POPL}} 2002},
  language = {en},
  publisher = {{The Association}},
  editor = {{Association for Computing Machinery} and {ACM Special Interest Group for Algorithms {and} Computation Theory} and {ACM Special Interest Group on Programming Languages}},
  year = {2002},
  keywords = {Computer programming,Congresses,Programming languages (Electronic computers)},
  file = {/Users/luigi/work/zotero/storage/AJRSV5ND/Association for Computing Machinery et al. - 2002 - Conference record of POPL 2002 the 29th ACM SIGPL.pdf},
  note = {OCLC: ocm48996054}
}

@incollection{pittsOperationalSemanticsProgram2002,
  address = {Berlin, Heidelberg},
  title = {Operational {{Semantics}} and {{Program Equivalence}}},
  volume = {2395},
  isbn = {978-3-540-44044-4 978-3-540-45699-5},
  abstract = {This tutorial paper discusses a particular style of operational semantics that enables one to give a `syntax-directed' inductive definition of termination which is very useful for reasoning about operational equivalence of programs. We restrict attention to contextual equivalence of expressions in the ML family of programming languages, concentrating on functions involving local state. A brief tour of structural operational semantics culminates in a structural definition of termination via an abstract machine using `frame stacks'. Applications of this to reasoning about contextual equivalence are given.},
  language = {en},
  booktitle = {Applied {{Semantics}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Pitts, Andrew M.},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Barthe, Gilles and Dybjer, Peter and Pinto, Lu\'is and Saraiva, Jo\~ao},
  year = {2002},
  pages = {378-412},
  file = {/Users/luigi/work/zotero/storage/D3H68C55/Pitts - 2002 - Operational Semantics and Program Equivalence.pdf},
  doi = {10.1007/3-540-45699-6_8}
}

@article{toffolaPerformanceProblemsYou,
  title = {Performance {{Problems You Can Fix}}: {{A Dynamic Analysis}} of {{Memoization Opportunities}}},
  abstract = {Performance bugs are a prevalent problem and recent research proposes various techniques to identify such bugs. This paper addresses a kind of performance problem that often is easy to address but difficult to identify: redundant computations that may be avoided by reusing already computed results for particular inputs, a technique called memoization. To help developers find and use memoization opportunities, we present MemoizeIt, a dynamic analysis that identifies methods that repeatedly perform the same computation. The key idea is to compare inputs and outputs of method calls in a scalable yet precise way. To avoid the overhead of comparing objects at all method invocations in detail, MemoizeIt first compares objects without following any references and iteratively increases the depth of exploration while shrinking the set of considered methods. After each iteration, the approach ignores methods that cannot benefit from memoization, allowing it to analyze calls to the remaining methods in more detail. For every memoization opportunity that MemoizeIt detects, it provides hints on how to implement memoization, making it easy for the developer to fix the performance issue. Applying MemoizeIt to eleven real-world Java programs reveals nine profitable memoization opportunities, most of which are missed by traditional CPU time profilers, conservative compiler optimizations, and other existing approaches for finding performance bugs. Adding memoization as proposed by MemoizeIt leads to statistically significant speedups by factors between 1.04x and 12.93x.},
  language = {en},
  author = {Toffola, Luca Della and Pradel, Michael and Gross, Thomas R},
  pages = {16},
  file = {/Users/luigi/work/zotero/storage/9ZH44SWJ/Toffola et al. - Performance Problems You Can Fix A Dynamic Analys.pdf}
}

@article{nystromConstrainedTypesObjectOriented,
  title = {Constrained {{Types}} for {{Object}}-{{Oriented Languages}}},
  abstract = {X10 is a modern object-oriented language designed for productivity and performance in concurrent and distributed systems. In this setting, dependent types offer significant opportunities for detecting design errors statically, documenting design decisions, eliminating costly run-time checks (e.g., for array bounds, null values), and improving the quality of generated code.},
  language = {en},
  author = {Nystrom, Nathaniel and Saraswat, Vijay and Grothoff, Christian and Palsberg, Jens},
  pages = {18},
  file = {/Users/luigi/work/zotero/storage/XJZ9UCP7/Nystrom et al. - Constrained Types for Object-Oriented Languages.pdf}
}

@article{stroustrupWhyNotJust,
  title = {Why {{C}}++ Is Not Just an {{Object}}-{{Oriented Programming Language}}},
  language = {en},
  author = {Stroustrup, Bjarne},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/C9BZ8G7X/Stroustrup - Why C++ is not just an Object-Oriented Programming.pdf}
}

@inproceedings{aldrichTypestateorientedProgramming2009,
  address = {Orlando, Florida, USA},
  title = {Typestate-Oriented Programming},
  isbn = {978-1-60558-768-4},
  doi = {10.1145/1639950.1640073},
  abstract = {Objects model the world, and state is fundamental to a faithful modeling. Engineers use state machines to understand and reason about state transitions, but programming languages provide little support for reasoning about or implementing these state machines, causing software defects and lost productivity when objects are misused.},
  language = {en},
  booktitle = {Proceeding of the 24th {{ACM SIGPLAN}} Conference Companion on {{Object}} Oriented Programming Systems Languages and Applications - {{OOPSLA}} '09},
  publisher = {{ACM Press}},
  author = {Aldrich, Jonathan and Sunshine, Joshua and Saini, Darpan and Sparks, Zachary},
  year = {2009},
  pages = {1015},
  file = {/Users/luigi/work/zotero/storage/7N2CZG8I/Aldrich et al. - 2009 - Typestate-oriented programming.pdf}
}

@book{acminternationalsymposiumonnewideasOnward15Proceedings2015,
  title = {Onward!'15: Proceedings of the 2015 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}} : {{October}} 25-30, 2015, {{Pittsburgh}}, {{PA}}, {{USA}}},
  shorttitle = {Onward!'15},
  language = {en},
  author = {{ACM International Symposium on New Ideas}, {and} Reflections on Programming {and} Software, New Paradigms and Murphy, Gail C and Steele, Guy L and {ACM Special Interest Group on Programming Languages} and {Association for Computing Machinery} and {SPLASH (Conference)}},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/VR273AMG/ACM International Symposium on New Ideas et al. - 2015 - Onward!'15 proceedings of the 2015 ACM Internatio.pdf},
  note = {OCLC: 961117640}
}

@inproceedings{bondOCTETCapturingControlling2013,
  address = {Indianapolis, Indiana, USA},
  title = {{{OCTET}}: Capturing and Controlling Cross-Thread Dependences Efficiently},
  isbn = {978-1-4503-2374-1},
  shorttitle = {{{OCTET}}},
  doi = {10.1145/2509136.2509519},
  abstract = {Parallel programming is essential for reaping the benefits of parallel hardware, but it is notoriously difficult to develop and debug reliable, scalable software systems. One key challenge is that modern languages and systems provide poor support for ensuring concurrency correctness properties\textemdash{}atomicity, sequential consistency, and multithreaded determinism\textemdash{}because all existing approaches are impractical. Dynamic, software-based approaches slow programs by up to an order of magnitude because capturing and controlling cross-thread dependences (i.e., conflicting accesses to shared memory) requires synchronization at virtually every access to potentially shared memory.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{ACM SIGPLAN}} International Conference on {{Object}} Oriented Programming Systems Languages \& Applications - {{OOPSLA}} '13},
  publisher = {{ACM Press}},
  author = {Bond, Michael D. and Kulkarni, Milind and Cao, Man and Zhang, Minjia and Fathi Salmi, Meisam and Biswas, Swarnendu and Sengupta, Aritra and Huang, Jipeng},
  year = {2013},
  pages = {693-712},
  file = {/Users/luigi/work/zotero/storage/EQF7H3A5/Bond et al. - 2013 - OCTET capturing and controlling cross-thread depe.pdf;/Users/luigi/work/zotero/storage/NW52RUY2/Bond et al. - 2013 - OCTET capturing and controlling cross-thread depe.pdf}
}

@inproceedings{aldrichPowerInteroperabilityWhy2013,
  address = {Indianapolis, Indiana, USA},
  title = {The Power of Interoperability: Why Objects Are Inevitable},
  isbn = {978-1-4503-2472-4},
  shorttitle = {The Power of Interoperability},
  doi = {10.1145/2509578.2514738},
  abstract = {Three years ago in this venue, Cook argued that in their essence, objects are what Reynolds called procedural data structures. His observation raises a natural question: if procedural data structures are the essence of objects, has this contributed to the empirical success of objects, and if so, how? This essay attempts to answer that question. After reviewing Cook's definition, I propose the term service abstractions to capture the essential nature of objects. This terminology emphasizes, following Kay, that objects are not primarily about representing and manipulating data, but are more about providing services in support of higher-level goals. Using examples taken from object-oriented frameworks, I illustrate the unique design leverage that service abstractions provide: the ability to define abstractions that can be extended, and whose extensions are interoperable in a first-class way. The essay argues that the form of interoperable extension supported by service abstractions is essential to modern software: many modern frameworks and ecosystems could not have been built without service abstractions. In this sense, the success of objects was not a coincidence: it was an inevitable consequence of their service abstraction nature.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{ACM}} International Symposium on {{New}} Ideas, New Paradigms, and Reflections on Programming \& Software - {{Onward}}! '13},
  publisher = {{ACM Press}},
  author = {Aldrich, Jonathan},
  year = {2013},
  pages = {101-116},
  file = {/Users/luigi/work/zotero/storage/CPK2MGE5/Aldrich - 2013 - The power of interoperability why objects are ine.pdf;/Users/luigi/work/zotero/storage/DMHLJMDX/Aldrich - 2013 - The power of interoperability why objects are ine.pdf}
}

@article{coburnNVHeapsMakingPersistent,
  title = {{{NV}}-{{Heaps}}: {{Making Persistent Objects Fast}} and {{Safe}} with {{Next}}-{{Generation}}, {{Non}}-{{Volatile Memories}}},
  abstract = {Persistent, user-defined objects present an attractive abstraction for working with non-volatile program state. However, the slow speed of persistent storage (i.e., disk) has restricted their design and limited their performance. Fast, byte-addressable, non-volatile technologies, such as phase change memory, will remove this constraint and allow programmers to build high-performance, persistent data structures in non-volatile storage that is almost as fast as DRAM. Creating these data structures requires a system that is lightweight enough to expose the performance of the underlying memories but also ensures safety in the presence of application and system failures by avoiding familiar bugs such as dangling pointers, multiple free()s, and locking errors. In addition, the system must prevent new types of hard-to-find pointer safety bugs that only arise with persistent objects. These bugs are especially dangerous since any corruption they cause will be permanent.},
  language = {en},
  author = {Coburn, Joel and Caulfield, Adrian M and Akel, Ameen and Grupp, Laura M and Gupta, Rajesh K and Jhala, Ranjit and Swanson, Steven},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/GED6DSVL/Coburn et al. - NV-Heaps Making Persistent Objects Fast and Safe .pdf;/Users/luigi/work/zotero/storage/UZCJNXF7/Coburn et al. - NV-Heaps Making Persistent Objects Fast and Safe .pdf}
}

@article{chughNestedRefinementsLogic,
  title = {Nested {{Refinements}}: {{A Logic}} for {{Duck Typing}}},
  abstract = {Programs written in dynamic languages make heavy use of features \textemdash{} run-time type tests, value-indexed dictionaries, polymorphism, and higher-order functions \textemdash{} that are beyond the reach of type systems that employ either purely syntactic or purely semantic reasoning. We present a core calculus, System D, that merges these two modes of reasoning into a single powerful mechanism of nested refinement types wherein the typing relation is itself a predicate in the refinement logic. System D coordinates SMT-based logical implication and syntactic subtyping to automatically typecheck sophisticated dynamic language programs. By coupling nested refinements with McCarthy's theory of finite maps, System D can precisely reason about the interaction of higher-order functions, polymorphism, and dictionaries. The addition of type predicates to the refinement logic creates a circularity that leads to unique technical challenges in the metatheory, which we solve with a novel stratification approach that we use to prove the soundness of System D.},
  language = {en},
  author = {Chugh, Ravi and Rondon, Patrick M and Jhala, Ranjit},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/FS8VE88M/Chugh et al. - Nested Reﬁnements A Logic for Duck Typing.pdf;/Users/luigi/work/zotero/storage/ZCHWXNAX/Chugh et al. - Nested Reﬁnements A Logic for Duck Typing.pdf}
}

@article{burnimNDSeqRuntimeChecking,
  title = {{{NDSeq}}: {{Runtime Checking}} for {{Nondeterministic Sequential Specifications}} of {{Parallel Correctness}}},
  abstract = {We propose to specify the correctness of a program's parallelism using a sequential version of the program with controlled nondeterminism. Such a nondeterministic sequential specification allows (1) the correctness of parallel interference to be verified independently of the program's functional correctness, and (2) the functional correctness of a program to be understood and verified on a sequential version of the program, one with controlled nondeterminism but no interleaving of parallel threads.},
  language = {en},
  author = {Burnim, Jacob and Elmas, Tayfun and Necula, George and Sen, Koushik},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/BDVSR8ZR/Burnim et al. - NDSeq Runtime Checking for Nondeterministic Seque.pdf}
}

@article{altenkirchNormalisationEvaluationDependent2016,
  title = {Normalisation by {{Evaluation}} for {{Dependent Types}}},
  abstract = {We develop normalisation by evaluation (NBE) for dependent types based on presheaf categories. Our construction is formulated using internal type theory using quotient inductive types. We use a typed presentation hence there are no preterms or realizers in our construction. NBE for simple types is using a logical relation between the syntax and the presheaf interpretation. In our construction, we merge the presheaf interpretation and the logical relation into a proof-relevant logical predicate. We have formalized parts of the construction in Agda.},
  language = {en},
  author = {Altenkirch, Thorsten and Kaposi, Ambrus},
  year = {2016},
  pages = {16},
  file = {/Users/luigi/work/zotero/storage/YUWV8BFU/Altenkirch and Kaposi - 2016 - Normalisation by Evaluation for Dependent Types.pdf}
}

@article{allenTypeCheckingModular,
  title = {Type {{Checking Modular Multiple Dispatch}} with {{Parametric Polymorphism}} and {{Multiple Inheritance}}},
  abstract = {In previous work, we presented rules for defining overloaded functions that ensure type safety under symmetric multiple dispatch in an object-oriented language with multiple inheritance, and we showed how to check these rules without requiring the entire type hierarchy to be known, thus supporting modularity and extensibility. In this work, we extend these rules to a language that supports parametric polymorphism on both classes and functions.},
  language = {en},
  author = {Allen, Eric and Hilburn, Justin and Kilpatrick, Scott and Luchangco, Victor and Ryu, Sukyoung and Chase, David and Jr, Guy L Steele},
  pages = {20},
  file = {/Users/luigi/work/zotero/storage/ZA8RPTKC/Allen et al. - Type Checking Modular Multiple Dispatch with Param.pdf}
}

@article{calcagnoMovingFastSoftware,
  title = {Moving {{Fast}} with {{Software Verification}}},
  abstract = {For organisations like Facebook, high quality software is important. However, the pace of change and increasing complexity of modern code makes it difficult to produce error-free software. Available tools are often lacking in helping programmers develop more reliable and secure applications.},
  language = {en},
  author = {Calcagno, Cristiano and Distefano, Dino and Dubreil, Jeremy and Gabi, Dominik and Luca, Martino and O'Hearn, Peter and Papakonstantinou, Irene and Rodriguez, Dulma},
  pages = {9},
  file = {/Users/luigi/work/zotero/storage/EMAIGNWP/Calcagno et al. - Moving Fast with Software Veriﬁcation.pdf}
}

@article{moretPolymorphicBytecodeInstrumentation,
  title = {Polymorphic Bytecode Instrumentation},
  abstract = {Bytecode instrumentation is a widely used technique to implement aspect weaving and dynamic analyses in virtual machines such as the Java Virtual Machine. Aspect weavers and other instrumentations are usually developed independently and combining them often requires significant engineering effort, if at all possible. In this paper we introduce polymorphic bytecode instrumentation (PBI), a simple but effective technique that allows dynamic dispatch amongst several, possibly independent instrumentations. PBI enables complete bytecode coverage, that is, any method with a bytecode representation can be instrumented. We illustrate further benefits of PBI with three case studies. First, we provide an implementation of execution levels for AspectJ, which avoid infinite regression and unwanted interference between aspects. Second, we present a framework for adaptive dynamic analysis, where the analysis to be performed can be changed at runtime by the user. Third, we describe how PBI can be used to support a form of dynamic mixin layers. We provide thorough performance evaluations with dynamic analysis aspects applied to standard benchmarks. We show that PBI-based execution levels are much faster than control flow pointcuts to avoid interference between aspects, and that their efficient integration in a practical aspect language is possible. We also demonstrate that PBI enables adaptive dynamic analysis tools that are more reactive to user inputs than existing tools that rely on dynamic aspect-oriented programming with runtime weaving.},
  language = {en},
  author = {Moret, Philippe and Binder, Walter},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/6UCEIZAG/Moret and Binder - Polymorphic bytecode instrumentation.pdf;/Users/luigi/work/zotero/storage/TXI55HU8/Moret and Binder - Polymorphic bytecode instrumentation.pdf}
}

@article{huttonMonadicParserCombinators,
  title = {Monadic {{Parser Combinators}}},
  abstract = {In functional programming, a popular approach to building recursive descent parsers is to model parsers as functions, and to define higher-order functions (or combinators) that implement grammar constructions such as sequencing, choice, and repetition. Such parsers form an instance of a monad , an algebraic structure from mathematics that has proved useful for addressing a number of computational problems. The purpose of this article is to provide a step-by-step tutorial on the monadic approach to building functional parsers, and to explain some of the benefits that result from exploiting monads. No prior knowledge of parser combinators or of monads is assumed. Indeed, this article can also be viewed as a first introduction to the use of monads in programming.},
  language = {en},
  author = {Hutton, Graham and Meijer, Erik},
  pages = {38},
  file = {/Users/luigi/work/zotero/storage/A4VL77QL/Hutton and Meijer - Monadic Parser Combinators.pdf}
}

@inproceedings{saiedMiningMultilevelAPI2015,
  address = {Montreal, QC, Canada},
  title = {Mining {{Multi}}-Level {{API Usage Patterns}}},
  isbn = {978-1-4799-8469-5},
  doi = {10.1109/SANER.2015.7081812},
  abstract = {Software developers need to cope with complexity of Application Programming Interfaces (APIs) of external libraries or frameworks. However, typical APIs provide several thousands of methods to their client programs, and such large APIs are difficult to learn and use. An API method is generally used within client programs along with other methods of the API of interest. Despite this, co-usage relationships between API methods are often not documented. We propose a technique for mining Multi-Level API Usage Patterns (MLUP) to exhibit the co-usage relationships between methods of the API of interest across interfering usage scenarios. We detect multi-level usage patterns as distinct groups of API methods, where each group is uniformly used across variable client programs, independently of usage contexts. We evaluated our technique through the usage of four APIs having up to 22 client programs per API. For all the studied APIs, our technique was able to detect usage patterns that are, almost all, highly consistent and highly cohesive across a considerable variability of client programs.},
  language = {en},
  booktitle = {2015 {{IEEE}} 22nd {{International Conference}} on {{Software Analysis}}, {{Evolution}}, and {{Reengineering}} ({{SANER}})},
  publisher = {{IEEE}},
  author = {Saied, Mohamed Aymen and Benomar, Omar and Abdeen, Hani and Sahraoui, Houari},
  month = mar,
  year = {2015},
  pages = {23-32},
  file = {/Users/luigi/work/zotero/storage/7ES3RJQN/Saied et al. - 2015 - Mining Multi-level API Usage Patterns.pdf}
}

@article{jonssonStrengtheningSupercompilationCallByValue,
  title = {Strengthening {{Supercompilation For Call}}-{{By}}-{{Value Languages}}},
  abstract = {A termination preserving supercompiler for a call-by-value language sometimes fails to remove intermediate structures that a supercompiler for a call-by-name language would remove. This discrepancy in power stems from the fact that many function bodies are either non-linear in use of an important variable or often start with a pattern match on their first argument and are therefore not strict in all their arguments. As a consequence, intermediate structures are left in the output program, making it slower. We present a revised supercompilation algorithm for a call-by-value language that propagates let-bindings into case-branches and uses termination analysis to remove dead code. This allows the algorithm to remove all intermediate structures for common examples where previous algorithms for call-by-value languages had to leave the intermediate structures in place.},
  language = {en},
  author = {Jonsson, Peter A and Nordlander, Johan},
  pages = {18},
  file = {/Users/luigi/work/zotero/storage/JG7Y3MJG/Jonsson and Nordlander - Strengthening Supercompilation For Call-By-Value L.pdf}
}

@article{sheardTemplateMetaprogrammingHaskell,
  title = {Template {{Meta}}-Programming for {{Haskell}}},
  abstract = {We propose a new extension to the purely functional programming language Haskell that supports compile-time meta-programming. The purpose of the system is to support the algorithmic construction of programs at compile-time.},
  language = {en},
  author = {Sheard, Tim and Jones, Simon Peyton},
  pages = {16},
  file = {/Users/luigi/work/zotero/storage/VJ8ZRGXQ/Sheard and Jones - Template Meta-programming for Haskell.pdf}
}

@article{marschallDetectingMethodsTest,
  title = {Detecting the {{Methods}} under {{Test}} in {{Java}}},
  language = {en},
  author = {Marschall, Philippe},
  pages = {25},
  file = {/Users/luigi/work/zotero/storage/ST66ZTAN/Marschall - Detecting the Methods under Test in Java.pdf}
}

@inproceedings{marekDiSLDomainspecificLanguage2012,
  address = {Potsdam, Germany},
  title = {{{DiSL}}: A Domain-Specific Language for Bytecode Instrumentation},
  isbn = {978-1-4503-1092-5},
  shorttitle = {{{DiSL}}},
  doi = {10.1145/2162049.2162077},
  language = {en},
  booktitle = {Proceedings of the 11th Annual International Conference on {{Aspect}}-Oriented {{Software Development}} - {{AOSD}} '12},
  publisher = {{ACM Press}},
  author = {Marek, Luk\'a{\v s} and Villaz\'on, Alex and Zheng, Yudi and Ansaloni, Danilo and Binder, Walter and Qi, Zhengwei},
  year = {2012},
  pages = {239},
  file = {/Users/luigi/work/zotero/storage/BZRB2V4G/Marek et al. - 2012 - DiSL a domain-specific language for bytecode inst.pdf;/Users/luigi/work/zotero/storage/ZNSL6N4S/Marek et al. - 2012 - DiSL a domain-specific language for bytecode inst.pdf}
}

@article{halpinLogicalDataModeling,
  title = {Logical {{Data Modeling}}: {{Part}}},
  language = {en},
  author = {Halpin, Terry},
  pages = {9},
  file = {/Users/luigi/work/zotero/storage/4MMCAGYP/Halpin - Logical Data Modeling Part.pdf}
}

@inproceedings{chaudhuriLanguagebasedSecurityAndroid2009,
  address = {Dublin, Ireland},
  title = {Language-Based Security on {{Android}}},
  isbn = {978-1-60558-645-8},
  doi = {10.1145/1554339.1554341},
  abstract = {In this paper, we initiate a formal study of security on Android: Google's new open-source platform for mobile devices. Specifically, we present a core typed language to describe Android applications, and to reason about their dataflow security properties. Our operational semantics and type system provide some necessary foundations to help both users and developers of Android applications deal with their security concerns.},
  language = {en},
  booktitle = {Proceedings of the {{ACM SIGPLAN Fourth Workshop}} on {{Programming Languages}} and {{Analysis}} for {{Security}} - {{PLAS}} '09},
  publisher = {{ACM Press}},
  author = {Chaudhuri, Avik},
  year = {2009},
  pages = {1},
  file = {/Users/luigi/work/zotero/storage/86PVECQH/Chaudhuri - 2009 - Language-based security on Android.pdf}
}

@inproceedings{erdwegFrameworkExtensibleLanguages2013,
  address = {Indianapolis, Indiana, USA},
  title = {A Framework for Extensible Languages},
  isbn = {978-1-4503-2373-4},
  doi = {10.1145/2517208.2517210},
  abstract = {Extensible programming languages such as SugarJ or Racket enable programmers to introduce customary language features as extensions of the base language. Traditionally, systems that support language extensions are either (i) agnostic to the base language or (ii) only support a single base language. In this paper, we present a framework for language extensibility that turns a non-extensible language into an extensible language featuring library-based extensible syntax, extensible static analyses, and extensible editor support. To make a language extensible, our framework only requires knowledge of the base language's grammar, the syntax for import statements (which activate extensions), and how to compile baselanguage programs. We have evaluated the generality of our framework by instantiating it for Java, Haskell, Prolog, JavaScript, and System F{$\omega$}, and by studying existing module-system features and their support in our framework.},
  language = {en},
  booktitle = {Proceedings of the 12th International Conference on {{Generative}} Programming: Concepts \& Experiences - {{GPCE}} '13},
  publisher = {{ACM Press}},
  author = {Erdweg, Sebastian and Rieger, Felix},
  year = {2013},
  pages = {3-12},
  file = {/Users/luigi/work/zotero/storage/2R4II8TG/Erdweg and Rieger - 2013 - A framework for extensible languages.pdf;/Users/luigi/work/zotero/storage/ZTWTM63M/Erdweg and Rieger - 2013 - A framework for extensible languages.pdf}
}

@article{lohTutorialImplementationDependently,
  title = {A Tutorial Implementation of a Dependently Typed Lambda Calculus},
  abstract = {We present the type rules for a dependently typed core calculus together with a straightforward implementation in Haskell. We explicitly highlight the changes necessary to shift from a simply-typed lambda calculus to the dependently typed lambda calculus. We also describe how to extend our core language with data types and write several small example programs. The article is accompanied by an executable interpreter and example code that allows immediate experimentation with the system we describe.},
  language = {en},
  author = {Loh, Andres and McBride, Conor and Swierstra, Wouter},
  pages = {31},
  file = {/Users/luigi/work/zotero/storage/3G25M6MZ/Loh et al. - A tutorial implementation of a dependently typed l.pdf;/Users/luigi/work/zotero/storage/T8UQKZ9J/Loh et al. - A tutorial implementation of a dependently typed l.pdf}
}

@article{gallierWHATSPECIALKRUSKAL,
  title = {{{WHAT}}'{{S SO SPECIAL ABOUT KRUSKAL}}'{{S THEOREM AND THE ORDINAL $\Gamma$0}}? {{A SURVEY OF SOME RESULTS IN PROOF THEORY}}},
  abstract = {This paper consists primarily of a survey of results of Harvey Friedman about some proof theoretic aspects of various forms of Kruskal's tree theorem, and in particular the connection with the ordinal {$\Gamma$}0. We also include a fairly extensive treatment of normal functions on the countable ordinals, and we give a glimpse of Veblen hierarchies, some subsystems of second-order logic, slow-growing and fast-growing hierarchies including Girard's result, and Goodstein sequences. The central theme of this paper is a powerful theorem due to Kruskal, the ``tree theorem'', as well as a ``finite miniaturization'' of Kruskal's theorem due to Harvey Friedman. These versions of Kruskal's theorem are remarkable from a proof-theoretic point of view because they are not provable in relatively strong logical systems. They are examples of so-called ``natural independence phenomena'', which are considered by most logicians as more natural than the metamathematical incompleteness results first discovered by G\textasciidieresis{}odel. Kruskal's tree theorem also plays a fundamental role in computer science, because it is one of the main tools for showing that certain orderings on trees are well founded. These orderings play a crucial role in proving the termination of systems of rewrite rules and the correctness of Knuth-Bendix completion procedures. There is also a close connection between a certain infinite countable ordinal called {$\Gamma$}0 and Kruskal's theorem. Previous definitions of the function involved in this connection are known to be incorrect, in that, the function is not monotonic. We offer a repaired definition of this function, and explore briefly the consequences of its existence.},
  language = {en},
  author = {Gallier, Jean H},
  pages = {70},
  file = {/Users/luigi/work/zotero/storage/ZFIZY7UY/Gallier - WHAT’S SO SPECIAL ABOUT KRUSKAL’S THEOREM AND THE .pdf}
}

@article{lindholmJavaVirtualMachine,
  title = {The {{Java}}\textregistered{} {{Virtual Machine Specification}}},
  language = {en},
  author = {Lindholm, Tim and Yellin, Frank and Bracha, Gilad and Buckley, Alex},
  pages = {626},
  file = {/Users/luigi/work/zotero/storage/3HYU9EJI/Lindholm et al. - The Java® Virtual Machine Specification.pdf;/Users/luigi/work/zotero/storage/9QX78FTZ/Lindholm et al. - The Java® Virtual Machine Specification.pdf;/Users/luigi/work/zotero/storage/EBAUUPQ5/Lindholm et al. - The Java® Virtual Machine Specification.pdf;/Users/luigi/work/zotero/storage/S8FHSSP4/Lindholm et al. - The Java® Virtual Machine Specification.pdf}
}

@incollection{albertVerificationJavaBytecode2006,
  address = {Berlin, Heidelberg},
  title = {Verification of {{Java Bytecode Using Analysis}} and {{Transformation}} of {{Logic Programs}}},
  volume = {4354},
  isbn = {978-3-540-69608-7 978-3-540-69611-7},
  abstract = {State of the art analyzers in the Logic Programming (LP) paradigm are nowadays mature and sophisticated. They allow inferring a wide variety of global properties including termination, bounds on resource consumption, etc. The aim of this work is to automatically transfer the power of such analysis tools for LP to the analysis and verification of Java bytecode (jvml). In order to achieve our goal, we rely on well-known techniques for meta-programming and program specialization. More precisely, we propose to partially evaluate a jvml interpreter implemented in LP together with (an LP representation of) a jvml program and then analyze the residual program. Interestingly, at least for the examples we have studied, our approach produces very simple LP representations of the original jvml programs. This can be seen as a decompilation from jvml to high-level LP source. By reasoning about such residual programs, we can automatically prove in the CiaoPP system some non-trivial properties of jvml programs such as termination, run-time error freeness and infer bounds on its resource consumption. We are not aware of any other system which is able to verify such advanced properties of Java bytecode.},
  language = {en},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Albert, Elvira and {G\'omez-Zamalloa}, Miguel and Hubert, Laurent and Puebla, Germ\'an},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Hanus, Michael},
  year = {2006},
  pages = {124-139},
  file = {/Users/luigi/work/zotero/storage/DGUPEELC/Albert et al. - 2006 - Verification of Java Bytecode Using Analysis and T.pdf},
  doi = {10.1007/978-3-540-69611-7_8}
}

@article{felleisenStructureInterpretationComputer2004,
  title = {The Structure and Interpretation of the Computer Science Curriculum},
  volume = {14},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796804005076},
  abstract = {Twenty years ago Abelson and Sussman's Structure and Interpretation of Computer Programs radically changed the intellectual landscape of introductory computing courses. Instead of teaching some currently fashionable programming language, it employed Scheme and functional programming to teach important ideas. Introductory courses based on the book showed up around the world and made Scheme and functional programming popular. Unfortunately, these courses quickly disappeared again due to shortcomings of the book and the whimsies of Scheme. Worse, the experiment left people with a bad impression of Scheme and functional programming in general.},
  language = {en},
  number = {4},
  journal = {Journal of functional Programming},
  author = {Felleisen, Matthias and Findler, Robert Bruce and Flatt, Matthew and Krishnamurthi, Shriram},
  month = jul,
  year = {2004},
  pages = {365-378},
  file = {/Users/luigi/work/zotero/storage/AY37N6D4/Felleisen et al. - 2004 - The structure and interpretation of the computer s.pdf}
}

@article{vytiniotisOutsideInModularType2011,
  title = {{{OutsideIn}}({{X}}) {{Modular}} Type Inference with Local Assumptions},
  volume = {21},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796811000098},
  abstract = {Advanced type system features, such as GADTs, type classes, and type families, have proven to be invaluable language extensions for ensuring data invariants and program correctness. Unfortunately, they pose a tough problem for type inference when they are used as local type assumptions. Local type assumptions often result in the lack of principal types and cast the generalisation of local let-bindings prohibitively difficult to implement and specify. User-declared axioms only make this situation worse. In this article, we explain the problems and \textendash{} perhaps controversially \textendash{} argue for abandoning local let-binding generalisation. We give empirical results that local let generalisation is only sporadically used by Haskell programmers.},
  language = {en},
  number = {4-5},
  journal = {Journal of Functional Programming},
  author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Schrijvers, Tom and Sulzmann, Martin},
  month = sep,
  year = {2011},
  pages = {333-412},
  file = {/Users/luigi/work/zotero/storage/SIZUUZRJ/Vytiniotis et al. - 2011 - OutsideIn(X) Modular type inference with local ass.pdf}
}

@article{shaliHybridPartialEvaluation,
  title = {Hybrid {{Partial Evaluation}}},
  abstract = {Hybrid partial evaluation (HPE) is a pragmatic approach to partial evaluation that borrows ideas from both online and offline partial evaluation. HPE performs offline-style specialization using an online approach without static binding time analysis. The goal of HPE is to provide a practical and predictable level of optimization for programmers, with an implementation strategy that fits well within existing compilers or interpreters. HPE requires the programmer to specify where partial evaluation should be applied. It provides no termination guarantee and reports errors in situations that violate simple binding time rules, or have incorrect use of side effects in compile-time code. We formalize HPE for a small imperative object-oriented language and describe Civet, a straightforward implementation of HPE as a relatively simple extension of a Java compiler. Code optimized by Civet performs as well as and in some cases better than the output of a state-of-the-art offline partial evaluator.},
  language = {en},
  author = {Shali, Amin and Cook, William R},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/XYAIVQHA/Shali and Cook - Hybrid Partial Evaluation.pdf}
}

@incollection{klimovJavaSupercompilerIts2010,
  address = {Berlin, Heidelberg},
  title = {A {{Java Supercompiler}} and {{Its Application}} to {{Verification}} of {{Cache}}-{{Coherence Protocols}}},
  volume = {5947},
  isbn = {978-3-642-11485-4 978-3-642-11486-1},
  abstract = {The Java Supercompiler (JScp) is a specializer of Java programs based on the Turchin's supercompilation method and extended to support imperative and object-oriented notions absent in functional languages. It has been successfully applied to verification of a number of parameterized models including cache-coherence protocols. Protocols are modeled in Java following the method by G. Delzanno and experiments by A. Lisitsa and A. Nemytykh on verification of protocol models by means of the Refal Supercompiler SCP4. The part of the supercompilation method relevant to the protocol verification is reviewed. It deals with an imperative subset of Java.},
  language = {en},
  booktitle = {Perspectives of {{Systems Informatics}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Klimov, Andrei V.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Pnueli, Amir and Virbitskaite, Irina and Voronkov, Andrei},
  year = {2010},
  pages = {185-192},
  file = {/Users/luigi/work/zotero/storage/T7ANECM4/Klimov - 2010 - A Java Supercompiler and Its Application to Verifi.pdf},
  doi = {10.1007/978-3-642-11486-1_16}
}

@article{goetzJavaTheoryPractice,
  title = {Java Theory and Practice: {{To}} Mutate or Not to Mutate?},
  language = {en},
  author = {Goetz, Brian and Corp, Quiotix},
  pages = {8},
  file = {/Users/luigi/work/zotero/storage/8AZQPJ2C/Goetz and Corp - Java theory and practice To mutate or not to muta.pdf}
}

@inproceedings{dallmeierGeneratingTestCases2010,
  address = {Trento, Italy},
  title = {Generating Test Cases for Specification Mining},
  isbn = {978-1-60558-823-0},
  doi = {10.1145/1831708.1831719},
  abstract = {Dynamic specification mining observes program executions to infer models of normal program behavior. What makes us believe that we have seen sufficiently many executions? The TAUTOKO1 typestate miner generates test cases that cover previously unobserved behavior, systematically extending the execution space and enriching the specification. To our knowledge, this is the first combination of systematic test case generation and typestate mining\textendash{}a combination with clear benefits: On a sample of 800 defects seeded into six Java subjects, a static typestate verifier fed with enriched models would report significantly more true positives, and significantly fewer false positives than the initial models.},
  language = {en},
  booktitle = {Proceedings of the 19th International Symposium on {{Software}} Testing and Analysis - {{ISSTA}} '10},
  publisher = {{ACM Press}},
  author = {Dallmeier, Valentin and Knopp, Nikolai and Mallon, Christoph and Hack, Sebastian and Zeller, Andreas},
  year = {2010},
  pages = {85},
  file = {/Users/luigi/work/zotero/storage/DX8JZHQY/Dallmeier et al. - 2010 - Generating test cases for specification mining.pdf}
}

@inproceedings{alnusairEffectiveAPINavigation2010,
  address = {Las Vegas, NV, USA},
  title = {Effective {{API}} Navigation and Reuse},
  isbn = {978-1-4244-8097-5},
  doi = {10.1109/IRI.2010.5558972},
  abstract = {Most reuse libraries come with few source-code examples that demonstrate how the library at hand should be used. We have developed a source-code recommendation approach for constructing and delivering relevant code snippets that programmers can use to complete a certain programming task. Our approach is semantic-based; relying on an explicit ontological representation of sourcecode. We argue that such representation opens new doors for an improved recommendation mechanism that ensures relevancy and accuracy. Current recommendation systems require an existing repository of relevant code samples. However, for many libraries, such a repository does not exist. Therefore, we instead utilize points-to analysis to infer precise type information of library components. We have backed our approach with a tool that has been tested on multiple libraries. The obtained results are promising and demonstrate the effectiveness of our approach.},
  language = {en},
  booktitle = {2010 {{IEEE International Conference}} on {{Information Reuse}} \& {{Integration}}},
  publisher = {{IEEE}},
  author = {Alnusair, Awny and Zhao, Tian and Bodden, Eric},
  month = aug,
  year = {2010},
  pages = {7-12},
  file = {/Users/luigi/work/zotero/storage/9RJF5HNT/Alnusair et al. - 2010 - Effective API navigation and reuse.pdf}
}

@inproceedings{pradelStaticallyCheckingAPI2012,
  address = {Zurich},
  title = {Statically Checking {{API}} Protocol Conformance with Mined Multi-Object Specifications},
  isbn = {978-1-4673-1066-6 978-1-4673-1067-3},
  doi = {10.1109/ICSE.2012.6227127},
  abstract = {Programmers using an API often must follow protocols that specify when it is legal to call particular methods. Several techniques have been proposed to find violations of such protocols based on mined specifications. However, existing techniques either focus on single-object protocols or on particular kinds of bugs, such as missing method calls. There is no practical technique to find multi-object protocol bugs without a priori known specifications. In this paper, we combine a dynamic analysis that infers multi-object protocols and a static checker of API usage constraints into a fully automatic protocol conformance checker. The combined system statically detects illegal uses of an API without human-written specifications. Our approach finds 41 bugs and code smells in mature, real-world Java programs with a true positive rate of 51\%. Furthermore, we show that the analysis reveals bugs not found by state of the art approaches.},
  language = {en},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  publisher = {{IEEE}},
  author = {Pradel, Michael and Jaspan, Ciera and Aldrich, Jonathan and Gross, Thomas R.},
  month = jun,
  year = {2012},
  pages = {925-935},
  file = {/Users/luigi/work/zotero/storage/GS6SXGNB/Pradel et al. - 2012 - Statically checking API protocol conformance with .pdf;/Users/luigi/work/zotero/storage/W5VSX2J4/Pradel et al. - 2012 - Statically checking API protocol conformance with .pdf}
}

@inproceedings{malonyParallelPerformanceMeasurement2011,
  address = {Taipei, Taiwan},
  title = {Parallel {{Performance Measurement}} of {{Heterogeneous Parallel Systems}} with {{GPUs}}},
  isbn = {978-1-4577-1336-1},
  doi = {10.1109/ICPP.2011.71},
  abstract = {The power of GPUs is giving rise to heterogeneous parallel computing, with new demands on programming environments, runtime systems, and tools to deliver high-performing applications. This paper studies the problems associated with performance measurement of heterogeneous machines with GPUs. A heterogeneous computation model and alternative host-GPU measurement approaches are discussed to set the stage for reporting new capabilities for heterogeneous parallel performance measurement in three leading HPC tools: PAPI, Vampir, and the TAU Performance System. Our work leverages the new CUPTI tool support in NVIDIA's CUDA device library. Heterogeneous benchmarks from the SHOC suite are used to demonstrate the measurement methods and tool support.},
  language = {en},
  booktitle = {2011 {{International Conference}} on {{Parallel Processing}}},
  publisher = {{IEEE}},
  author = {Malony, Allen D. and Biersdorff, Scott and Shende, Sameer and Jagode, Heike and Tomov, Stanimire and Juckeland, Guido and Dietrich, Robert and Poole, Duncan and Lamb, Christopher},
  month = sep,
  year = {2011},
  pages = {176-185},
  file = {/Users/luigi/work/zotero/storage/HP4X5BBW/Malony et al. - 2011 - Parallel Performance Measurement of Heterogeneous .pdf;/Users/luigi/work/zotero/storage/S2HL2YPK/Malony et al. - 2011 - Parallel Performance Measurement of Heterogeneous .pdf}
}

@inproceedings{spicugliaLoadBalancingMixaware2013,
  address = {Prague, Czech Republic},
  title = {On Load Balancing: A Mix-Aware Algorithm for Heterogeneous Systems},
  isbn = {978-1-4503-1636-1},
  shorttitle = {On Load Balancing},
  doi = {10.1145/2479871.2479884},
  abstract = {Today's web services are commonly hosted on clusters of servers that are often located within computing clouds, whose computational and storage resources can be highly heterogeneous. The workload served typically exhibits disparate computation patterns (e.g., CPU-intensive or IO-intensive), that fluctuate both in terms of volume and mix. The system heterogeneity together with workload diversity further exacerbates the challenge of effective distribution of load within a computing cloud. This paper presents a novel, mixaware load-balancing algorithm, which aims to distribute requests sent by multiple applications in heterogeneous servers such that the application response times are minimized and system resources (e.g., CPU and IO) are equally utilized. To this end, the presented algorithm tries to not only balance the total number of requests seen by each server, but also to shape the requests received by each server into a certain ``mix", that is analytically shown to be optimal for response time minimization. Our experimental results\textemdash{}based both on simulation and on a prototype implementation\textemdash{}show that the mix-aware algorithm achieves robust performance in most workload mixes as well as a consistent performance improvement in comparison with one of the most robust load-balancing schemes of the Apache server.},
  language = {en},
  booktitle = {Proceedings of the {{ACM}}/{{SPEC}} International Conference on {{International}} Conference on Performance Engineering - {{ICPE}} '13},
  publisher = {{ACM Press}},
  author = {Spicuglia, Sebastiano and Bj\"oerkqvist, Mathias and Chen, Lydia Y. and Serazzi, Giuseppe and Binder, Walter and Smirni, Evgenia},
  year = {2013},
  pages = {71},
  file = {/Users/luigi/work/zotero/storage/8X6CZCSR/Spicuglia et al. - 2013 - On load balancing a mix-aware algorithm for heter.pdf}
}

@article{jonssonSupercompilingOverloadedFunctions,
  title = {Supercompiling {{Overloaded Functions}}},
  abstract = {A na\textasciidieresis\i{}ve translation of recursive cases in type class instances will give code that allocates a new dictionary and copies the data from the input dictionary to the new dictionary for the recursive call. This record creation allocates memory and leads to bad performance for a conceptually simple and rather common operation, for example equality between lists. We present a positive supercompilation algorithm, parametrized with respect to evaluation order, and show how this algorithm can remove these allocations resulting in both time and space savings. We also show how a small extension to the folding mechanism of our supercompiler can give similar effects the static argument transformation at nearly no extra cost.},
  language = {en},
  author = {Jonsson, Peter A and Nordlander, Johan},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/RBK2MAE4/Jonsson and Nordlander - Supercompiling Overloaded Functions.pdf}
}

@inproceedings{jiwonseoSociaLiteDatalogExtensions2013,
  address = {Brisbane, QLD},
  title = {{{SociaLite}}: {{Datalog}} Extensions for Efficient Social Network Analysis},
  isbn = {978-1-4673-4910-9 978-1-4673-4909-3 978-1-4673-4908-6},
  shorttitle = {{{SociaLite}}},
  doi = {10.1109/ICDE.2013.6544832},
  abstract = {With the rise of social networks, large-scale graph analysis becomes increasingly important. Because SQL lacks the expressiveness and performance needed for graph algorithms, lower-level, general-purpose languages are often used instead.},
  language = {en},
  booktitle = {2013 {{IEEE}} 29th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  publisher = {{IEEE}},
  author = {{Jiwon Seo} and Guo, S. and Lam, M. S.},
  month = apr,
  year = {2013},
  pages = {278-289},
  file = {/Users/luigi/work/zotero/storage/ZQ3C5BWE/Jiwon Seo et al. - 2013 - SociaLite Datalog extensions for efficient social.pdf}
}

@article{deoliveiraHowMuchDoes,
  title = {How {{Much Does Memory Layout Impact Performance}}? {{A Wide Study}}},
  abstract = {Memory layout variations are often cited as a threat to the reproducibility of computer performance experiments. Previous research has shown worst-case effects as large as 300\% of execution time, among other surprising cases. While these are worrying results, how frequent and widespread are these large effects? To answer this question, this paper presents a widescale evaluation of memory layout effects on the performance of a large subset of the SPECCPU2006 benchmark suite on a wide array of diverse machines. We find that on average, these benchmarks are not as susceptible to memory layout effects as the worst-case analysis found in the literature suggests. Finally, we re-execute our own experiment and demonstrate why the reproducibility of an experiment's result should not depend on its data being perfectly reproducible.},
  language = {en},
  author = {{de Oliveira}, Augusto Born and Petkovich, Jean-Christophe},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/NNRHPCFA/de Oliveira and Petkovich - How Much Does Memory Layout Impact Performance A .pdf}
}

@incollection{hoareHintsProgrammingLanguage1983,
  address = {Berlin, Heidelberg},
  title = {Hints on {{Programming Language Design}}},
  isbn = {978-3-662-09509-6 978-3-662-09507-2},
  language = {en},
  booktitle = {Programming {{Languages}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Hoare, C. A. R.},
  editor = {Horowitz, Ellis},
  year = {1983},
  pages = {31-40},
  file = {/Users/luigi/work/zotero/storage/VNZSN5KY/Hoare - 1983 - Hints on Programming Language Design.pdf},
  doi = {10.1007/978-3-662-09507-2_3}
}

@article{hoareAxiomaticBasisComputer1969,
  title = {An {{Axiomatic Basis}} for {{Computer Programming}}},
  volume = {12},
  abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and practical, may follow from a pursuance of these topics.},
  language = {en},
  number = {10},
  author = {Hoare, C A R},
  year = {1969},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/8TL3MQNN/Hoare - 1969 - An Axiomatic Basis for Computer Programming.pdf}
}

@inproceedings{kiselyovStronglyTypedHeterogeneous2004,
  address = {Snowbird, Utah, USA},
  title = {Strongly Typed Heterogeneous Collections},
  isbn = {978-1-58113-850-4},
  doi = {10.1145/1017472.1017488},
  abstract = {A heterogeneous collection is a datatype that is capable of storing data of different types, while providing operations for look-up, update, iteration, and others. There are various kinds of heterogeneous collections, differing in representation, invariants, and access operations. We describe HLIST \textemdash{} a Haskell library for strongly typed heterogeneous collections including extensible records. We illustrate HLIST's benefits in the context of type-safe database access in Haskell. The HLIST library relies on common extensions of Haskell 98. Our exploration raises interesting issues regarding Haskell's type system, in particular, avoidance of overlapping instances, and reification of type equality and type unification.},
  language = {en},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} Workshop on {{Haskell}}  - {{Haskell}} '04},
  publisher = {{ACM Press}},
  author = {Kiselyov, Oleg and L\"ammel, Ralf and Schupke, Keean},
  year = {2004},
  pages = {96},
  file = {/Users/luigi/work/zotero/storage/D44986GG/Kiselyov et al. - 2004 - Strongly typed heterogeneous collections.pdf}
}

@article{nordstromMartinLTypeTheory,
  title = {Martin-{{L}}\textasciidieresis{}of's {{Type Theory}}},
  language = {en},
  author = {Nordstrom, B and Petersson, K and Smith, J M},
  pages = {38},
  file = {/Users/luigi/work/zotero/storage/FWQ947WC/Nordstrom et al. - Martin-L¨of’s Type Theory.pdf}
}

@inproceedings{symeLeveragingNETMetaprogramming2006,
  address = {Portland, Oregon, USA},
  title = {Leveraging .{{NET}} Meta-Programming Components from {{F}}\#: Integrated Queries and Interoperable Heterogeneous Execution},
  isbn = {978-1-59593-483-3},
  shorttitle = {Leveraging .{{NET}} Meta-Programming Components from {{F}}\#},
  doi = {10.1145/1159876.1159884},
  abstract = {Language-integrated meta-programming and extensible compilation have been recurring themes of programming languages since the invention of LISP. A recent real-world application of these techniques is the use of small meta-programs to specify database queries, as used in the Microsoft LINQ extensions for .NET. It is important that .NET languages such as F\# are able to leverage the functionality provided by LINQ and related components for heterogeneous execution, both for pragmatic reasons and as a first step toward applying more disciplined, formal approaches to these problems. This paper explores the use of a modest metaprogramming extension to F\# to access and leverage the functionality of LINQ and other components. We do this by demonstrating an implementation of language integrated SQL queries using the LINQ/SQLMetal libraries. We also sketch two other applications: the execution of data-parallel quoted F\# programs on a GPU via the Accelerator libraries, and dynamic native-code compilation via LINQ.},
  language = {en},
  booktitle = {Proceedings of the 2006 Workshop on {{ML}}  - {{ML}} '06},
  publisher = {{ACM Press}},
  author = {Syme, Don},
  year = {2006},
  pages = {43},
  file = {/Users/luigi/work/zotero/storage/JQAB8QLB/Syme - 2006 - Leveraging .NET meta-programming components from F.pdf}
}

@article{aftandilianHeapvizInteractiveHeap,
  title = {Heapviz: {{Interactive Heap Visualization}} for {{Program Understanding}} and {{Debugging}}},
  abstract = {Understanding the data structures in a program is crucial to understanding how the program works, or why it doesn't work. Inspecting the code that implements the data structures, however, is an arduous task and often fails to yield insights into the global organization of a program's data. Inspecting the actual contents of the heap solves these problems but presents a significant challenge of its own: finding an effective way to present the enormous number of objects it contains.},
  language = {en},
  author = {Aftandilian, Edward E and Kelley, Sean and Gramazio, Connor and Guyer, Samuel Z},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/3QD2PTMZ/Aftandilian et al. - Heapviz Interactive Heap Visualization for Progra.pdf;/Users/luigi/work/zotero/storage/I7X5ERZF/Aftandilian et al. - Heapviz Interactive Heap Visualization for Progra.pdf}
}

@article{ekbladDeclarativeWeb,
  title = {Towards a {{Declarative Web}}},
  language = {en},
  author = {Ekblad, Anton},
  pages = {70},
  file = {/Users/luigi/work/zotero/storage/ZN69KPTP/Ekblad - Towards a Declarative Web.pdf}
}

@article{rompfLightweightModularStaging,
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but developing high-quality program generators takes a large effort, even if a multi-stage programming language is used.},
  language = {en},
  author = {Rompf, Tiark and Odersky, Martin},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/TA9P8E84/Rompf and Odersky - Lightweight Modular Staging A Pragmatic Approach .pdf}
}

@book{gpceGPCE15Proceedings2015,
  address = {New York, NY},
  series = {{{SIGPLAN}} Notices},
  title = {{{GPCE}}'15: Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}: {{October}} 26-27, 2015, {{Pittsburgh}}, {{PA}}, {{USA}}},
  isbn = {978-1-4503-3687-1},
  shorttitle = {{{GPCE}}'15},
  language = {en},
  number = {volume 51, number 3},
  publisher = {{ACM, Association for Computing Machinery}},
  author = {GPCE},
  editor = {K\"astner, Christian and Gokh\=al\'e, Anir\"uddh\=a},
  collaborator = {{Association for Computing Machinery}},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/R6XXIPVT/GPCE - 2015 - GPCE'15 proceedings of the 2015 ACM SIGPLAN Inter.pdf},
  note = {OCLC: 951091681}
}

@inproceedings{efftingeXbaseImplementingDomainspecific2012,
  address = {Dresden, Germany},
  title = {Xbase: Implementing Domain-Specific Languages for {{Java}}},
  isbn = {978-1-4503-1129-8},
  shorttitle = {Xbase},
  doi = {10.1145/2371401.2371419},
  abstract = {Xtext is an open-source framework for implementing external, textual domain-specific languages (DSLs). So far, most DSLs implemented with Xtext and similar tools focus on structural aspects such as service specifications and entities. Because behavioral aspects are significantly more complicated to implement, they are often delegated to generalpurpose programming languages. This approach introduces complex integration patterns and the DSL's high level of abstraction is compromised.},
  language = {en},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}} - {{GPCE}} '12},
  publisher = {{ACM Press}},
  author = {Efftinge, Sven and Eysholdt, Moritz and K\"ohnlein, Jan and Zarnekow, Sebastian and {von Massow}, Robert and Hasselbring, Wilhelm and Hanus, Michael},
  year = {2012},
  pages = {112},
  file = {/Users/luigi/work/zotero/storage/UX4I8PJE/Efftinge et al. - 2012 - Xbase implementing domain-specific languages for .pdf;/Users/luigi/work/zotero/storage/WP85DWPT/Efftinge et al. - 2012 - Xbase implementing domain-specific languages for .pdf}
}

@article{dietrichJavaMinute,
  title = {For {{Java}} in {{Under}} a {{Minute}}},
  abstract = {Computing a precise points-to analysis for very large Java1 programs remains challenging despite the large body of research on points-to analysis. Any approach must solve an underlying dynamic graph reachability problem, for which the best algorithms have near-cubic worst-case runtime complexity, and, hence, previous work does not scale to programs with millions of lines of code.},
  language = {en},
  author = {Dietrich, Jens and Hollingum, Nicholas and Scholz, Bernhard},
  pages = {17},
  file = {/Users/luigi/work/zotero/storage/FKQ87GZQ/Dietrich et al. - for Java in Under a Minute.pdf}
}

@article{garciaAbstractingGradualTyping,
  title = {Abstracting {{Gradual Typing}}},
  abstract = {Language researchers and designers have extended a wide variety of type systems to support gradual typing, which enables languages to seamlessly combine dynamic and static checking. These efforts consistently demonstrate that designing a satisfactory gradual counterpart to a static type system is challenging, and this challenge only increases with the sophistication of the type system. Gradual type system designers need more formal tools to help them conceptualize, structure, and evaluate their designs.},
  language = {en},
  author = {Garcia, Ronald and Clark, Alison M},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/RUCQ6KW3/Garcia and Clark - Abstracting Gradual Typing.pdf}
}

@article{emanuelssonComparativeStudyIndustrial,
  title = {A {{Comparative Study}} of {{Industrial Static Analysis Tools}} ({{Extended Version}})},
  language = {en},
  author = {Emanuelsson, Par and Nilsson, Ulf},
  pages = {34},
  file = {/Users/luigi/work/zotero/storage/5ABJPV4R/Emanuelsson and Nilsson - A Comparative Study of Industrial Static Analysis .pdf}
}

@inproceedings{joshiEffectiveDynamicAnalysis2010,
  address = {Santa Fe, New Mexico, USA},
  title = {An Effective Dynamic Analysis for Detecting Generalized Deadlocks},
  isbn = {978-1-60558-791-2},
  doi = {10.1145/1882291.1882339},
  abstract = {We present an effective dynamic analysis for finding a broad class of deadlocks, including the well-studied lock-only deadlocks as well as the less-studied, but no less widespread or insidious, deadlocks involving condition variables. Our analysis consists of two stages. In the first stage, our analysis observes a multi-threaded program execution and generates a simple multi-threaded program, called a trace program, that only records operations observed during the execution that are deemed relevant to finding deadlocks. Such operations include lock acquire and release, wait and notify, thread start and join, and change of values of user-identified synchronization predicates associated with condition variables. In the second stage, our analysis uses an off-the-shelf model checker to explore all possible thread interleavings of the trace program and check if any of them deadlocks. A key advantage of our technique is that it discards most of the program logic which usually causes state-space explosion in model checking, and retains only the relevant synchronization logic in the trace program, which is sufficient for finding deadlocks. We have implemented our analysis for Java, and have applied it to twelve real-world multi-threaded Java programs. Our analysis is effective in practice, finding thirteen previously known as well as four new deadlocks.},
  language = {en},
  booktitle = {Proceedings of the Eighteenth {{ACM SIGSOFT}} International Symposium on {{Foundations}} of Software Engineering - {{FSE}} '10},
  publisher = {{ACM Press}},
  author = {Joshi, Pallavi and Naik, Mayur and Sen, Koushik and Gay, David},
  year = {2010},
  pages = {327},
  file = {/Users/luigi/work/zotero/storage/DTHWG9ZU/Joshi et al. - 2010 - An effective dynamic analysis for detecting genera.pdf;/Users/luigi/work/zotero/storage/TY7XVGPD/Joshi et al. - 2010 - An effective dynamic analysis for detecting genera.pdf}
}

@inproceedings{johannFreeTheoremsPresence2004,
  address = {Venice, Italy},
  title = {Free Theorems in the Presence of {\emph{Seq}}},
  isbn = {978-1-58113-729-3},
  doi = {10.1145/964001.964010},
  abstract = {Parametric polymorphism constrains the behavior of pure functional programs in a way that allows the derivation of interesting theorems about them solely from their types, i.e., virtually for free. Unfortunately, the standard parametricity theorem fails for nonstrict languages supporting a polymorphic strict evaluation primitive like Haskell's seq. Contrary to the folklore surrounding seq and parametricity, we show that not even quantifying only over strict and bottom-reflecting relations in the {$\forall$}-clause of the underlying logical relation \textemdash{} and thus restricting the choice of functions with which such relations are instantiated to obtain free theorems to strict and total ones \textemdash{} is sufficient to recover from this failure. By addressing the subtle issues that arise when propagating up the type hierarchy restrictions imposed on a logical relation in order to accommodate the strictness primitive, we provide a parametricity theorem for the subset of Haskell corresponding to a Girard-Reynolds-style calculus with fixpoints, algebraic datatypes, and seq. A crucial ingredient of our approach is the use of an asymmetric logical relation, which leads to ``inequational'' versions of free theorems enriched by preconditions guaranteeing their validity in the described setting. Besides the potential to obtain corresponding preconditions for standard equational free theorems by combining some new inequational ones, the latter also have value in their own right, as is exemplified with a careful analysis of seq's impact on familiar program transformations.},
  language = {en},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN}}-{{SIGACT}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '04},
  publisher = {{ACM Press}},
  author = {Johann, Patricia and Voigtl\"ander, Janis},
  year = {2004},
  pages = {99-110},
  file = {/Users/luigi/work/zotero/storage/727XRR4P/Johann and Voigtländer - 2004 - Free theorems in the presence of iseqi.pdf}
}

@inproceedings{killianFindingLatentPerformance2010,
  address = {Santa Fe, New Mexico, USA},
  title = {Finding Latent Performance Bugs in Systems Implementations},
  isbn = {978-1-60558-791-2},
  doi = {10.1145/1882291.1882297},
  abstract = {Robust distributed systems commonly employ high-level recovery mechanisms enabling the system to recover from a wide variety of problematic environmental conditions such as node failures, packet drops and link disconnections. Unfortunately, these recovery mechanisms also effectively mask additional serious design and implementation errors, disguising them as latent performance bugs that severely degrade end-to-end system performance. These bugs typically go unnoticed due to the challenge of distinguishing between a bug and an intermittent environmental condition that must be tolerated by the system. We present techniques that can automatically pinpoint latent performance bugs in systems implementations, in the spirit of recent advances in model checking by systematic state space exploration. The techniques proceed by automating the process of conducting random simulations, identifying performance anomalies, and analyzing anomalous executions to pinpoint the circumstances leading to performance degradation. By focusing our implementation on the MACE toolkit, MACEPC can be used to test our implementations directly, without modification. We have applied MACEPC to five thoroughly tested and trusted distributed systems implementations. MACEPC was able to find significant, previously unknown, long-standing performance bugs in each of the systems, and led to fixes that significantly improved the end-to-end performance of the systems.},
  language = {en},
  booktitle = {Proceedings of the Eighteenth {{ACM SIGSOFT}} International Symposium on {{Foundations}} of Software Engineering - {{FSE}} '10},
  publisher = {{ACM Press}},
  author = {Killian, Charles and Nagaraj, Karthik and Pervez, Salman and Braud, Ryan and Anderson, James W. and Jhala, Ranjit},
  year = {2010},
  pages = {17},
  file = {/Users/luigi/work/zotero/storage/3ZNDSNM4/Killian et al. - 2010 - Finding latent performance bugs in systems impleme.pdf;/Users/luigi/work/zotero/storage/YET9SZ7F/Killian et al. - 2010 - Finding latent performance bugs in systems impleme.pdf}
}

@techreport{morrisTypeClassesInstance2000,
  title = {Type {{Classes}} and {{Instance Chains}}: {{A Relational Approach}}},
  shorttitle = {Type {{Classes}} and {{Instance Chains}}},
  abstract = {Type classes, first proposed during the design of the Haskell programming language, extend standard type systems to support overloaded functions. Since their introduction, type classes have been used to address a range of problems, from typing ordering and arithmetic operators to describing heterogeneous lists and limited subtyping. However, while type class programming is useful for a variety of practical problems, its wider use is limited by the inexpressiveness and hidden complexity of current mechanisms. We propose two improvements to existing class systems. First, we introduce several novel language features, instance chains and explicit failure, that increase the expressiveness of type classes while providing more direct expression of current idioms. To validate these features, we have built an implementation of these features, demonstrating their use in a practical setting and their integration with type reconstruction for a Hindley-Milner type system. Second, we define a set-based semantics for type classes that provides a sound basis for reasoning about type class systems, their implementations, and the meanings of programs that use them.},
  language = {en},
  author = {Morris, John Garrett},
  month = jan,
  year = {2000},
  file = {/Users/luigi/work/zotero/storage/JTMPC9XL/Morris - 2000 - Type Classes and Instance Chains A Relational App.pdf},
  doi = {10.15760/etd.1010}
}

@inproceedings{weirichSystemFCExplicit2013,
  address = {Boston, Massachusetts, USA},
  title = {System {{FC}} with Explicit Kind Equality},
  isbn = {978-1-4503-2326-0},
  doi = {10.1145/2500365.2500599},
  abstract = {System FC, the core language of the Glasgow Haskell Compiler, is an explicitly-typed variant of System F with first-class type equality proofs called coercions. This extensible proof system forms the foundation for type system extensions such as type families (typelevel functions) and Generalized Algebraic Datatypes (GADTs). Such features, in conjunction with kind polymorphism and datatype promotion, support expressive compile-time reasoning.},
  language = {en},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming - {{ICFP}} '13},
  publisher = {{ACM Press}},
  author = {Weirich, Stephanie and Hsu, Justin and Eisenberg, Richard A.},
  year = {2013},
  pages = {275},
  file = {/Users/luigi/work/zotero/storage/FMTQ9PSZ/Weirich et al. - 2013 - System FC with explicit kind equality.pdf}
}

@article{tateEqualitySaturationNew,
  title = {Equality {{Saturation}}: A {{New Approach}} to {{Optimization}}},
  abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
  language = {en},
  author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/JJK8C2H2/Tate et al. - Equality Saturation a New Approach to Optimizatio.pdf}
}

@article{tiarkLightweightModularStaging,
  title = {Lightweight {{Modular Staging}} and {{Embedded Compilers}}: {{Abstraction}} without {{Regret}} for {{High}}-{{Level High}}-{{Performance Programming}}},
  abstract = {Programs expressed in a high-level programming language need to be translated to a low-level machine dialect for execution. This translation is usually accomplished by a compiler, which is able to translate any legal program to equivalent low-level code. But for individual source programs, automatic translation does not always deliver good results: Software engineering practice demands generalization and abstraction, whereas high performance demands specialization and concretization. These goals are at odds, and compilers can only rarely translate expressive high-level programs to modern hardware platforms in a way that makes best use of the available resources. Explicit program generation is a promising alternative to fully automatic translation. Instead of writing down the program and relying on a compiler for translation, developers write a program generator, which produces a specialized, efficient, low-level program as its output. However, developing high-quality program generators requires a very large effort that is often hard to amortize. In this thesis, we propose a hybrid design: Integrate compilers into programs so that programs can take control of the translation process, but rely on libraries of common compiler functionality for help. We present Lightweight Modular Staging (LMS), a generative programming approach that lowers the development effort significantly. LMS combines program generator logic with the generated code in a single program, using only types to distinguish the two stages of execution. Through extensive use of component technology, LMS makes a reusable and extensible compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process, with common generic optimizations provided by the framework. Compared to previous work on program generation, a key aspect of our design is the use of staging not only as a front-end, but also as a way to implement internal compiler passes and optimizations, many of which can be combined into powerful joint simplification passes. LMS is well suited to develop embedded domain specific languages (DSLs) and has been used to develop powerful performance-oriented DSLs for demanding domains such as machine learning, with code generation for heterogeneous platforms including GPUs. LMS has also been used to generate SQL for embedded database queries and JavaScript for web applications.},
  author = {Tiark, Rompf},
  pages = {191},
  file = {/Users/luigi/work/zotero/storage/BVVSFPUJ/Tiark - Lightweight Modular Staging and Embedded Compilers.pdf}
}

@inproceedings{mendellExtendingGeneralpurposeStreaming2012,
  address = {Berlin, Germany},
  title = {Extending a General-Purpose Streaming System for {{XML}}},
  isbn = {978-1-4503-0790-1},
  doi = {10.1145/2247596.2247659},
  abstract = {General-purpose streaming systems support diverse application domains with powerful and user-defined stream operators. Most general-purpose streaming systems have their own, non-XML, internal data representation. However, streaming input is often either a sequence of small XML documents, or a scan of a huge document. Prior work on XML streaming focuses on filtering, not transforming, XML, and does not describe how to integrate with a general-purpose streaming system. This paper describes how to integrate an XML transformer with a streaming system by designing a specification syntax that is both consistent with the existing system and familiar to XML users. After type-checking the specification, we compile it to an efficient automaton driven by SAX events. Our approach extends the underlying streaming system with XML support without changing its core architecture, and the same technique could be used for other extensions beyond XML.},
  language = {en},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Extending Database Technology}} - {{EDBT}} '12},
  publisher = {{ACM Press}},
  author = {Mendell, Mark and Nasgaard, Howard and Bouillet, Eric and Hirzel, Martin and Gedik, Bu{\v g}ra},
  year = {2012},
  pages = {534},
  file = {/Users/luigi/work/zotero/storage/6BIB9VJ4/Mendell et al. - 2012 - Extending a general-purpose streaming system for X.pdf;/Users/luigi/work/zotero/storage/ZUPFRC4N/Mendell et al. - 2012 - Extending a general-purpose streaming system for X.pdf}
}

@incollection{wilsonDynamicStorageAllocation1995,
  address = {Berlin, Heidelberg},
  title = {Dynamic Storage Allocation: {{A}} Survey and Critical Review},
  volume = {986},
  isbn = {978-3-540-60368-9 978-3-540-45511-0},
  shorttitle = {Dynamic Storage Allocation},
  language = {en},
  booktitle = {Memory {{Management}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Wilson, Paul R. and Johnstone, Mark S. and Neely, Michael and Boles, David},
  editor = {Goos, Gerhard and Hartmanis, Juris and Leeuwen, Jan and Baler, Henry G.},
  year = {1995},
  pages = {1-116},
  file = {/Users/luigi/work/zotero/storage/5V983BM4/Wilson et al. - 1995 - Dynamic storage allocation A survey and critical .pdf},
  doi = {10.1007/3-540-60368-9_19}
}

@incollection{smaragdakisUsingDatalogFast2011,
  address = {Berlin, Heidelberg},
  title = {Using {{Datalog}} for {{Fast}} and {{Easy Program Analysis}}},
  volume = {6702},
  isbn = {978-3-642-24205-2 978-3-642-24206-9},
  abstract = {Our recent work introduced the D framework for points-to analysis of Java programs. Although Datalog has been used for points-to analyses before, D is the first implementation to express full end-to-end context-sensitive analyses in Datalog. This includes key elements such as call-graph construction as well as the logic dealing with various semantic complexities of the Java language (native methods, reflection, threading, etc.).},
  language = {en},
  booktitle = {Datalog {{Reloaded}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Smaragdakis, Yannis and Bravenboer, Martin},
  editor = {{de Moor}, Oege and Gottlob, Georg and Furche, Tim and Sellers, Andrew},
  year = {2011},
  pages = {245-251},
  file = {/Users/luigi/work/zotero/storage/4GRTWRS4/Smaragdakis and Bravenboer - 2011 - Using Datalog for Fast and Easy Program Analysis.pdf},
  doi = {10.1007/978-3-642-24206-9_14}
}

@book{DLS15Roceedings2015,
  address = {New York},
  title = {{{DLS}}'15: Roceedings of the 11th {{Symposium}} on {{Dynamic Languages}}: {{October}} 27, 2015, {{Pittsburgh}}, {{PA}}},
  isbn = {978-1-4503-3690-1},
  shorttitle = {{{DLS}}'15},
  language = {en},
  publisher = {{Assoication for Computing Machinery}},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/HM43QQU4/2015 - DLS'15 roceedings of the 11th Symposium on Dynami.pdf}
}

@article{bachrachDExpressionsLispPower,
  title = {D-{{Expressions}}: {{Lisp Power}}, {{Dylan Style}}},
  language = {en},
  author = {Bachrach, Jonathan and Playford, Keith and Street, Chandler},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/QIS496V6/Bachrach et al. - D-Expressions Lisp Power, Dylan Style.pdf}
}

@article{kawaguchiDeterministicParallelismLiquid,
  title = {Deterministic {{Parallelism}} via {{Liquid Effects}}},
  abstract = {Shared memory multithreading is a popular approach to parallel programming, but also fiendishly hard to get right. We present Liquid Effects, a type-and-effect system based on refinement types which allows for fine-grained, low-level, shared memory multithreading while statically guaranteeing that a program is deterministic. Liquid Effects records the effect of an expression as a formula in first-order logic, making our type-and-effect system highly expressive. Further, effects like Read and Write are recorded in Liquid Effects as ordinary uninterpreted predicates, leaving the effect system open to extension by the user. By building our system as an extension to an existing dependent refinement type system, our system gains precise value- and branch-sensitive reasoning about effects. Finally, our system exploits the Liquid Types refinement type inference technique to automatically infer refinement types and effects. We have implemented our type-and-effect checking techniques in CSOLVE, a refinement type inference system for C programs. We demonstrate how CSOLVE uses Liquid Effects to prove the determinism of a variety of benchmarks.},
  language = {en},
  author = {Kawaguchi, Ming and Rondon, Patrick and Bakst, Alexander and Jhala, Ranjit},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/7XW4UG45/Kawaguchi et al. - Deterministic Parallelism via Liquid Effects.pdf;/Users/luigi/work/zotero/storage/TAF2ZRNN/Kawaguchi et al. - Deterministic Parallelism via Liquid Effects.pdf}
}

@incollection{conditDependentTypesLowLevel2007,
  address = {Berlin, Heidelberg},
  title = {Dependent {{Types}} for {{Low}}-{{Level Programming}}},
  volume = {4421},
  isbn = {978-3-540-71314-2 978-3-540-71316-6},
  abstract = {In this paper, we describe the key principles of a dependent type system for low-level imperative languages. The major contributions of this work are (1) a sound type system that combines dependent types and mutation for variables and for heap-allocated structures in a more flexible way than before and (2) a technique for automatically inferring dependent types for local variables. We have applied these general principles to design Deputy, a dependent type system for C that allows the user to describe bounded pointers and tagged unions. Deputy has been used to annotate and check a number of real-world C programs.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Condit, Jeremy and Harren, Matthew and Anderson, Zachary and Gay, David and Necula, George C.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Rangan, C. Pandu and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and De Nicola, Rocco},
  year = {2007},
  pages = {520-535},
  file = {/Users/luigi/work/zotero/storage/GI4KFYZ3/Condit et al. - 2007 - Dependent Types for Low-Level Programming.pdf},
  doi = {10.1007/978-3-540-71316-6_35}
}

@incollection{boveDependentTypesWork2009,
  address = {Berlin, Heidelberg},
  title = {Dependent {{Types}} at {{Work}}},
  volume = {5520},
  isbn = {978-3-642-03152-6 978-3-642-03153-3},
  abstract = {In these lecture notes we give an introduction to functional programming with dependent types. We use the dependently typed programming language Agda which is an extension of Martin-Lo\textasciidieresis{}f type theory. First we show how to do simply typed functional programming in the style of Haskell and ML. Some di{$\carriagereturn$}erences between Agda's type system and the Hindley-Milner type system of Haskell and ML are also discussed. Then we show how to use dependent types for programming and we explain the basic ideas behind type-checking dependent types. We go on to explain the Curry-Howard identification of propositions and types. This is what makes Agda a programming logic and not only a programming language. According to Curry-Howard, we identify programs and proofs, something which is possible only by requiring that all program terminate. However, at the end of these notes we present a method for encoding partial and general recursive functions as total functions using dependent types.},
  language = {en},
  booktitle = {Language {{Engineering}} and {{Rigorous Software Development}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bove, Ana and Dybjer, Peter},
  editor = {Bove, Ana and Barbosa, Lu\'is Soares and Pardo, Alberto and Pinto, Jorge Sousa},
  year = {2009},
  pages = {57-99},
  file = {/Users/luigi/work/zotero/storage/EEDHTD5N/Bove and Dybjer - 2009 - Dependent Types at Work.pdf},
  doi = {10.1007/978-3-642-03153-3_2}
}

@inproceedings{gillShortCutDeforestation1993,
  address = {Copenhagen, Denmark},
  title = {A Short Cut to Deforestation},
  isbn = {978-0-89791-595-3},
  doi = {10.1145/165180.165214},
  abstract = {Lists are often used as \textbackslash{}glue" to connect separate parts of a program together. We propose an automatic technique for improving the e ciency of such programs, by removing many of these intermediate lists, based on a single, simple, local transformation. We have implemented the method in the Glasgow Haskell compiler.},
  language = {en},
  booktitle = {Proceedings of the Conference on {{Functional}} Programming Languages and Computer Architecture  - {{FPCA}} '93},
  publisher = {{ACM Press}},
  author = {Gill, Andrew and Launchbury, John and Peyton Jones, Simon L.},
  year = {1993},
  pages = {223-232},
  file = {/Users/luigi/work/zotero/storage/Y4CM8HS3/Gill et al. - 1993 - A short cut to deforestation.pdf}
}

@article{gottliebDesignImplementationModern,
  title = {The Design and Implementation of a Modern Systems Programming Language},
  language = {en},
  author = {Gottlieb, Eli},
  pages = {45},
  file = {/Users/luigi/work/zotero/storage/2Y22B7FQ/Gottlieb - The design and implementation of a modern systems .pdf;/Users/luigi/work/zotero/storage/4NEGTE4T/Gottlieb - The design and implementation of a modern systems .pdf}
}

@inproceedings{souleDynamicExpressivityStatic2013,
  address = {Arlington, Texas, USA},
  title = {Dynamic Expressivity with Static Optimization for Streaming Languages},
  isbn = {978-1-4503-1758-0},
  doi = {10.1145/2488222.2488255},
  abstract = {Developers increasingly use streaming languages to write applications that process large volumes of data with high throughput. Unfortunately, when picking which streaming language to use, they face a di cult choice. On the one hand, dynamically scheduled languages allow developers to write a wider range of applications, but cannot take advantage of many crucial optimizations. On the other hand, statically scheduled languages are extremely performant, but have di culty expressing many important streaming applications.},
  language = {en},
  booktitle = {Proceedings of the 7th {{ACM}} International Conference on {{Distributed}} Event-Based Systems - {{DEBS}} '13},
  publisher = {{ACM Press}},
  author = {Soul\'e, Robert and Gordon, Michael I. and Amarasinghe, Saman and Grimm, Robert and Hirzel, Martin},
  year = {2013},
  pages = {159},
  file = {/Users/luigi/work/zotero/storage/GF7SRBV4/Soulé et al. - 2013 - Dynamic expressivity with static optimization for .pdf;/Users/luigi/work/zotero/storage/VT5GCE4Q/Soulé et al. - 2013 - Dynamic expressivity with static optimization for .pdf}
}

@incollection{meijerFunctionalProgrammingBananas1991,
  address = {Berlin, Heidelberg},
  title = {Functional Programming with Bananas, Lenses, Envelopes and Barbed Wire},
  volume = {523},
  isbn = {978-3-540-54396-1 978-3-540-47599-6},
  abstract = {We develop a calculus for lazy functional programming based on recursion operators associated with data type de nitions. For these operators we derive various algebraic laws that are useful in deriving and manipulating programs. We shall show that all example functions in Bird and Wadler's \textbackslash{}Introduction to Functional Programming" can be expressed using these operators.},
  language = {en},
  booktitle = {Functional {{Programming Languages}} and {{Computer Architecture}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Meijer, Erik and Fokkinga, Maarten and Paterson, Ross},
  editor = {Goos, Gerhard and Hartmanis, Juris and Hughes, John},
  year = {1991},
  pages = {124-144},
  file = {/Users/luigi/work/zotero/storage/944G2JDI/Meijer et al. - 1991 - Functional programming with bananas, lenses, envel.pdf},
  doi = {10.1007/3540543961_7}
}

@inproceedings{albertApplyingSourceLevel2014,
  address = {Cracow, Poland},
  title = {Applying Source Level Auto-Vectorization to {{Aparapi Java}}},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647519},
  abstract = {Parallelism dominates modern hardware design, from multi-core CPUs to SIMD and GPGPU. This bring with it, however, a need to program this hardware in a programmer-friendly manner. Traditionally, managed languages like Java have struggled to take advantage of data-parallel hardware, but projects like Aparapi provide a programming model that lets the programmer easily express the parallelism within their code, while still programming in a highlevel language.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Albert, Curt and Murray, Alastair and Ravindran, Binoy},
  year = {2014},
  pages = {122-132},
  file = {/Users/luigi/work/zotero/storage/BAQC55LF/Albert et al. - 2014 - Applying source level auto-vectorization to Aparap.pdf}
}

@inproceedings{marquesCooperariToolCooperative2014,
  address = {Cracow, Poland},
  title = {Cooperari: A Tool for Cooperative Testing of Multithreaded {{Java}} Programs},
  isbn = {978-1-4503-2926-2},
  shorttitle = {Cooperari},
  doi = {10.1145/2647508.2647523},
  abstract = {Bugs in multithreaded application can be elusive. They are often hard to trace and replicate, given the usual non-determinism and irreproducibility of scheduling decisions at runtime. We present Cooperari, a tool for deterministic testing of multithreaded Java code based on cooperative execution. In a cooperative execution, threads voluntarily suspend (yield) at interference points (e.g., lock acquisition), and code between two consecutive yield points of each thread always executes serially as a transaction. A cooperative scheduler takes over control at yield points and deterministically selects the next thread to run. An application test runs multiple times, until it either fails or the state-space of schedules is deemed as covered by a configurable policy that is responsible for the scheduling decisions. Beyond failed assertions in software tests, deadlocks and races are also detected as soon as they are exposed in the cooperative execution. Cooperari effectively finds, characterizes, and deterministically reproduces bugs that are not detected under unconstrained preemptive semantics, as illustrated by standard benchmark examples.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Marques, Eduardo R. B. and Martins, Francisco and Sim\~oes, Miguel},
  year = {2014},
  pages = {200-206},
  file = {/Users/luigi/work/zotero/storage/U9TF6TP8/Marques et al. - 2014 - Cooperari a tool for cooperative testing of multi.pdf}
}

@inproceedings{hartmannEfficientCodeManagement2014,
  address = {Cracow, Poland},
  title = {Efficient Code Management for Dynamic Multi-Tiered Compilation Systems},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647513},
  abstract = {Managed runtime environments (MREs) like web browsers or the Java Virtual Machine (JVM) include multiple dynamic compilers (or compiler configurations) to provide a good tradeoff between compilation speed and generated code quality: Longer compilation times typically imply better code quality and hence better peak performance than shorter compilation times, which provide good warmup/startup performance and lower peak performance. A system that uses multiple compilers is referred to as a multi-tiered compilation system.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Hartmann, Tobias and Noll, Albert and Gross, Thomas},
  year = {2014},
  pages = {51-62},
  file = {/Users/luigi/work/zotero/storage/VR2P3RG7/Hartmann et al. - 2014 - Efficient code management for dynamic multi-tiered.pdf}
}

@inproceedings{grimmerTruffleCDynamicExecution2014,
  address = {Cracow, Poland},
  title = {{{TruffleC}}: Dynamic Execution of {{C}} on a {{Java}} Virtual Machine},
  isbn = {978-1-4503-2926-2},
  shorttitle = {{{TruffleC}}},
  doi = {10.1145/2647508.2647528},
  abstract = {This paper presents TruffleC, a C interpreter that allows the dynamic execution of C code on top of a Java Virtual Machine (JVM). Rather than producing a static build of a C application, TruffleC is a self-optimizing abstract syntax tree (AST) interpreter combined with a just-in-time (JIT) compiler. Our self-optimizing interpreter specializes the AST based on run-time feedback. AST specialization relies on optimistic assumptions and allows us to build inline caches for polymorphic function pointer calls, to profile runtime values and to potentially replace them with constants, or to speculatively remove code that was not executed yet.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Grimmer, Matthias and Rigger, Manuel and Schatz, Roland and Stadler, Lukas and M\"ossenb\"ock, Hanspeter},
  year = {2014},
  pages = {17-26},
  file = {/Users/luigi/work/zotero/storage/V7BZXR54/Grimmer et al. - 2014 - TruffleC dynamic execution of C on a Java virtual.pdf}
}

@inproceedings{duboscqSpeculationRegretReducing2014,
  address = {Cracow, Poland},
  title = {Speculation without Regret: Reducing Deoptimization Meta-Data in the {{Graal}} Compiler},
  isbn = {978-1-4503-2926-2},
  shorttitle = {Speculation without Regret},
  doi = {10.1145/2647508.2647521},
  abstract = {Speculative optimizations are used in most Just In Time (JIT) compilers in order to take advantage of dynamic runtime feedback. These speculative optimizations usually require the compiler to produce meta-data that the Virtual Machine (VM) can use as fallback when a speculation fails. This meta-data can be large and incurs a significant memory overhead since it needs to be stored alongside the machine code for as long as the machine code lives. The design of the Graal compiler leads to many speculations falling back to a similar state and location. In this paper we present deoptimization grouping, an optimization using this property of the Graal compiler to reduce the amount of meta-data that must be stored by the VM without having to modify the VM. We compare our technique with existing meta-data compression techniques from the HotSpot Virtual Machine and study how well they combine. In order to make informed decisions about speculation meta-data, we present an empirical analysis of the origin, impact and usages of this meta-data.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Duboscq, Gilles and W\"urthinger, Thomas and M\"ossenb\"ock, Hanspeter},
  year = {2014},
  pages = {187-193},
  file = {/Users/luigi/work/zotero/storage/QRFLDZLH/Duboscq et al. - 2014 - Speculation without regret reducing deoptimizatio.pdf}
}

@book{associationforcomputingmachineryProceedings2014International2014,
  title = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}}.},
  isbn = {978-1-4503-2926-2},
  language = {en},
  editor = {{Association for Computing Machinery} and {Special Interest Group on Programming Languages} and {ACM Special Interest Group on Applied Computing}},
  year = {2014},
  file = {/Users/luigi/work/zotero/storage/D5NW8CFQ/Association for Computing Machinery et al. - 2014 - Proceedings of the 2014 International Conference o.pdf},
  note = {OCLC: 994260694}
}

@inproceedings{chapmanClosedOpenNested2014,
  address = {Cracow, Poland},
  title = {Closed and Open Nested Atomic Actions for {{Java}}: Language Design and Prototype Implementation},
  isbn = {978-1-4503-2926-2},
  shorttitle = {Closed and Open Nested Atomic Actions for {{Java}}},
  doi = {10.1145/2647508.2647525},
  abstract = {We describe the design and prototype implementation of a dialect of Java, XJ, that supports both closed and open nested transactions. As we have previously advocated, open nesting most naturally attaches to the class as the primary abstraction mechanism of Java. The resulting design allows natural expression of layered abstractions for concurrent data structures, while promoting improved concurrency for operations on those abstractions. Moreover, we describe our approach to constructing a prototype implementation of XJ that runs on standard Java virtual machines, by grafting support for transactions onto both application code and library code via load-time bytecode rewriting, for full execution coverage. We rely on extensions to the javac compiler, a JVMTI run-time agent to intercept and rewrite Java classes as they are loaded into the virtual machine, and a run-time library that tracks and manages all transaction meta-data. The resulting prototype will allow further exploration of implementation alternatives for open and closed nested transactions in Java. Our design also addresses the issue of internal deadlock caused by accessing the same data in both closed and open nesting fashion by carefully disallowing such access.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Chapman, Keith and Hosking, Antony L. and Moss, J. Eliot B. and Richards, Tim},
  year = {2014},
  pages = {169-180},
  file = {/Users/luigi/work/zotero/storage/D29AVG9M/Chapman et al. - 2014 - Closed and open nested atomic actions for Java la.pdf}
}

@inproceedings{bonoTraitorientedProgrammingJava2014,
  address = {Cracow, Poland},
  title = {Trait-Oriented Programming in {{Java}} 8},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647520},
  abstract = {Java 8 was released recently. Along with lambda expressions, a new language construct is introduced: default methods in interfaces. The intent of this feature is to allow interfaces to be extended over time preserving backward compatibility. In this paper, we show a possible, different use of these interfaces: we introduce a trait-oriented programming style based on an interface-as-trait idea, with the aim of improving code modularity. Starting from the most common operators on traits, we introduce some programming patterns mimicking such operators and discuss this approach.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {Bono, Viviana and Mensa, Enrico and Naddeo, Marco},
  year = {2014},
  pages = {181-186},
  file = {/Users/luigi/work/zotero/storage/L8IIZQMK/Bono et al. - 2014 - Trait-oriented programming in Java 8.pdf}
}

@inproceedings{wossObjectStorageModel2014,
  address = {Cracow, Poland},
  title = {An Object Storage Model for the Truffle Language Implementation Framework},
  isbn = {978-1-4503-2926-2},
  doi = {10.1145/2647508.2647517},
  abstract = {Truffle is a Java-based framework for developing high-performance language runtimes. Language implementers aiming at developing new runtimes have to design all the runtime mechanisms for managing dynamically typed objects from scratch. This not only leads to potential code duplication, but also impacts the actual time needed to develop a fully-fledged runtime.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{International Conference}} on {{Principles}} and {{Practices}} of {{Programming}} on the {{Java}} Platform {{Virtual}} Machines, {{Languages}}, and {{Tools}} - {{PPPJ}} '14},
  publisher = {{ACM Press}},
  author = {W\"o\ss, Andreas and Wirth, Christian and Bonetta, Daniele and Seaton, Chris and Humer, Christian and M\"ossenb\"ock, Hanspeter},
  year = {2014},
  pages = {133-144},
  file = {/Users/luigi/work/zotero/storage/EJ3L9PHI/Wöß et al. - 2014 - An object storage model for the truffle language i.pdf}
}

@article{raemaekersMiningMetricsChanges,
  title = {Mining {{Metrics}}, {{Changes}} and {{Dependencies}} from the {{Maven Dependency Dataset}}},
  abstract = {We present the Maven Dependency Dataset (MDD), containing metrics, changes and dependencies of 148,253 jar files. Metrics and changes have been calculated at the level of individual methods, classes and packages of multiple library versions. A complete call graph is also presented which includes call, inheritance, composition and historical relationships between all units of the entire repository. In this paper, we describe our dataset and the methodology used to obtain it. We present different conceptual views of MDD and we also describe limitations and data quality issues that researchers using this data should be aware of.},
  language = {en},
  author = {Raemaekers, Steven},
  pages = {8},
  file = {/Users/luigi/work/zotero/storage/C58E848T/Raemaekers - Mining Metrics, Changes and Dependencies from the .pdf;/Users/luigi/work/zotero/storage/EEYTHAJF/Raemaekers - Mining Metrics, Changes and Dependencies from the .pdf}
}

@article{turonUnderstandingExpressingScalable,
  title = {Understanding and Expressing Scalable Concurrency},
  language = {en},
  author = {Turon, Aaron},
  pages = {299},
  file = {/Users/luigi/work/zotero/storage/WE3H8LKF/Turon - Understanding and expressing scalable concurrency.pdf}
}

@article{titzerHarmonizingClassesFunctions,
  title = {Harmonizing {{Classes}}, {{Functions}}, {{Tuples}}, and {{Type Parameters}} in {{Virgil III}}},
  abstract = {Languages are becoming increasingly multi-paradigm. Subtype polymorphism in statically-typed object-oriented languages is being supplemented with parametric polymorphism in the form of generics. Features like first-class functions and lambdas are appearing everywhere. Yet existing languages like Java, C\#, C++, D, and Scala seem to accrete ever more complexity when they reach beyond their original paradigm into another; inevitably older features have some rough edges that lead to nonuniformity and pitfalls. Given a fresh start, a new language designer is faced with a daunting array of potential features. Where to start? What is important to get right first, and what can be added later? What features must work together, and what features are orthogonal? We report on our experience with Virgil III, a practical language with a careful balance of classes, functions, tuples and type parameters. Virgil intentionally lacks many advanced features, yet we find its core feature set enables new species of design patterns that bridge multiple paradigms and emulate features not directly supported such as interfaces, abstract data types, ad hoc polymorphism, and variant types. Surprisingly, we find variance for function types and tuple types often replaces the need for other kinds of type variance when libraries are designed in a more functional style.},
  language = {en},
  author = {Titzer, Ben L},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/XYA8W335/Titzer - Harmonizing Classes, Functions, Tuples, and Type P.pdf}
}

@article{sewellTranslationValidationVerified,
  title = {Translation {{Validation}} for a {{Verified OS Kernel}}},
  abstract = {We extend the existing formal verification of the seL4 operating system microkernel from 9 500 lines of C source code to the binary level. We handle all functions that were part of the previous verification. Like the original verification, we currently omit the assembly routines and volatile accesses used to control system hardware. More generally, we present an approach for proving refinement between the formal semantics of a program on the C source level and its formal semantics on the binary level, thus checking the validity of compilation, including some optimisations, and linking, and extending static properties proved of the source code to the executable. We make use of recent improvements in SMT solvers to almost fully automate this process.},
  language = {en},
  author = {Sewell, Thomas and Myreen, Magnus and Klein, Gerwin},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/QFL4MLD8/Sewell et al. - Translation Validation for a Veriﬁed OS Kernel.pdf}
}

@article{seweNewScalaInstance,
  title = {New {{Scala}}() Instance of {{Java}}: A Comparison of the Memory Behaviour of {{Java}} and {{Scala}} Programs},
  abstract = {While often designed with a single language in mind, managed runtimes like the Java virtual machine (JVM) have become the target of not one but many languages, all of which benefit from the runtime's services. One of these services is automatic memory management. In this paper, we compare and contrast the memory behaviour of programs written in Java and Scala, respectively, two languages which both target the same platform: the JVM. We both analyze core object demographics like object lifetimes as well as secondary properties of objects like their associated monitors and identity hash-codes. We find that objects in Scala programs have lower survival rates and higher rates of immutability, which is only partly explained by the memory behaviour of objects representing closures or boxed primitives. Other metrics vary more by benchmark than language.},
  language = {en},
  author = {Sewe, Andreas and Mezini, Mira and Sarimbekov, Aibek and Ansaloni, Danilo and Binder, Walter and Ricci, Nathan and Guyer, Samuel Z},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/U5ZYKMFX/Sewe et al. - new Scala() instance of Java a comparison of the .pdf}
}

@inproceedings{sarimbekovComprehensiveToolchainWorkload2013,
  address = {Seattle, Washington},
  title = {A Comprehensive Toolchain for Workload Characterization across {{JVM}} Languages},
  isbn = {978-1-4503-2128-0},
  doi = {10.1145/2462029.2462033},
  abstract = {The Java Virtual Machine (JVM) today hosts implementations of numerous languages. To achieve high performance, JVM implementations rely on heuristics in choosing compiler optimizations and adapting garbage collection behavior. Historically, these heuristics have been tuned to suit the dynamics of Java programs only. This leads to unnecessarily poor performance in case of non-Java languages, which often exhibit systematic difference in workload behavior. Dynamic metrics characterizing the workload help identify and quantify useful optimizations, but so far, no cohesive suite of metrics has adequately covered properties that vary systematically between Java and non-Java workloads. We present a suite of such metrics, justifying our choice with reference to a range of guest languages. These metrics are implemented on a common portable infrastructure which ensures ease of deployment and customization.},
  language = {en},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN}}-{{SIGSOFT Workshop}} on {{Program Analysis}} for {{Software Tools}} and {{Engineering}} - {{PASTE}} '13},
  publisher = {{ACM Press}},
  author = {Sarimbekov, Aibek and Sewe, Andreas and Kell, Stephen and Zheng, Yudi and Binder, Walter and Bulej, Lubom\'ir and Ansaloni, Danilo},
  year = {2013},
  pages = {9-16},
  file = {/Users/luigi/work/zotero/storage/MWSLGLVT/Sarimbekov et al. - 2013 - A comprehensive toolchain for workload characteriz.pdf}
}

@inproceedings{rossbachDandelionCompilerRuntime2013,
  address = {Farminton, Pennsylvania},
  title = {Dandelion: A Compiler and Runtime for Heterogeneous Systems},
  isbn = {978-1-4503-2388-8},
  shorttitle = {Dandelion},
  doi = {10.1145/2517349.2522715},
  abstract = {Computer systems increasingly rely on heterogeneity to achieve greater performance, scalability and energy efficiency. Because heterogeneous systems typically comprise multiple execution contexts with different programming abstractions and runtimes, programming them remains extremely challenging.},
  language = {en},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth ACM Symposium}} on {{Operating Systems Principles}} - {{SOSP}} '13},
  publisher = {{ACM Press}},
  author = {Rossbach, Christopher J. and Yu, Yuan and Currey, Jon and Martin, Jean-Philippe and Fetterly, Dennis},
  year = {2013},
  pages = {49-68},
  file = {/Users/luigi/work/zotero/storage/GVJJ2MXK/Rossbach et al. - 2013 - Dandelion a compiler and runtime for heterogeneou.pdf}
}

@inproceedings{pienaarJSWhizStaticAnalysis2013,
  address = {Shenzhen},
  title = {{{JSWhiz}}: {{Static}} Analysis for {{JavaScript}} Memory Leaks},
  isbn = {978-1-4673-5525-4 978-1-4673-5524-7},
  shorttitle = {{{JSWhiz}}},
  doi = {10.1109/CGO.2013.6495007},
  abstract = {JavaScript is the dominant language for implementing dynamic web pages in browsers. Even though it is standardized, many browsers implement language and browser bindings in different and incompatible ways. As a result, a plethora of web development frameworks were developed to hide cross-browser issues and to ease development of large web applications. An unwelcome side-effect of these frameworks is that they can introduce memory leaks, despite the fact that JavaScript is garbage collected. Memory bloat is a major issue for web applications, as it affects user perceived latency and may even prevent large web applications from running on devices with limited resources.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  publisher = {{IEEE}},
  author = {Pienaar, J. A. and Hundt, R.},
  month = feb,
  year = {2013},
  pages = {1-11},
  file = {/Users/luigi/work/zotero/storage/Y6UFRVY3/Pienaar and Hundt - 2013 - JSWhiz Static analysis for JavaScript memory leak.pdf}
}

@article{paiImprovingGPGPUConcurrency,
  title = {Improving {{GPGPU}} Concurrency with Elastic Kernels},
  abstract = {Each new generation of GPUs vastly increases the resources available to GPGPU programs. GPU programming models (like CUDA) were designed to scale to use these resources. However, we find that CUDA programs actually do not scale to utilize all available resources, with over 30\% of resources going unused on average for programs of the Parboil2 suite that we used in our work. Current GPUs therefore allow concurrent execution of kernels to improve utilization. In this work, we study concurrent execution of GPU kernels using multiprogram workloads on current NVIDIA Fermi GPUs. On two-program workloads from the Parboil2 benchmark suite we find concurrent execution is often no better than serialized execution. We identify that the lack of control over resource allocation to kernels is a major serialization bottleneck. We propose transformations that convert CUDA kernels into elastic kernels which permit fine-grained control over their resource usage. We then propose several elastic-kernel aware concurrency policies that offer significantly better performance and concurrency compared to the current CUDA policy. We evaluate our proposals on real hardware using multiprogrammed workloads constructed from benchmarks in the Parboil 2 suite. On average, our proposals increase system throughput (STP) by 1.21x and improve the average normalized turnaround time (ANTT) by 3.73x for two-program workloads when compared to the current CUDA concurrency implementation.},
  language = {en},
  author = {Pai, Sreepathi and Thazhuthaveetil, Matthew J and Govindarajan, R},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/2NMAXLFS/Pai et al. - Improving GPGPU concurrency with elastic kernels.pdf}
}

@inproceedings{mistryValarBenchmarkSuite2013,
  address = {Houston, Texas},
  title = {Valar: A Benchmark Suite to Study the Dynamic Behavior of Heterogeneous Systems},
  isbn = {978-1-4503-2017-7},
  shorttitle = {Valar},
  doi = {10.1145/2458523.2458529},
  abstract = {Heterogeneous systems have grown in popularity within the commercial platform and application developer communities. We have seen a growing number of systems incorporating CPUs, Graphics Processors (GPUs) and Accelerated Processing Units (APUs combine a CPU and GPU on the same chip). These emerging class of platforms are now being targeted to accelerate applications where the host processor (typically a CPU) and compute device (typically a GPU) cooperate on a computation. In this scenario, the performance of the application is not only dependent on the processing power of the respective heterogeneous processors, but also on the efficient interaction and communication between them.},
  language = {en},
  booktitle = {Proceedings of the 6th {{Workshop}} on {{General Purpose Processor Using Graphics Processing Units}} - {{GPGPU}}-6},
  publisher = {{ACM Press}},
  author = {Mistry, Perhaad and Ukidave, Yash and Schaa, Dana and Kaeli, David},
  year = {2013},
  pages = {54-65},
  file = {/Users/luigi/work/zotero/storage/UM3DE9HP/Mistry et al. - 2013 - Valar a benchmark suite to study the dynamic beha.pdf}
}

@inproceedings{liDynamicallyValidatingStatic2013,
  address = {Lugano, Switzerland},
  title = {Dynamically Validating Static Memory Leak Warnings},
  isbn = {978-1-4503-2159-4},
  doi = {10.1145/2483760.2483778},
  abstract = {Memory leaks have significant impact on software availability, performance, and security. Static analysis has been widely used to find memory leaks in C/C++ programs. Although a static analysis is able to find all potential leaks in a program, it often reports a great number of false warnings. Manually validating these warnings is a daunting task, which significantly limits the practicality of the analysis. In this paper, we develop a novel dynamic technique that automatically validates and categorizes such warnings to unleash the power of static memory leak detectors. Our technique analyzes each warning that contains information regarding the leaking allocation site and the leaking path, generates test cases to cover the leaking path, and tracks objects created by the leaking allocation site. Eventually, warnings are classified into four categories: MUST-LEAK, LIKELY-NOT-LEAK, BLOAT, and MAYLEAK. Warnings in MUST-LEAK are guaranteed by our analysis to be true leaks. Warnings in LIKELY-NOT-LEAK are highly likely to be false warnings. Although we cannot provide any formal guarantee that they are not leaks, we have high confidence that this is the case. Warnings in BLOAT are also not likely to be leaks but they should be fixed to improve performance. Using our approach, the developer's manual validation effort needs to be focused only on warnings in the category MAY-LEAK, which is often much smaller than the original set.},
  language = {en},
  booktitle = {Proceedings of the 2013 {{International Symposium}} on {{Software Testing}} and {{Analysis}} - {{ISSTA}} 2013},
  publisher = {{ACM Press}},
  author = {Li, Mengchen and Chen, Yuanjun and Wang, Linzhang and Xu, Guoqing},
  year = {2013},
  pages = {112},
  file = {/Users/luigi/work/zotero/storage/RJHE2CQQ/Li et al. - 2013 - Dynamically validating static memory leak warnings.pdf}
}

@inproceedings{vegaTransparentCPUGPUCollaboration2013,
  address = {Edinburgh},
  title = {Transparent {{CPU}}-{{GPU}} Collaboration for Data-Parallel Kernels on Heterogeneous Systems},
  isbn = {978-1-4799-1018-2 978-1-4799-1021-2},
  doi = {10.1109/PACT.2013.6618814},
  abstract = {Heterogeneous computing on CPUs and GPUs has traditionally used fixed roles for each device: the GPU handles data parallel work by taking advantage of its massive number of cores while the CPU handles non data-parallel work, such as the sequential code or data transfer management. Unfortunately, this work distribution can be a poor solution as it under utilizes the CPU, has difficulty generalizing beyond the single CPU-GPU combination, and may waste a large fraction of time transferring data. Further, CPUs are performance competitive with GPUs on many workloads, thus simply partitioning work based on the fixed roles may be a poor choice. In this paper, we present the single kernel multiple devices (SKMD) system, a framework that transparently orchestrates collaborative execution of a single data-parallel kernel across multiple asymmetric CPUs and GPUs. The programmer is responsible for developing a single dataparallel kernel in OpenCL, while the system automatically partitions the workload across an arbitrary set of devices, generates kernels to execute the partial workloads, and efficiently merges the partial outputs together. The goal is performance improvement by maximally utilizing all available resources to execute the kernel. SKMD handles the difficult challenges of exposed data transfer costs and the performance variations GPUs have with respect to input size. On real hardware, SKMD achieves an average speedup of 29\% on a system with one multicore CPU and two asymmetric GPUs compared to a fastest device execution strategy for a set of popular OpenCL kernels.},
  language = {en},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  publisher = {{IEEE}},
  author = {Vega, Augusto and Buyuktosunoglu, Alper and Bose, Pradip},
  month = oct,
  year = {2013},
  pages = {245-256},
  file = {/Users/luigi/work/zotero/storage/EZJQZ6K7/Vega et al. - 2013 - Transparent CPU-GPU collaboration for data-paralle.pdf}
}

@article{jogOWLCooperativeThread,
  title = {{{OWL}}: Cooperative Thread Array Aware Scheduling Techniques for Improving {{GPGPU}} Performance},
  abstract = {Emerging GPGPU architectures, along with programming models like CUDA and OpenCL, offer a cost-effective platform for many applications by providing high thread level parallelism at lower energy budgets. Unfortunately, for many general-purpose applications, available hardware resources of a GPGPU are not efficiently utilized, leading to lost opportunity in improving performance. A major cause of this is the inefficiency of current warp scheduling policies in tolerating long memory latencies.},
  language = {en},
  author = {Jog, Adwait and Kayiran, Onur and Nachiappan, Nachiappan Chidambaram and Mishra, Asit K and Kandemir, Mahmut T and Mutlu, Onur and Iyer, Ravishankar and Das, Chita R},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/4ELFX3AD/Jog et al. - OWL cooperative thread array aware scheduling tec.pdf}
}

@article{howardRelativisticRedBlackTrees,
  title = {Relativistic {{Red}}-{{Black Trees}}},
  abstract = {Operating system performance and scalability on sharedmemory many-core systems depends critically on efficient access to shared data structures. Scalability has proven difficult to achieve for many data structures. In this paper we present a novel and highly scalable concurrent red-black tree.},
  language = {en},
  author = {Howard, Philip W and Walpole, Jonathan},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/7ENPHKTY/Howard and Walpole - Relativistic Red-Black Trees.pdf}
}

@article{delimitrouParagonQoSawareScheduling,
  title = {Paragon: {{QoS}}-Aware Scheduling for Heterogeneous Datacenters},
  abstract = {Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty to match applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online and do not scale beyond few applications.},
  language = {en},
  author = {Delimitrou, Christina and Kozyrakis, Christos},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/K6CFFSSI/Delimitrou and Kozyrakis - Paragon QoS-aware scheduling for heterogeneous da.pdf}
}

@article{Intel64IA32,
  title = {{{Intel}}\textregistered{} 64 and {{IA}}-32 {{Architectures Software Developer}}'s {{Manual}}, {{Volume 3A}}: {{System Programming Guide}}, {{Part}} 1},
  language = {en},
  pages = {458},
  file = {/Users/luigi/work/zotero/storage/E2YYP6HY/Intel® 64 and IA-32 Architectures Software Develop.pdf}
}

@article{choiEfficientSchedulingScheme2013,
  title = {An Efficient Scheduling Scheme Using Estimated Execution Time for Heterogeneous Computing Systems},
  volume = {65},
  issn = {0920-8542, 1573-0484},
  doi = {10.1007/s11227-013-0870-6},
  language = {en},
  number = {2},
  journal = {The Journal of Supercomputing},
  author = {Choi, Hong Jun and Son, Dong Oh and Kang, Seung Gu and Kim, Jong Myon and Lee, Hsien-Hsin and Kim, Cheol Hong},
  month = aug,
  year = {2013},
  pages = {886-902},
  file = {/Users/luigi/work/zotero/storage/SXVZYX4Z/Choi et al. - 2013 - An efficient scheduling scheme using estimated exe.pdf}
}

@article{bradyProgrammingIDRISTutorial,
  title = {Programming in {{IDRIS}}: A Tutorial},
  language = {en},
  author = {Brady, Edwin},
  pages = {47},
  file = {/Users/luigi/work/zotero/storage/SEP6AFKJ/Brady - Programming in IDRIS a tutorial.pdf}
}

@incollection{bouajjaniVerifyingConcurrentPrograms2013,
  address = {Berlin, Heidelberg},
  title = {Verifying {{Concurrent Programs}} against {{Sequential Specifications}}},
  volume = {7792},
  isbn = {978-3-642-37035-9 978-3-642-37036-6},
  abstract = {We investigate the algorithmic feasibility of checking whether concurrent implementations of shared-memory objects adhere to their given sequential specifications; sequential consistency, linearizability, and conflict serializability are the canonical variations of this problem. While verifying sequential consistency of systems with unbounded concurrency is known to be undecidable, we demonstrate that conflict serializability, and linearizability with fixed linearization points are EXPSPACEcomplete, while the general linearizability problem is undecidable.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bouajjani, Ahmed and Emmi, Michael and Enea, Constantin and Hamza, Jad},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Felleisen, Matthias and Gardner, Philippa},
  year = {2013},
  pages = {290-309},
  file = {/Users/luigi/work/zotero/storage/RZPPRG2L/Bouajjani et al. - 2013 - Verifying Concurrent Programs against Sequential S.pdf},
  doi = {10.1007/978-3-642-37036-6_17}
}

@article{bonettaTigerQuollParallelEventbased,
  title = {{{TigerQuoll}}: {{Parallel Event}}-Based {{JavaScript}}},
  abstract = {JavaScript, the most popular language on the Web, is rapidly moving to the server-side, becoming even more pervasive. Still, JavaScript lacks support for shared memory parallelism, making it challenging for developers to exploit multicores present in both servers and clients. In this paper we present TigerQuoll, a novel API and runtime for parallel programming in JavaScript. TigerQuoll features an event-based API and a parallel runtime allowing applications to exploit a mutable shared memory space. The programming model of TigerQuoll features automatic consistency and concurrency management, such that developers do not have to deal with shared-data synchronization. TigerQuoll supports an innovative transaction model that allows for eventual consistency to speed up high-contention workloads. Experiments show that TigerQuoll applications scale well, allowing one to implement common parallelism patterns in JavaScript.},
  language = {en},
  author = {Bonetta, Daniele and Binder, Walter and Pautasso, Cesare},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/443W7XS9/Bonetta et al. - TigerQuoll Parallel Event-based JavaScript.pdf}
}

@article{anandOrchestratedSurveyAutomated,
  title = {An {{Orchestrated Survey}} on {{Automated Software Test Case Generation}} \$},
  abstract = {Test case generation is among the most labour-intensive tasks in software testing and also one that has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has also been one of the most active topics in the research on software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by worldrenowned active researchers on the technique, and briefly covers the basic ideas underlying the technique, the current state of art, a discussion of the open research problems, and a perspective of the future development in the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring comprehensiveness and authoritativeness.},
  language = {en},
  author = {Anand, Saswat and Burke, Edmund and Chen, Tsong Yueh and Clark, John and Cohen, Myra B and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and McMinn, Phil and Bertolino, Antonia and Li, J Jenny and Zhu, Hong},
  pages = {28},
  file = {/Users/luigi/work/zotero/storage/MSJ7JEF4/Anand et al. - An Orchestrated Survey on Automated Software Test .pdf}
}

@article{albertHeapSpaceAnalysis2013,
  title = {Heap Space Analysis for Garbage Collected Languages},
  volume = {78},
  issn = {01676423},
  doi = {10.1016/j.scico.2012.10.008},
  abstract = {Accurately predicting the dynamic memory consumption (or heap space) of programs can be critical during software development. It is well-known that garbage collection (GC) complicates such problem. The peak heap consumption of a program is the maximum size of the data on the heap during its execution, i.e., the minimum amount of heap space needed to safely run the program. Existing heap space analyses either do not take deallocation into account or adopt specific models of garbage collectors which do not necessarily correspond to the actual memory usage. This paper presents a novel static analysis for garbagecollected imperative languages that infers accurate upper bounds on the peak heap usage, including exponential, logarithmic and polynomial bounds. A unique characteristic of the analysis is that it is parametric on the notion of object lifetime, i.e., on when objects become collectible.},
  language = {en},
  number = {9},
  journal = {Science of Computer Programming},
  author = {Albert, Elvira and Genaim, Samir and {G\'omez-Zamalloa}, Miguel},
  month = sep,
  year = {2013},
  pages = {1427-1448},
  file = {/Users/luigi/work/zotero/storage/FT5FJG2Y/Albert et al. - 2013 - Heap space analysis for garbage collected language.pdf}
}

@article{robinsonRewriteRulesDisciplined,
  title = {Rewrite Rules for the {{Disciplined Disciple Compiler}}},
  language = {en},
  author = {Robinson, Amos and Lippmeier, Ben and Chakravarty, Manuel},
  pages = {56},
  file = {/Users/luigi/work/zotero/storage/PAM69Q9A/Robinson et al. - Rewrite rules for the Disciplined Disciple Compile.pdf}
}

@incollection{zhengTurboDiSLPartial2012,
  address = {Berlin, Heidelberg},
  title = {Turbo {{DiSL}}: {{Partial Evaluation}} for {{High}}-{{Level Bytecode Instrumentation}}},
  volume = {7304},
  isbn = {978-3-642-30560-3 978-3-642-30561-0},
  shorttitle = {Turbo {{DiSL}}},
  abstract = {Bytecode instrumentation is a key technique for the implementation of dynamic program analysis tools such as profilers and debuggers. Traditionally, bytecode instrumentation has been supported by low-level bytecode engineering libraries that are difficult to use. Recently, the domain-specific aspect language DiSL has been proposed to provide high-level abstractions for the rapid development of efficient bytecode instrumentations. While DiSL supports user-defined expressions that are evaluated at weave-time, the DiSL programming model requires these expressions to be implemented in separate classes, thus increasing code size and impairing code readability and maintenance. In addition, the DiSL weaver may produce a significant amount of dead code, which may impair some optimizations performed by the runtime. In this paper we introduce Turbo, a novel partial evaluator for DiSL, which processes the generated instrumentation code, performs constant propagation, conditional reduction, and pattern-based code simplification, and executes pure methods at weave-time. With Turbo, it is often unnecessary to wrap expressions for evaluation at weave-time in separate classes, thus simplifying the programming model. We present Turbo's partial evaluation algorithm and illustrate its benefits with several case studies. We evaluate the impact of Turbo on weave-time performance and on runtime performance of the instrumented application.},
  language = {en},
  booktitle = {Objects, {{Models}}, {{Components}}, {{Patterns}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Zheng, Yudi and Ansaloni, Danilo and Marek, Lukas and Sewe, Andreas and Binder, Walter and Villaz\'on, Alex and Tuma, Petr and Qi, Zhengwei and Mezini, Mira},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Furia, Carlo A. and Nanz, Sebastian},
  year = {2012},
  pages = {353-368},
  file = {/Users/luigi/work/zotero/storage/MPP6KGTQ/Zheng et al. - 2012 - Turbo DiSL Partial Evaluation for High-Level Byte.pdf},
  doi = {10.1007/978-3-642-30561-0_24}
}

@inproceedings{zhangThreelevelComponentModel2012,
  address = {Dresden, Germany},
  title = {A Three-Level Component Model in Component Based Software Development},
  isbn = {978-1-4503-1129-8},
  doi = {10.1145/2371401.2371412},
  abstract = {Component-based development promotes a software development process that focuses on component reuse. How to describe a desired component before searching in the repository? How to find an existing component that fulfills the required functionalities? How to capture the system personalization based on its constitutive components' customization? To answer these questions, this paper claims that components should be described using three different forms at three development stages: architecture specification, configuration and assembly. However, no architecture description language proposes such a detailed description for components that supports such a three step component-based development. This paper proposes a three-level Adl, named Dedal, that enables the explicit and separate definitions of component roles, component classes, and component instances.},
  language = {en},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}} - {{GPCE}} '12},
  publisher = {{ACM Press}},
  author = {Zhang, Huaxi (Yulin) and Zhang, Lei and Urtado, Christelle and Vauttier, Sylvain and Huchard, Marianne},
  year = {2012},
  pages = {70},
  file = {/Users/luigi/work/zotero/storage/G9IRISI6/Zhang et al. - 2012 - A three-level component model in component based s.pdf}
}

@inproceedings{zaparanuksEssenceMeasureMaintainability2012,
  address = {Zurich, Switzerland},
  title = {Is Essence a Measure of Maintainability?},
  isbn = {978-1-4673-1859-4 978-1-4673-1858-7},
  doi = {10.1109/USER.2012.6226579},
  abstract = {We recently published a paper at ECOOP presenting a new software design metric, essence, that quantifies the amount of indirection in a software design. The reviews were overwhelmingly positive and included statements such as ``The evaluation of the metric is fantastic.'' However, we also received feedback from senior researchers who do not believe that we have meaningfully evaluated our metric. This paper represents our effort towards a meaningful evaluation of essence. Given our lack of experience in human-subject studies, we hope to receive valuable feedback on our proposed study design.},
  language = {en},
  booktitle = {2012 {{First International Workshop}} on {{User Evaluation}} for {{Software Engineering Researchers}} ({{USER}})},
  publisher = {{IEEE}},
  author = {Zaparanuks, Dmitrijs and Hauswirth, Matthias},
  month = jun,
  year = {2012},
  pages = {31-34},
  file = {/Users/luigi/work/zotero/storage/A4EXU3NN/Zaparanuks and Hauswirth - 2012 - Is essence a measure of maintainability.pdf}
}

@article{zaparanuksAlgorithmicProfiling,
  title = {Algorithmic Profiling},
  abstract = {Traditional profilers identify where a program spends most of its resources. They do not provide information about why the program spends those resources or about how resource consumption would change for different program inputs. In this paper we introduce the idea of algorithmic profiling. While a traditional profiler determines a set of measured cost values, an algorithmic profiler determines a cost function. It does that by automatically determining the ``inputs'' of a program, by measuring the program's ``cost'' for any given input, and by inferring an empirical cost function.},
  language = {en},
  author = {Zaparanuks, Dmitrijs and Hauswirth, Matthias},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/HSRXMF69/Zaparanuks and Hauswirth - Algorithmic profiling.pdf}
}

@article{wamhoffFASTLANEStreamliningTransactions,
  title = {{{FASTLANE}}: {{Streamlining Transactions}} for {{Low Thread Counts}}},
  abstract = {Software transactional memory (STM) can lead to scalable implementations of concurrent programs, as the relative performance of an application increases with the number of threads that support it. However, the absolute performance is typically impaired by the overheads of transaction management and instrumented accesses to shared memory. This often leads STM-based programs with low thread counts to perform worse than a sequential, non-instrumented version of the same application.},
  language = {en},
  author = {Wamhoff, Jons-Tobias and Fetzer, Christof and Felber, Pascal and Riviere, Etienne and Muller, Gilles},
  pages = {8},
  file = {/Users/luigi/work/zotero/storage/2MIN8ZRS/Wamhoff et al. - FASTLANE Streamlining Transactions for Low Thread.pdf}
}

@article{turonReagentsExpressingComposing,
  title = {Reagents: {{Expressing}} and {{Composing Fine}}-Grained {{Concurrency}}},
  abstract = {Efficient communication and synchronization is crucial for finegrained parallelism. Libraries providing such features, while indispensable, are difficult to write, and often cannot be tailored or composed to meet the needs of specific users. We introduce reagents, a set of combinators for concisely expressing concurrency algorithms. Reagents scale as well as their hand-coded counterparts, while providing the composability existing libraries lack.},
  language = {en},
  author = {Turon, Aaron},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/5WE8V7FN/Turon - Reagents Expressing and Composing Fine-grained Co.pdf}
}

@incollection{streitSambambaRuntimeSystem2012,
  address = {Berlin, Heidelberg},
  title = {Sambamba: {{A Runtime System}} for {{Online Adaptive Parallelization}}},
  volume = {7210},
  isbn = {978-3-642-28651-3 978-3-642-28652-0},
  shorttitle = {Sambamba},
  abstract = {How can we exploit a microprocessor as efficiently as possible? The ``classic'' approach is static optimization at compile-time, optimizing a program for all possible uses. Further optimization can only be achieved by anticipating the actual usage profile: If we know, for instance, that two computations will be independent, we can run them in parallel. In the Sambamba project, we replace anticipation by adaptation. Our runtime system provides the infrastructure for implementing runtime adaptive and speculative transformations. We demonstrate our framework in the context of adaptive parallelization. We show the fully automatic parallelization of a small irregular C program in combination with our adaptive runtime system. The result is a parallel execution which adapts to the availability of idle system resources. In our example, this enables a 1.92 fold speedup on two cores while still preventing oversubscription of the system.},
  language = {en},
  booktitle = {Compiler {{Construction}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Streit, Kevin and Hammacher, Clemens and Zeller, Andreas and Hack, Sebastian},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and O'Boyle, Michael},
  year = {2012},
  pages = {240-243},
  file = {/Users/luigi/work/zotero/storage/CA9JILVV/Streit et al. - 2012 - Sambamba A Runtime System for Online Adaptive Par.pdf},
  doi = {10.1007/978-3-642-28652-0_13}
}

@article{strattonParboilRevisedBenchmark,
  title = {Parboil: {{A Revised Benchmark Suite}} for {{Scientific}} and {{Commercial Throughput Computing}}},
  abstract = {The Parboil benchmarks are a set of throughput computing applications useful for studying the performance of throughput computing architecture and compilers. The name comes from the culinary term for a partial cooking process, which represents our belief that useful throughput computing benchmarks must be ``cooked'', or preselected to implement a scalable algorithm with fine-grained parallel tasks. But useful benchmarks for this field cannot be ``fully cooked'', because the architectures and programming models and supporting tools are evolving rapidly enough that static benchmark codes will lose relevance very quickly.},
  language = {en},
  author = {Stratton, John A and Rodrigues, Christopher and Sung, I-Jui and Obeid, Nady and Chang, Li-Wen and Anssari, Nasser and Liu, Geng Daniel and Hwu, Wen-mei W},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/MR73VCPZ/Stratton et al. - Parboil A Revised Benchmark Suite for Scientiﬁc a.pdf}
}

@inproceedings{spacekInheritanceSystemStructural2012,
  address = {Dresden, Germany},
  title = {An Inheritance System for Structural \& Behavioral Reuse in Component-Based Software Programming},
  isbn = {978-1-4503-1129-8},
  doi = {10.1145/2371401.2371411},
  abstract = {In the context of Component-based Programming, which addresses the implementation stage of a component-based software engineering development process, this paper describes a specification and an operational integration of an inheritance system into a self-contained new componentbased programming language named COMPO. Our proposal completes and extends related works by making it possible to apply inheritance to the full description of components, i.e. both to structural (description of provisions and requirements, of component architecture) and behavioral (full implementations of services) parts in component descriptions. Inheritance in COMPO is designed to be used in conjunction with composition to maximize reuse capabilities and expressive power. COMPO implementation proposes a clear operational solution for inheritance and for achieving and testing substitutions.},
  language = {en},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}} - {{GPCE}} '12},
  publisher = {{ACM Press}},
  author = {Spacek, Petr and Dony, Christophe and Tibermacine, Chouki and Fabresse, Luc},
  year = {2012},
  pages = {60},
  file = {/Users/luigi/work/zotero/storage/ZN7R4WU2/Spacek et al. - 2012 - An inheritance system for structural & behavioral .pdf}
}

@article{skrupskyDonRepeatYourself,
  title = {Don't {{Repeat Yourself}}: {{Automatically Synthesizing Client}}-Side {{Validation Code}} for {{Web Applications}}},
  abstract = {We outline the groundwork for a new software development approach where developers author the serverside application logic and rely on tools to automatically synthesize the corresponding client-side application logic. Our approach uses program analysis techniques to extract a logical specification from the server and synthesizes client code from that specification. Our implementation (WAVES) synthesizes interactive client interfaces that include asynchronous callbacks whose performance and coverage rival that of manually written clients, while ensuring that no new security vulnerabilities are introduced.},
  language = {en},
  author = {Skrupsky, Nazari and Monshizadeh, Maliheh and Venkatakrishnan, V N and Bisht, Prithvi and Hinrichs, Timothy and Zuck, Lenore},
  pages = {2},
  file = {/Users/luigi/work/zotero/storage/KYQTIBTT/Skrupsky et al. - Don’t Repeat Yourself Automatically Synthesizing .pdf}
}

@article{simPerformanceAnalysisFramework,
  title = {A {{Performance Analysis Framework}} for {{Identifying Potential Benefits}} in {{GPGPU Applications}}},
  abstract = {Tuning code for GPGPU and other emerging many-core platforms is a challenge because few models or tools can precisely pinpoint the root cause of performance bottlenecks. In this paper, we present a performance analysis framework that can help shed light on such bottlenecks for GPGPU applications. Although a handful of GPGPU profiling tools exist, most of the traditional tools, unfortunately, simply provide programmers with a variety of measurements and metrics obtained by running applications, and it is often difficult to map these metrics to understand the root causes of slowdowns, much less decide what next optimization step to take to alleviate the bottleneck. In our approach, we first develop an analytical performance model that can precisely predict performance and aims to provide programmer-interpretable metrics. Then, we apply static and dynamic profiling to instantiate our performance model for a particular input code and show how the model can predict the potential performance benefits. We demonstrate our framework on a suite of micro-benchmarks as well as a variety of computations extracted from real codes.},
  language = {en},
  author = {Sim, Jaewoong and Dasgupta, Aniruddha and Kim, Hyesoon and Vuduc, Richard},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/7PLIGCUF/Sim et al. - A Performance Analysis Framework for Identifying P.pdf}
}

@article{samadiAdaptiveInputawareCompilation,
  title = {Adaptive {{Input}}-Aware {{Compilation}} for {{Graphics Engines}}},
  abstract = {While graphics processing units (GPUs) provide low-cost and efficient platforms for accelerating high performance computations, the tedious process of performance tuning required to optimize applications is an obstacle to wider adoption of GPUs. In addition to the programmability challenges posed by GPU's complex memory hierarchy and parallelism model, a well-known application design problem is target portability across different GPUs. However, even for a single GPU target, changing a program's input characteristics can make an already-optimized implementation of a program perform poorly. In this work, we propose Adaptic, an adaptive input-aware compilation system to tackle this important, yet overlooked, input portability problem. Using this system, programmers develop their applications in a high-level streaming language and let Adaptic undertake the difficult task of input portable optimizations and code generation. Several input-aware optimizations are introduced to make efficient use of the memory hierarchy and customize thread composition. At runtime, a properly optimized version of the application is executed based on the actual program input. We perform a head-to-head comparison between the Adaptic generated and hand-optimized CUDA programs. The results show that Adaptic is capable of generating codes that can perform on par with their hand-optimized counterparts over certain input ranges and outperform them when the input falls out of the hand-optimized programs' ``comfort zone''. Furthermore, we show that input-aware results are sustainable across different GPU targets making it possible to write and optimize applications once and run them anywhere.},
  language = {en},
  author = {Samadi, Mehrzad and Hormati, Amir and Mehrara, Mojtaba and Lee, Janghaeng and Mahlke, Scott},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/7KEEZ3GW/Samadi et al. - Adaptive Input-aware Compilation for Graphics Engi.pdf}
}

@inproceedings{raysideSynthesizingIteratorsAbstraction2012,
  address = {Dresden, Germany},
  title = {Synthesizing Iterators from Abstraction Functions},
  isbn = {978-1-4503-1129-8},
  doi = {10.1145/2371401.2371407},
  abstract = {A technique for synthesizing iterators from declarative abstraction functions written in a relational logic specification language is described. The logic includes a transitive closure operator that makes it convenient for expressing reachability queries on linked data structures. Some optimizations, including tuple elimination, iterator flattening, and traversal state reduction, are used to improve performance of the generated iterators.},
  language = {en},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}} - {{GPCE}} '12},
  publisher = {{ACM Press}},
  author = {Rayside, Derek and Montaghami, Vajihollah and Leung, Francesca and Yuen, Albert and Xu, Kevin and Jackson, Daniel},
  year = {2012},
  pages = {31},
  file = {/Users/luigi/work/zotero/storage/4RZK6JFR/Rayside et al. - 2012 - Synthesizing iterators from abstraction functions.pdf}
}

@inproceedings{ramachandraProgramAnalysisTransformation2012,
  address = {Beijing, China},
  title = {Program Analysis and Transformation for Holistic Optimization of Database Applications},
  isbn = {978-1-4503-1490-9},
  doi = {10.1145/2259051.2259057},
  abstract = {We describe DBridge, a novel program analysis and transformation tool to optimize database and web service access. Traditionally, rewrite of queries and programs are done independently, by the database query optimizer and the language compiler respectively, leaving out many optimization opportunities. Our tool aims to bridge this gap by performing holistic transformations, which include both program and query rewrite.},
  language = {en},
  booktitle = {Proceedings of the {{ACM SIGPLAN International Workshop}} on {{State}} of the {{Art}} in {{Java Program}} Analysis - {{SOAP}} '12},
  publisher = {{ACM Press}},
  author = {Ramachandra, Karthik and Guravannavar, Ravindra and Sudarshan, S.},
  year = {2012},
  pages = {39-44},
  file = {/Users/luigi/work/zotero/storage/CA5TH2D3/Ramachandra et al. - 2012 - Program analysis and transformation for holistic o.pdf}
}

@inproceedings{pradelStaticDetectionBrittle2012,
  address = {Minneapolis, MN, USA},
  title = {Static Detection of Brittle Parameter Typing},
  isbn = {978-1-4503-1454-1},
  doi = {10.1145/2338965.2336785},
  abstract = {To avoid receiving incorrect arguments, a method specifies the expected type of each formal parameter. However, some parameter types are too general and have subtypes that the method does not expect as actual argument types. For example, this may happen if there is no common supertype that precisely describes all expected types. As a result of such brittle parameter typing, a caller may accidentally pass arguments unexpected by the callee without any warnings from the type system. This paper presents a fully automatic, static analysis to find brittle parameter typing and unexpected arguments given to brittle parameters. First, the analysis infers from callers of a method the types that arguments commonly have. Then, the analysis reports potentially unexpected arguments that stand out by having an unusual type. We apply the approach to 21 real-world Java programs that use the Swing API, an API providing various methods with brittle parameters. The analysis reveals 15 previously unknown bugs and code smells where programmers pass arguments that are compatible with the declared parameter type but nevertheless unexpected by the callee. The warnings reported by the analysis have 47\% precision and 83\% recall.},
  language = {en},
  booktitle = {Proceedings of the 2012 {{International Symposium}} on {{Software Testing}} and {{Analysis}} - {{ISSTA}} 2012},
  publisher = {{ACM Press}},
  author = {Pradel, Michael and Heiniger, Severin and Gross, Thomas R.},
  year = {2012},
  pages = {265},
  file = {/Users/luigi/work/zotero/storage/9YI4H7GN/Pradel et al. - 2012 - Static detection of brittle parameter typing.pdf}
}

@article{kimPerformanceAnalysisTuning2012,
  title = {Performance {{Analysis}} and {{Tuning}} for {{General Purpose Graphics Processing Units}} ({{GPGPU}})},
  volume = {7},
  issn = {1935-3235, 1935-3243},
  doi = {10.2200/S00451ED1V01Y201209CAC020},
  language = {en},
  number = {2},
  journal = {Synthesis Lectures on Computer Architecture},
  author = {Kim, Hyesoon and Vuduc, Richard and Baghsorkhi, Sara and Choi, Jee and Hwu, Wen-mei},
  month = nov,
  year = {2012},
  pages = {1-96},
  file = {/Users/luigi/work/zotero/storage/YQENIY6U/Kim et al. - 2012 - Performance Analysis and Tuning for General Purpos.pdf}
}

@inproceedings{kellJVMNotObservable2012,
  address = {Tucson, Arizona, USA},
  title = {The {{JVM}} Is Not Observable Enough (and What to Do about It)},
  isbn = {978-1-4503-1633-0},
  doi = {10.1145/2414740.2414747},
  abstract = {Bytecode instrumentation is a preferred technique for building profiling, debugging and monitoring tools targeting the Java Virtual Machine (JVM), yet is fundamentally dangerous. We illustrate its dangers with several examples gathered while building the DiSL instrumentation framework. We argue that no Java platform mechanism provides simultaneously adequate performance, reliability and expressiveness, but that this weakness is fixable. To elaborate, we contrast internal with external observation, and sketch some approaches and requirements for a hybrid mechanism.},
  language = {en},
  booktitle = {Proceedings of the Sixth {{ACM}} Workshop on {{Virtual}} Machines and Intermediate Languages - {{VMIL}} '12},
  publisher = {{ACM Press}},
  author = {Kell, Stephen and Ansaloni, Danilo and Binder, Walter and Marek, Luk\'a{\v s}},
  year = {2012},
  pages = {33},
  file = {/Users/luigi/work/zotero/storage/XRLRWGZT/Kell et al. - 2012 - The JVM is not observable enough (and what to do a.pdf}
}

@article{imamIntegratingTaskParallelism,
  title = {Integrating Task Parallelism with Actors},
  abstract = {This paper introduces a unified concurrent programming model combining the previously developed Actor Model (AM) and the task-parallel Async-Finish Model (AFM). With the advent of multi-core computers, there is a renewed interest in programming models that can support a wide range of parallel programming patterns. The proposed unified model shows how the divide-and-conquer approach of the AFM and the no-shared mutable state and event-driven philosophy of the AM can be combined to solve certain classes of problems more efficiently and productively than either of the aforementioned models individually. The unified model adds actor creation and coordination to the AFM, while also enabling parallelization within actors. This paper describes two implementations of the unified model as extensions of Habanero-Java and Habanero-Scala. The unified model adds to the foundations of parallel programs, and to the tools available for the programmer to aid in productivity and performance while developing parallel software.},
  language = {en},
  author = {Imam, Shams M and Sarkar, Vivek},
  pages = {19},
  file = {/Users/luigi/work/zotero/storage/YMWGD8NL/Imam and Sarkar - Integrating task parallelism with actors.pdf}
}

@inproceedings{freemanHotDrinkLibraryWeb2012,
  address = {Dresden, Germany},
  title = {{{HotDrink}}: A Library for Web User Interfaces},
  isbn = {978-1-4503-1129-8},
  shorttitle = {{{HotDrink}}},
  doi = {10.1145/2371401.2371413},
  abstract = {HotDrink is a JavaScript library for constructing forms, dialogs, and other common user interfaces for Web applications. With HotDrink, instead of writing event handlers, developers declare a ``view-model'' in JavaScript and a set of ``bindings'' between the view-model and the HTML elements comprising the view. These specifications tend to be small, but they are enough for HotDrink to provide a fully operational GUI with multi-way dataflows, enabling/disabling of values, activation/deactivation of commands, and data validation. HotDrink implements these rich behaviors, expected of high-quality user interfaces, as generic reusable algorithms. This paper/tool demonstration introduces developers to the HotDrink library by stepping through the construction of an example web application GUI.},
  language = {en},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}} - {{GPCE}} '12},
  publisher = {{ACM Press}},
  author = {Freeman, John and J\"arvi, Jaakko and Foust, Gabriel},
  year = {2012},
  pages = {80},
  file = {/Users/luigi/work/zotero/storage/FELES2VW/Freeman et al. - 2012 - HotDrink a library for web user interfaces.pdf}
}

@inproceedings{dietlVerificationGamesMaking2012,
  address = {Beijing, China},
  title = {Verification Games: Making Verification Fun},
  isbn = {978-1-4503-1272-1},
  shorttitle = {Verification Games},
  doi = {10.1145/2318202.2318210},
  abstract = {Program verification is the only way to be certain that a given piece of software is free of (certain types of) errors \textemdash{} errors that could otherwise disrupt operations in the field. To date, formal verification has been done by specially-trained engineers. Labor costs have heretofore made formal verification too costly to apply beyond small, critical software components.},
  language = {en},
  booktitle = {Proceedings of the 14th {{Workshop}} on {{Formal Techniques}} for {{Java}}-like {{Programs}} - {{FTfJP}} '12},
  publisher = {{ACM Press}},
  author = {Dietl, Werner and Dietzel, Stephanie and Ernst, Michael D. and Mote, Nathaniel and Walker, Brian and Cooper, Seth and Pavlik, Timothy and Popovi\'c, Zoran},
  year = {2012},
  pages = {42-49},
  file = {/Users/luigi/work/zotero/storage/HX44LTMV/Dietl et al. - 2012 - Verification games making verification fun.pdf}
}

@article{chenAchievingApplicationCentricPerformance,
  title = {Achieving {{Application}}-{{Centric Performance Targets}} via {{Consolidation}} on {{Multicores}}: {{Myth}} or {{Reality}}?},
  abstract = {Consolidation of multiple applications with diverse and changing resource requirements is common in multicore systems as hardware resources are abundant and opportunities for better system usage are plenty. Can we maximize resource usage in such a system while respecting individual application performance targets or is it an oxymoron to simultaneously meet such conflicting measures? In this work we provide a solution to the above difficult problem by constructing a queueing-theory based tool that we use to accurately predict application scalability on multicores and that can also provide the optimal consolidation suggestions to maximize system resource usage while meeting simultaneously application performance targets. The proposed methodology is light-weight and relies on capturing application resource demands using standard tools, via nonintrusive low-level measurements. We evaluate our approach on an IBM Power7 system using the DaCapo and SPECjvm benchmark suites where each benchmark exhibits different patterns of parallelism. From 900 different consolidations of application instances, our tool accurately predicts the average iteration time of collocated applications with an average error below 10\%.},
  language = {en},
  author = {Chen, Lydia Y and Ansaloni, Danilo and Smirni, Evgenia},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/SVXJH26Z/Chen et al. - Achieving Application-Centric Performance Targets .pdf}
}

@article{bonettaParallelEventbasedJavaScript,
  title = {Parallel {{Event}}-Based {{JavaScript}}},
  abstract = {JavaScript, the most popular language on the Web, is also rapidly moving to the serverside, becoming even more pervasive. However, JavaScript lacks support for shared memory parallelism, making it challenging for developers to exploit multicores present in both the server and the client. In this paper we present TQ, a novel API for parallel programming in JavaScript. TQ features a low-level event-based programming model allowing applications to exploit shared memory parallelism, which can be conveniently used for expressing any arbitrary parallel programming pattern. This document presents the TQ programming model, and describes how the TQ API can be exploited to build some popular higher-level parallel programming patterns in JavaScript.},
  language = {en},
  author = {Bonetta, Daniele and Pautasso, Cesare and Binder, Walter},
  pages = {17},
  file = {/Users/luigi/work/zotero/storage/U4KQ4PE8/Bonetta et al. - Parallel Event-based JavaScript.pdf}
}

@article{bonettaScriptingLanguageHighPerformance,
  title = {S: A {{Scripting Language}} for {{High}}-{{Performance RESTful Web Services}}},
  abstract = {There is an urgent need for novel programming abstractions to leverage the parallelism in modern multicore machines. We introduce S, a new domain-specific language targeting the server-side scripting of high-performance RESTful Web services. S promotes an innovative programming model based on explicit (control-flow) and implicit (process-level) parallelism control, allowing the service developer to specify which portions of the control-flow should be executed in parallel. For each service, the choice of the best level of parallelism is left to the runtime system. We assess performance and scalability by implementing two non-trivial composite Web services in S. Experiments show that S-based Web services can handle thousands of concurrent client requests on a modern multicore machine.},
  language = {en},
  author = {Bonetta, Daniele and Peternier, Achille and Pautasso, Cesare and Binder, Walter},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/BCZ6CDL2/Bonetta et al. - S a Scripting Language for High-Performance RESTf.pdf}
}

@inproceedings{axelsenPackageTemplatesDefinition2012,
  address = {Dresden, Germany},
  title = {Package Templates: A Definition by Semantics-Preserving Source-to-Source Transformations to Efficient {{Java}} Code},
  isbn = {978-1-4503-1129-8},
  shorttitle = {Package Templates},
  doi = {10.1145/2371401.2371409},
  abstract = {Package Templates (PT) is a mechanism designed for writing reusable modules, called templates, each consisting of a set of classes that can be adapted to their use in a program through compile-time specialization. A template must be instantiated in a program before its classes can be used. The mechanism supports type-safe renaming, merging, type parameterization and refinement in the form of static additions and overrides that are orthogonal to the corresponding concepts of ordinary inheritance.},
  language = {en},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}} - {{GPCE}} '12},
  publisher = {{ACM Press}},
  author = {Axelsen, Eyvind W. and Krogdahl, Stein},
  year = {2012},
  pages = {50},
  file = {/Users/luigi/work/zotero/storage/SWCQIVH5/Axelsen and Krogdahl - 2012 - Package templates a definition by semantics-prese.pdf}
}

@incollection{atigWhatDecidableWeak2012,
  address = {Berlin, Heidelberg},
  title = {What's {{Decidable}} about {{Weak Memory Models}}?},
  volume = {7211},
  isbn = {978-3-642-28868-5 978-3-642-28869-2},
  abstract = {We investigate the decidability of the state reachability problem in finite-state programs running under weak memory models. In [3], we have shown that this problem is decidable for TSO and its extension with the write-to-write order relaxation, but beyond these models nothing is known to be decidable. Moreover, we have shown that relaxing the program order by allowing reads or writes to overtake reads leads to undecidability. In this paper, we refine these results by sharpening the (un)decidability frontiers on both sides. On the positive side, we introduce a new memory model NSW (for non-speculative writes) that extends TSO with the write-to-write relaxation, the read-to-read relaxation, and support for partial fences. We present a backtrack-free operational model for NSW, and prove that it does not allow causal cycles (thus barring pathological out-of-thin-air effects). On the negative side, we show that adding the read-to-write relaxation to TSO causes undecidability, and that adding non-atomic writes to NSW also causes undecidability. Our results establish that NSW is the first known hardware-centric memory model that is relaxed enough to permit both delayed execution of writes and early execution of reads for which the reachability problem is decidable.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Atig, Mohamed Faouzi and Bouajjani, Ahmed and Burckhardt, Sebastian and Musuvathi, Madanlal},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Seidl, Helmut},
  year = {2012},
  pages = {26-46},
  file = {/Users/luigi/work/zotero/storage/278HESAI/Atig et al. - 2012 - What’s Decidable about Weak Memory Models.pdf},
  doi = {10.1007/978-3-642-28869-2_2}
}

@article{aroraRedefiningRoleCPU2012,
  title = {Redefining the {{Role}} of the {{CPU}} in the {{Era}} of {{CPU}}-{{GPU Integration}}},
  volume = {32},
  issn = {0272-1732},
  doi = {10.1109/MM.2012.57},
  language = {en},
  number = {6},
  journal = {IEEE Micro},
  author = {Arora, Manish and Nath, Siddhartha and Mazumdar, Subhra and Baden, Scott B. and Tullsen, Dean M.},
  month = nov,
  year = {2012},
  pages = {4-16},
  file = {/Users/luigi/work/zotero/storage/J2S3PVJT/Arora et al. - 2012 - Redefining the Role of the CPU in the Era of CPU-G.pdf}
}

@inproceedings{ansaloniDeferredMethodsAccelerating2012,
  address = {San Jose, California},
  title = {Deferred Methods: Accelerating Dynamic Program Analysis on Multicores},
  isbn = {978-1-4503-1206-6},
  shorttitle = {Deferred Methods},
  doi = {10.1145/2259016.2259048},
  abstract = {Parallelization is attractive for speeding up dynamic program analysis on multicores. However, inter-thread communication overhead may outweigh any benefit from parallel execution. We propose deferred methods, a high-level Java framework to accelerate dynamic analysis on multicores. To minimize inter-thread communication overhead, invocations to analysis methods are automatically aggregated in thread-local buffers that are processed when full. In contrast to other approaches, our framework supports custom buffer processing strategies, eases pre-processing of buffers to reduce contention on shared data structures, and offers a synchronization mechanism to wait for the completion of previously invoked deferred methods. We also present a novel adaptive buffer processing strategy that parallelizes the analysis only when the observed workload leaves some CPU cores under-utilized. Using a profiler as case study, we show that deferred methods with the adaptive buffer processing strategy yield an average speedup of factor 4.09 on a quad-core machine. The speedup stems both from parallelization and from reduced contention.},
  language = {en},
  booktitle = {Proceedings of the {{Tenth International Symposium}} on {{Code Generation}} and {{Optimization}} - {{CHO}} '12},
  publisher = {{ACM Press}},
  author = {Ansaloni, Danilo and Binder, Walter and Heydarnoori, Abbas and Chen, Lydia Y.},
  year = {2012},
  pages = {242},
  file = {/Users/luigi/work/zotero/storage/5RJ96VAE/Ansaloni et al. - 2012 - Deferred methods accelerating dynamic program ana.pdf}
}

@article{aftandilianExploitingVirtualMachine,
  title = {Exploiting {{Virtual Machine Infrastructure To Implement Low}}-{{Overhead Error Checking Tools}}},
  language = {en},
  author = {Aftandilian, Edward Emil},
  pages = {135},
  file = {/Users/luigi/work/zotero/storage/9XMGSMEJ/Aftandilian - Exploiting Virtual Machine Infrastructure To Imple.pdf}
}

@incollection{zaparanuksBeautyBeastSeparating2011,
  address = {Berlin, Heidelberg},
  title = {The {{Beauty}} and the {{Beast}}: {{Separating Design}} from {{Algorithm}}},
  volume = {6813},
  isbn = {978-3-642-22654-0 978-3-642-22655-7},
  shorttitle = {The {{Beauty}} and the {{Beast}}},
  abstract = {We present an approach that partitions a software system into its algorithmically essential parts and the parts that manifest its design. Our approach is inspired by the notion of an algorithm and its asymptotic complexity. However, we do not propose a metric for measuring asymptotic complexity (efficiency). Instead, we use the one aspect of algorithms that drives up their asymptotic complexity \textendash{} repetition, in the form of loops and recursions \textendash{} to determine the algorithmically essential parts of a software system. Those parts of a system that are not algorithmically essential represent aspects of the design. A large fraction of inessential parts is indicative of ``overdesign'', where a small fraction indicates a lack of modularization. We present a metric, relative essence, to quantify the fraction of the program that is algorithmically essential. We evaluate our approach by studying the algorithmic essence of a large corpus of software system, and by comparing the measured essence to an intuitive view of design ``overhead''.},
  language = {en},
  booktitle = {{{ECOOP}} 2011 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Zaparanuks, Dmitrijs and Hauswirth, Matthias},
  editor = {Mezini, Mira},
  year = {2011},
  pages = {27-51},
  file = {/Users/luigi/work/zotero/storage/P6KBFTT4/Zaparanuks and Hauswirth - 2011 - The Beauty and the Beast Separating Design from A.pdf},
  doi = {10.1007/978-3-642-22655-7_3}
}

@article{wurthingerSafeAtomicRuntime,
  title = {Safe and Atomic Run-Time Code Evolution for {{Java}} and Its Application to Dynamic {{AOP}}},
  abstract = {Dynamic updates to running programs improve development productivity and reduce downtime of long-running applications. This feature is however severely limited in current virtual machines for object-oriented languages. In particular, changes to classes often apply only to methods invoked after a class change, but not to active methods on the call stack of threads. Additionally, adding and removing methods as well as fields is often not supported.},
  language = {en},
  author = {Wurthinger, Thomas and Ansaloni, Danilo and Binder, Walter and Wimmer, Christian and Mossenbock, Hanspeter},
  pages = {20},
  file = {/Users/luigi/work/zotero/storage/IEQFPWZI/Wurthinger et al. - Safe and atomic run-time code evolution for Java a.pdf}
}

@article{szafarynExperiencesAchievingPortability,
  title = {Experiences with {{Achieving Portability}} across {{Heterogeneous Architectures}}},
  abstract = {The increasing computational needs of parallel applications inevitably require portability across popular parallel architectures, which are becoming heterogeneous. The lack of a common parallel framework results in divergent code bases, difficulty in porting, higher maintenance cost, and, thus difficulty achieving optimal performance on target architectures.},
  language = {en},
  author = {Szafaryn, Lukasz G and Gamblin, Todd and {de Supinski}, Bronis R and Skadron, Kevin},
  pages = {8},
  file = {/Users/luigi/work/zotero/storage/Q2TFTJYN/Szafaryn et al. - Experiences with Achieving Portability across Hete.pdf}
}

@article{seweCapoConScala,
  title = {Da {{Capo}} Con {{Scala}}: {{Design}} and {{Analysis}} of a {{Scala Benchmark Suite}} for the {{Java Virtual Machine}}},
  abstract = {Originally conceived as the target platform for Java alone, the Java Virtual Machine (JVM) has since been targeted by other languages, one of which is Scala. This trend, however, is not yet reflected by the benchmark suites commonly used in JVM research. In this paper, we thus present the design and analysis of the first full-fledged benchmark suite for Scala. We furthermore compare the benchmarks contained therein with those from the well-known DaCapo 9.12 benchmark suite and show where the differences are between Scala and Java code\textemdash{}and where not.},
  language = {en},
  author = {Sewe, Andreas and Mezini, Mira and Sarimbekov, Aibek and Binder, Walter},
  pages = {20},
  file = {/Users/luigi/work/zotero/storage/R9UW5MIW/Sewe et al. - Da Capo con Scala Design and Analysis of a Scala .pdf}
}

@inproceedings{mistryAnalyzingProgramFlow2011,
  address = {Newport Beach, California},
  title = {Analyzing Program Flow within a Many-Kernel {{OpenCL}} Application},
  isbn = {978-1-4503-0569-3},
  doi = {10.1145/1964179.1964193},
  abstract = {Many developers have begun to realize that heterogeneous multi-core and many-core computer systems can provide significant performance opportunities to a range of applications. Typical applications possess multiple components that can be parallelized; developers need to be equipped with proper performance tools to analyze program flow and identify application bottlenecks. In this paper, we analyze and profile the components of the Speeded Up Robust Features (SURF) Computer Vision algorithm written in OpenCL. Our profiling framework is developed using built-in OpenCL API function calls, without the need for an external profiler. We show we can begin to identify performance bottlenecks and performance issues present in individual components on different hardware platforms. We demonstrate that by using run-time profiling using the OpenCL specification, we can provide an application developer with a fine-grained look at performance, and that this information can be used to tailor performance improvements for specific platforms.},
  language = {en},
  booktitle = {Proceedings of the {{Fourth Workshop}} on {{General Purpose Processing}} on {{Graphics Processing Units}} - {{GPGPU}}-4},
  publisher = {{ACM Press}},
  author = {Mistry, Perhaad and Gregg, Chris and Rubin, Norman and Kaeli, David and Hazelwood, Kim},
  year = {2011},
  pages = {1},
  file = {/Users/luigi/work/zotero/storage/WVI5KBAJ/Mistry et al. - 2011 - Analyzing program flow within a many-kernel OpenCL.pdf}
}

@article{meyerMultiGPUImplementationPerformance,
  title = {A Multi-{{GPU}} Implementation and Performance Model for the Standard Simplex Method},
  abstract = {The standard simplex method is a well-known optimization algorithm for solving linear programming models in the field of operational research. It is part of software that is often employed by businesses for solving scheduling or assignment problems. But their always increasing complexity and size drives the demand for more computational power. In the past few years, GPUs have gained a lot of popularity as they offer an opportunity to accelerate many algorithms. In this paper we present a mono and a multi-GPU implementation of the standard simplex method, which is based on CUDA. Measurements show that it outperforms the CLP solver provided the problem size is large enough. We also derive a performance model and establish its accurateness. To our knowledge, only the revised simplex method has so far been implemented on a GPU.},
  language = {en},
  author = {Meyer, Xavier and Albuquerque, Paul and Chopard, Bastien},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/WLES8MIB/Meyer et al. - A multi-GPU implementation and performance model f.pdf}
}

@inproceedings{mengGROPHECYGPUPerformance2011,
  address = {Seattle, Washington},
  title = {{{GROPHECY}}: {{GPU}} Performance Projection from {{CPU}} Code Skeletons},
  isbn = {978-1-4503-0771-0},
  shorttitle = {{{GROPHECY}}},
  doi = {10.1145/2063384.2063402},
  abstract = {We propose GROPHECY, a GPU performance projection framework that can estimate the performance benefit of GPU acceleration without actual GPU programming or hardware. Users need only to skeletonize pieces of CPU code that are targets for GPU acceleration. Code skeletons are automatically transformed in various ways to mimic tuned GPU codes with characteristics resembling real implementations. The synthesized characteristics are used by an existing analytical model to project GPU performance. The cost and benefit of GPU development can then be estimated according to the transformed code skeleton that yields the best projected performance. With GROPHECY, users can leap toward GPU acceleration only when the cost-benefit makes sense. We validate the framework using microbenchmarks as well as data-parallel codes in legacy high-performance computing applications. The measured performance of manually tuned codes deviates from the projected performance by 17\% in geometric mean.},
  language = {en},
  booktitle = {Proceedings of 2011 {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} on - {{SC}} '11},
  publisher = {{ACM Press}},
  author = {Meng, Jiayuan and Morozov, Vitali A. and Kumaran, Kalyan and Vishwanath, Venkatram and Uram, Thomas D.},
  year = {2011},
  pages = {1},
  file = {/Users/luigi/work/zotero/storage/6GRMWA7L/Meng et al. - 2011 - GROPHECY GPU performance projection from CPU code.pdf}
}

@inproceedings{kichererCostawareFunctionMigration2011,
  address = {Heraklion, Greece},
  title = {Cost-Aware Function Migration in Heterogeneous Systems},
  isbn = {978-1-4503-0241-8},
  doi = {10.1145/1944862.1944883},
  abstract = {Today's approaches towards heterogeneous computing rely on either the programmer or dedicated programming models to efficiently integrate heterogeneous components. In this work, we propose an adaptive cost-aware function-migration mechanism built on top of a light-weight hardware abstraction layer. With this mechanism, the highly dynamic task of choosing the most beneficial processing unit will be hidden from the programmer while causing only minor variation in the work and program flow. The migration mechanism transparently adapts to the current workload and system environment without the necessity of JIT compilation or binary translation. Evaluation shows that our approach successfully adapts to new circumstances and predicts the most beneficial processing unit (PU). Through fine-grained PU selection, our solution achieves a speedup of up to 2.27 for the average kernel execution time but introduces only a marginal overhead in case its services are not required.},
  language = {en},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{High Performance}} and {{Embedded Architectures}} and {{Compilers}} - {{HiPEAC}} '11},
  publisher = {{ACM Press}},
  author = {Kicherer, Mario and Buchty, Rainer and Karl, Wolfgang},
  year = {2011},
  pages = {137},
  file = {/Users/luigi/work/zotero/storage/Q9NAXTR5/Kicherer et al. - 2011 - Cost-aware function migration in heterogeneous sys.pdf}
}

@inproceedings{kellVirtualMachinesShould2011,
  address = {Portland, Oregon, USA},
  title = {Virtual Machines Should Be Invisible},
  isbn = {978-1-4503-1183-0},
  doi = {10.1145/2095050.2095099},
  abstract = {Current VM designs prioritise implementor freedom and performance, at the expense of other concerns of the end programmer. We motivate an alternative approach to VM design aiming to be unobtrusive in general, and prioritising two key concerns specifically: foreign function interfacing and support for runtime analysis tools (such as debuggers, profilers etc.). We describe our experiences building a Python VM in this manner, and identify some simple constraints that help enable low-overhead foreign function interfacing and direct use of native tools. We then discuss how to extend this towards a higher-performance VM suitable for Java or similar languages.},
  language = {en},
  booktitle = {Proceedings of the Compilation of the Co-Located Workshops on {{DSM}}'11, {{TMC}}'11, {{AGERE}}!'11, {{AOOPES}}'11, {{NEAT}}'11, \& {{VMIL}}'11 - {{SPLASH}} '11 {{Workshops}}},
  publisher = {{ACM Press}},
  author = {Kell, Stephen and Irwin, Conrad},
  year = {2011},
  pages = {289},
  file = {/Users/luigi/work/zotero/storage/W337CP3G/Kell and Irwin - 2011 - Virtual machines should be invisible.pdf}
}

@article{isenbergStudyDualScaleData2011,
  title = {A {{Study}} on {{Dual}}-{{Scale Data Charts}}},
  volume = {17},
  issn = {1077-2626},
  doi = {10.1109/TVCG.2011.160},
  abstract = {We present the results of a user study that compares different ways of representing Dual-Scale data charts. DualScale charts incorporate two different data resolutions into one chart in order to emphasize data in regions of interest or to enable the comparison of data from distant regions. While some design guidelines exist for these types of charts, there is currently little empirical evidence on which to base their design. We fill this gap by discussing the design space of Dual-Scale cartesian-coordinate charts and by experimentally comparing the performance of different chart types with respect to elementary graphical perception tasks such as comparing lengths and distances. Our study suggests that cut-out charts which include collocated full context and focus are the best alternative, and that superimposed charts in which focus and context overlap on top of each other should be avoided.},
  language = {en},
  number = {12},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  author = {Isenberg, P. and Bezerianos, A. and Dragicevic, Pierre and Fekete, Jean-Daniel},
  month = dec,
  year = {2011},
  pages = {2469-2478},
  file = {/Users/luigi/work/zotero/storage/NWVAMD9C/Isenberg et al. - 2011 - A Study on Dual-Scale Data Charts.pdf}
}

@article{hauswirthMovingVisualizationTeaching,
  title = {Moving from {{Visualization}} for {{Teaching}} to {{Visualization}} for {{Learning}}},
  abstract = {Good teachers choose from a rich set of visualizations to clearly communicate ideas and concepts to students. Good educational visualizations are designed in a way that supports the teaching process: they help to tell a story that leads to understanding.},
  language = {en},
  author = {Hauswirth, Matthias},
  pages = {2},
  file = {/Users/luigi/work/zotero/storage/ICIGCH5Y/Hauswirth - Moving from Visualization for Teaching to Visualiz.pdf}
}

@inproceedings{greggWhereDataWhy2011,
  address = {Austin, TX, USA},
  title = {Where Is the Data? {{Why}} You Cannot Debate {{CPU}} vs. {{GPU}} Performance without the Answer},
  isbn = {978-1-61284-367-4},
  shorttitle = {Where Is the Data?},
  doi = {10.1109/ISPASS.2011.5762730},
  abstract = {General purpose GPU Computing (GPGPU) has taken off in the past few years, with great promises for increased desktop processing power due to the large number of fast computing cores on high-end graphics cards. Many publications have demonstrated phenomenal performance and have reported speedups as much as 1000x over code running on multi-core CPUs. Other studies have claimed that well-tuned CPU code reduces the performance gap significantly. We demonstrate that this important discussion is missing a key aspect, specifically the question of where in the system data resides, and the overhead to move the data to where it will be used, and back again if necessary. We have benchmarked a broad set of GPU kernels on a number of platforms with different GPUs and our results show that when memory transfer times are included, it can easily take between 2 to 50x longer to run a kernel than the GPU processing time alone. Therefore, it is necessary to either include memory transfer overhead when reporting GPU performance, or to explain why this is not relevant for the application in question. We suggest a taxonomy for future CPU/GPU comparisons, and we argue that this is not only germane for reporting performance, but is important to heterogeneous scheduling research in general.},
  language = {en},
  booktitle = {({{IEEE ISPASS}}) {{IEEE INTERNATIONAL SYMPOSIUM ON PERFORMANCE ANALYSIS OF SYSTEMS AND SOFTWARE}}},
  publisher = {{IEEE}},
  author = {Gregg, Chris and Hazelwood, Kim},
  month = apr,
  year = {2011},
  pages = {134-144},
  file = {/Users/luigi/work/zotero/storage/M4PT3RMJ/Gregg and Hazelwood - 2011 - Where is the data Why you cannot debate CPU vs. G.pdf}
}

@inproceedings{dietlBuildingUsingPluggable2011,
  address = {Waikiki, Honolulu, HI, USA},
  title = {Building and Using Pluggable Type-Checkers},
  isbn = {978-1-4503-0445-0},
  doi = {10.1145/1985793.1985889},
  abstract = {This paper describes practical experience building and using pluggable type-checkers. A pluggable type-checker refines (strengthens) the built-in type system of a programming language. This permits programmers to detect and prevent, at compile time, defects that would otherwise have been manifested as run-time errors. The prevented defects may be generally applicable to all programs, such as null pointer dereferences. Or, an application-specific pluggable type system may be designed for a single application.},
  language = {en},
  booktitle = {Proceeding of the 33rd International Conference on {{Software}} Engineering - {{ICSE}} '11},
  publisher = {{ACM Press}},
  author = {Dietl, Werner and Dietzel, Stephanie and Ernst, Michael D. and Mu{\c s}lu, Kivan{\c c} and Schiller, Todd W.},
  year = {2011},
  pages = {681},
  file = {/Users/luigi/work/zotero/storage/PP5R89G2/Dietl et al. - 2011 - Building and using pluggable type-checkers.pdf}
}

@inproceedings{ansaloniAutonomicConsolidationHeterogeneous2011,
  address = {Lisbon, Portugal},
  title = {Towards Autonomic Consolidation of Heterogeneous Workloads},
  isbn = {978-1-4503-1073-4},
  doi = {10.1145/2088960.2088972},
  language = {en},
  booktitle = {Proceedings of the {{Workshop}} on {{Posters}} and {{Demos Track}} - {{PDT}} '11},
  publisher = {{ACM Press}},
  author = {Ansaloni, Danilo and Chen, Lydia Y. and Smirni, Evgenia and Binder, Walter},
  year = {2011},
  pages = {1-2},
  file = {/Users/luigi/work/zotero/storage/PI2U7BDS/Ansaloni et al. - 2011 - Towards autonomic consolidation of heterogeneous w.pdf}
}

@inproceedings{zhangBPGenAutomatedBreakpoint2010,
  address = {Cape Town, South Africa},
  title = {{{BPGen}}: An Automated Breakpoint Generator for Debugging},
  volume = {2},
  isbn = {978-1-60558-719-6},
  shorttitle = {{{BPGen}}},
  doi = {10.1145/1810295.1810351},
  abstract = {During debugging processes, breakpoints are frequently used to inspect and understand runtime behaviors of programs. Although most development environments offer convenient breakpoint facilities, the use of these environments usually requires considerable human efforts in order to generate useful breakpoints. Before setting breakpoints or typing breakpoint conditions, developers usually have to make some judgements and hypotheses on the basis of their observations and experience. To reduce this kind of efforts we present a tool, named BPGen, to automatically generate breakpoints for debugging. BPGen uses three well-known dynamic fault localization techniques in tandem to identify suspicious program statements and states, through which both conditional and unconditional breakpoints are generated. BPGen is implemented as an Eclipse plugin for supplementing the existing Eclipse JDT debugger.},
  language = {en},
  booktitle = {Proceedings of the 32nd {{ACM}}/{{IEEE International Conference}} on {{Software Engineering}} - {{ICSE}} '10},
  publisher = {{ACM Press}},
  author = {Zhang, Cheng and Yan, Dacong and Zhao, Jianjun and Chen, Yuting and Yang, Shengqian},
  year = {2010},
  pages = {271},
  file = {/Users/luigi/work/zotero/storage/KZQ5ACYK/Zhang et al. - 2010 - BPGen an automated breakpoint generator for debug.pdf}
}

@article{liangDynamicEvaluationPrecision,
  title = {A Dynamic Evaluation of the Precision of Static Heap Abstractions},
  abstract = {The quality of a static analysis of heap-manipulating programs is largely determined by its heap abstraction. Object allocation sites are a commonly-used abstraction, but are too coarse for some clients. The goal of this paper is to investigate how various refinements of allocation sites can improve precision. In particular, we consider abstractions that use call stack, object recency, and heap connectivity information. We measure the precision of these abstractions dynamically for four different clients motivated by concurrency and on nine Java programs chosen from the DaCapo benchmark suite. Our dynamic results shed new light on aspects of heap abstractions that matter for precision, which allows us to more effectively navigate the large space of possible heap abstractions.},
  language = {en},
  author = {Liang, Percy and Tripp, Omer and Naik, Mayur and Sagiv, Mooly},
  pages = {17},
  file = {/Users/luigi/work/zotero/storage/QBP9IKXZ/Liang et al. - A dynamic evaluation of the precision of static he.pdf}
}

@article{greggContentionAwareSchedulingParallel,
  title = {Contention-{{Aware Scheduling}} of {{Parallel Code}} for {{Heterogeneous Systems}}},
  abstract = {A typical consumer desktop computer has a multi-core CPU with at least two and possibly up to eight processing elements over two processors, and a multi-core GPU with up to 512 processing elements. Both the CPU and the GPU are capable of running parallel code, yet it is not obvious when to utilize one processor or the other because of workload considerations and, as importantly, contention on each device. This paper demonstrates a method for dynamically deciding whether to run a given parallel workload on the CPU or the GPU depending on the state of the system when the code is launched. To achieve this, we tested a selection of parallel OpenCL code on a multi-core CPU and a multi-core GPU, as part of a larger program that runs on the CPU. When the parallel code is launched, the runtime makes a dynamic decision about which processor to run the code on, given system state and historical data. We demonstrate a method for using meta-data available to the runtime and historical data from code profiling to make the dynamic decision, and we outline the runtime information necessary for making effective dynamic decisions, suggest hardware, operating system, and driver support.},
  language = {en},
  author = {Gregg, Chris and Brantley, Jeff and Hazelwood, Kim},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/VIC3WXB2/Gregg et al. - Contention-Aware Scheduling of Parallel Code for H.pdf}
}

@inproceedings{goswamiExploringGPGPUWorkloads2010,
  address = {Atlanta, GA, USA},
  title = {Exploring {{GPGPU}} Workloads: {{Characterization}} Methodology, Analysis and Microarchitecture Evaluation Implications},
  isbn = {978-1-4244-9297-8},
  shorttitle = {Exploring {{GPGPU}} Workloads},
  doi = {10.1109/IISWC.2010.5649549},
  abstract = {The GPUs are emerging as a general-purpose high-performance computing device. Growing GPGPU research has made numerous GPGPU workloads available. However, a systematic approach to characterize these benchmarks and analyze their implication on GPU microarchitecture design evaluation is still lacking. In this research, we propose a set of microarchitecture agnostic GPGPU workload characteristics to represent them in a microarchitecture independent space. Correlated dimensionality reduction process and clustering analysis are used to understand these workloads. In addition, we propose a set of evaluation metrics to accurately evaluate the GPGPU design space. With growing number of GPGPU workloads, this approach of analysis provides meaningful, accurate and thorough simulation for a proposed GPU architecture design choice. Architects also benefit by choosing a set of workloads to stress their intended functional block of the GPU microarchitecture. We present a diversity analysis of GPU benchmark suites such as Nvidia CUDA SDK, Parboil and Rodinia. Our results show that with a large number of diverse kernels, workloads such as Similarity Score, Parallel Reduction, and Scan of Large Arrays show diverse characteristics in different workload spaces. We have also explored diversity in different workload subspaces (e.g. memory coalescing and branch divergence). Similarity Score, Scan of Large Arrays, MUMmerGPU, Hybrid Sort, and Nearest Neighbor workloads exhibit relatively large variation in branch divergence characteristics compared to others. Memory coalescing behavior is diverse in Scan of Large Arrays, K-Means, Similarity Score and Parallel Reduction.},
  language = {en},
  booktitle = {{{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}}'10)},
  publisher = {{IEEE}},
  author = {Goswami, Nilanjan and Shankar, Ramkumar and Joshi, Madhura and Li, Tao},
  month = dec,
  year = {2010},
  pages = {1-10},
  file = {/Users/luigi/work/zotero/storage/UR4DG97T/Goswami et al. - 2010 - Exploring GPGPU workloads Characterization method.pdf}
}

@incollection{dezani-ciancagliniSessionsSessionTypes2010,
  address = {Berlin, Heidelberg},
  title = {Sessions and {{Session Types}}: {{An Overview}}},
  volume = {6194},
  isbn = {978-3-642-14457-8 978-3-642-14458-5},
  shorttitle = {Sessions and {{Session Types}}},
  abstract = {We illustrate the concepts of sessions and session types as they have been developed in the setting of the {$\pi$}-calculus. Motivated by the goal of obtaining a formalisation closer to existing standards and aiming at their enhancement and strengthening, several extensions of the original core system have been proposed, which we survey together with the embodying of sessions into functional and object-oriented languages, as well as some implementations.},
  language = {en},
  booktitle = {Web {{Services}} and {{Formal Methods}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{Dezani-Ciancaglini}, Mariangiola and {de'Liguoro}, Ugo},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Laneve, Cosimo and Su, Jianwen},
  year = {2010},
  pages = {1-28},
  file = {/Users/luigi/work/zotero/storage/QREZYABT/Dezani-Ciancaglini and de’Liguoro - 2010 - Sessions and Session Types An Overview.pdf},
  doi = {10.1007/978-3-642-14458-5_1}
}

@inproceedings{danalisScalableHeterogeneousComputing2010,
  address = {Pittsburgh, Pennsylvania},
  title = {The {{Scalable Heterogeneous Computing}} ({{SHOC}}) Benchmark Suite},
  isbn = {978-1-60558-935-0},
  doi = {10.1145/1735688.1735702},
  abstract = {Scalable heterogeneous computing systems, which are composed of a mix of compute devices, such as commodity multicore processors, graphics processors, reconfigurable processors, and others, are gaining attention as one approach to continuing performance improvement while managing the new challenge of energy efficiency. As these systems become more common, it is important to be able to compare and contrast architectural designs and programming systems in a fair and open forum. To this end, we have designed the Scalable HeterOgeneous Computing benchmark suite (SHOC). SHOC's initial focus is on systems containing graphics processing units (GPUs) and multi-core processors, and on the new OpenCL programming standard. SHOC is a spectrum of programs that test the performance and stability of these scalable heterogeneous computing systems. At the lowest level, SHOC uses microbenchmarks to assess architectural features of the system. At higher levels, SHOC uses application kernels to determine system-wide performance including many system features such as intranode and internode communication among devices. SHOC includes benchmark implementations in both OpenCL and CUDA in order to provide a comparison of these programming models.},
  language = {en},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{General}}-{{Purpose Computation}} on {{Graphics Processing Units}} - {{GPGPU}} '10},
  publisher = {{ACM Press}},
  author = {Danalis, Anthony and Marin, Gabriel and McCurdy, Collin and Meredith, Jeremy S. and Roth, Philip C. and Spafford, Kyle and Tipparaju, Vinod and Vetter, Jeffrey S.},
  year = {2010},
  pages = {63},
  file = {/Users/luigi/work/zotero/storage/EXU7WFS3/Danalis et al. - 2010 - The Scalable Heterogeneous Computing (SHOC) benchm.pdf}
}

@inproceedings{schaaExploringMultipleGPUDesign2009,
  address = {Rome, Italy},
  title = {Exploring the Multiple-{{GPU}} Design Space},
  isbn = {978-1-4244-3751-1},
  doi = {10.1109/IPDPS.2009.5161068},
  abstract = {Graphics Processing Units (GPUs) have been growing in popularity due to their impressive processing capabilities, and with general purpose programming languages such as NVIDIA's CUDA interface, are becoming the platform of choice in the scientific computing community. Previous studies that used GPUs focused on obtaining significant performance gains from execution on a single GPU. These studies employed low-level, architecture-specific tuning in order to achieve sizeable benefits over multicore CPU execution.},
  language = {en},
  booktitle = {2009 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}},
  publisher = {{IEEE}},
  author = {Schaa, Dana and Kaeli, David},
  month = may,
  year = {2009},
  pages = {1-12},
  file = {/Users/luigi/work/zotero/storage/YZL4ETVM/Schaa and Kaeli - 2009 - Exploring the multiple-GPU design space.pdf}
}

@inproceedings{reissVisualizingJavaHeap2009,
  address = {Edmonton, AB, Canada},
  title = {Visualizing the {{Java}} Heap to Detect Memory Problems},
  isbn = {978-1-4244-5027-5},
  doi = {10.1109/VISSOF.2009.5336418},
  abstract = {Many of the problems that occur in long-running systems involve the way that the system uses memory. We have developed a framework for extracting and building a model of the heap from a running Java system. Such a model is only useful if the programmer can extract from it the information they need to understand, find, and eventually fix memory-related problems in their system. This paper describes the visualization strategy we use for interactively displaying the model and related information to achieve these goals.},
  language = {en},
  booktitle = {2009 5th {{IEEE International Workshop}} on {{Visualizing Software}} for {{Understanding}} and {{Analysis}}},
  publisher = {{IEEE}},
  author = {Reiss, Steven P.},
  month = sep,
  year = {2009},
  pages = {73-80},
  file = {/Users/luigi/work/zotero/storage/LIK83ZTA/Reiss - 2009 - Visualizing the Java heap to detect memory problem.pdf}
}

@article{pradelDynamicallyInferringRefining,
  title = {Dynamically {{Inferring}}, {{Refining}}, and {{Checking API Usage Protocols}}},
  abstract = {Using a set of API methods often requires compliance with a protocol, whose violation can lead to errors in the program. However, most APIs lack explicit and formal definitions of these protocols. We propose a dynamic program analysis for automatically inferring and refining specifications of correct method call sequences. Our experiments with several Java programs show that we can infer meaningful protocols, such as widely respected programming rules. Furthermore, our analysis finds violations of the inferred specifications that point out potential bugs to the programmer.},
  language = {en},
  author = {Pradel, Michael},
  pages = {2},
  file = {/Users/luigi/work/zotero/storage/ID92AR6C/Pradel - Dynamically Inferring, Reﬁning, and Checking API U.pdf}
}

@article{norellDependentlyTypedProgramming,
  title = {Dependently {{Typed Programming}} in {{Agda}}},
  language = {en},
  author = {Norell, Ulf and Chapman, James},
  pages = {41},
  file = {/Users/luigi/work/zotero/storage/N7WNZ7GE/Norell and Chapman - Dependently Typed Programming in Agda.pdf}
}

@incollection{jimenezPredictiveRuntimeCode2009,
  address = {Berlin, Heidelberg},
  title = {Predictive {{Runtime Code Scheduling}} for {{Heterogeneous Architectures}}},
  volume = {5409},
  isbn = {978-3-540-92989-5 978-3-540-92990-1},
  abstract = {Heterogeneous architectures are currently widespread. With the advent of easy-to-program general purpose GPUs, virtually every recent desktop computer is a heterogeneous system. Combining the CPU and the GPU brings great amounts of processing power. However, such architectures are often used in a restricted way for domain-specific applications like scientific applications and games, and they tend to be used by a single application at a time. We envision future heterogeneous computing systems where all their heterogeneous resources are continuously utilized by different applications with versioned critical parts to be able to better adapt their behavior and improve execution time, power consumption, response time and other constraints at runtime. Under such a model, adaptive scheduling becomes a critical component.},
  language = {en},
  booktitle = {High {{Performance Embedded Architectures}} and {{Compilers}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jim\'enez, V\'ictor J. and Vilanova, Llu\'is and Gelado, Isaac and Gil, Marisa and Fursin, Grigori and Navarro, Nacho},
  editor = {Seznec, Andr\'e and Emer, Joel and O'Boyle, Michael and Martonosi, Margaret and Ungerer, Theo},
  year = {2009},
  pages = {19-33},
  file = {/Users/luigi/work/zotero/storage/2D7JKDFL/Jiménez et al. - 2009 - Predictive Runtime Code Scheduling for Heterogeneo.pdf},
  doi = {10.1007/978-3-540-92990-1_4}
}

@incollection{hoenickeItDoomedWe2009,
  address = {Berlin, Heidelberg},
  title = {It's {{Doomed}}; {{We Can Prove It}}},
  volume = {5850},
  isbn = {978-3-642-05088-6 978-3-642-05089-3},
  abstract = {Programming errors found early are the cheapest. Tools applying to the early stage of code development exist but either they suffer from false positives (``noise'') or they require strong user interaction. We propose to avoid this deficiency by defining a new class of errors. A program fragment is doomed if its execution will inevitably fail, in whatever state it is started. We use a formal verification method to identify such errors fully automatically and, most significantly, without producing noise. We report on preliminary experiments with a prototype tool.},
  language = {en},
  booktitle = {{{FM}} 2009: {{Formal Methods}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Hoenicke, Jochen and Leino, K. Rustan M. and Podelski, Andreas and Sch\"af, Martin and Wies, Thomas},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Cavalcanti, Ana and Dams, Dennis R.},
  year = {2009},
  pages = {338-353},
  file = {/Users/luigi/work/zotero/storage/CES47PKN/Hoenicke et al. - 2009 - It’s Doomed\; We Can Prove It.pdf},
  doi = {10.1007/978-3-642-05089-3_22}
}

@article{hallerScalaActorsUnifying2009,
  title = {Scala {{Actors}}: {{Unifying}} Thread-Based and Event-Based Programming},
  volume = {410},
  issn = {03043975},
  shorttitle = {Scala {{Actors}}},
  doi = {10.1016/j.tcs.2008.09.019},
  abstract = {There is an impedance mismatch between message-passing concurrency and virtual machines, such as the JVM. VMs usually map their threads to heavyweight OS processes. Without a lightweight process abstraction, users are often forced to write parts of concurrent applications in an event-driven style which obscures control flow, and increases the burden on the programmer.},
  language = {en},
  number = {2-3},
  journal = {Theoretical Computer Science},
  author = {Haller, Philipp and Odersky, Martin},
  month = feb,
  year = {2009},
  pages = {202-220},
  file = {/Users/luigi/work/zotero/storage/YLWMV6UT/Haller and Odersky - 2009 - Scala Actors Unifying thread-based and event-base.pdf}
}

@article{flanaganFastTrackEfficientPrecise,
  title = {{{FastTrack}}: {{Efficient}} and {{Precise Dynamic Race Detection}}},
  abstract = {Multithreaded programs are notoriously prone to race conditions. Prior work on dynamic race detectors includes fast but imprecise race detectors that report false alarms, as well as slow but precise race detectors that never report false alarms. The latter typically use expensive vector clock operations that require time linear in the number of program threads.},
  language = {en},
  author = {Flanagan, Cormac and Freund, Stephen N},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/NKEAWWKH/Flanagan and Freund - FastTrack Efﬁcient and Precise Dynamic Race Detec.pdf}
}

@article{dietlUniverseTypesTopology,
  title = {Universe {{Types}}: {{Topology}}, {{Encapsulation}}, {{Genericity}}, and {{Tools}}},
  language = {en},
  author = {Dietl, Werner M},
  pages = {208},
  file = {/Users/luigi/work/zotero/storage/DHQUTRB7/Dietl - Universe Types Topology, Encapsulation, Genericit.pdf}
}

@incollection{vandenbrandModelDrivenEngineeringMeets2009,
  address = {Berlin, Heidelberg},
  title = {Model-{{Driven Engineering Meets Generic Language Technology}}},
  volume = {5452},
  isbn = {978-3-642-00433-9 978-3-642-00434-6},
  abstract = {One of the key points of model-driven engineering is raising the level of abstraction in software development. This phenomenon is not new. In the sixties of the previous century, the first high-level programming languages were developed and they also increased the abstraction level of software development. The development of high-level programming languages initiated research on compilers and programming environments. This research eventually matured into generic language technology: the description of (programming) languages and tooling to generate compilers and programming environments. The model-driven engineering community is developing tools to analyze models and to transform models into code. The application of generic language technology, or at least the lessons learnt by this community, can be beneficial for the model-driven engineering community. By means of a number of case studies it will be shown how generic language technology research can be useful for the development of model-driven engineering technology.},
  language = {en},
  booktitle = {Software {{Language Engineering}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{van den Brand}, M. G. J.},
  editor = {Ga{\v s}evi\'c, Dragan and L\"ammel, Ralf and Van Wyk, Eric},
  year = {2009},
  pages = {8-15},
  file = {/Users/luigi/work/zotero/storage/YRCKMYUZ/van den Brand - 2009 - Model-Driven Engineering Meets Generic Language Te.pdf},
  doi = {10.1007/978-3-642-00434-6_2}
}

@article{anselPetaBricksLanguageCompiler,
  title = {{{PetaBricks}}: {{A Language}} and {{Compiler}} for {{Algorithmic Choice}}},
  abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarsegrained, library level selections or hand coded cutoffs between base cases and recursive cases.},
  language = {en},
  author = {Ansel, Jason and Chan, Cy and Wong, Yee Lok and Olszewski, Marek and Zhao, Qin and Edelman, Alan and Amarasinghe, Saman},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/DKJW2G3S/Ansel et al. - PetaBricks A Language and Compiler for Algorithmi.pdf}
}

@article{schulteActiveDocumentsOrgMode2011,
  title = {Active {{Documents}} with {{Org}}-{{Mode}}},
  volume = {13},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2011.41},
  language = {en},
  number = {3},
  journal = {Computing in Science \& Engineering},
  author = {Schulte, Eric and Davison, Dan},
  month = may,
  year = {2011},
  pages = {66-73},
  file = {/Users/luigi/work/zotero/storage/A96EBHH2/Schulte and Davison - 2011 - Active Documents with Org-Mode.pdf}
}

@article{dominikCompactOrgmodeGuide,
  title = {The Compact {{Org}}-Mode {{Guide}}},
  language = {en},
  author = {Dominik, Carsten},
  pages = {45},
  file = {/Users/luigi/work/zotero/storage/9I44KL6A/Dominik - The compact Org-mode Guide.pdf}
}

@article{marlowMakingFastCurry2006,
  title = {Making a Fast Curry: Push/Enter vs. Eval/Apply for Higher-Order Languages},
  volume = {16},
  issn = {0956-7968, 1469-7653},
  shorttitle = {Making a Fast Curry},
  doi = {10.1017/S0956796806005995},
  abstract = {Higher-order languages that encourage currying are typically implemented using one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition and qualitative judgements to choose one model or the other. Our goal in this paper is to provide, for the first time, a more substantial basis for this choice, based on our qualitative and quantitative experience of implementing both models in a state-of-the-art compiler for Haskell. Our conclusion is simple, and contradicts our initial intuition: compiled implementations should use eval/apply.},
  language = {en},
  number = {4\&5},
  journal = {Journal of Functional Programming},
  author = {Marlow, Simon and Jones, Simon Peyton},
  month = jul,
  year = {2006},
  pages = {415},
  file = {/Users/luigi/work/zotero/storage/DZJFRF42/Marlow and Jones - 2006 - Making a fast curry pushenter vs. evalapply for.pdf}
}

@article{abadiDynamicTypingStatically,
  title = {Dynamic Typing in a Statically Typed Language},
  abstract = {Statically typed programming languages allow earlier error checking, better enforcement of disciplined programming styles, and generation of more e cient object code than languages where all type consistency checks are performed at run time. However, even in statically typed languages, there is often the need to deal with data whose type cannot be determined at compile time. To handle such situations safely, we propose to add a type Dynamic whose values are pairs of a value v and a type tag T where v has the type denoted by T. Instances of Dynamic are built with an explicit tagging construct and inspected with a type safe typecase construct.},
  language = {en},
  author = {Abadi, Martin and Cardelli, Luca and Pierce, Benjamin C and Plotkin, Gordon D},
  pages = {35},
  file = {/Users/luigi/work/zotero/storage/YWVN85GR/Abadi et al. - Dynamic typing in a statically typed language.pdf}
}

@article{peytonjonesSecretsGlasgowHaskell2002,
  title = {Secrets of the {{Glasgow Haskell Compiler}} Inliner},
  volume = {12},
  issn = {0956-7968, 1469-7653},
  doi = {10.1017/S0956796802004331},
  abstract = {Higher-order languages, such as Haskell, encourage the programmer to build abstractions by composing functions. A good compiler must inline many of these calls to recover an e ciently executable program.},
  language = {en},
  number = {4-5},
  journal = {Journal of Functional Programming},
  author = {Peyton Jones, Simon and Marlow, Simon},
  month = jul,
  year = {2002},
  file = {/Users/luigi/work/zotero/storage/MN34Q6LW/Peyton Jones and Marlow - 2002 - Secrets of the Glasgow Haskell Compiler inliner.pdf}
}

@inproceedings{visserBuildingProgramOptimizers1998,
  address = {New York, NY, USA},
  series = {{{ICFP}} '98},
  title = {Building {{Program Optimizers}} with {{Rewriting Strategies}}},
  isbn = {978-1-58113-024-9},
  doi = {10.1145/289423.289425},
  abstract = {We describe a language for defining term rewriting strategies, and its application to the production of program optimizers. Valid transformations on program terms can be described by a set of rewrite rules; rewriting strategies are used to describe when and how the various rules should be applied in order to obtain the desired optimization effects. Separating rules from strategies in this fashion makes it easier to reason about the behavior of the optimizer as a whole, compared to traditional monolithic optimizer implementations. We illustrate the expressiveness of our language by using it to describe a simple optimizer for an ML-like intermediate representation.The basic strategy language uses operators such as sequential composition, choice, and recursion to build transformers from a set of labeled unconditional rewrite rules. We also define an extended language in which the side-conditions and contextual rules that arise in realistic optimizer specifications can themselves be expressed as strategy-driven rewrites. We show that the features of the basic and extended languages can be expressed by breaking down the rewrite rules into their primitive building blocks, namely matching and building terms in variable binding environments. This gives us a low-level core language which has a clear semantics, can be implemented straightforwardly and can itself be optimized. The current implementation generates C code from a strategy specification.},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Visser, Eelco and Benaissa, Zine-el-Abidine and Tolmach, Andrew},
  year = {1998},
  pages = {13--26},
  file = {/Users/luigi/work/zotero/storage/2NPKIPDZ/Visser et al. - 1998 - Building Program Optimizers with Rewriting Strateg.pdf}
}

@incollection{ballandTomPiggybackingRewriting2007,
  address = {Berlin, Heidelberg},
  title = {Tom: {{Piggybacking Rewriting}} on {{Java}}},
  volume = {4533},
  isbn = {978-3-540-73447-5 978-3-540-73449-9},
  shorttitle = {Tom},
  abstract = {We present the Tom language that extends Java with the purpose of providing high level constructs inspired by the rewriting community. Tom furnishes a bridge between a general purpose language and higher level specifications that use rewriting. This approach was motivated by the promotion of rewriting techniques and their integration in large scale applications. Powerful matching capabilities along with a rich strategy language are among Tom's strong points, making it easy to use and competitive with other rule based languages.},
  language = {en},
  booktitle = {Term {{Rewriting}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Balland, Emilie and Brauner, Paul and Kopetz, Radu and Moreau, Pierre-Etienne and Reilles, Antoine},
  editor = {Baader, Franz},
  year = {2007},
  pages = {36-47},
  file = {/Users/luigi/work/zotero/storage/8HMZS75T/Balland et al. - 2007 - Tom Piggybacking Rewriting on Java.pdf},
  doi = {10.1007/978-3-540-73449-9_5}
}

@inproceedings{hudakModularDomainSpecific1998,
  title = {Modular Domain Specific Languages and Tools},
  doi = {10.1109/ICSR.1998.685738},
  abstract = {A domain specific language (DSL) allows one to develop software for a particular application domain quickly and effectively, yielding programs that are easy to understand, reason about, and maintain. On the other hand, there may be a significant overhead in creating the infrastructure needed to support a DSL. To solve this problem, a methodology is described for building domain specific embedded languages (DSELs), in which a DSL is designed within an existing, higher-order and typed, programming language such as Haskell or ML. In addition, techniques are described for building modular interpreters and tools for DSELs. The resulting methodology facilitates reuse of syntax semantics, implementation code, software tools, as well as look-and-feel.},
  booktitle = {Proceedings. {{Fifth International Conference}} on {{Software Reuse}} ({{Cat}}. {{No}}.{{98TB100203}})},
  author = {Hudak, P.},
  month = jun,
  year = {1998},
  keywords = {Application software,Computer languages,Software maintenance,Computer science,Haskell,Buildings,software maintenance,Programming,Hardware,software reusability,Costs,software tools,methodology,Domain specific languages,functional languages,DSL,domain specific embedded languages,higher-order typed programming language,implementation code,ML,modular domain specific languages,modular interpreters,program interpreters,program understanding,semantics,software reuse,syntax},
  pages = {134-142},
  file = {/Users/luigi/work/zotero/storage/WAQ87KND/Hudak - 1998 - Modular domain specific languages and tools.pdf;/Users/luigi/work/zotero/storage/WJTYRA5Q/685738.html}
}

@inproceedings{baconFastStaticAnalysis1996,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '96},
  title = {Fast {{Static Analysis}} of {{C}}++ {{Virtual Function Calls}}},
  isbn = {978-0-89791-788-9},
  doi = {10.1145/236337.236371},
  abstract = {Virtual functions make code easier for programmers to reuse but also make it harder for compilers to analyze. We investigate the ability of three static analysis algorithms to improve C++ programs by resolving virtual function calls, thereby reducing compiled code size and reducing program complexity so as to improve both human and automated program understanding and analysis. In measurements of seven programs of significant size (5000 to 20000 lines of code each) we found that on average the most precise of the three algorithms resolved 71\% of the virtual function calls and reduced compiled code size by 25\%. This algorithm is very fast: it analyzes 3300 source lines per second on an 80 MHz PowerPC 601. Because of its accuracy and speed, this algorithm is an excellent candidate for inclusion in production C++ compilers.},
  booktitle = {Proceedings of the 11th {{ACM SIGPLAN Conference}} on {{Object}}-Oriented {{Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Bacon, David F. and Sweeney, Peter F.},
  year = {1996},
  pages = {324--341},
  file = {/Users/luigi/work/zotero/storage/4A4FSIHF/Bacon and Sweeney - 1996 - Fast Static Analysis of C++ Virtual Function Calls.pdf}
}

@article{LambdaUltimateImperative,
  title = {Lambda: {{The Ultimate Imperative}}},
  language = {en},
  pages = {43},
  file = {/Users/luigi/work/zotero/storage/FBTG4I2G/Lambda The Ultimate Imperative.pdf}
}

@inproceedings{cheneyLightweightImplementationGenerics2002,
  address = {Pittsburgh, Pennsylvania},
  title = {A Lightweight Implementation of Generics and Dynamics},
  isbn = {978-1-58113-605-0},
  doi = {10.1145/581690.581698},
  abstract = {The recent years have seen a number of proposals for extending statically typed languages by dynamics or generics. Most proposals \textemdash{} if not all \textemdash{} require significant extensions to the underlying language. In this paper we show that this need not be the case. We propose a particularly lightweight extension that supports both dynamics and generics. Furthermore, the two features are smoothly integrated: dynamic values, for instance, can be passed to generic functions. Our proposal makes do with a standard Hindley-Milner type system augmented by existential types. Building upon these ideas we have implemented a small library that is readily usable both with Hugs and with the Glasgow Haskell compiler.},
  language = {en},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} Workshop on {{Haskell}}  - {{Haskell}} '02},
  publisher = {{ACM Press}},
  author = {Cheney, James and Hinze, Ralf},
  year = {2002},
  pages = {90-104},
  file = {/Users/luigi/work/zotero/storage/MH237C48/Cheney and Hinze - 2002 - A lightweight implementation of generics and dynam.pdf}
}

@article{lammelScrapYourBoilerplate2003,
  title = {Scrap Your Boilerplate: A Practical Approach to Generic Programming},
  shorttitle = {Scrap Your Boilerplate},
  abstract = {We describe a design pattern that for writing programs that traverse data structures built from rich mutually-recursive data types. Such programs often have a great deal of ``boilerplate'' code that simply walks the structure, hiding a small amount of ``real'' code that constitutes the reason for the traversal. Our technique allows most of this boilerplate \ldots{}},
  language = {en-US},
  journal = {Microsoft Research},
  author = {L\"ammel, Ralf and Jones, Simon Peyton},
  month = jan,
  year = {2003},
  file = {/Users/luigi/work/zotero/storage/VWJD9RUH/Lämmel and Jones - 2003 - Scrap your boilerplate a practical approach to ge.pdf;/Users/luigi/work/zotero/storage/FUAA9KAZ/scrap-your-boilerplate-a-practical-approach-to-generic-programming.html}
}

@article{dietrichXCorpusExecutableCorpus2017,
  title = {{{XCorpus}} \textendash{} {{An}} Executable {{Corpus}} of {{Java Programs}}.},
  volume = {16},
  issn = {1660-1769},
  doi = {10.5381/jot.2017.16.4.a1},
  abstract = {Empirical studies on code require standardized datasets of significant size extracted from real-world programs in order to be reproducible and generalisable. We argue that there is a need for such data sets that are executable and can therefore be used for experiments using static and dynamic analysis. A harness for such a data set should have high coverage in order to facilitate the construction of comprehensive models of program execution.},
  language = {en},
  number = {4},
  journal = {The Journal of Object Technology},
  author = {Dietrich, Jens and Schole, Henrik and Sui, Li and Tempero, Ewan},
  year = {2017},
  pages = {1:1},
  file = {/Users/luigi/work/zotero/storage/8Z42F235/Dietrich et al. - 2017 - XCorpus – An executable Corpus of Java Programs..pdf}
}

@inproceedings{allamanisMiningSourceCode2013,
  address = {San Francisco, CA, USA},
  title = {Mining Source Code Repositories at Massive Scale Using Language Modeling},
  isbn = {978-1-4673-2936-1 978-1-4799-0345-0},
  doi = {10.1109/MSR.2013.6624029},
  abstract = {The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by finding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the first giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is significantly better at the code suggestion task than previous models. More broadly, our approach provides a new ``lens'' for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program's core logic based solely on general information theoretic criteria.},
  language = {en},
  booktitle = {2013 10th {{Working Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  publisher = {{IEEE}},
  author = {Allamanis, Miltiadis and Sutton, Charles},
  month = may,
  year = {2013},
  pages = {207-216},
  file = {/Users/luigi/work/zotero/storage/BVXB6PWU/Allamanis and Sutton - 2013 - Mining source code repositories at massive scale u.pdf}
}

@inproceedings{bajracharyaSourcererInternetscaleSoftware2009,
  title = {Sourcerer: {{An}} Internet-Scale Software Repository},
  shorttitle = {Sourcerer},
  doi = {10.1109/SUITE.2009.5070010},
  abstract = {Vast quantities of open source code are now available online, presenting a great potential resource for software developers. Yet the current generation of open source code search engines fail to take advantage of the rich structural information contained in the code they index. We have developed Sourcerer, an infrastructure for large-scale indexing and analysis of open source code. By taking full advantage of this structural information, Sourcerer provides a foundation upon which state of the art search engines and related tools easily be built. We describe the Sourcerer infrastructure, present the applications that we have built on top of it, and discuss how existing tools could benefit from using Sourcerer.},
  booktitle = {Tools and {{Evaluation}} 2009 {{ICSE Workshop}} on {{Search}}-{{Driven Development}}-{{Users}}, {{Infrastructure}}},
  author = {Bajracharya, S. and Ossher, J. and Lopes, Cristina},
  month = may,
  year = {2009},
  keywords = {Java,Libraries,Computer architecture,Internet,public domain software,Open source software,software developers,Search engines,Feature extraction,indexing,Indexing,Information retrieval,Internet-scale software repository,large-scale indexing,Large-scale systems,open source code,search engines,Sourcerer,structural information},
  pages = {1-4},
  file = {/Users/luigi/work/zotero/storage/C3KR2Z45/Bajracharya et al. - 2009 - Sourcerer An internet-scale software repository.pdf;/Users/luigi/work/zotero/storage/YDENGFHV/5070010.html}
}

@inproceedings{linares-vasquezSupportingEvolutionMaintenance2014,
  address = {Hyderabad, India},
  title = {Supporting Evolution and Maintenance of {{Android}} Apps},
  isbn = {978-1-4503-2768-8},
  doi = {10.1145/2591062.2591092},
  language = {en},
  booktitle = {Companion {{Proceedings}} of the 36th {{International Conference}} on {{Software Engineering}} - {{ICSE Companion}} 2014},
  publisher = {{ACM Press}},
  author = {{Linares-V\'asquez}, Mario},
  year = {2014},
  pages = {714-717},
  file = {/Users/luigi/work/zotero/storage/BIPT8QIF/Linares-Vásquez - 2014 - Supporting evolution and maintenance of Android ap.pdf}
}

@inproceedings{tanSafeJavaNative2006,
  title = {Safe {{Java}} Native Interface},
  abstract = {Type safety is a promising approach to enhancing software security. Programs written in type-safe programming languages such as Java are type-safe by construction. However, in practice, many complex applications are heterogeneous, i.e., they contain components written in different languages. The Java Native Interface (JNI) allows type-safe Java code to interact with unsafe C code. When a type-safe language interacts with an unsafe language in the same address space, in general, the overall application becomes unsafe. In this work, we propose a framework called Safe Java Native Interface (SafeJNI) that ensures type safety of heterogeneous programs that contain Java and C components. We identify the loopholes of using JNI that would permit C code to bypass the type safety of Java. The proposed SafeJNI system fixes these loopholes and guarantees type safety when native C methods are called. The overall approach consists of (i) retro-fitting the native C methods to make them safe, and (ii) developing an enhanced system that captures additional invariants that must be satisfied to guarantee safe interoperation. The SafeJNI framework is implemented through a combination of static and dynamic checks on the C code. We have measured our system's effectiveness and performance on a set of benchmarks. During our experiments on the Zlib open source compression library, our system identified one vulnerability in the glue code between Zlib and Java. This vulnerability could be exploited to crash a large number of commercially deployed Java Virtual Machines (JVMs). The performance impact of SafeJNI on Zlib, while considerable, is less than reimplementing the C code},
  booktitle = {In {{Proceedings}} of the 2006 {{IEEE International Symposium}} on {{Secure Software Engineering}}},
  author = {Tan, Gang and Chakradhar, Srimat and Srivaths, Raghunathan and Wang, Ravi Daniel},
  year = {2006},
  pages = {97--106},
  file = {/Users/luigi/work/zotero/storage/USMVR42L/Tan et al. - 2006 - Safe Java native interface.pdf;/Users/luigi/work/zotero/storage/A2TII8T4/summary.html}
}

@inproceedings{allendeCastInsertionStrategies2013,
  address = {Indianapolis, Indiana, USA},
  title = {Cast Insertion Strategies for Gradually-Typed Objects},
  isbn = {978-1-4503-2433-5},
  doi = {10.1145/2508168.2508171},
  abstract = {Gradual typing enables a smooth and progressive integration of static and dynamic typing. The semantics of a gradually-typed program is given by translation to an intermediate language with casts: runtime type checks that control the boundaries between statically- and dynamically-typed portions of a program. This paper studies the performance of different cast insertion strategies in the context of Gradualtalk, a gradually-typed Smalltalk. We first implement the strategy specified by Siek and Taha, which inserts casts at call sites. We then study the dual approach, which consists in performing casts in callees. Based on the observation that both strategies perform well in different scenarios, we design a hybrid strategy that combines the best of each approach. We evaluate these three strategies using both micro- and macro-benchmarks. We also discuss the impact of these strategies on memory, modularity, and inheritance. The hybrid strategy constitutes a promising cast insertion strategy for adding gradual types to existing dynamicallytyped languages.},
  language = {en},
  booktitle = {Proceedings of the 9th Symposium on {{Dynamic}} Languages - {{DLS}} '13},
  publisher = {{ACM Press}},
  author = {Allende, Esteban and Fabry, Johan and Tanter, \'Eric},
  year = {2013},
  pages = {27-36},
  file = {/Users/luigi/work/zotero/storage/W88RA9R8/Allende et al. - 2013 - Cast insertion strategies for gradually-typed obje.pdf}
}

@article{krishnamurthiTeachingProgrammingLanguages2008,
  title = {Teaching Programming Languages in a Post-Linnaean Age},
  volume = {43},
  issn = {03621340},
  doi = {10.1145/1480828.1480846},
  abstract = {Programming language ``paradigms'' are a moribund and tedious legacy of a bygone age. Modern language designers pay them no respect, so why do our courses slavishly adhere to them? This paper argues that we should abandon this method of teaching languages, offers an alternative, reconciles an important split in programming language education, and describes a textbook that explores these matters.},
  language = {en},
  number = {11},
  journal = {ACM SIGPLAN Notices},
  author = {Krishnamurthi, Shriram},
  month = nov,
  year = {2008},
  pages = {81},
  file = {/Users/luigi/work/zotero/storage/J4UY2W4Q/Krishnamurthi - 2008 - Teaching programming languages in a post-linnaean .pdf}
}

@inproceedings{gaoTypeNotType2017,
  address = {Buenos Aires},
  title = {To {{Type}} or {{Not}} to {{Type}}: {{Quantifying Detectable Bugs}} in {{JavaScript}}},
  isbn = {978-1-5386-3868-2},
  shorttitle = {To {{Type}} or {{Not}} to {{Type}}},
  doi = {10.1109/ICSE.2017.75},
  abstract = {JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error.},
  language = {en},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  publisher = {{IEEE}},
  author = {Gao, Zheng and Bird, Christian and Barr, Earl T.},
  month = may,
  year = {2017},
  pages = {758-769},
  file = {/Users/luigi/work/zotero/storage/DKTWE9EJ/Gao et al. - 2017 - To Type or Not to Type Quantifying Detectable Bug.pdf}
}

@article{nanevskiYnotDependentTypes,
  title = {Ynot : {{Dependent Types}} for {{Imperative Programs}}},
  abstract = {We describe an axiomatic extension to the Coq proof assistant, that supports writing, reasoning about, and extracting higher-order, dependently-typed programs with side-effects. Coq already includes a powerful functional language that supports dependent types, but that language is limited to pure, total functions. The key contribution of our extension, which we call Ynot, is the added support for computations that may have effects such as non-termination, accessing a mutable store, and throwing/catching exceptions.},
  language = {en},
  author = {Nanevski, Aleksandar and Morrisett, Greg and Shinnar, Avi and Govereau, Paul and Birkedal, Lars},
  pages = {12},
  file = {/Users/luigi/work/zotero/storage/RMU9P2W6/Nanevski et al. - Ynot  Dependent Types for Imperative Programs.pdf}
}

@article{stefikMethodologicalIrregularitiesProgramming,
  title = {Methodological {{Irregularities}} in {{Programming}}- {{Language Research}} Implement New Processes and Poli- Cies, and Users\textemdash{}Whether Enterprises,},
  language = {en},
  author = {Stefik, Andreas and Hanenberg, Stefan},
  pages = {4},
  file = {/Users/luigi/work/zotero/storage/S4Y2I7X8/Stefik and Hanenberg - Methodological Irregularities in Programming- Lang.pdf}
}

@article{cohenJTLJavaTools,
  title = {{{JTL}} \textendash{} the {{Java Tools Language}}},
  abstract = {We present an overview of JTL (the Java Tools Language, pronounced ``Gee-tel''), a novel language for querying JAVA [8] programs. JTL was designed to serve the development of source code software tools for JAVA, and as a small language to aid programming language extensions to JAVA. Applications include definition of pointcuts for aspect-oriented programming, fixing type constraints for generic programming, specification of encapsulation policies, definition of micro-patterns, etc. We argue that the JTL expression of each of these is systematic, concise, intuitive and general.},
  language = {en},
  author = {Cohen, Tal and Maman, Itay},
  pages = {20},
  file = {/Users/luigi/work/zotero/storage/JCBAHC8R/Cohen and Maman - JTL – the Java Tools Language.pdf}
}

@article{gousiosToolsMethodsLarge,
  title = {Tools and Methods for Large Scale Empirical Software Engineering Research},
  language = {en},
  author = {Gousios, Georgios},
  pages = {160},
  file = {/Users/luigi/work/zotero/storage/5CS7LEE8/Gousios - Tools and methods for large scale empirical softwa.pdf}
}

@book{jesusLargeScaleEmpiricalStudies2013,
  title = {Large-{{Scale Empirical Studies}} of {{Mobile Apps}}},
  abstract = {Mobile apps (or apps) are software applications developed to run on mobile devices such as smartphones and tablets, among other devices. The number of apps has grown tremendously since Apple opened the first app store in 2008. For example, in March of 2009 the Google Play app store (formerly known as Android Market) had only 2,300 apps, and by mid of 2013 there were more than 800,000 apps. Given the accelerated rate of growth in the number of apps, new software engineering challenges have emerged in order to help ease the software development practices of app developers. In this thesis we examine three examples of these challenges, namely code reuse in mobile apps, app ratings, and the use of ad libraries within apps. We carry out our case studies on thousands of Android apps from the Google Play market. We find that code reuse in mobile apps is considerably higher than in desktop/server apps. However, identical copies of mobile apps are rare. We find that the current ratings system is not able to capture the dynamics of the evolving nature of apps. Thus, we were able to show the need for a more elaborate rating system for the apps. Finally, we observe},
  author = {Jesus, Israel and Ruiz, Mojica and Israel, Copyright and Ruiz, Jesus Mojica},
  year = {2013},
  file = {/Users/luigi/work/zotero/storage/YNL52757/Jesus et al. - 2013 - Large-Scale Empirical Studies of Mobile Apps.pdf;/Users/luigi/work/zotero/storage/NJP8DVZ6/summary.html}
}

@phdthesis{sajnaniLargeScaleCodeClone2016,
  title = {Large-{{Scale Code Clone Detection}}},
  abstract = {Clone detection locates exact or similar pieces of code, known as clones, within or between software systems. With the amount of source code increasing steadily, large-scale clone detection has become a necessity. Large code bases and repositories of projects have led to several new use cases of clone detection including mining library candidates, detecting similar mobile applications, detection of license violations, reverse engineering product lines, finding the provenance of a component, and code search. While several techniques have been proposed for clone detection over many years, accuracy and scalability of clone detection tools and techniques still remains an active area of research. Specifically, there is a marked lack in clone detectors that scale to large systems or repositories, particularly for detecting near-miss clones where significant editing activities may have taken place in the cloned code. The problem stated above motivates the need for clone detection techniques and tools that satisfy the following requirements: (1) accurate detection of near-miss clones, where minor to significant editing changes occur in the copy/pasted fragments; (2) scalability to hundreds of millions of lines of code and several thousand projects; and (3) minimal dependency on programming languages. To that effect, this dissertation presents SourcererCC, an accurate, near-miss clone detection tool that scales to hundreds of millions of lines of code (MLOC) on a single standard machine. The core idea of SourcererCC is to build an optimized index of code blocks and compare them using a simple bag-of-tokens strategy, which is very effective in detecting near-miss clones. Coupled with several filtering heuristics that reduce the size of the index, this approach is also very efficient, as it reduces the number of code block comparisons to detect the clones. This dissertation evaluates scalability, execution time, and accuracy of SourcererCC against four state-of-the-art open-source tools: CCFinderX, Deckard, iClones, and NiCad. To measure scalability, the performance of the tools is evaluated on inter-project software repository IJaDataset-2.0, consisting of 25,000 projects, containing 3 million files and 250 MLOC. To measure precision and recall, two recent benchmarks are used: (1) a benchmark of real clones, BigCloneBench, that spans the four primary clone types and the full spectrum of syntactical similarity in three different languages (Java, C, and C\#); and (2) a Mutation/Injection-based framework of thousands of fine-grained artificial clones. The results of these experiments suggest that SourcererCC improves the state-of-the-art in code clone detection by being the most scalable technique known so far, with accuracy at par with the current state-of-the-art tools.Additionally, this dissertation presents two tools built on top of SourcererCC: (i) SourcererCC-D: a distributed version of SourcererCC that exploits the inherent parallelism present in SourcererCC's approach to scale horizontally on a cluster of commodity machines for large scale code clone detection. Our experiments demonstrate SourcererCC-D's ability to achieve ideal speed-up and near linear scale-up on large datasets; and (ii) SourcererCC-I: an interactive and real-time version of SourcererCC that is integrated with the Eclipse development environment. SourcererCC-I is built to support developers in clone-aware development and maintenance activities. Finally, this dissertation concludes by presenting two empirical studies conducted using SourcererCC to demonstrate its effectiveness in practice.},
  language = {en},
  school = {UC Irvine},
  author = {Sajnani, Hitesh},
  year = {2016},
  file = {/Users/luigi/work/zotero/storage/LJTTR3DW/Sajnani - 2016 - Large-Scale Code Clone Detection.pdf}
}

@article{pekCorpusbasedEmpiricalResearch,
  title = {Corpus-Based {{Empirical Research}} in {{Software Engineering}}},
  language = {en},
  author = {Pek, Ekaterina},
  pages = {260},
  file = {/Users/luigi/work/zotero/storage/CG73TSPN/Pek - Corpus-based Empirical Research in Software Engine.pdf}
}

@article{mamanFormalPatternsJava2012,
  title = {Formal {{Patterns}} in {{Java Programs}}},
  language = {en},
  author = {Maman, Itay},
  year = {2012},
  pages = {230},
  file = {/Users/luigi/work/zotero/storage/4DTXQRDP/Maman - 2012 - Formal Patterns in Java Programs.pdf}
}

@article{symeProvingJavaType1997,
  title = {Proving {{Java Type Soundness}}},
  abstract = {This technical report describes a machine checked proof of the type soundness of a subset of the Java language called Javas. A formal semantics for this subset has been develooped by Drossopoulou and Eisenbach, and they have sketched an outline of the type soundness proof. The formulation developed here complements their written semantics and proff \ldots{}},
  language = {en-US},
  journal = {Microsoft Research},
  author = {Syme, Don},
  month = jan,
  year = {1997},
  file = {/Users/luigi/work/zotero/storage/GC76NFTT/Syme - 1997 - Proving Java Type Soundness.pdf;/Users/luigi/work/zotero/storage/RJXQ5CIL/proving-java-type-soundness.html}
}

@inproceedings{urmaProgrammingLanguageEvolution2012,
  address = {Tucson, Arizona, USA},
  title = {Programming Language Evolution via Source Code Query Languages},
  isbn = {978-1-4503-1631-6},
  doi = {10.1145/2414721.2414728},
  abstract = {Programming languages evolve just like programs. Language features are added and removed, for example when programs using them are shown to be error-prone. When language features are modified, deprecated, removed or even deemed unsuitable for the project at hand, it is necessary to analyse programs to identify occurrences to refactor.},
  language = {en},
  booktitle = {Proceedings of the {{ACM}} 4th Annual Workshop on {{Evaluation}} and Usability of Programming Languages and Tools - {{PLATEAU}} '12},
  publisher = {{ACM Press}},
  author = {Urma, Raoul-Gabriel and Mycroft, Alan},
  year = {2012},
  pages = {35},
  file = {/Users/luigi/work/zotero/storage/YM65MUEQ/Urma and Mycroft - 2012 - Programming language evolution via source code que.pdf}
}

@inproceedings{volderJqueryGenericCode2006,
  title = {Jquery: {{A}} Generic Code Browser with a Declarative Configuration Language},
  shorttitle = {Jquery},
  abstract = {Abstract. Modern IDEs have an open-ended plugin architecture to allow customizability. However, developing a plugin is costly in terms of effort and expertise required by the customizer. We present a two-pronged approach that allows for open-ended customizations while keeping the customization cost low. First, we explicitly limit the portion of the design space targeted by the configuration mechanism. This reduces customization cost by simplifying the configuration interface. Second, we use a declarative programming language as our configuration language. This facilitates open-ended specification of behavior without burdening the user with operational details.},
  booktitle = {In {{Practical Aspects}} of {{Declarative Languages}}, 8th {{International Symposium}}, {{PADL}} 2006},
  publisher = {{Springer}},
  author = {Volder, Kris De},
  year = {2006},
  pages = {88--102},
  file = {/Users/luigi/work/zotero/storage/RSHFEEMM/Volder - 2006 - Jquery A generic code browser with a declarative .pdf;/Users/luigi/work/zotero/storage/Y5EB5LWS/summary.html}
}

@inproceedings{moorKeynoteAddressQL2007,
  title = {Keynote {{Address}}: .{{QL}} for {{Source Code Analysis}}},
  shorttitle = {Keynote {{Address}}},
  doi = {10.1109/SCAM.2007.31},
  abstract = {Many tasks in source code analysis can be viewed as evaluating queries over a relational representation of the code. Here we present an object-oriented query language, named .QL, and demonstrate its use for general navigation, bug finding and enforcing coding conventions. We then focus on the particular problem of specifying metrics as queries.},
  booktitle = {Seventh {{IEEE International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}} 2007)},
  author = {de Moor, Oege and Verbaere, M. and Hajiyev, E. and Avgustinov, P. and Ekman, T. and Ongkingco, N. and Sereni, D. and Tibble, J.},
  month = sep,
  year = {2007},
  keywords = {software metrics,program debugging,code navigation,object-oriented languages,source code analysis,query languages,bug finding,.QL object-oriented query language,query metrics specification,relational code representation},
  pages = {3-16},
  file = {/Users/luigi/work/zotero/storage/HYT97E8F/Moor et al. - 2007 - Keynote Address .QL for Source Code Analysis.pdf;/Users/luigi/work/zotero/storage/KEPDRBMD/4362893.html}
}

@article{krishnamurthiRealSoftwareCrisis2015,
  title = {The Real Software Crisis: Repeatability as a Core Value},
  volume = {58},
  issn = {00010782},
  shorttitle = {The Real Software Crisis},
  doi = {10.1145/2658987},
  language = {en},
  number = {3},
  journal = {Communications of the ACM},
  author = {Krishnamurthi, Shriram and Vitek, Jan},
  month = feb,
  year = {2015},
  pages = {34-36},
  file = {/Users/luigi/work/zotero/storage/3CLZYBG6/Krishnamurthi and Vitek - 2015 - The real software crisis repeatability as a core .pdf}
}

@article{sandoz-personal-communication,
  title = {Personal Communication},
  author = {Sandoz, Paul},
  year = {2015}
}

@article{psandoz14,
  title = {Safety {{Not Guaranteed}}: Sun.Misc.{{Unsafe}} and the Quest for Safe Alternatives},
  author = {Sandoz, Paul},
  year = {2014},
  note = {Oracle Inc. [Online; accessed 29-January-2015]}
}

@article{jep193,
  title = {{{JEP}} 193: {{Enhanced Volatiles}}},
  author = {Lea, Doug},
  year = {2014}
}

@article{jep189,
  title = {{{JEP}} 189: {{Shenandoah}}: {{An Ultra}}-{{Low}}-{{Pause}}-{{Time Garbage Collector}}},
  author = {Christine H. Flood, Roman Kennke},
  year = {2014}
}

@article{valuetypes,
  title = {State of the {{Values}}},
  author = {Rose, John and Goetz, Brian and Steele, Guy},
  year = {2014}
}

@article{arrays20,
  title = {Arrays 2.0},
  author = {Rose, John R.},
  year = {2012}
}

@article{layouts,
  title = {Project {{Sumatra}}},
  author = {{OpenJDK}},
  year = {2013}
}

@article{panama,
  title = {The Isthmus in the {{VM}}},
  author = {Rose, John R.},
  year = {2014}
}

@article{jep191,
  title = {{{JEP}} 191: {{Foreign Function Interface}}},
  author = {Nutter, Charles Oliver},
  year = {2014}
}

@misc{tanEmpiricalSecurityStudy2008,
  title = {An {{Empirical Security Study}} of the {{Native Code}} in the {{JDK}}},
  abstract = {It is well known that the use of native methods in Java defeats Java's guarantees of safety and security, which is why the default policy of Java applets, for example, does not allow loading non-local native code. However, there is already a large amount of trusted native C/C++ code that comprises a significant portion of the Java Development Kit (JDK). We have carried out an empirical security study on a portion of the native code in Sun's JDK 1.6. By applying static analysis tools and manual inspection, we have identified in this security-critical code previously undiscovered bugs. Based on our study, we describe a taxonomy to classify bugs. Our taxonomy provides guidance to construction of automated and accurate bug-finding tools. We also suggest systematic remedies that can mediate the threats posed by the native code.},
  journal = {undefined},
  howpublished = {/paper/An-Empirical-Security-Study-of-the-Native-Code-in-Tan-Croft/4c3a84729bd09db6a90a862846bb29e937ec2ced},
  author = {Tan, Gang and Croft, Jason},
  year = {2008},
  file = {/Users/luigi/work/zotero/storage/QL9WQR3U/Tan and Croft - 2008 - An Empirical Security Study of the Native Code in .pdf;/Users/luigi/work/zotero/storage/WLY2LCQ5/4c3a84729bd09db6a90a862846bb29e937ec2ced.html}
}

@inproceedings{kondohFindingBugsJava2008,
  address = {New York, NY, USA},
  series = {{{ISSTA}} '08},
  title = {Finding {{Bugs}} in {{Java Native Interface Programs}}},
  isbn = {978-1-60558-050-0},
  doi = {10.1145/1390630.1390645},
  abstract = {In this paper, we describe static analysis techniques for finding bugs in programs using the Java Native Interface (JNI). The JNI is both tedious and error-prone because there are many JNI-specific mistakes that are not caught by a native compiler. This paper is focused on four kinds of common mistakes. First, explicit statements to handle a possible exception need to be inserted after a statement calling a Java method. However, such statements tend to be forgotten. We present a typestate analysis to detect this exception-handling mistake. Second, while the native code can allocate resources in a Java VM, those resources must be manually released, unlike Java. Mistakes in resource management cause leaks and other errors. To detect Java resource errors, we used the typestate analysis also used for detecting general memory errors. Third, if a reference to a Java resource lives across multiple native method invocations, it should be converted into a global reference. However, programmers sometimes forget this rule and, for example, store a local reference in a global variable for later uses. We provide a syntax checker that detects this bad coding practice. Fourth, no JNI function should be called in a critical region. If called there, the current thread might block and cause a deadlock. Misinterpreting the end of the critical region, programmers occasionally break this rule. We present a simple typestate analysis to detect an improper JNI function call in a critical region. We have implemented our analysis techniques in a bug-finding tool called BEAM, and executed it on opensource software including JNI code. In the experiment, our analysis techniques found 86 JNI-specific bugs without any overhead and increased the total number of bug reports by 76\%.},
  booktitle = {Proceedings of the 2008 {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  publisher = {{ACM}},
  author = {Kondoh, Goh and Onodera, Tamiya},
  year = {2008},
  keywords = {static analysis,java native interface,typestate analysis},
  pages = {109--118},
  file = {/Users/luigi/work/zotero/storage/L4WM5GFV/Kondoh and Onodera - 2008 - Finding Bugs in Java Native Interface Programs.pdf}
}

@inproceedings{ogataStudyJavaNonJava2010,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} '10},
  title = {A {{Study}} of {{Java}}'s Non-{{Java Memory}}},
  isbn = {978-1-4503-0203-6},
  doi = {10.1145/1869459.1869477},
  abstract = {A Java application sometimes raises an out-of-memory ex-ception. This is usually because it has exhausted the Java heap. However, a Java application can raise an out-of-memory exception when it exhausts the memory used by Java that is not in the Java heap. We call this area non-Java memory. For example, an out-of-memory exception in the non-Java memory can happen when the JVM attempts to load too many classes. Although it is relatively rare to ex-haust the non-Java memory compared to exhausting the Java heap, a Java application can consume a considerable amount of non-Java memory. This paper presents a quantitative analysis of non-Java memory. To the best of our knowledge, this is the first in-depth analysis of the non-Java memory. To do this we cre-ated a tool called Memory Analyzer for Redundant, Unused, and String Areas (MARUSA), which gathers memory statis-tics from both the OS and the Java virtual machine, break-ing down and visualizing the non-Java memory usage. We studied the use of non-Java memory for a wide range of Java applications, including the DaCapo benchmarks and Apache DayTrader. Our study is based on the IBM J9 Java Virtual Machine for Linux. Although some of our results may be specific to this combination, we believe that most of our observations are applicable to other platforms as well.},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  publisher = {{ACM}},
  author = {Ogata, Kazunori and Mikurube, Dai and Kawachiya, Kiyokuni and Trent, Scott and Onodera, Tamiya},
  year = {2010},
  keywords = {java,java native memory,memory footprint analysis,non-java memory},
  pages = {191--204},
  file = {/Users/luigi/work/zotero/storage/HECZXFUA/Ogata et al. - 2010 - A Study of Java's non-Java Memory.pdf}
}

@inproceedings{sunNativeGuardProtectingAndroid2014,
  address = {New York, NY, USA},
  series = {{{WiSec}} '14},
  title = {{{NativeGuard}}: {{Protecting Android Applications}} from {{Third}}-Party {{Native Libraries}}},
  isbn = {978-1-4503-2972-9},
  shorttitle = {{{NativeGuard}}},
  doi = {10.1145/2627393.2627396},
  abstract = {Android applications often include third-party libraries written in native code. However, current native components are not well managed by Android's security architecture. We present NativeGuard, a security framework that isolates native libraries from other components in Android applications. Leveraging the process-based protection in Android, NativeGuard isolates native libraries of an Android application into a second application where unnecessary privileges are eliminated. NativeGuard requires neither modifications to Android nor access to the source code of an application. It addresses multiple technical issues to support various interfaces that Android provides to the native world. Experimental results demonstrate that our framework works well with a set of real-world applications, and incurs only modest overhead on benchmark programs.},
  booktitle = {Proceedings of the 2014 {{ACM Conference}} on {{Security}} and {{Privacy}} in {{Wireless}} \& {{Mobile Networks}}},
  publisher = {{ACM}},
  author = {Sun, Mengtao and Tan, Gang},
  year = {2014},
  keywords = {java native interface,android,privilege isolation},
  pages = {165--176},
  file = {/Users/luigi/work/zotero/storage/TMVMV6SR/Sun and Tan - 2014 - NativeGuard Protecting Android Applications from .pdf}
}

@inproceedings{liFindingBugsExceptional2009,
  address = {New York, NY, USA},
  series = {{{CCS}} '09},
  title = {Finding {{Bugs}} in {{Exceptional Situations}} of {{JNI Programs}}},
  isbn = {978-1-60558-894-0},
  doi = {10.1145/1653662.1653716},
  abstract = {Software flaws in native methods may defeat Java's guarantees of safety and security. One common kind of flaws in native methods results from the discrepancy on how exceptions are handled in Java and in native methods. Unlike exceptions in Java, exceptions raised in the native code through the Java Native Interface (JNI) are not controlled by the Java Virtual Machine (JVM). Only after the native code finishes execution will the JVM's mechanism for exceptions take over. This discrepancy makes handling of JNI exceptions an error prone process and can cause serious security flaws in software written using the JNI. We propose a novel static analysis framework to examine exceptions and report errors in JNI programs. We have built a complete tool consisting of exception analysis, static taint analysis, and warning recovery. Experimental results demonstrated this tool allows finding of mishandling of exceptions with high accuracy (15.4\% false-positive rate on over 260k lines of code). Our framework can be easily applied to analyzing software written in other foreign function interfaces, including the Python/C interface and the OCaml/C interface.},
  booktitle = {Proceedings of the 16th {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  publisher = {{ACM}},
  author = {Li, Siliang and Tan, Gang},
  year = {2009},
  keywords = {static analysis,java native interface,taint analysis},
  pages = {442--452},
  file = {/Users/luigi/work/zotero/storage/2BIIY6CS/Li and Tan - 2009 - Finding Bugs in Exceptional Situations of JNI Prog.pdf}
}

@incollection{krishnamurthiSynthesizingObjectorientedFunctional1998,
  address = {Berlin, Heidelberg},
  title = {Synthesizing Object-Oriented and Functional Design to Promote Re-Use},
  volume = {1445},
  isbn = {978-3-540-64737-9 978-3-540-69064-1},
  abstract = {Many problems require recursively speci ed types of data and a collection of tools that operate on those data. Over time, these problems evolve so that the programmer must extend the toolkit or extend the types and adjust the existing tools accordingly. Ideally, this should be done without modifying existing code. Unfortunately, the prevailing program design strategies do not support both forms of extensibility: functional programming accommodates the addition of tools, while object-oriented programming supports either adding new tools or extending the data set, but not both. In this paper, we present a composite design pattern that synthesizes the best of both approaches and in the process resolves the tension between the two design strategies. We also show how this protocol suggests a new set of linguistic facilities for languages that support class systems.},
  language = {en},
  booktitle = {{{ECOOP}}'98 \textemdash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Krishnamurthi, Shriram and Felleisen, Matthias and Friedman, Daniel P.},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Jul, Eric},
  year = {1998},
  pages = {91-113},
  file = {/Users/luigi/work/zotero/storage/MJ9ZQEBY/Krishnamurthi et al. - 1998 - Synthesizing object-oriented and functional design.pdf},
  doi = {10.1007/BFb0054088}
}

@inproceedings{azadmaneshSQLDeepDynamic2015,
  address = {New York, NY, USA},
  series = {{{WODA}} 2015},
  title = {{{SQL}} for {{Deep Dynamic Analysis}}?},
  isbn = {978-1-4503-3909-4},
  doi = {10.1145/2823363.2823365},
  abstract = {If we develop a new dynamic analysis tool, how should we expose its functionalities? Through an interactive user interface, a DSL, a specific API, or in some other way? In this paper, we discuss how to use an already existing language familiar to most software engineers, SQL, to perform deep dynamic analyses. The goal is to explore the trade-off between expressiveness and ease-of-use. We use BLAST as the dynamic analysis tool and map its trace information to a relational database. We find that, even though SQL is expressive enough for deep analysis of program executions and information flow, it is not quite straight forward to express some of the queries software engineers might be interested in. However, it removes the burden of learning a new language from scratch, which could make it worthwhile as an option in some cases.},
  booktitle = {Proceedings of the 13th {{International Workshop}} on {{Dynamic Analysis}}},
  publisher = {{ACM}},
  author = {Azadmanesh, Mohammad R. and Hauswirth, Matthias},
  year = {2015},
  keywords = {Dynamic analysis,SQL,program trace,query-based analysis},
  pages = {2--7},
  file = {/Users/luigi/work/zotero/storage/EKSTV2HS/Azadmanesh and Hauswirth - 2015 - SQL for Deep Dynamic Analysis.pdf}
}

@inproceedings{tsantalisJDeodorantIdentificationRemoval2008,
  title = {{{JDeodorant}}: {{Identification}} and {{Removal}} of {{Type}}-{{Checking Bad Smells}}},
  shorttitle = {{{JDeodorant}}},
  doi = {10.1109/CSMR.2008.4493342},
  abstract = {In this demonstration, we present an Eclipse plug-in that automatically identifies type-checking bad smells in Java source code, and resolves them by applying the "replace conditional with polymorphism" or "replace type code with state/strategy " refactorings. To the best of our knowledge there is a lack of tools that identify type-checking bad smells. Moreover, none of the state-of-the-art IDEs support the refactorings that resolve such kind of bad smells.},
  booktitle = {2008 12th {{European Conference}} on {{Software Maintenance}} and {{Reengineering}}},
  author = {Tsantalis, N. and Chaikalis, T. and Chatzigeorgiou, A.},
  month = apr,
  year = {2008},
  keywords = {Java,Programming profession,Runtime,Software maintenance,software maintenance,Informatics,software quality,Software quality,polymorphism,Eclipse plug-in,Switches,Employment,Gettering,Java source code,JDeodorant,Object oriented programming,refactorings,source coding,type-checking bad smells},
  pages = {329-331},
  file = {/Users/luigi/work/zotero/storage/Z5FIUELP/Tsantalis et al. - 2008 - JDeodorant Identification and Removal of Type-Che.pdf;/Users/luigi/work/zotero/storage/SLQVVJ7Q/4493342.html}
}

@article{tufanoWhenWhyYour2017,
  title = {When and {{Why Your Code Starts}} to {{Smell Bad}} (and {{Whether}} the {{Smells Go Away}})},
  volume = {43},
  issn = {0098-5589, 1939-3520},
  doi = {10.1109/TSE.2017.2653105},
  abstract = {Technical debt is a metaphor introduced by Cunningham to indicate ``not quite right code which we postpone making it right''. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80\% of smells survive in the system. Also, among the 20\% of removed instances, only 9\% are removed as a direct consequence of refactoring operations.},
  language = {en},
  number = {11},
  journal = {IEEE Transactions on Software Engineering},
  author = {Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Oliveto, Rocco and Penta, Massimiliano Di and De Lucia, Andrea and Poshyvanyk, Denys},
  month = nov,
  year = {2017},
  pages = {1063-1088},
  file = {/Users/luigi/work/zotero/storage/RCSXYBAB/Tufano et al. - 2017 - When and Why Your Code Starts to Smell Bad (and Wh.pdf}
}

@inproceedings{palombaDetectingBadSmells2013,
  address = {Silicon Valley, CA, USA},
  title = {Detecting Bad Smells in Source Code Using Change History Information},
  isbn = {978-1-4799-0215-6},
  doi = {10.1109/ASE.2013.6693086},
  abstract = {Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately.},
  language = {en},
  booktitle = {2013 28th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  publisher = {{IEEE}},
  author = {Palomba, Fabio and Bavota, Gabriele and Di Penta, Massimiliano and Oliveto, Rocco and De Lucia, Andrea and Poshyvanyk, Denys},
  month = nov,
  year = {2013},
  pages = {268-278},
  file = {/Users/luigi/work/zotero/storage/HBG3G5I9/Palomba et al. - 2013 - Detecting bad smells in source code using change h.pdf}
}

@article{palombaTheyReallySmell,
  title = {Do They {{Really Smell Bad}}? {{A Study}} on {{Developers}}' {{Perception}} of {{Code Bad Smells}}},
  abstract = {In the last decade several catalogues have been defined to characterize code bad smells, i.e., symptoms of poor design and implementation choices. On top of such catalogues, researchers have defined methods and tools to automatically detect and/or remove bad smells. Nevertheless, there is an ongoing debate regarding the extent to which developers perceive bad smells as serious design problems. Indeed, there seems to be a gap between theory and practice, i.e., what is believed to be a problem (theory) and what is actually a problem (practice).},
  language = {en},
  author = {Palomba, Fabio and Bavota, Gabriele},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/58FNVZK9/Palomba and Bavota - Do they Really Smell Bad A Study on Developers’ P.pdf}
}

@article{palombaDiffusenessImpactMaintainability2018,
  title = {On the Diffuseness and the Impact on Maintainability of Code Smells: A Large Scale Empirical Investigation},
  volume = {23},
  issn = {1573-7616},
  shorttitle = {On the Diffuseness and the Impact on Maintainability of Code Smells},
  doi = {10.1007/s10664-017-9535-z},
  abstract = {Code smells are symptoms of poor design and implementation choices that may hinder code comprehensibility and maintainability. Despite the effort devoted by the research community in studying code smells, the extent to which code smells in software systems affect software maintainability remains still unclear. In this paper we present a large scale empirical investigation on the diffuseness of code smells and their impact on code change- and fault-proneness. The study was conducted across a total of 395 releases of 30 open source projects and considering 17,350 manually validated instances of 13 different code smell kinds. The results show that smells characterized by long and/or complex code (e.g., Complex Class) are highly diffused, and that smelly classes have a higher change- and fault-proneness than smell-free classes.},
  language = {en},
  number = {3},
  journal = {Empirical Software Engineering},
  author = {Palomba, Fabio and Bavota, Gabriele and Penta, Massimiliano Di and Fasano, Fausto and Oliveto, Rocco and Lucia, Andrea De},
  month = jun,
  year = {2018},
  keywords = {Empirical studies,Code smells,Mining software repositories},
  pages = {1188-1221},
  file = {/Users/luigi/work/zotero/storage/Q6BGGVSL/Palomba et al. - 2018 - On the diffuseness and the impact on maintainabili.pdf}
}

@inproceedings{palombaLandfillOpenDataset2015,
  title = {Landfill: {{An Open Dataset}} of {{Code Smells}} with {{Public Evaluation}}},
  shorttitle = {Landfill},
  doi = {10.1109/MSR.2015.69},
  abstract = {Code smells are symptoms of poor design and implementation choices that may hinder code comprehension and possibly increase change- and fault-proneness of source code. Several techniques have been proposed in the literature for detecting code smells. These techniques are generally evaluated by comparing their accuracy on a set of detected candidate code smells against a manually-produced oracle. Unfortunately, such comprehensive sets of annotated code smells are not available in the literature with only few exceptions. In this paper we contribute (i) a dataset of 243 instances of five types of code smells identified from 20 open source software projects, (ii) a systematic procedure for validating code smell datasets, (iii) LANDFILL, a Web-based platform for sharing code smell datasets, and (iv) a set of APIs for programmatically accessing LANDFILL's contents. Anyone can contribute to Landfill by (i) improving existing datasets (e.g., Adding missing instances of code smells, flagging possibly incorrectly classified instances), and (ii) sharing and posting new datasets. Landfill is available at www.sesa.unisa.it/landfill/, while the video demonstrating its features in action is available at http://www.sesa.unisa.it/tools/landfill.jsp.},
  booktitle = {2015 {{IEEE}}/{{ACM}} 12th {{Working Conference}} on {{Mining Software Repositories}}},
  author = {Palomba, F. and Nucci, D. Di and Tufano, M. and Bavota, G. and Oliveto, R. and Poshyvanyk, D. and Lucia, A. De},
  month = may,
  year = {2015},
  keywords = {History,Buildings,software engineering,Internet,source code (software),Androids,Humanoid robots,Manuals,candidate code smells,LANDFILL,open dataset,open source software projects,Software systems,systematic procedure,Web-based platform},
  pages = {482-485},
  file = {/Users/luigi/work/zotero/storage/HPAKNKJG/Palomba et al. - 2015 - Landfill An Open Dataset of Code Smells with Publ.pdf;/Users/luigi/work/zotero/storage/5S3KDBEG/7180123.html}
}

@article{palombaMiningVersionHistories2015,
  title = {Mining {{Version Histories}} for {{Detecting Code Smells}}},
  volume = {41},
  issn = {0098-5589},
  doi = {10.1109/TSE.2014.2372760},
  abstract = {Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase changeand fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change overtime. In this paper, we propose Historical Information for Smell deTection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system's snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers' perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.},
  number = {5},
  journal = {IEEE Transactions on Software Engineering},
  author = {Palomba, F. and Bavota, G. and Penta, M. D. and Oliveto, R. and Poshyvanyk, D. and Lucia, A. De},
  month = may,
  year = {2015},
  keywords = {program compilers,History,data mining,Detectors,mining software repositories,empirical studies,public domain software,Accuracy,Feature extraction,code analysis,open source project,Code smells,Association rules,blob,code smell detection,divergent change,feature envy,HIST,historical information for smell detection,mining version history,parallel inheritance,shotgun surgery,single system snapshot,Surgery},
  pages = {462-489},
  file = {/Users/luigi/work/zotero/storage/AUZV6YWD/Palomba et al. - 2015 - Mining Version Histories for Detecting Code Smells.pdf;/Users/luigi/work/zotero/storage/IQNCJTN3/6963448.html}
}

@article{bavotaAreTestSmells2015,
  title = {Are Test Smells Really Harmful? {{An}} Empirical Study},
  volume = {20},
  issn = {1573-7616},
  shorttitle = {Are Test Smells Really Harmful?},
  doi = {10.1007/s10664-014-9313-0},
  abstract = {Bad code smells have been defined as indicators of potential problems in source code. Techniques to identify and mitigate bad code smells have been proposed and studied. Recently bad test code smells (test smells for short) have been put forward as a kind of bad code smell specific to tests such a unit tests. What has been missing is empirical investigation into the prevalence and impact of bad test code smells. Two studies aimed at providing this missing empirical data are presented. The first study finds that there is a high diffusion of test smells in both open source and industrial software systems with 86 \% of JUnit tests exhibiting at least one test smell and six tests having six distinct test smells. The second study provides evidence that test smells have a strong negative impact on program comprehension and maintenance. Highlights from this second study include the finding that comprehension is 30 \% better in the absence of test smells.},
  language = {en},
  number = {4},
  journal = {Empirical Software Engineering},
  author = {Bavota, Gabriele and Qusef, Abdallah and Oliveto, Rocco and De Lucia, Andrea and Binkley, Dave},
  month = aug,
  year = {2015},
  keywords = {Mining software repositories,Controlled experiments,Test smells,Unit testing},
  pages = {1052-1094},
  file = {/Users/luigi/work/zotero/storage/TFQAL7FV/Bavota et al. - 2015 - Are test smells really harmful An empirical study.pdf}
}

@inproceedings{baxterCloneDetectionUsing1998,
  address = {Bethesda, MD, USA},
  title = {Clone Detection Using Abstract Syntax Trees},
  isbn = {978-0-8186-8779-2},
  doi = {10.1109/ICSM.1998.738528},
  abstract = {Existing research suggests that a considerable fraction (5-10\%) of the source code of large-scale computer programs is duplicate code (``clones''). Detection and removal of such clones promises decreased software maintenance costs of possibly the same magnitude. Previous work was limited to detection of either nearmisses differing only in single lexems, or near misses only between complete functions. This paper presents simple and practical methods for detecting exact and near miss clones over arbitrary program fragments in program source code by using abstract syntax trees. Previous work also did not suggest practical means for removing detected clones. Since our methods operate in terms of the program structure, clones could be removed by mechanical methods producing in-lined procedures or standard preprocessor macros.},
  language = {en},
  booktitle = {Proceedings. {{International Conference}} on {{Software Maintenance}} ({{Cat}}. {{No}}. {{98CB36272}})},
  publisher = {{IEEE Comput. Soc}},
  author = {Baxter, I.D. and Yahin, A. and Moura, L. and Sant'Anna, M. and Bier, L.},
  year = {1998},
  pages = {368-377},
  file = {/Users/luigi/work/zotero/storage/CV5ZFVVG/Baxter et al. - 1998 - Clone detection using abstract syntax trees.pdf}
}

@article{chenReplicationReproductionCode,
  title = {A {{Replication}} and {{Reproduction}} of {{Code Clone Detection Studies}}},
  abstract = {Code clones, fragments of code that are similar in some way, are regarded as costly. In order to understand the level of threat and opportunity of clones, we need to be able to efficiently detect clones in existing code. Recently, a new clone detection technique, CMCD, has been proposed. Our goal is to evaluate it and, if possible, improve on the original. We replicate the original study to evaluate the effectiveness of basic CMCD technique, improve it based on our experience with the replication, and apply it to a 43 open-source Java code from the Qualitas Corpus. We find the original technique is quite effective. Applying our improved technique we find that that 1 in 2 systems had at least 10\% cloned code, not counting the original, indicating that cloned code is quite common. We believe the CMCD technique provides a very promising means to support research into code clones.},
  language = {en},
  author = {Chen, Xiliang and Wang, Alice Yuchen and Tempero, Ewan},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/S5YLARIC/Chen et al. - A Replication and Reproduction of Code Clone Detec.pdf}
}

@inproceedings{yuanCMCDCountMatrix2011,
  title = {{{CMCD}}: {{Count}} Matrix Based Code Clone Detection},
  shorttitle = {{{CMCD}}},
  doi = {10.1109/APSEC.2011.13},
  abstract = {This paper introduces CMCD, a Count Matrix based technique to detect clones in program code. The key concept behind CMCD is Count Matrix, which is created while counting the occurrence frequencies of every variable in situations specified by pre-determined counting conditions. Because the characteristics of the count matrix do not change due to variable name replacements or even switching of statements, CMCD works well on many hard-to-detect code clones, such as swapping statements or deleting a few lines, which are dfficult for other state-of-the-art detection techniques. We have obtained the following interesting results using CMCD: (1) we successfully detected all 16 clone scenarios proposed by C. Roy et al., (2) we discovered two clone clusters with three copies each from 29 student-submitted compiler lab projects, (3) we identified 174 code clone clusters and a potential bug from JDK 1.6 source files.},
  booktitle = {Proceedings - {{Asia}}-{{Pacific Software Engineering Conference}}, {{APSEC}}},
  author = {Yuan, Yang and Guo, Yao},
  month = dec,
  year = {2011},
  pages = {250-257},
  file = {/Users/luigi/work/zotero/storage/MAKXIQ44/Yuan and Guo - 2011 - CMCD Count matrix based code clone detection.pdf}
}

@article{riegerVisualDetectionDuplicated,
  title = {Visual {{Detection}} of {{Duplicated Code}}},
  abstract = {Code duplication is considered as bad practice that complicates the maintenance and evolution of software. Detecting duplicated code is a difficult task because of the large amount of data to be checked and the fact that a priori it is unknown which code part has been duplicated. In this paper, we present a tool called DUPLOC that supports code duplication detection in a visual and exploratory or an automatic way.},
  language = {en},
  author = {Rieger, Matthias and Ducasse, Stephane},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/FR5BHYTG/Rieger and Ducasse - Visual Detection of Duplicated Code.pdf}
}

@incollection{demeyerReengineeringPatterns2003,
  title = {Reengineering {{Patterns}}},
  isbn = {978-1-55860-639-5},
  language = {en},
  booktitle = {Object-{{Oriented Reengineering Patterns}}},
  publisher = {{Elsevier}},
  author = {Demeyer, Serge and Ducasse, St\'ephane and Nierstrasz, Oscar},
  year = {2003},
  pages = {1-14},
  file = {/Users/luigi/work/zotero/storage/324UD3I9/Demeyer et al. - 2003 - Reengineering Patterns.pdf},
  doi = {10.1016/B978-155860639-5/50006-7}
}

@inproceedings{dieckmannStudyAllocationBehavior1999,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A Study of the {{Allocation Behavior}} of the {{SPECjvm98 Java Benchmarks}}},
  isbn = {978-3-540-48743-2},
  abstract = {We present an analysis of the memory usage for six of the Java programs in the SPECjvm98 benchmark suite. Most of the programs are realworld applications with high demands on the memory system. For each program, we measured as much low level data as possible, including age and size distribution, type distribution, and the overhead of object alignment. Among other things, we found that non-pointer data usually represents more than 50\% of the allocated space for instance objects, that Java objects tend to live longer than objects in Smalltalk or ML, and that they are fairly small.},
  language = {en},
  booktitle = {{{ECOOP}}' 99 \textemdash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Dieckmann, Sylvia and H\"olzle, Urs},
  editor = {Guerraoui, Rachid},
  year = {1999},
  keywords = {Garbage Collection,Class Pointer,Garbage Collector,Instance Object,Reference Array},
  pages = {92-115},
  file = {/Users/luigi/work/zotero/storage/V6L6LDMF/Dieckmann and Hölzle - 1999 - A study of the Allocation Behavior of the SPECjvm9.pdf}
}

@article{forresterEmpiricalStudyRobustness2000,
  title = {An {{Empirical Study}} of the {{Robustness}} of {{Windows NT Applications Using Random Testing}}},
  abstract = {We report on the third in a series of studies on the reliability of application programs in the face of random input. In 1990 and 1995, we studied the reliability of UNIX application programs, both command line and X-Window based (GUI). In this study, we apply our testing techniques to applications running on the Windows NT operating system. Our testing is simple black-box random input testing; by any measure, it is a crude technique, but it seems to be effective at locating bugs in real programs.},
  language = {en},
  author = {Forrester, Justin E and Miller, Barton P},
  year = {2000},
  pages = {10},
  file = {/Users/luigi/work/zotero/storage/GDWB8JGN/Forrester and Miller - 2000 - An Empirical Study of the Robustness of Windows NT.pdf}
}

@techreport{millerFuzzRevisitedReexamination1995,
  title = {Fuzz Revisited: {{A}} Re-Examination of the Reliability of {{UNIX}} Utilities and Services},
  shorttitle = {Fuzz Revisited},
  abstract = {We have tested the reliability of a large collection of basic UNIX utility programs, X-Window applications and servers, and network services. We used a simple testing method of subjecting these programs to a random input stream. Our testing methods and tools are largely automatic and simple to use. We tested programs on nine versions of the UNIX operating system, including seven commercial systems and the freely-available GNU utilities and Linux. We report which programs failed on which systems, and identify and categorize the causes of these failures. The result of our testing is that we can crash (with core dump) or hang (infinite loop) over 40 \% (in the worst case) of the basic programs and over 25 \% of the X-Window applications. We were not able to crash any of the network services that we tested nor any of X-Window servers. This study parallels our 1990 study (that tested only the basic UNIX utilities); all systems that we compared between 1990 and 1995 noticeably improved in reliability, but still had significant rates of failure. The reliability of the basic utilities from GNU and Linux were noticeably better than those of the commercial systems. We also tested how utility programs checked their return codes from the memory allocation library routines by simulating the unavailability of virtual memory. We could crash almost half of the programs that we tested in this way.},
  author = {Miller, Barton P. and Koski, David and Pheow, Cjin and Maganty, Lee Vivekananda and Murthy, Ravi and Natarajan, Ajitkumar and Steidl, Jeff},
  year = {1995},
  file = {/Users/luigi/work/zotero/storage/JQQ357QR/Miller et al. - 1995 - Fuzz revisited A re-examination of the reliabilit.pdf;/Users/luigi/work/zotero/storage/Z3ZN42MZ/summary.html}
}

@article{millerEmpiricalStudyReliability1990,
  title = {An Empirical Study of the Reliability of {{UNIX}} Utilities},
  volume = {33},
  issn = {00010782},
  doi = {10.1145/96267.96279},
  abstract = {Operating system facilities, such as the kernel and utility programs, are typically assumed to be reliable. In our recent experiments, we have been able to crash 25-33\% of the utility programs on any version of UNIX that was tested. This report describes these tests and an analysis of the program bugs that caused the crashes.},
  language = {en},
  number = {12},
  journal = {Communications of the ACM},
  author = {Miller, Barton P. and Fredriksen, Louis and So, Bryan},
  month = dec,
  year = {1990},
  pages = {32-44},
  file = {/Users/luigi/work/zotero/storage/BIMXP8I2/Miller et al. - 1990 - An empirical study of the reliability of UNIX util.pdf}
}

@inproceedings{nagappanEmpiricalStudyGoto2015,
  address = {New York, NY, USA},
  series = {{{ESEC}}/{{FSE}} 2015},
  title = {An {{Empirical Study}} of {{Goto}} in {{C Code}} from {{GitHub Repositories}}},
  isbn = {978-1-4503-3675-8},
  doi = {10.1145/2786805.2786834},
  abstract = {It is nearly 50 years since Dijkstra argued that goto obscures the flow of control in program execution and urged programmers to abandon the goto statement. While past research has shown that goto is still in use, little is known about whether goto is used in the unrestricted manner that Dijkstra feared, and if it is `harmful' enough to be a part of a post-release bug. We, therefore, conduct a two part empirical study - (1) qualitatively analyze a statistically rep- resentative sample of 384 files from a population of almost 250K C programming language files collected from over 11K GitHub repositories and find that developers use goto in C files for error handling (80.21{$\pm$}5\%) and cleaning up resources at the end of a procedure (40.36 {$\pm$} 5\%); and (2) quantitatively analyze the commit history from the release branches of six OSS projects and find that no goto statement was re- moved/modified in the post-release phase of four of the six projects. We conclude that developers limit themselves to using goto appropriately in most cases, and not in an unrestricted manner like Dijkstra feared, thus suggesting that goto does not appear to be harmful in practice.},
  booktitle = {Proceedings of the 2015 10th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}},
  publisher = {{ACM}},
  author = {Nagappan, Meiyappan and Robbes, Romain and Kamei, Yasutaka and Tanter, \'Eric and McIntosh, Shane and Mockus, Audris and Hassan, Ahmed E.},
  year = {2015},
  keywords = {Github,Dijkstra,Empirical SE,Use of goto statements},
  pages = {404--414},
  file = {/Users/luigi/work/zotero/storage/EPHCZBXJ/Nagappan et al. - 2015 - An Empirical Study of Goto in C Code from GitHub R.pdf}
}

@article{weiEmpiricalStudyDynamic2016,
  title = {Empirical Study of the Dynamic Behavior of {{JavaScript}} Objects},
  volume = {46},
  copyright = {Copyright \textcopyright{} 2015\,John Wiley \& Sons, Ltd.},
  issn = {1097-024X},
  doi = {10.1002/spe.2334},
  abstract = {Despite the popularity of JavaScript for client-side web applications, there is a lack of effective software tools supporting JavaScript development and testing. The dynamic characteristics of JavaScript pose software engineering challenges such as program understanding and security. One important feature of JavaScript is that its objects support flexible mechanisms such as property changes at runtime and prototype-based inheritance, making it difficult to reason about object behavior. We have performed an empirical study on real JavaScript applications to understand the dynamic behavior of JavaScript objects. We present metrics to measure behavior of JavaScript objects during execution (e.g., operations associated with an object, object size, and property type changes). We also investigated the behavioral patterns of observed objects to understand the coding or user interaction practices in JavaScript software. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {7},
  journal = {Software: Practice and Experience},
  author = {Wei, Shiyi and Xhakaj, Franceska and Ryder, Barbara G.},
  month = jul,
  year = {2016},
  keywords = {JavaScript,object behavioral metrics and patterns,study of websites},
  pages = {867-889},
  file = {/Users/luigi/work/zotero/storage/LJ2JZU3V/Wei et al. - 2016 - Empirical study of the dynamic behavior of JavaScr.pdf;/Users/luigi/work/zotero/storage/XGGHUGGG/spe.html}
}

@inproceedings{kiselyovStreamFusionCompleteness2017,
  address = {New York, NY, USA},
  series = {{{POPL}} 2017},
  title = {Stream {{Fusion}}, to {{Completeness}}},
  isbn = {978-1-4503-4660-3},
  doi = {10.1145/3009837.3009880},
  abstract = {Stream processing is mainstream (again): Widely-used stream libraries are now available for virtually all modern OO and functional languages, from Java to C\# to Scala to OCaml to Haskell. Yet expressivity and performance are still lacking. For instance, the popular, well-optimized Java 8 streams do not support the zip operator and are still an order of magnitude slower than hand-written loops.   We present the first approach that represents the full generality of stream processing and eliminates overheads, via the use of staging. It is based on an unusually rich semantic model of stream interaction. We support any combination of zipping, nesting (or flat-mapping), sub-ranging, filtering, mapping\textemdash{}of finite or infinite streams. Our model captures idiosyncrasies that a programmer uses in optimizing stream pipelines, such as rate differences and the choice of a ``for'' vs. ``while'' loops. Our approach delivers hand-written\textendash{}like code, but automatically. It explicitly avoids the reliance on black-box optimizers and sufficiently-smart compilers, offering highest, guaranteed and portable performance.   Our approach relies on high-level concepts that are then readily mapped into an implementation. Accordingly, we have two distinct implementations: an OCaml stream library, staged via MetaOCaml, and a Scala library for the JVM, staged via LMS. In both cases, we derive libraries richer and simultaneously many tens of times faster than past work. We greatly exceed in performance the standard stream libraries available in Java, Scala and OCaml, including the well-optimized Java 8 streams.},
  booktitle = {Proceedings of the 44th {{ACM SIGPLAN Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  author = {Kiselyov, Oleg and Biboudis, Aggelos and Palladinos, Nick and Smaragdakis, Yannis},
  year = {2017},
  keywords = {optimization,Code generation,multi-stage programming,stream fusion,streams},
  pages = {285--299},
  file = {/Users/luigi/work/zotero/storage/PAGNQ6M9/Kiselyov et al. - 2017 - Stream Fusion, to Completeness.pdf}
}

@inproceedings{ohLearningStrategyAdapting2015,
  address = {New York, NY, USA},
  series = {{{OOPSLA}} 2015},
  title = {Learning a {{Strategy}} for {{Adapting}} a {{Program Analysis}} via {{Bayesian Optimisation}}},
  isbn = {978-1-4503-3689-5},
  doi = {10.1145/2814270.2814309},
  abstract = {Building a cost-effective static analyser for real-world programs is still regarded an art. One key contributor to this grim reputation is the difficulty in balancing the cost and the precision of an analyser. An ideal analyser should be adaptive to a given analysis task, and avoid using techniques that unnecessarily improve precision and increase analysis cost. However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts. In this paper we present a new approach for building an adaptive static analyser. In our approach, the analyser includes a sophisticated parameterised strategy that decides, for each part of a given program, whether to apply a precision-improving technique to that part or not. We present a method for learning a good parameter for such a strategy from an existing codebase via Bayesian optimisation. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow- and context-sensitive variants of a realistic C static analyser. The experimental results demonstrate that using Bayesian optimisation is crucial for learning from an existing codebase. Also, they show that among all program queries that require flow- or context-sensitivity, our partially flow- and context-sensitive analysis answers the 75\% of them, while increasing the analysis cost only by 3.3x of the baseline flow- and context-insensitive analysis, rather than 40x or more of the fully sensitive version.},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Object}}-{{Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  publisher = {{ACM}},
  author = {Oh, Hakjoo and Yang, Hongseok and Yi, Kwangkeun},
  year = {2015},
  keywords = {Bayesian Optimization,rogram Analysis},
  pages = {572--588},
  file = {/Users/luigi/work/zotero/storage/8SIQA4GX/Oh et al. - 2015 - Learning a Strategy for Adapting a Program Analysi.pdf}
}

@inproceedings{kuutilaReviewingLiteratureTime2017,
  address = {Piscataway, NJ, USA},
  series = {{{SEmotion}} '17},
  title = {Reviewing {{Literature}} on {{Time Pressure}} in {{Software Engineering}} and {{Related Professions}}: {{Computer Assisted Interdisciplinary Literature Review}}},
  isbn = {978-1-5386-2793-8},
  shorttitle = {Reviewing {{Literature}} on {{Time Pressure}} in {{Software Engineering}} and {{Related Professions}}},
  doi = {10.1109/SEmotion.2017...11},
  abstract = {During the past few years, psychological diseases related to unhealthy work environments, such as burnouts, have drawn more and more public attention. One of the known causes of these affective problems is time pressure. In order to form a theoretical background for time pressure detection in software repositories, this paper combines interdisciplinary knowledge by analyzing 1270 papers found on Scopus database and containing terms related to time pressure. By clustering those papers based on their abstract, we show that time pressure has been widely studied across different fields, but relatively little in software engineering. From a literature review of the most relevant papers, we infer a list of testable hypotheses that we want to verify in future studies in order to assess the impact of time pressures on software developers' mental health.},
  booktitle = {Proceedings of the {{2Nd International Workshop}} on {{Emotion Awareness}} in {{Software Engineering}}},
  publisher = {{IEEE Press}},
  author = {Kuutila, Miikka and M\"antyl\"a, Mika V. and Claes, Ma\"elick and Elovainio, Marko},
  year = {2017},
  keywords = {software engineering,time pressure,deadline pressure,job demands-resources model,speed-accuracy tradeoff,topic analysis},
  pages = {54--59},
  file = {/Users/luigi/work/zotero/storage/9DII9RBW/Kuutila et al. - 2017 - Reviewing Literature on Time Pressure in Software .pdf}
}

@article{oostvogelsStaticTypingComplex2018,
  title = {Static {{Typing}} of {{Complex Presence Constraints}} in {{Interfaces}} ({{Artifact}})},
  volume = {4},
  issn = {2509-8195},
  doi = {10.4230/DARTS.4.3.3},
  number = {3},
  journal = {Dagstuhl Artifacts Series},
  author = {Oostvogels, Nathalie and Koster, Joeri De and Meuter, Wolfgang De},
  year = {2018},
  keywords = {type system,dependency logic,interfaces},
  pages = {3:1--3:2},
  file = {/Users/luigi/work/zotero/storage/RUHM4JXQ/Oostvogels et al. - 2018 - Static Typing of Complex Presence Constraints in I.pdf;/Users/luigi/work/zotero/storage/RQHMTKI4/9242.html}
}

@inproceedings{oostvogelsStaticTypingComplex2018a,
  address = {Dagstuhl, Germany},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  title = {Static {{Typing}} of {{Complex Presence Constraints}} in {{Interfaces}}},
  volume = {109},
  isbn = {978-3-95977-079-8},
  doi = {10.4230/LIPIcs.ECOOP.2018.14},
  booktitle = {32nd {{European Conference}} on {{Object}}-{{Oriented Programming}} ({{ECOOP}} 2018)},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  author = {Oostvogels, Nathalie and Koster, Joeri De and Meuter, Wolfgang De},
  editor = {Millstein, Todd},
  year = {2018},
  keywords = {type checking,dependency logic,interfaces},
  pages = {14:1--14:27},
  file = {/Users/luigi/work/zotero/storage/IR6RDRSB/Oostvogels et al. - 2018 - Static Typing of Complex Presence Constraints in I.pdf;/Users/luigi/work/zotero/storage/QFIWVHN7/9219.html}
}

@inproceedings{cabralExceptionHandlingField2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Exception {{Handling}}: {{A Field Study}} in {{Java}} and .{{NET}}},
  isbn = {978-3-540-73589-2},
  shorttitle = {Exception {{Handling}}},
  abstract = {Most modern programming languages rely on exceptions for dealing with abnormal situations. Although exception handling was a significant improvement over other mechanisms like checking return codes, it is far from perfect. In fact, it can be argued that this mechanism is seriously limited, if not, flawed. This paper aims to contribute to the discussion by providing quantitative measures on how programmers are currently using exception handling. We examined 32 different applications, both for Java and .NET. The major conclusion for this work is that exceptions are not being correctly used as an error recovery mechanism. Exception handlers are not specialized enough for allowing recovery and, typically, programmers just do one of the following actions: logging, user notification and application termination. To our knowledge, this is the most comprehensive study done on exception handling to date, providing a quantitative measure useful for guiding the development of new error handling mechanisms.},
  language = {en},
  booktitle = {{{ECOOP}} 2007 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Cabral, Bruno and Marques, Paulo},
  editor = {Ernst, Erik},
  year = {2007},
  keywords = {Programming Languages,Exception Handling Mechanisms},
  pages = {151-175},
  file = {/Users/luigi/work/zotero/storage/XLFGTEBS/Cabral and Marques - 2007 - Exception Handling A Field Study in Java and .NET.pdf}
}

@inproceedings{ryderStaticStudyJava2000,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Static Study}} of {{Java Exceptions Using JESP}}},
  isbn = {978-3-540-46423-5},
  abstract = {JESP is a tool for statically examining the usage of user thrown exceptions in Java source code. Reported here are the first findings over a dataset of 31 publicly available Java codes, including the JavaSpecs. Of greatest interest to compiler writers are the findings that most Java exceptions are thrown across method boundaries, trys and catches occur in equal numbers, finallys are rare, and programs fall into one of two categories, those dominated by throw statements and those dominated by catch statements.},
  language = {en},
  booktitle = {Compiler {{Construction}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ryder, Barbara G. and Smith, Donald and Kremer, Ulrich and Gordon, Michael and Shah, Nirav},
  editor = {Watt, David A.},
  year = {2000},
  keywords = {Exception Handling,Java Program,Exception Tree,Java Code,Legacy Code},
  pages = {67-81},
  file = {/Users/luigi/work/zotero/storage/XNHAWIG5/Ryder et al. - 2000 - A Static Study of Java Exceptions Using JESP.pdf}
}

@misc{zhitnitskyTop10Exception2016,
  title = {The {{Top}} 10 {{Exception Types}} in {{Production Java Applications}} - {{Based}} on {{1B Events}}},
  abstract = {The Pareto logging principle: 97\% of logged error statements are caused by 3\% of unique errors},
  language = {en-US},
  journal = {OverOps Blog},
  howpublished = {https://blog.takipi.com/the-top-10-exceptions-types-in-production-java-applications-based-on-1b-events/},
  author = {Zhitnitsky, Alex},
  month = jun,
  year = {2016},
  file = {/Users/luigi/work/zotero/storage/LCH5YSXD/the-top-10-exceptions-types-in-production-java-applications-based-on-1b-events.html}
}

@inproceedings{fourtounisStaticAnalysisJava2018,
  address = {New York, NY, USA},
  series = {{{ISSTA}} 2018},
  title = {Static {{Analysis}} of {{Java Dynamic Proxies}}},
  isbn = {978-1-4503-5699-2},
  doi = {10.1145/3213846.3213864},
  abstract = {The dynamic proxy API is one of Java's most widely-used dynamic features, permitting principled run-time code generation and link- ing. Dynamic proxies can implement any set of interfaces and for- ward method calls to a special object that handles them reflectively. The flexibility of dynamic proxies, however, comes at the cost of having a dynamically generated layer of bytecode that cannot be penetrated by current static analyses. In this paper, we observe that the dynamic proxy API is stylized enough to permit static analysis. We show how the semantics of dynamic proxies can be modeled in a straightforward manner as logical rules in the Doop static analysis framework. This concise set of rules enables Doop's standard analyses to process code behind dynamic proxies. We evaluate our approach by analyzing XCorpus, a corpus of real-world Java programs: we fully handle 95\% of its reported proxy creation sites. Our handling results in the analysis of significant portions of previously unreachable or incompletely- modeled code.},
  booktitle = {Proceedings of the 27th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  publisher = {{ACM}},
  author = {Fourtounis, George and Kastrinis, George and Smaragdakis, Yannis},
  year = {2018},
  keywords = {static analysis,reflection,dynamic proxy},
  pages = {209--220},
  file = {/Users/luigi/work/zotero/storage/IBFZ5JWH/Fourtounis et al. - 2018 - Static Analysis of Java Dynamic Proxies.pdf}
}

@article{meijerStaticTypingWhere,
  title = {Static {{Typing Where Possible}}, {{Dynamic Typing When Needed}}: {{The End}} of the {{Cold War Between Programming Languages}}},
  abstract = {This paper argues that we should seek the golden middle way between dynamically and statically typed languages.},
  language = {en},
  author = {Meijer, Erik and Drayton, Peter},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/XD3EDYBQ/Meijer and Drayton - Static Typing Where Possible, Dynamic Typing When .pdf}
}

@article{cardelliUnderstandingTypesData1985,
  title = {On Understanding Types, Data Abstraction, and Polymorphism},
  volume = {17},
  issn = {03600300},
  doi = {10.1145/6041.6042},
  abstract = {Our objective is to understand the notion of type in programming languages, present a model of typed, polymorphic programming languages that reflects recent research in type theory, and examine the relevance of recent research to the design of practical programming languages.},
  language = {en},
  number = {4},
  journal = {ACM Computing Surveys},
  author = {Cardelli, Luca and Wegner, Peter},
  month = dec,
  year = {1985},
  pages = {471-523},
  file = {/Users/luigi/work/zotero/storage/4BA5Q6Q9/Cardelli and Wegner - 1985 - On understanding types, data abstraction, and poly.pdf}
}

@inproceedings{doJustintimeStaticAnalysis2017,
  address = {New York, NY, USA},
  series = {{{ISSTA}} 2017},
  title = {Just-in-Time {{Static Analysis}}},
  isbn = {978-1-4503-5076-1},
  doi = {10.1145/3092703.3092705},
  abstract = {We present the concept of Just-In-Time (JIT) static analysis that interleaves code development and bug fixing in an integrated development environment. Unlike traditional batch-style analysis tools, a JIT analysis tool presents warnings to code developers over time, providing the most relevant results quickly, and computing less relevant results incrementally later. In this paper, we describe general guidelines for designing JIT analyses. We also present a general recipe for transforming static data-flow analyses to JIT analyses through a concept of layered analysis execution. We illustrate this transformation through CHEETAH, a JIT taint analysis for Android applications. Our empirical evaluation of CHEETAH on real-world applications shows that our approach returns warnings quickly enough to avoid disrupting the normal workflow of developers. This result is confirmed by our user study, in which developers fixed data leaks twice as fast when using CHEETAH compared to an equivalent batch-style analysis.},
  booktitle = {Proceedings of the 26th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  publisher = {{ACM}},
  author = {Do, Lisa Nguyen Quang and Ali, Karim and Livshits, Benjamin and Bodden, Eric and Smith, Justin and {Murphy-Hill}, Emerson},
  year = {2017},
  keywords = {Static analysis,Just-in-Time,Layered analysis},
  pages = {307--317},
  file = {/Users/luigi/work/zotero/storage/4HWV7YU4/Do et al. - 2017 - Just-in-time Static Analysis.pdf}
}

@inproceedings{inostrozaModularInterpretersMasses2015,
  address = {New York, NY, USA},
  series = {{{GPCE}} 2015},
  title = {Modular {{Interpreters}} for the {{Masses}}: {{Implicit Context Propagation Using Object Algebras}}},
  isbn = {978-1-4503-3687-1},
  shorttitle = {Modular {{Interpreters}} for the {{Masses}}},
  doi = {10.1145/2814204.2814209},
  abstract = {Modular interpreters have the potential to achieve component-based language development: instead of writing language interpreters from scratch, they can be assembled from reusable, semantic building blocks. Unfortunately, traditional language interpreters are hard to extend because different language constructs may require different interpreter signatures. For instance, arithmetic interpreters produce a value without any context information, whereas binding constructs require an additional environment. In this paper, we present a practical solution to this problem based on implicit context propagation. By structuring denotational-style interpreters as Object Algebras, base interpreters can be retroactively lifted into new interpreters that have an extended signature. The additional parameters are implicitly propagated behind the scenes, through the evaluation of the base interpreter. Interpreter lifting enables a flexible style of component-based language development. The technique works in mainstream object-oriented languages, does not sacrifice type safety or separate compilation, and can be easily automated. We illustrate implicit context propagation using a modular definition of Featherweight Java and its extension to support side-effects.},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN International Conference}} on {{Generative Programming}}: {{Concepts}} and {{Experiences}}},
  publisher = {{ACM}},
  author = {Inostroza, Pablo and van der Storm, Tijs},
  year = {2015},
  keywords = {implicit propagation,Modular interpreters,object algebras},
  pages = {171--180},
  file = {/Users/luigi/work/zotero/storage/PEFTLIKZ/Inostroza and Storm - 2015 - Modular Interpreters for the Masses Implicit Cont.pdf}
}

@inproceedings{saiedCouldWeInfer2015,
  title = {Could {{We Infer Unordered API Usage Patterns Only Using}} the {{Library Source Code}}?},
  doi = {10.1109/ICPC.2015.16},
  abstract = {Learning to use existing or new software libraries is a difficult task for software developers, which would impede their productivity. Much existing work has provided different techniques to mine API usage patterns from client programs in order to help developers on understanding and using existing libraries. However, considering only client programs to identify API usage patterns is a strong constraint as the client programs source code is not always available or the clients themselves do not exist yet for newly released APIs. In this paper, we propose a technique for mining Non Client-based Usage Patterns (NCBUP miner). We detect unordered API usage patterns as distinct groups of API methods that are structurally and semantically related and thus may contribute together to the implementation of a particular functionality for potential client programs. We evaluated our technique through four APIs. The obtained results are comparable to those of client-based approaches in terms of usage-patterns cohesion.},
  booktitle = {2015 {{IEEE}} 23rd {{International Conference}} on {{Program Comprehension}}},
  author = {Saied, M. A. and Abdeen, H. and Benomar, O. and Sahraoui, H.},
  month = may,
  year = {2015},
  keywords = {Java,software libraries,data mining,application program interfaces,Measurement,Security,Semantics,source code (software),API Usage,API Documentation,client programs,Context,library source code,Software Clustering,Usage Pattern,Clustering algorithms,Matrix decomposition,NCBUP miner,non client-based usage patterns,unordered API usage patterns},
  pages = {71-81},
  file = {/Users/luigi/work/zotero/storage/ZAYHZ4WW/Saied et al. - 2015 - Could We Infer Unordered API Usage Patterns Only U.pdf;/Users/luigi/work/zotero/storage/LN2PVC53/7181434.html}
}

@inproceedings{gurfinkelSeaHornVerificationFramework2015,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {The {{SeaHorn Verification Framework}}},
  isbn = {978-3-319-21690-4},
  abstract = {In this paper, we present SeaHorn, a software verification framework. The key distinguishing feature of SeaHorn is its modular design that separates the concerns of the syntax of the programming language, its operational semantics, and the verification semantics. SeaHorn encompasses several novelties: it (a) encodes verification conditions using an efficient yet precise inter-procedural technique, (b) provides flexibility in the verification semantics to allow different levels of precision, (c) leverages the state-of-the-art in software model checking and abstract interpretation for verification, and (d) uses Horn-clauses as an intermediate language to represent verification conditions which simplifies interfacing with multiple verification tools based on Horn-clauses. SeaHorn provides users with a powerful verification tool and researchers with an extensible and customizable framework for experimenting with new software verification techniques. The effectiveness and scalability of SeaHorn are demonstrated by an extensive experimental evaluation using benchmarks from SV-COMP 2015 and real avionics code.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer International Publishing}},
  author = {Gurfinkel, Arie and Kahsai, Temesghen and Komuravelli, Anvesh and Navas, Jorge A.},
  editor = {Kroening, Daniel and P{\u a}s{\u a}reanu, Corina S.},
  year = {2015},
  keywords = {Operational Semantic,Proof Obligation,Abstract Domain,Abstract Interpretation,Buffer Overflow},
  pages = {343-361},
  file = {/Users/luigi/work/zotero/storage/25CJ6S9Y/Gurfinkel et al. - 2015 - The SeaHorn Verification Framework.pdf}
}

@article{mantylaEvolutionSentimentAnalysis2018,
  title = {The Evolution of Sentiment Analysis\textemdash{{A}} Review of Research Topics, Venues, and Top Cited Papers},
  volume = {27},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2017.10.002},
  abstract = {Sentiment analysis is one of the fastest growing research areas in computer science, making it challenging to keep track of all the activities in the area. We present a computer-assisted literature review, where we utilize both text mining and qualitative coding, and analyze 6996 papers from Scopus. We find that the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th century and in the text subjectivity analysis performed by the computational linguistics community in 1990's. However, the outbreak of computer-based sentiment analysis only occurred with the availability of subjective texts on the Web. Consequently, 99\% of the papers have been published after 2004. Sentiment analysis papers are scattered to multiple publication venues, and the combined number of papers in the top-15 venues only represent ca. 30\% of the papers in total. We present the top-20 cited papers from Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has shifted from analyzing online product reviews to social media texts from Twitter and Facebook. Many topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering and cyberbullying extend the utilization of sentiment analysis.},
  journal = {Computer Science Review},
  author = {M\"antyl\"a, Mika V. and Graziotin, Daniel and Kuutila, Miikka},
  month = feb,
  year = {2018},
  keywords = {Bibliometric study,Latent Dirichlet Allocation,Literature review,Opinion mining,Qualitative analysis,Sentiment analysis,Text mining,Topic modeling},
  pages = {16-32},
  file = {/Users/luigi/work/zotero/storage/H6X5ZZD4/Mäntylä et al. - 2018 - The evolution of sentiment analysis—A review of re.pdf;/Users/luigi/work/zotero/storage/J5AAEPEE/S1574013717300606.html}
}

@inproceedings{huppeMiningComplexTemporal2017,
  title = {Mining {{Complex Temporal API Usage Patterns}}: {{An Evolutionary Approach}}},
  shorttitle = {Mining {{Complex Temporal API Usage Patterns}}},
  doi = {10.1109/ICSE-C.2017.147},
  abstract = {Learning to use existing or new software libraries is a difficult task for software developers, which would impede their productivity. Much existing work has provided different techniques to mine API usage patterns from client programs inorder to help developers on understanding and using existinglibraries. However, these techniques produce incomplete patterns, i.e., without temporal properties, or simple ones. In this paper, we propose a new formulation of the problem of API temporal pattern mining and a new approach to solve it. Indeed, we learn complex temporal patterns using a genetic programming approach. Our preliminary results show that across a considerable variability of client programs, our approach has been able to infer non-trivial patterns that reflect informative temporal properties.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering Companion}} ({{ICSE}}-{{C}})},
  author = {Huppe, S. and Saied, M. A. and Sahraoui, H.},
  month = may,
  year = {2017},
  keywords = {Data mining,Libraries,software libraries,data mining,application program interfaces,Software,Software engineering,Conferences,API documentation,API usage pattern,client program variability,complex temporal API usage pattern mining,evolutionary approach,genetic algorithms,Genetic programming,genetic programming approach,incomplete patterns,informative temporal properties,Linear temporal logic},
  pages = {274-276},
  file = {/Users/luigi/work/zotero/storage/M895YF8C/Huppe et al. - 2017 - Mining Complex Temporal API Usage Patterns An Evo.pdf;/Users/luigi/work/zotero/storage/FBLXIUPY/7965328.html}
}

@inproceedings{madsenDatalogFlixDeclarative2016,
  address = {New York, NY, USA},
  series = {{{PLDI}} '16},
  title = {From {{Datalog}} to {{Flix}}: {{A Declarative Language}} for {{Fixed Points}} on {{Lattices}}},
  isbn = {978-1-4503-4261-2},
  shorttitle = {From {{Datalog}} to {{Flix}}},
  doi = {10.1145/2908080.2908096},
  abstract = {We present Flix, a declarative programming language for specifying and solving least fixed point problems, particularly static program analyses. Flix is inspired by Datalog and extends it with lattices and monotone functions. Using Flix, implementors of static analyses can express a broader range of analyses than is currently possible in pure Datalog, while retaining its familiar rule-based syntax. We define a model-theoretic semantics of Flix as a natural extension of the Datalog semantics. This semantics captures the declarative meaning of Flix programs without imposing any specific evaluation strategy. An efficient strategy is semi-naive evaluation which we adapt for Flix. We have implemented a compiler and runtime for Flix, and used it to express several well-known static analyses, including the IFDS and IDE algorithms. The declarative nature of Flix clearly exposes the similarity between these two algorithms.},
  booktitle = {Proceedings of the 37th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Madsen, Magnus and Yee, Ming-Ho and Lhot\'ak, Ond{\v r}ej},
  year = {2016},
  keywords = {static analysis,Datalog,logic programming},
  pages = {194--208},
  file = {/Users/luigi/work/zotero/storage/8BKYCP6Z/Madsen et al. - 2016 - From Datalog to Flix A Declarative Language for F.pdf}
}

@article{ujhelyiPerformanceComparisonQuerybased2015,
  title = {Performance Comparison of Query-Based Techniques for Anti-Pattern Detection},
  volume = {65},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2015.01.003},
  abstract = {Context
Program queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Abstract Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries.
Objective
Our paper investigates the costs and benefits of using the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by four different general-purpose model query techniques based on native Java code, OCL evaluation and (incremental) graph pattern matching.
Method
We provide in-depth comparison of these techniques on the source code of 28 Java projects using anti-pattern queries taken from refactoring operations in different usage profiles.
Results
Our results show that general purpose model queries can outperform hand-coded queries by 2\textendash{}3 orders of magnitude, with the trade-off of an increased in memory consumption and model load time of up to an order of magnitude.
Conclusion
The measurement results of usage profiles can be used as guidelines for selecting the appropriate query technologies in concrete scenarios.},
  journal = {Information and Software Technology},
  author = {Ujhelyi, Zolt\'an and Sz{\H o}ke, G\'abor and Horv\'ath, \'Akos and Csisz\'ar, Norbert Istv\'an and Vid\'acs, L\'aszl\'o and Varr\'o, D\'aniel and Ferenc, Rudolf},
  month = sep,
  year = {2015},
  keywords = {Anti-patterns,Columbus,EMF-IncQuery,OCL,Performance measurements,Refactoring},
  pages = {147-165},
  file = {/Users/luigi/work/zotero/storage/U9MFI4NZ/Ujhelyi et al. - 2015 - Performance comparison of query-based techniques f.pdf;/Users/luigi/work/zotero/storage/FLHJ4ZM7/S0950584915000051.html}
}

@book{ISO:2012:III,
  address = {Geneva, Switzerland},
  title = {{{ISO}}/{{IEC}} 14882:2011 {{Information}} Technology --- {{Programming}} Languages --- {{C}}++},
  publisher = {{International Organization for Standardization}},
  author = {{ISO}},
  month = feb,
  year = {2012},
  keywords = {C++ Specification Standard},
  bibdate = {Mon Dec 19 11:12:12 2011},
  bibsource = {http://www.math.utah.edu/pub/tex/bib/isostd.bib},
  biburl = {https://www.bibsonomy.org/bibtex/24b660c16d9a5ab0ad595b1555402c797/gron},
  day = {28},
  interhash = {ff5df6d7fa67f89d7d5ea964dab3e3c9},
  intrahash = {4b660c16d9a5ab0ad595b1555402c797},
  remark = {Revises ISO/IEC 14882:2003.}
}

@article{Naik11,
  title = {Chord: {{A Versatile Platform}} for {{Program Analysis}}},
  author = {Naik, Mayur},
  year = {2011},
  howpublished = {Tutorial at PLDI'11. http://pag-www.gtisc.gatech.edu/chord/pldi11/tutorial.pptx}
}

@book{Gosling:2013:JLS:2462622,
  title = {The {{Java Language Specification}}, {{Java SE}} 7 {{Edition}}},
  isbn = {0-13-326022-4 978-0-13-326022-9},
  publisher = {{Addison-Wesley Professional}},
  author = {Gosling, James and Joy, Bill and Steele, Jr., Guy L. and Bracha, Gilad and Buckley, Alex},
  year = {2013}
}

@inproceedings{lavilleLazyPatternMatching1987,
  address = {London, UK, UK},
  title = {Lazy {{Pattern Matching}} in the {{ML Language}}},
  isbn = {978-0-387-18625-2},
  booktitle = {Proc. {{Of}} the {{Seventh Conference}} on {{Foundations}} of {{Software Technology}} and {{Theoretical Computer Science}}},
  publisher = {{Springer-Verlag}},
  author = {Laville, Alain},
  year = {1987},
  pages = {400--419}
}

@article{blackEssenceInheritance2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.02059},
  primaryClass = {cs},
  title = {The {{Essence}} of {{Inheritance}}},
  abstract = {Programming languages serve a dual purpose: to communicate programs to computers, and to communicate programs to humans. Indeed, it is this dual purpose that makes programming language design a constrained and challenging problem. Inheritance is an essential aspect of that second purpose: it is a tool to improve communication. Humans understand new concepts most readily by first looking at a number of concrete examples, and later abstracting over those examples. The essence of inheritance is that it mirrors this process: it provides a formal mechanism for moving from the concrete to the abstract.},
  journal = {arXiv:1601.02059 [cs]},
  author = {Black, Andrew P. and Bruce, Kim B. and Noble, James},
  month = jan,
  year = {2016},
  keywords = {Computer Science - Programming Languages},
  file = {/Users/luigi/work/zotero/storage/EGDBIMNX/Black et al. - 2016 - The Essence of Inheritance.pdf;/Users/luigi/work/zotero/storage/L8GWQB6V/1601.html}
}

@inproceedings{andreasenSystematicApproachesIncreasing2017,
  address = {New York, NY, USA},
  series = {{{SOAP}} 2017},
  title = {Systematic {{Approaches}} for {{Increasing Soundness}} and {{Precision}} of {{Static Analyzers}}},
  isbn = {978-1-4503-5072-3},
  doi = {10.1145/3088515.3088521},
  abstract = {Building static analyzers for modern programming languages is difficult. Often soundness is a requirement, perhaps with some well-defined exceptions, and precision must be adequate for producing useful results on realistic input programs. Formally proving such properties of a complex static analysis implementation is rarely an option in practice, which raises the challenge of how to identify causes and importance of soundness and precision problems.   Through a series of examples, we present our experience with semi-automated methods based on delta debugging and dynamic analysis for increasing soundness and precision of a static analyzer for JavaScript. The individual methods are well known, but to our knowledge rarely used systematically and in combination.},
  booktitle = {Proceedings of the 6th {{ACM SIGPLAN International Workshop}} on {{State Of}} the {{Art}} in {{Program Analysis}}},
  publisher = {{ACM}},
  author = {Andreasen, Esben Sparre and M\o{}ller, Anders and Nielsen, Benjamin Barslev},
  year = {2017},
  keywords = {JavaScript,Static Analysis,Testing,Soundness},
  pages = {31--36},
  file = {/Users/luigi/work/zotero/storage/FS3J4SR8/Andreasen et al. - 2017 - Systematic Approaches for Increasing Soundness and.pdf}
}

@article{wangMachineLearningCompiler2018,
  title = {Machine {{Learning}} in {{Compiler Optimization}}},
  volume = {106},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2018.2817118},
  abstract = {In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.},
  number = {11},
  journal = {Proceedings of the IEEE},
  author = {Wang, Z. and O'Boyle, M.},
  month = nov,
  year = {2018},
  keywords = {Optimization,program compilers,optimisation,High performance computing,Data models,machine learning,Program processors,compiler,compiler optimization,Training data,Feature extraction,learning (artificial intelligence),Code optimization,Machine learning,machine-learning-based compilation,program tuning},
  pages = {1879-1901},
  file = {/Users/luigi/work/zotero/storage/XK4PIVDF/Wang and O’Boyle - 2018 - Machine Learning in Compiler Optimization.pdf;/Users/luigi/work/zotero/storage/ZM3KRENF/8357388.html}
}

@inproceedings{harmanStartupsScaleupsOpportunities2018,
  title = {From {{Start}}-Ups to {{Scale}}-Ups: {{Opportunities}} and {{Open Problems}} for {{Static}} and {{Dynamic Program Analysis}}},
  shorttitle = {From {{Start}}-Ups to {{Scale}}-Ups},
  doi = {10.1109/SCAM.2018.00009},
  abstract = {This paper describes some of the challenges and opportunities when deploying static and dynamic analysis at scale, drawing on the authors' experience with the Infer and Sapienz Technologies at Facebook, each of which started life as a research-led start-up that was subsequently deployed at scale, impacting billions of people worldwide. The paper identifies open problems that have yet to receive significant attention from the scientific community, yet which have potential for profound real world impact, formulating these as research questions that, we believe, are ripe for exploration and that would make excellent topics for research projects. Note: This paper accompanies the authors' joint keynote at the 18th IEEE International Working Conference on Source Code Analysis and Manipulation, September 23rd-24th, 2018 - Madrid, Spain.},
  booktitle = {2018 {{IEEE}} 18th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}})},
  author = {Harman, M. and O'Hearn, P.},
  month = sep,
  year = {2018},
  keywords = {verification,program diagnostics,dynamic analysis,Software,dynamic program analysis,Tools,Testing,Static analysis,Prototypes,static program analysis,social networking (online),Separation Logic,authors experience,compositional reasoning,Facebook,flaky test,Industries,Infer,open problems,opportunities,Sapienz,SBSE,scale-ups,scientific community,significant attention,start-ups,testing},
  pages = {1-23},
  file = {/Users/luigi/work/zotero/storage/64N5CXWJ/Harman and O'Hearn - 2018 - From Start-ups to Scale-ups Opportunities and Ope.pdf;/Users/luigi/work/zotero/storage/B4WGGYN3/8530713.html}
}

@article{daleyVirtualMemoryProcesses,
  title = {Virtual {{Memory}}, {{Processes}},and {{Sharing}} in {{MULTICS}}},
  abstract = {Some basic concepts involved in the design of the MULTICS operating system are introduced. MULTICS concepts of processes, address space, and virtual memory are defined and the use of paging and segmentation is explained. The means by which users may share procedures and data is discussed and the mechanism by which symbolic references are dynamically transformed into virtual machine addresses is described in detail.},
  language = {en},
  author = {Daley, Robert C and Dennis, Jack B},
  pages = {7},
  file = {/Users/luigi/work/zotero/storage/MQSS5J76/Daley and Dennis - Virtual Memory, Processes,and Sharing in MULTICS.pdf}
}

@article{dijkstraStructureMultiprogrammingSystem,
  title = {The {{Structure}} of the "{{THE}}"-{{Multiprogramming System}}},
  abstract = {A multiprogramming system is described in which all activities are divided over a number of sequential processes. These sequential processes are placed at various hierarchical levels, in each of which one or more independent abstractions have been implemented. The hierarchical structure proved to be vital for the verification of the logical soundness of the design and the correctness of its implementation.},
  language = {en},
  journal = {Communications of the ACM},
  author = {Dijkstra, Edsger W},
  pages = {6},
  file = {/Users/luigi/work/zotero/storage/Q85GFVLJ/Dijkstra - The Structure of the THE-Multiprogramming System.pdf}
}

@article{bensoussanMulticsVirtualMemory1972,
  title = {The {{Multics}} Virtual Memory: Concepts and Design},
  volume = {15},
  issn = {00010782},
  shorttitle = {The {{Multics}} Virtual Memory},
  doi = {10.1145/355602.361306},
  abstract = {As experience with use of on-line operating systems has grown, the need to share information among system users has become increasingly apparent. Many contemporary systems permit some degree of sharing. Usually, sharing is accomplished by allowing several users to share data via input and output of information stored in files kept in secondary storage. Through the use of segmentation, however, Multics provides direct hardware addressing by user and system programs of all information, independent of its physical storage location. Information is stored in segments each of which is potentially sharable and carries its own independent attributes of size and access privilege. Here, the design and implementation considerations of segmentation and sharing in Multics are first discussed under the assumption that all information resides in a large, segmented main memory. Since the size of main memory on contemporary systems is rather limited, it is then shown how the Multics software achieves the effect of a large segmented main memory through the use of the Honeywell 645 segmentation and paging hardware.},
  language = {en},
  number = {5},
  journal = {Communications of the ACM},
  author = {Bensoussan, A. and Clingen, C. T. and Daley, R. C.},
  month = may,
  year = {1972},
  pages = {308-318},
  file = {/Users/luigi/work/zotero/storage/BPZL6926/Bensoussan et al. - 1972 - The Multics virtual memory concepts and design.pdf}
}

@article{schroederHardwareArchitectureImplementing1972,
  title = {A {{Hardware Architecture}} for {{Implementing Protection Rings}}},
  volume = {15},
  abstract = {Protection of computations and information is an important aspect of a computer utility. In a system which uses segmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation. This paper describes hardware processor mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor. Automatic hardware validation of references across ring boundaries is also performed. Thus, a call by a user procedure to a protected subsystem (including the the supervisor) is identical to a call to a companion user procedure. The mechanisms of passing and referencing arguments are the same in both cases as well.},
  language = {en},
  number = {3},
  author = {Schroeder, Michael D},
  year = {1972},
  pages = {14},
  file = {/Users/luigi/work/zotero/storage/DQFNPS3H/Schroeder - 1972 - A Hardware Architecture for Implementing Protectio.pdf}
}

@inproceedings{sungTestingInterlayerIntertask2010,
  title = {Testing {{Inter}}-Layer and {{Inter}}-Task {{Interactions}} in {{RTES Applications}}},
  doi = {10.1109/APSEC.2010.38},
  abstract = {Real-time embedded systems (RTESs) are becoming increasingly ubiquitous, controlling a wide variety of popular and safety-critical devices. Effective testing techniques could improve the dependability of these systems. In this paper we present an approach for testing RTESs, intended specifically to help RTES application developers detect faults related to functional correctness. Our approach consists of two techniques that focus on exercising the interactions between system layers and between the multiple user tasks that enact application behaviors. We present results of an empirical study that shows that our techniques are effective at detecting faults.},
  booktitle = {2010 {{Asia Pacific Software Engineering Conference}}},
  author = {Sung, A. and {Srisa-an}, W. and Rothermel, G. and Yu, T.},
  month = nov,
  year = {2010},
  keywords = {Libraries,Kernel,Testing,Context,program testing,Algorithm design and analysis,embedded systems,fault detection,fault diagnosis,real time embedded system,Real time systems,real-time embedded systems,RTES,safety critical device,safety-critical software,software testing,system layers,system testing,ubiquitous computing,ubiquitous system},
  pages = {260-269},
  file = {/Users/luigi/work/zotero/storage/ICVB6YFF/Sung et al. - 2010 - Testing Inter-layer and Inter-task Interactions in.pdf;/Users/luigi/work/zotero/storage/VTSEEHSF/5693202.html}
}

@inproceedings{baumannMultikernelNewOS2009,
  address = {Big Sky, Montana, USA},
  title = {The Multikernel: A New {{OS}} Architecture for Scalable Multicore Systems},
  isbn = {978-1-60558-752-3},
  shorttitle = {The Multikernel},
  doi = {10.1145/1629575.1629579},
  abstract = {Commodity computer systems contain more and more processor cores and exhibit increasingly diverse architectural tradeoffs, including memory hierarchies, interconnects, instruction sets and variants, and IO configurations. Previous high-performance computing systems have scaled in specific cases, but the dynamic nature of modern client and server workloads, coupled with the impossibility of statically optimizing an OS for all workloads and hardware variants pose serious challenges for operating system structures.},
  language = {en},
  booktitle = {Proceedings of the {{ACM SIGOPS}} 22nd Symposium on {{Operating}} Systems Principles - {{SOSP}} '09},
  publisher = {{ACM Press}},
  author = {Baumann, Andrew and Barham, Paul and Dagand, Pierre-Evariste and Harris, Tim and Isaacs, Rebecca and Peter, Simon and Roscoe, Timothy and Sch\"upbach, Adrian and Singhania, Akhilesh},
  year = {2009},
  pages = {29},
  file = {/Users/luigi/work/zotero/storage/LWHGJSK7/Baumann et al. - 2009 - The multikernel a new OS architecture for scalabl.pdf}
}

@article{smansImplicitDynamicFrames2012,
  title = {Implicit {{Dynamic Frames}}},
  volume = {34},
  issn = {0164-0925},
  doi = {10.1145/2160910.2160911},
  abstract = {An important, challenging problem in the verification of imperative programs with shared, mutable state is the frame problem in the presence of data abstraction. That is, one must be able to specify and verify upper bounds on the set of memory locations a method can read and write without exposing that method's implementation. Separation logic is now widely considered the most promising solution to this problem. However, unlike conventional verification approaches, separation logic assertions cannot mention heap-dependent expressions from the host programming language, such as method calls familiar to many developers. Moreover, separation logic-based verifiers are often based on symbolic execution. These symbolic execution-based verifiers typically do not support non-separating conjunction, and some of them rely on the developer to explicitly fold and unfold predicate definitions. Furthermore, several researchers have wondered whether it is possible to use verification condition generation and standard first-order provers instead of symbolic execution to automatically verify conformance with a separation logic specification. In this article, we propose a variant of separation logic called implicit dynamic frames that supports heap-dependent expressions inside assertions. Conformance with an implicit dynamic frames specification can be checked by proving the validity of a number of first-order verification conditions. To show that these verification conditions can be discharged automatically by standard first-order provers, we have implemented our approach in a verifier prototype and have used this prototype to verify several challenging examples from related work. Our prototype automatically folds and unfolds predicate definitions, as required, during the proof and can reason about non-separating conjunction which is used in the specifications of some of these examples. Finally, we prove the soundness of the approach.},
  number = {1},
  journal = {ACM Trans. Program. Lang. Syst.},
  author = {Smans, Jan and Jacobs, Bart and Piessens, Frank},
  month = may,
  year = {2012},
  keywords = {frame problem,Program verification,separation logic},
  pages = {2:1--2:58},
  file = {/Users/luigi/work/zotero/storage/2UYNVK4G/Smans et al. - 2012 - Implicit Dynamic Frames.pdf}
}

@article{bouckaertMappingOriginsExpansion2012,
  title = {Mapping the Origins and Expansion of the {{Indo}}-{{European}} Language Family},
  volume = {337},
  issn = {0036-8075},
  doi = {10.1126/science.1219669},
  abstract = {There are two competing hypotheses for the origin of the Indo-European language family. The conventional view places the homeland in the Pontic steppes approximately 6kya. An alternative hypothesis claims the languages spread from Anatolia with the expansion of farming 8\textendash{}9.5kya. Here we use Bayesian phylogeographic approaches together with basic vocabulary data from 103 ancient and contemporary Indo-European languages to explicitly model the expansion of the family and test between the homeland hypotheses. We find decisive support for an Anatolian over a steppe origin. Both the inferred timing and root location of the Indo-European language trees fit with an agricultural expansion from Anatolia beginning in the 9th millennium BP. These results highlight the critical role phylogeographic inference can play in resolving longstanding debates about human prehistory.},
  number = {6097},
  journal = {Science (New York, N.Y.)},
  author = {Bouckaert, Remco and Lemey, Philippe and Dunn, Michael and Greenhill, Simon J. and Alekseyenko, Alexander V. and Drummond, Alexei J. and Gray, Russell D. and Suchard, Marc A. and Atkinson, Quentin},
  month = aug,
  year = {2012},
  pages = {957-960},
  file = {/Users/luigi/work/zotero/storage/9PJHBLXT/Bouckaert et al. - 2012 - Mapping the origins and expansion of the Indo-Euro.pdf},
  pmid = {22923579},
  pmcid = {PMC4112997}
}

@book{johnsonTableContents1995,
  title = {Table of {{Contents}}},
  abstract = {This is a preliminary version of a chapter that appeared in the book Local Search in Combinatorial Optimization, E. H. L. Aarts and J. K. Lenstra (eds.), John Wiley and Sons, London, 1997, pp. 215-310. The traveling salesman problem (TSP) has been an early proving ground for many approaches to combinatorial optimization, including classical local optimization techniques as well as many of the more recent variants on local optimization, such as simulated annealing, tabu search, neural networks, and genetic algorithms. This chapter discusses how these various approaches have been adapted to the TSP and evaluates their relative success in this perhaps atypical domain from both a},
  author = {Johnson, David S. and Mcgeoch, Lyle A.},
  year = {1995},
  file = {/Users/luigi/work/zotero/storage/73QNQYAG/Johnson and Mcgeoch - 1995 - Table of Contents.pdf;/Users/luigi/work/zotero/storage/VHS8IL56/summary.html}
}

@article{solteszContainerbasedOperatingSystem,
  title = {Container-Based {{Operating System Virtualization}}: {{A Scalable}}, {{High}}-Performance {{Alternative}} to {{Hypervisors}}},
  abstract = {Hypervisors, popularized by Xen and VMware, are quickly becoming commodity. They are appropriate for many usage scenarios, but there are scenarios that require system virtualization with high degrees of both isolation and efficiency. Examples include HPC clusters, the Grid, hosting centers, and PlanetLab. We present an alternative to hypervisors that is better suited to such scenarios. The approach is a synthesis of prior work on resource containers and security containers applied to general-purpose, timeshared operating systems. Examples of such container-based systems include Solaris 10, Virtuozzo for Linux, and LinuxVServer. As a representative instance of container-based systems, this paper describes the design and implementation of Linux-VServer. In addition, it contrasts the architecture of Linux-VServer with current generations of Xen, and shows how Linux-VServer provides comparable support for isolation and superior system efficiency.},
  language = {en},
  author = {Soltesz, Stephen and P\"otzl, Herbert and Fiuczynski, Marc E},
  pages = {13},
  file = {/Users/luigi/work/zotero/storage/UUHPU34X/Soltesz et al. - Container-based Operating System Virtualization A.pdf}
}

@inproceedings{legoutRarestFirstChoke2006,
  address = {Rio de Janeriro, Brazil},
  title = {Rarest First and Choke Algorithms Are Enough},
  isbn = {978-1-59593-561-8},
  doi = {10.1145/1177080.1177106},
  abstract = {The performance of peer-to-peer file replication comes from its piece and peer selection strategies. Two such strategies have been introduced by the BitTorrent protocol: the rarest first and choke algorithms. Whereas it is commonly admitted that BitTorrent performs well, recent studies have proposed the replacement of the rarest first and choke algorithms in order to improve efficiency and fairness. In this paper, we use results from real experiments to advocate that the replacement of the rarest first and choke algorithms cannot be justified in the context of peer-to-peer file replication in the Internet.},
  language = {en},
  booktitle = {Proceedings of the 6th {{ACM SIGCOMM}} on {{Internet}} Measurement  - {{IMC}} '06},
  publisher = {{ACM Press}},
  author = {Legout, Arnaud and {Urvoy-Keller}, G. and Michiardi, P.},
  year = {2006},
  pages = {203},
  file = {/Users/luigi/work/zotero/storage/DH2FIR77/Legout et al. - 2006 - Rarest first and choke algorithms are enough.pdf}
}

@inproceedings{andImpactNATBitTorrentlike2009,
  title = {The Impact of {{NAT}} on {{BitTorrent}}-like {{P2P}} Systems},
  doi = {10.1109/P2P.2009.5284521},
  abstract = {BitTorrent nowadays is one of the most popular peer-to-peer (P2P) applications on the Internet; on the other hand, network address translation (NAT) has become pervasive in almost all networking scenarios. Despite the effort of NAT traversal, it is still very likely that P2P applications cannot receive incoming connection requests properly if they are behind NAT. Although this phenomenon has been widely observed, so far there is no quantitative study in the literature examining the impact of NAT on P2P applications. In this paper, we build analytical models to capture the performance of BitTorrent-like P2P systems with the presence of homogeneous and heterogeneous NAT peers. We further propose biased optimistic unchoke strategies in order to improve the overall system performance considerably. The analytical models have been validated by simulation results, which also reveal some interesting facts about the coexistence of NAT and public peers in P2P systems.},
  booktitle = {2009 {{IEEE Ninth International Conference}} on {{Peer}}-to-{{Peer Computing}}},
  author = {{and}},
  month = sep,
  year = {2009},
  keywords = {Internet,DSL,Analytical models,biased optimistic unchoke strategy,BitTorrent-like P2P system,File servers,IP networks,Modems,network address translation,Network address translation,Peer to peer computing,peer-to-peer application,peer-to-peer computing,System performance,Web server},
  pages = {242-251},
  file = {/Users/luigi/work/zotero/storage/4A2LFCQF/and - 2009 - The impact of NAT on BitTorrent-like P2P systems.pdf;/Users/luigi/work/zotero/storage/D2SN6ESI/5284521.html}
}

@article{turnerChurchThesisFunctional,
  title = {Church's {{Thesis}} and {{Functional Programming}}},
  abstract = {The earliest statement of Church's Thesis, from Church (1936) p356 is

We now define the notion, already discussed, of an effectively calculable function of positive integers by identifying it with the notion of a recursive function of positive integers (or of a lambda- definable function of positive integers).

The phrase in parentheses refers to the apparatus which Church had developed to investigate this and other problems in the foundations of mathematics: the calculus of lambda conversion. Both the Thesis and the lambda calculus have been of seminal influence on the development of Computing Science. The main subject of this article is the lambda calculus but I will begin with a brief sketch of the emergence of the Thesis.},
  journal = {Journal of Universal Computer Science},
  author = {Turner, David},
  pages = {2004},
  file = {/Users/luigi/work/zotero/storage/L7SHGFMC/Turner - Church’s Thesis and Functional Programming.pdf;/Users/luigi/work/zotero/storage/LSZ5DL8G/summary.html}
}

@article{leinoSpecificationVerificationObjectOriented,
  title = {Specification and {{Verification}} of {{Object}}-{{Oriented Software}}},
  abstract = {The specification of object-oriented and other pointer-based programs must be able to describe the structure of the program's dynamically allocated data as well as some abstract view of what the code implements. The verification of such programs can be done by generating logical verification conditions from the program and its specifications and then analyzing the verification conditions by a mechanical theorem prover.},
  language = {en},
  author = {Leino, K Rustan M},
  pages = {36},
  file = {/Users/luigi/work/zotero/storage/CTB2Y4ZI/Leino - Speciﬁcation and Veriﬁcation of Object-Oriented So.pdf}
}

@incollection{merzModelCheckingTutorial2001,
  address = {Berlin, Heidelberg},
  title = {Model {{Checking}}: {{A Tutorial Overview}}},
  volume = {2067},
  isbn = {978-3-540-42787-2 978-3-540-45510-3},
  shorttitle = {Model {{Checking}}},
  abstract = {We survey principles of model checking techniques for the automatic analysis of reactive systems. The use of model checking is exemplified by an analysis of the Needham-Schroeder public key protocol. We then formally define transition systems, temporal logic, {$\omega$}-automata, and their relationship. Basic model checking algorithms for linear- and branching-time temporal logics are defined, followed by an introduction to symbolic model checking and partial-order reduction techniques. The paper ends with a list of references to some more advanced topics.},
  language = {en},
  booktitle = {Modeling and {{Verification}} of {{Parallel Processes}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Merz, Stephan},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Cassez, Franck and Jard, Claude and Rozoy, Brigitte and Ryan, Mark Dermot},
  year = {2001},
  pages = {3-38},
  file = {/Users/luigi/work/zotero/storage/PIMALZP3/Merz - 2001 - Model Checking A Tutorial Overview.pdf},
  doi = {10.1007/3-540-45510-8_1}
}

@incollection{vanlamsweerdeObjectOrientationGoal2004,
  address = {Berlin, Heidelberg},
  title = {From {{Object Orientation}} to {{Goal Orientation}}: {{A Paradigm Shift}} for {{Requirements Engineering}}},
  volume = {2941},
  isbn = {978-3-540-21179-2 978-3-540-24626-8},
  shorttitle = {From {{Object Orientation}} to {{Goal Orientation}}},
  abstract = {Requirements engineering (RE) is concerned with the elicitation of the objectives to be achieved by the system envisioned, the operationalization of such objectives into specifications of services and constraints, the assignment of responsibilities for the resulting requirements to agents such as humans, devices and software, and the evolution of such requirements over time and across system families. Getting highquality requirements is difficult and critical. Recent surveys have confirmed the growing recognition of RE as an area of primary concern in software engineering research and practice. The paper reviews the important limitations of OO modeling and formal specification technology when applied to this early phase of the software lifecycle. It argues that goals are an essential abstraction for eliciting, elaborating, modeling, specifying, analyzing, verifying, negotiating and documenting robust and conflict-free requirements. A safety injection system for a nuclear power plant is used as a running example to illustrate the key role of goals while engineering requirements for high assurance systems.},
  language = {en},
  booktitle = {Radical {{Innovations}} of {{Software}} and {{Systems Engineering}} in the {{Future}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{van Lamsweerde}, Axel and Letier, Emmanuel},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Wirsing, Martin and Knapp, Alexander and Balsamo, Simonetta},
  year = {2004},
  pages = {325-340},
  file = {/Users/luigi/work/zotero/storage/Y58AW9GU/van Lamsweerde and Letier - 2004 - From Object Orientation to Goal Orientation A Par.pdf},
  doi = {10.1007/978-3-540-24626-8_23}
}

@inproceedings{vanlamsweerdeGoalorientedRequirementsEngineering2000,
  address = {Toronto, Ont., Canada},
  title = {Goal-Oriented Requirements Engineering: A Guided Tour},
  isbn = {978-0-7695-1125-2},
  shorttitle = {Goal-Oriented Requirements Engineering},
  doi = {10.1109/ISRE.2001.948567},
  abstract = {Goals capture, at different levels of abstraction, the various objectives the system under consideration should achieve. Goal-oriented requirements engineering is concerned with the use of goals for eliciting, elaborating, structuring, specifying, analyzing, negotiating, documenting, and modifying requirements. This area has received increasing attention over the past few years.},
  language = {en},
  booktitle = {Proceedings {{Fifth IEEE International Symposium}} on {{Requirements Engineering}}},
  publisher = {{IEEE Comput. Soc}},
  author = {{van Lamsweerde}, A.},
  year = {2000},
  pages = {249-262},
  file = {/Users/luigi/work/zotero/storage/5XV3LZU9/van Lamsweerde - 2000 - Goal-oriented requirements engineering a guided t.pdf}
}

@inproceedings{burckhardtEffectiveProgramVerification2008,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Effective {{Program Verification}} for {{Relaxed Memory Models}}},
  isbn = {978-3-540-70545-1},
  abstract = {Program verification for relaxed memory models is hard. The high degree of nondeterminism in such models challenges standard verification techniques. This paper proposes a new verification technique for the most common relaxation, store buffers. Crucial to this technique is the observation that all programmers, including those who use low-lock techniques for performance, expect their programs to be sequentially consistent. We first present a monitor algorithm that can detect the presence of program executions that are not sequentially consistent due to store buffers while only exploring sequentially consistent executions. Then, we combine this monitor with a stateless model checker that verifies that every sequentially consistent execution is correct. We have implemented this algorithm in a prototype tool called Sober and present experiments that demonstrate the precision and scalability of our method. We find relaxed memory model bugs in several programs, including two previously unknown bugs in a production-level concurrency library that would have been difficult to find by other means.},
  language = {en},
  booktitle = {Computer {{Aided Verification}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Burckhardt, Sebastian and Musuvathi, Madanlal},
  editor = {Gupta, Aarti and Malik, Sharad},
  year = {2008},
  keywords = {Transitive Closure,Coherence Index,Memory Model,Sequentially Consistent,Vector Clock},
  pages = {107-120},
  file = {/Users/luigi/work/zotero/storage/6F9262F2/Burckhardt and Musuvathi - 2008 - Effective Program Verification for Relaxed Memory .pdf}
}

@inproceedings{hummelGeneralDataDependence1994,
  address = {New York, NY, USA},
  series = {{{PLDI}} '94},
  title = {A {{General Data Dependence Test}} for {{Dynamic}}, {{Pointer}}-Based {{Data Structures}}},
  isbn = {978-0-89791-662-2},
  doi = {10.1145/178243.178262},
  abstract = {Optimizing compilers require accurate dependence testing to enable numerous, performance-enhancing transformations. However, data dependence testing is a difficult problem, particularly in the presence of pointers. Though existing approaches work well for pointers to named memory locations (i.e. other variables), they are overly conservative in the case of pointers to unnamed memory locations. The latter occurs in the context of dynamic, pointer-based data structures, used in a variety of applications ranging from system software to computational geometry to N-body and circuit simulations.
In this paper we present a new technique for performing more accurate data dependence testing in the presence of dynamic, pointer-based data structures. We will demonstrate its effectiveness by breaking false dependences that existing approaches cannot, and provide results which show that removing these dependences enables significant parallelization of a real application.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 1994 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  author = {Hummel, Joseph and Hendren, Laurie J. and Nicolau, Alexandru},
  year = {1994},
  pages = {218--229},
  file = {/Users/luigi/work/zotero/storage/4G63BL2R/Hummel et al. - 1994 - A General Data Dependence Test for Dynamic, Pointe.pdf}
}

@article{nystromSoftwareCompositionMultiple,
  title = {Software {{Composition}} with {{Multiple Nested Inheritance}}},
  abstract = {This paper introduces a programming language that makes it convenient to extend large software systems and even to compose them in a modular way. JX/MI supports multiple nested inheritance, building on earlier work on nested inheritance in the language JX. Nested inheritance permits modular, type-safe extension of a package (including nested packages and classes), while preserving existing type relationships. Multiple nested inheritance enables simultaneous extension of two or more classes or packages, composing their types and behavior while resolving conflicts with a relatively small amount of code. The utility of JX/MI is demonstrated by using it to construct two composable, extensible frameworks: a compiler framework for Java, and a peer-to-peer networking system. Both frameworks support composition of extensions. For example, two compilers adding different, domain-specific features to Java can be composed to obtain a compiler for a language that supports both sets of features.},
  language = {en},
  author = {Nystrom, Nathaniel and Qi, Xin and Myers, Andrew C},
  pages = {17},
  file = {/Users/luigi/work/zotero/storage/JCCITJDZ/Nystrom et al. - Software Composition with Multiple Nested Inherita.pdf}
}

@article{ryuScalableFrameworkParsing2016,
  title = {Scalable Framework for Parsing: From {{Fortress}} to {{JavaScript}}},
  volume = {46},
  issn = {1097-024X},
  shorttitle = {Scalable Framework for Parsing},
  doi = {10.1002/spe.2380},
  abstract = {Programming languages grow over time that requires frequent changes in language manipulations such as compilation, interpretation, and analysis. Because the very first step of most language manipulations is parsing, whether parsing can adapt to changes easily, quickly, and correctly, it affects the scalability of language manipulations. Even though various parsing techniques have been well studied theoretically, their practical experiences in scalable frameworks have not been available. In this paper, we present our experiences with parsing in scalable frameworks. We first describe our trials and errors using various parsing techniques in developing parsers for the Fortress programming language. Because Fortress was a scientific language under development, its mathematical and growable syntax introduced new challenges in parsing. We summarize the lessons learned from parsing Fortress, and we share our experience of applying the lessons to parsing the JavaScript programming language. While JavaScript is one of the most widely used languages, JavaScript itself and its diverse variants keep extending its syntax, and the extremely dynamic features of JavaScript also add new challenges in parsing. Using automatic generation tools and methods like staged parsing and automatic extraction and testing of examples in language specifications, our methodology for scalable parsing has shown to be very effective in practice. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  language = {en},
  number = {9},
  journal = {Software: Practice and Experience},
  author = {Ryu, Sukyoung},
  year = {2016},
  keywords = {JavaScript,automatic generation,Fortress,parsing,scalable framework,staged parsing},
  pages = {1219-1238},
  file = {/Users/luigi/work/zotero/storage/97JP439D/Ryu - 2016 - Scalable framework for parsing from Fortress to J.pdf;/Users/luigi/work/zotero/storage/JWYNGSEB/spe.html}
}

@article{urmaSourcecodeQueriesGraph2015,
  title = {Source-Code Queries with Graph Databases\textemdash{}with Application to Programming Language Usage and Evolution},
  volume = {97},
  issn = {01676423},
  doi = {10.1016/j.scico.2013.11.010},
  abstract = {Program querying and analysis tools are of growing importance, and occur in two main variants. Firstly there are source-code query languages which help software engineers to explore a system, or to find code in need of refactoring as coding standards evolve. These also enable language designers to understand the practical uses of language features and idioms over a software corpus. Secondly there are program analysis tools in the style of Coverity which perform deeper program analysis searching for bugs as well as checking adherence to coding standards such as MISRA.},
  language = {en},
  journal = {Science of Computer Programming},
  author = {Urma, Raoul-Gabriel and Mycroft, Alan},
  month = jan,
  year = {2015},
  pages = {127-134},
  file = {/Users/luigi/work/zotero/storage/5DCQ54M6/Urma and Mycroft - 2015 - Source-code queries with graph databases—with appl.pdf}
}

@incollection{tauberMemoryEfficientTailCalls2015,
  address = {Cham},
  title = {Memory-{{Efficient Tail Calls}} in the {{JVM}} with {{Imperative Functional Objects}}},
  volume = {9458},
  isbn = {978-3-319-26528-5 978-3-319-26529-2},
  abstract = {This paper presents FCore: a JVM implementation of System F with support for full tail-call elimination (TCE). Our compilation technique for FCore is innovative in two respects: it uses a new representation for first-class functions called imperative functional objects; and it provides a way to do TCE on the JVM using constant space.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer International Publishing}},
  author = {Tauber, Tom\'a{\v s} and Bi, Xuan and Shi, Zhiyuan and Zhang, Weixin and Li, Huang and Zhang, Zhenrui and Oliveira, Bruno C. D. S.},
  editor = {Feng, Xinyu and Park, Sungwoo},
  year = {2015},
  pages = {11-28},
  file = {/Users/luigi/work/zotero/storage/Y7GHZF4Y/Tauber et al. - 2015 - Memory-Efficient Tail Calls in the JVM with Impera.pdf},
  doi = {10.1007/978-3-319-26529-2_2}
}

@inproceedings{sadowskiTricorderBuildingProgram2015,
  title = {Tricorder: {{Building}} a {{Program Analysis Ecosystem}}},
  shorttitle = {Tricorder},
  booktitle = {International {{Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Sadowski, Caitlin and van Gogh, Jeffrey and Jaspan, Ciera and Soederberg, Emma and Winter, Collin},
  year = {2015},
  file = {/Users/luigi/work/zotero/storage/CFI2VQC2/Sadowski et al. - 2015 - Tricorder Building a Program Analysis Ecosystem.pdf}
}

@incollection{alpuimDisjointPolymorphism2017,
  address = {Berlin, Heidelberg},
  title = {Disjoint {{Polymorphism}}},
  volume = {10201},
  isbn = {978-3-662-54433-4 978-3-662-54434-1},
  abstract = {The combination of intersection types, a merge operator and parametric polymorphism enables important applications for programming. However, such combination makes it hard to achieve the desirable property of a coherent semantics: all valid reductions for the same expression should have the same value. Recent work proposed disjoint intersections types as a means to ensure coherence in a simply typed setting. However, the addition of parametric polymorphism was not studied.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Alpuim, Jo\~ao and Oliveira, Bruno C. d. S. and Shi, Zhiyuan},
  editor = {Yang, Hongseok},
  year = {2017},
  pages = {1-28},
  file = {/Users/luigi/work/zotero/storage/WNJ6YETL/Alpuim et al. - 2017 - Disjoint Polymorphism.pdf},
  doi = {10.1007/978-3-662-54434-1_1}
}

@article{bruceChallengingTypingIssues2003,
  title = {Some {{Challenging Typing Issues}} in {{Object}}-{{Oriented Languages}}},
  volume = {82},
  issn = {15710661},
  doi = {10.1016/S1571-0661(04)80799-0},
  abstract = {In this paper we discuss some of the remaining problems in the design of static type systems for object-oriented programming languages. We look at typing problems involved in writing a simple interpreter as a good example of a simple problem leading to difficult typing issues. The difficulties encountered seem to arise in situations where a programmer desires to simultaneously refine mutually interdependent classes and object types.},
  language = {en},
  number = {8},
  journal = {Electronic Notes in Theoretical Computer Science},
  author = {Bruce, Kim B.},
  month = oct,
  year = {2003},
  pages = {1-29},
  file = {/Users/luigi/work/zotero/storage/Y9AGCFV5/Bruce - 2003 - Some Challenging Typing Issues in Object-Oriented .pdf}
}

@article{zengerIndependentlyExtensibleSolutions,
  title = {Independently {{Extensible Solutions}} to the {{Expression Problem}}},
  abstract = {The expression problem is fundamental for the development of extensible software. Many (partial) solutions to this important problem have been proposed in the past. None of these approaches solves the problem of using different, independent extensions jointly. This paper proposes solutions to the expression problem that make it possible to combine independent extensions in a flexible, modular, and type-safe way. The solutions, formulated in the programming language Scala, are affected with only a small implementation overhead and are easy to implement by hand.},
  language = {en},
  author = {Zenger, Matthias and Odersky, Martin},
  pages = {11},
  file = {/Users/luigi/work/zotero/storage/4C6E5T8B/Zenger and Odersky - Independently Extensible Solutions to the Expressi.pdf}
}

@inproceedings{milnerProposalStandardML1984,
  address = {New York, NY, USA},
  series = {{{LFP}} '84},
  title = {A {{Proposal}} for {{Standard ML}}},
  isbn = {978-0-89791-142-9},
  doi = {10.1145/800055.802035},
  booktitle = {Proceedings of the 1984 {{ACM Symposium}} on {{LISP}} and {{Functional Programming}}},
  publisher = {{ACM}},
  author = {Milner, Robin},
  year = {1984},
  pages = {184--197},
  file = {/Users/luigi/work/zotero/storage/S5FUTEWN/Milner - 1984 - A Proposal for Standard ML.pdf}
}


